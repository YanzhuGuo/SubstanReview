{"id":1192,"text":"This paper describes a state-of-the-art CCG parsing model that decomposes into tagging and dependency scores, and has an efficient A* decoding algorithm. \nInterestingly, the paper slightly outperforms Lee et al. (2016)'s more expressive global parsing model, presumably because this factorization makes learning easier. It's great that they also report results on another language, showing large improvements over existing work on Japanese CCG parsing. One surprising original result is that modeling the first word of a constituent as the head substantially outperforms linguistically motivated head rules.  Overall this is a good paper that makes a nice contribution. I only have a few suggestions: -I liked the way that the dependency and supertagging models interact, but it would be good to include baseline results for simpler variations (e.g. not conditioning the tag on the head dependency).\n-The paper achieves new state-of-the-art results on Japanese by a large margin. However, there has been a lot less work on this data - would it also be possible to train the Lee et al. parser on this data for comparison?\n-Lewis, He and Zettlemoyer (2015) explore combined dependency and supertagging models for CCG and SRL, and may be worth citing. ","rid":1,"scores":{"overall":"4","SUBSTANCE":"4","APPROPRIATENESS":"5","PRESENTATION_FORMAT":"Oral Presentation","SOUNDNESS_CORRECTNESS":"5"},"label":[[320,380,"Eval_pos_1"],[381,452,"Jus_pos_1"],[609,669,"Major_claim"]],"Comments":["1;1"]}
{"id":1193,"text":"The paper considers a synergistic combination of two non-HMM based speech recognition techniques: CTC and attention-based seq2seq networks. The combination is two-fold: 1. first, similarly to Kim et al. 2016 multitask learning is used to train a model with a joint CTC and seq2seq cost. \n2. second (novel contribution), the scores of the CTC model and seq2seq model are ensembled during decoding (results of beam search over the seq2seq model are rescored with the CTC model).\nThe main novelty of the paper is in using the CTC model not only as an auxiliary training objective (originally proposed by Kim et al. 2016), but also during decoding.\n- Strengths: The paper identifies several problems stemming from the flexibility offered by the attention mechanism and shows that by combining the seq2seq network with CTC the problems are mitigated.\n- Weaknesses: The paper is an incremental improvement over Kim et al. 2016 (since two models are trained, their outputs can just as well be ensembled). However, it is nice to see that such a simple change offers important performance improvements of ASR systems.\n- General Discussion: A lot of the paper is spent on explaining the well-known, classical ASR systems. A description of the core improvement of the paper (better decoding algorithm) starts to appear only on p. 5.  The description of CTC is nonstandard and maybe should either be presented in a more standard way, or the explanation should be expanded. Typically, the relation p(C|Z) (eq. 5) is deterministic - there is one and only one character sequence that corresponds to the blank-expanded form Z. I am also unsure about the last transformation of the eq. 5. ","rid":1,"scores":{"overall":"3","SUBSTANCE":"4","APPROPRIATENESS":"5","PRESENTATION_FORMAT":"Poster","SOUNDNESS_CORRECTNESS":"5"},"label":[[658,845,"Eval_pos_1"],[859,920,"Eval_neg_1"],[922,995,"Jus_neg_1"]],"Comments":["1;1"]}
{"id":1194,"text":"The authors propose ‘morph-fitting’, a method that retrofits any given set of trained word embeddings based on a morphologically-driven objective that (1) pulls inflectional forms of the same word together (as in ‘slow’ and ‘slowing’) and (2) pushes derivational antonyms apart (as in ‘expensive’ and ‘inexpensive’). With this, the authors aim to improve the representation of low-frequency inflections of words as well as mitigate the tendency of corpus-based word embeddings to assign similar representations to antonyms. The method is based on relatively simple manually-constructed morphological rules and is demonstrated on both English, German, Italian and Russian. The experiments include intrinsic word similarity benchmarks, showing notable performance improvements achieved by applying morph-fitting to several different corpus-based embeddings. Performance improvement yielding new state-of-the-art results is also demonstrated for German and Italian on an extrinsic task - dialog state tracking.  Strengths: - The proposed method is simple and shows nice performance improvements across a number of evaluations and in several languages. Compared to previous knowledge-based retrofitting approaches (Faruqui et al., 2015), it relies on a few manually-constructed rules, instead of a large-scale knowledge base, such as an ontology.\n- Like previous retrofitting approaches, this method is easy to apply to existing sets of embeddings and therefore it seems like the software that the authors intend to release could be useful to the community.\n- The method and experiments are clearly described.   Weaknesses: - I was hoping to see some analysis of why the morph-fitted embeddings worked better in the evaluation, and how well that corresponds with the intuitive motivation of the authors.  - The authors introduce a synthetic word similarity evaluation dataset, Morph-SimLex. They create it by applying their presumably semantic-meaning-preserving morphological rules to SimLex999 to generate many more pairs with morphological variability. They do not manually annotate these new pairs, but rather use the original similarity judgements from SimLex999. \nThe obvious caveat with this dataset is that the similarity scores are presumed and therefore less reliable. Furthermore, the fact that this dataset was generated by the very same rules that are used in this work to morph-fit word embeddings, means that the results reported on this dataset in this work should be taken with a grain of salt. The authors should clearly state this in their paper.\n- (Soricut and Och, 2015) is mentioned as a future source for morphological knowledge, but in fact it is also an alternative approach to the one proposed in this paper for generating morphologically-aware word representations. The authors should present it as such and differentiate their work.\n- The evaluation does not include strong morphologically-informed embedding baselines.  General Discussion: With the few exceptions noted, I like this work and I think it represents a nice contribution to the community. The authors presented a simple approach and showed that it can yield nice improvements using various common embeddings on several evaluations and four different languages. I’d be happy to see it in the conference.\nMinor comments: - Line 200: I found this phrasing unclear: “We then query … of linguistic constraints”.\n- Section 2.1: I suggest to elaborate a little more on what the delta is between the model used in this paper and the one it is based on in Wieting 2015. It seemed to me that this was mostly the addition of the REPEL part.\n- Line 217: “The method’s cost function consists of three terms” - I suggest to spell this out in an equation.\n- Line 223:  x and t in this equation (and following ones) are the vector representations of the words. I suggest to denote that somehow. Also, are the vectors L2-normalized before this process? Also, when computing ‘nearest neighbor’ examples do you use cosine or dot-product? Please share these details.\n- Line 297-299: I suggest to move this text to Section 3, and make the note that you did not fine-tune the params in the main text and not in a footnote.\n- Line 327: (create, creates) seems like a wrong example for that rule.   - I have read the author response ","rid":1,"scores":{"overall":"4","SUBSTANCE":"4","APPROPRIATENESS":"5","PRESENTATION_FORMAT":"Oral Presentation","SOUNDNESS_CORRECTNESS":"5"},"label":[[1022,1148,"Eval_pos_1"],[1345,1457,"Jus_pos_2"],[1457,1553,"Eval_pos_2"],[1556,1604,"Eval_pos_3"],[1621,1799,"Eval_neg_1"],[2166,2419,"Jus_neg_2"],[2420,2507,"Eval_neg_2"],[2965,3076,"Major_claim"],[3249,3290,"Major_claim"]],"Comments":["2;2"]}
{"id":1195,"text":"This paper outlines a method to learn sense embeddings from unannotated corpora using a modular sense selection and representation process. The learning is achieved by a message passing scheme between the two modules that is cast as a reinforcement learning problem by the authors.\n- Strengths: The paper is generally well written, presents most of its ideas clearly and makes apt comparisons to related work where required. The experiments are well structured and the results are overall good, though not outstanding. However, there are several problems with the paper that prevent me from endorsing it completely.\n- Weaknesses: My main concern with the paper is the magnification of its central claims, beyond their actual worth.\n1) The authors use the term \"deep\" in their title and then several times in the paper. But they use a skip-gram architecture (which is not deep). This is misrepresentation.\n2) Also reinforcement learning is one of the central claims of this paper. \nHowever, to the best of my understanding, the motivation and implementation lacks clarity. Section 3.2 tries to cast the task as a reinforcement learning problem but goes on to say that there are 2 major drawbacks, due to which a Q-learning algorithm is used. This algorithm does not relate to the originally claimed policy.\nFurthermore, it remains unclear how novel their modular approach is. Their work seems to be very similar to EM learning approaches, where an optimal sense is selected in the E step and an objective is optimized in the M step to yield better sense representations. The authors do not properly distinguish their approach, nor motivative why RL should be preferred over EM in the first place.\n3) The authors make use of the term pure-sense representations multiple times, and claim this as a central contribution of their paper. I am not sure what this means, or why it is beneficial.\n4) They claim linear-time sense selection in their model. Again, it is not clear to me how this is the case. A highlighting of this fact in the relevant part of the paper would be helpful.  5) Finally, the authors claim state-of-the-art results. However, this is only on a single MaxSimC metric. Other work has achieved overall better results using the AvgSimC metric. So, while state-of-the-art isn't everything about a paper, the claim that this paper achieves it - in the abstract and intro - is at least a little misleading. ","rid":0,"scores":{"overall":"3","SUBSTANCE":"3","APPROPRIATENESS":"5","PRESENTATION_FORMAT":"Poster","SOUNDNESS_CORRECTNESS":"5"},"label":[[295,330,"Eval_pos_1"],[332,424,"Jus_pos_1"],[425,460,"Eval_pos_2"],[465,493,"Eval_pos_3"],[519,615,"Major_claim"],[630,731,"Eval_neg_1"],[732,2417,"Jus_neg_1"]],"Comments":["3;3"]}
{"id":1196,"text":"- Strengths: This is  a well written paper. \nThe paper is very clear for the most part. \nThe experimental comparisons are very well done. \nThe experiments are well designed and executed. \nThe idea of using KD for zero-resource NMT is impressive.\n- Weaknesses: There were many sentences in the abstract and in other places in the paper where the authors stuff too much information into a single sentence. This could be avoided. One can always use an extra sentence to be more clear. \nThere could have been a section where the actual method used could be explained in a more detailed. This explanation is glossed over in the paper. It's non-trivial to guess the idea from reading the sections alone. \nDuring test time, you need the source-pivot corpus as well. This is a major disadvantage of this approach. This is played down - in fact it's not mentioned at all. I could strongly encourage the authors to mention this and comment on it.  - General Discussion: This paper uses knowledge distillation to improve zero-resource translation. \nThe techniques used in this paper are very similar to the one proposed in Yoon Kim et. al. The innovative part is that they use it for doing zero-resource translation. They compare against other prominent works in the field. Their approach also eliminates the need to do double decoding.\nDetailed comments: -Line 21-27 - the authors could have avoided this complicated structure for two simple sentences. \nLine 41 - Johnson et. al has SOTA on English-French and German-English. \nLine 77-79 there is no evidence provided as to why combination of multiple languages increases complexity. Please retract this statement or provide more evidence. Evidence in literature seems to suggest the opposite.\nLine 416-420 - The two lines here are repeated again. They were first mentioned in the previous paragraph. \nLine 577 - Figure 2 not 3! ","rid":2,"scores":{"overall":"4","SUBSTANCE":"3","APPROPRIATENESS":"5","PRESENTATION_FORMAT":"Oral Presentation","SOUNDNESS_CORRECTNESS":"4"},"label":[[13,44,"Eval_pos_1"],[45,88,"Eval_pos_2"],[89,138,"Eval_pos_3"],[139,187,"Eval_pos_4"],[188,245,"Eval_pos_5"],[259,482,"Eval_neg_1"],[759,826,"Jus_neg_3"],[827,936,"Eval_neg_3"]],"Comments":["1;2"]}
{"id":1197,"text":"- Strengths:  * Elaborate evaluation data creation and evaluation scheme. \n * Range of compared techniques: baseline\/simple\/complex - Weaknesses:  * No in-depth analysis beyond overall evaluation results.\n- General Discussion: This paper compares several techniques for robust HPSG parsing.\nSince the main contribution of the paper is not a novel parsing technique but the empirical evaluation, I would like to see a more in-depth analysis of the results summarized in Table 1 and 2. \nIt would be nice to show some representative example sentences and sketches of its analyses, on which the compared methods behaved differently.\nPlease add EDM precision and recall figures to Table 2. \nThe EDM F1 score is a result of a mixed effects of (overall and partial) coverage, parse ranking, efficiency of search, etc. \nThe overall coverage figures in Table 1 are helpful but addition of EDM recall to Table 2 would make the situations clearer.\nMinor comment: -Is 'pacnv+ut' in Table 1 and 2 the same as 'pacnv' described in 3.4.3? ","rid":0,"scores":{"overall":"3","SUBSTANCE":"4","APPROPRIATENESS":"5","PRESENTATION_FORMAT":"Oral Presentation","SOUNDNESS_CORRECTNESS":"3"},"label":[[16,74,"Eval_pos_1"],[149,204,"Eval_neg_1"]],"Comments":["1;1"]}
{"id":1198,"text":"The paper proposes a task of selecting the most appropriate textual description for a given scene\/image from a list of similar options. It also proposes couple of baseline models, an evaluation metrics and human evaluation score.  - Strengths: The paper is well-written and well-structured. \nIt is clear with its contributions and well supports them by empirical evidence. So the paper is very easy to read.  The paper is well motivated. A method of selecting the most appropriate caption given a list of misleading candidates will benefit other image-caption\/understanding models, by acting as a post-generation re-ranking method.  - Weaknesses: I am not sure if the proposed algorithm for decoys generation is effective, which as a consequence puts the paper on questions.\nFor each target caption, the algorithm basically picks out those with similar representation and surface form but do not belong to the same image. But a fundamentally issue with this approach is: not belonging to the image-A does not mean not appropriate to describe image-A, especially when the representation and surface form are close. So the ground-truth labels might not be valid. As we can see in Figure-1, the generated decoys are either too far from the target to be a *good* decoy (*giraffe* vs *elephant*), or fair substitutes for the target (*small boy playing kites* vs *boy flies a kite*).\nThus, I am afraid that the dataset generated with this algorithm can not train a model to really *go beyond key word recognition*, which was claimed as contribution in this paper. As shown in Figure-1, most decoys can be filtered by key word mismatch---*giraffe vs elephant*, *pan vs bread*, *frisbee vs kite*, etc. And when they can not be separated by *key word match*, they look very tempting to be a correct option.\nFurthermore, it is interesting that humans only do correctly on 82.8% on a sampled test set. Does it mean that those examples are really too hard even for human to correctly classify? Or are some of the *decoys* in fact good enough to be the target's substitute (or even better) so that human choose them over ground-truth targets?\n- General Discussion: I think this is a well-written paper with clear motivation and substantial experiments. \nThe major issue is that the data-generating algorithm and the generated dataset do not seem helpful for the motivation. This in turn makes the experimental conclusions less convincing. So I tend to reject this paper unless my concerns can be fully addressed in rebuttal. ","rid":0,"scores":{"overall":"2","SUBSTANCE":"4","APPROPRIATENESS":"5","PRESENTATION_FORMAT":"Poster","SOUNDNESS_CORRECTNESS":"5"},"label":[[244,291,"Eval_pos_1"],[292,372,"Jus_pos_1"],[409,437,"Eval_pos_2"],[438,635,"Jus_pos_2"],[647,774,"Eval_neg_2"],[775,1160,"Jus_neg_2"],[1161,1377,"Jus_neg_2"],[1378,1557,"Eval_neg_3"],[1558,1797,"Jus_neg_3"],[2151,2240,"Eval_pos_4"],[2241,2425,"Eval_neg_4"],[2426,2511,"Major_claim"]],"Comments":["3;1"]}
{"id":1199,"text":"This paper presents a corpus of annotated essay revisions.  It includes two examples of application for the corpus: 1) Student Revision Behavior Analysis and 2) Automatic Revision Identification The latter is essentially a text classification task using an SVM classifier and a variety of features. The authors state that the corpus will be freely available for research purposes.\nThe paper is well-written and clear. A detailed annotation scheme was used by two annotators to annotate the corpus which added value to it. I believe the resource might be interesting to researcher working on writing process research and related topics. I also liked that you provided two very clear usage scenarios for the corpus.  I have two major criticisms. The first could be easily corrected in case the paper is accepted, but the second requires more work.\n1) There are no statistics about the corpus in this paper. This is absolutely paramount. When you describe a corpus, there are some information that should be there.  I am talking about number of documents (I assume the corpus has 180 documents (60 essays x 3 drafts), is that correct?), number of tokens (around 400 words each essay?), number of sentences, etc.  I assume we are talking about 60 unique essays x 400 words, so about 24,000 words in total. Is that correct? If we take the 3 drafts we end up with about 72,000 words but probably with substantial overlap between drafts.\nA table with this information should be included in the paper.\n2) If the aforementioned figures are correct, we are talking about a very small corpus. I understand the difficulty of producing hand-annotated data, and I think this is one of the strengths of your work, but I am not sure about how helpful this resource is for the NLP community as a whole. Perhaps such a resource would be better presented in a specialised workshop such as BEA or a specialised conference on language resources like LREC instead of a general NLP conference like ACL.\nYou mentioned in the last paragraph that you would like to augment the corpus with more annotation. Are you also willing to include more essays?\nComments\/Minor: - As you have essays by native and non-native speakers, one further potential application of this corpus is native language identification (NLI).\n- p. 7: \"where the unigram feature was used as the baseline\" - \"word unigram\". \nBe more specific.\n- p. 7: \"and the SVM classifier was used as the classifier.\" - redundant. ","rid":0,"scores":{"overall":"2","SUBSTANCE":"3","APPROPRIATENESS":"3","PRESENTATION_FORMAT":"Poster","SOUNDNESS_CORRECTNESS":"3"},"label":[[381,417,"Eval_pos_1"],[522,635,"Eval_pos_2"],[635,713,"Eval_pos_3"],[715,845,"Major_claim"],[846,934,"Eval_neg_1"],[935,1493,"Jus_neg_1"],[1497,1581,"Jus_neg_2"],[1582,1703,"Eval_neg_2"],[1703,1785,"Eval_neg_2"],[1786,1979,"Major_claim"]],"Comments":["2;1"]}
{"id":1200,"text":"This paper presents several weakly supervised methods for developing NERs. The methods rely on some form of projection from English into another language. The overall approach is not new and the individual methods proposed are improvements of existing methods. For an ACL paper I would have expected more novel approaches.\nOne of the contributions of the paper is the data selection scheme. The formula used to calculate the quality score is quite straightforward and this is not a bad thing. However, it is unclear how the thresholds were calculated for Table 2. The paper says only that different thresholds were tried. Was this done on a development set? There is no mention of this in the paper. The evaluation results show clearly that data selection is very important, but one may not know how to tune the parameters for a new data set or a new language pair.  Another contribution of the paper is the combination of the outputs of the two systems developed in the paper. I tried hard to understand how it works, but the description provided is not clear.  The paper presents a number of variants for each of the methods proposed. Does it make sense to combine more than two weakly supervised systems? Did the authors try anything in this direction.\nIt would be good to know a bit more about the types of texts that are in the \"in-house\" dataset. ","rid":0,"scores":{"overall":"3","SUBSTANCE":"4","APPROPRIATENESS":"5","PRESENTATION_FORMAT":"Poster","SOUNDNESS_CORRECTNESS":"4"},"label":[[155,260,"Eval_neg_3"],[261,322,"Eval_neg_3"],[323,492,"Eval_pos_1"],[493,563,"Eval_neg_1"],[564,699,"Jus_neg_1"],[978,1062,"Eval_neg_2"],[1063,1353,"Jus_neg_2"]],"Comments":["1;1"]}
{"id":1201,"text":"The paper proposes a model for the Stanford Natural Language Inference (SNLI) dataset, that builds on top of sentence encoding models and the decomposable word level alignment model by Parikh et al. (2016). The proposed improvements include performing decomposable attention on the output of a BiLSTM and feeding the attention output to another BiLSTM, and augmenting this network with a parallel tree variant.\n- Strengths: This approach outperforms several strong models previously proposed for the task. The authors have tried a large number of experiments, and clearly report the ones that did not work, and the hyperparameter settings of the ones that did. This paper serves as a useful empirical study for a popular problem.\n- Weaknesses: Unfortunately, there are not many new ideas in this work that seem useful beyond the scope the particular dataset used. While the authors claim that the proposed network architecture is simpler than many previous models, it is worth noting that the model complexity (in terms of the number of parameters) is fairly high. Due to this reason, it would help to see if the empirical gains extend to other datasets as well. In terms of ablation studies, it would help to see 1) how well the tree-variant of the model does on its own and 2) the effect of removing inference composition from the model.\nOther minor issues: 1) The method used to enhance local inference (equations 14 and 15) seem very similar to the heuristic matching function used by Mou et al., 2015 (Natural Language Inference by Tree-Based Convolution and Heuristic Matching). You may want to cite them.\n2) The first sentence in section 3.2 is an unsupported claim. This either needs a citation, or needs to be stated as a hypothesis.\nWhile the work is not very novel, the the empirical study is rigorous for the most part, and could be useful for researchers working on similar problems. \nGiven these strengths, I am changing my recommendation score to 3. I have read the authors' responses. ","rid":1,"scores":{"overall":"3","SUBSTANCE":"3","APPROPRIATENESS":"5","PRESENTATION_FORMAT":"Poster","SOUNDNESS_CORRECTNESS":"5"},"label":[[424,505,"Eval_pos_3"],[506,660,"Eval_pos_1"],[661,729,"Eval_pos_2"],[744,863,"Eval_neg_1"],[864,1064,"Jus_neg_1"],[1615,1673,"Eval_neg_2"],[1674,1742,"Jus_neg_2"],[1743,1897,"Major_claim"]],"Comments":["1;2"]}
{"id":1202,"text":"- Strengths: This paper contributes to the field of knowledge base-based question answering (KB-QA), which is to tackle the problem of retrieving results from a structured KB based on a natural language question. KB-QA is an important and challenging task.\nThe authors clearly identify the contributions and the novelty of their work, provide a good overview of the previous work and performance comparison of their approach to the related methods.\nPrevious approaches to NN-based KB-QA represent questions and answers as fixed length vectors, merely as a bag of words, which limits the expressiveness of the models. And previous work also don’t leverage unsupervised training over KG, which potentially can help a trained model to generalize. \nThis paper makes two major innovative points on the Question Answering problem.\n1) The backbone of the architecture of the proposed approach is a cross-attention based neural network, where attention is used for capture different parts of questions and answer aspects. The cross-attention model contains two parts, benefiting each other. The A-Q attention part tries to dynamically capture different aspects of the question, thus leading to different embedding representations of the question. And the Q-A attention part also offer different attention weight of the question towards the answer aspects when computing their Q-A similarity score. \n2) Answer embeddings are not only learnt on the QA task but also modeled using TransE which allows to integrate more prior knowledge on the KB side. \nExperimental results are obtained on Web questions and the proposed approach exhibits better behavior than state-of-the-art end-to-end methods. The two contributions were made particularly clear by ablation experiment. Both the cross-attention mechanism and global information improve QA performance by large margins.\nThe paper contains a lot of contents. The proposed framework is quite impressive and novel compared with the previous works.\n- Weaknesses: The paper is well-structured, the language is clear and correct. Some minor typos are provided below. \n1. Page 5, column 1, line 421:                                       re-read                      reread 2. Page 5, column 2, line 454: pairs be    pairs to be - General Discussion: In Equation 2: the four aspects of candidate answer aspects share the same W and b. How about using separate W and b for each aspect? \nI would suggest considering giving a name to your approach instead of \"our approach\", something like ANN or CA-LSTM…(yet something different from Table 2).   In general, I think it is a good idea to capture the different aspects for question answer similarity, and cross-attention based NN model is a novel solution for the above task. The experimental results also demonstrate the effectiveness of the authors’ approach. Although the overall performance is weaker than SP-based methods or some other integrated systems, I think this paper is a good attempt in end-to-end KB-QA area and should be encouraged. ","rid":1,"scores":{"overall":"4","SUBSTANCE":"4","APPROPRIATENESS":"5","PRESENTATION_FORMAT":"Oral Presentation","SOUNDNESS_CORRECTNESS":"5"},"label":[[257,333,"Eval_pos_1"],[334,448,"Eval_pos_2"],[745,824,"Eval_pos_3"],[825,1858,"Jus_pos_3"],[1859,1983,"Major_claim"],[1997,2062,"Eval_pos_4"],[2941,3029,"Major_claim"]],"Comments":["2;2"]}
{"id":1153,"text":"paper_summary\nSince only minor revisions have been made to the paper, my views of the paper have not changed. For details, please see my previous review comments.\nThe author’s response has answered my previous questions very well and added relevant analysis to the revised draft. In my opinion, the analysis of the negative phenomenon on NLU corpora in this paper is comprehensive. But as its contribution is incremental, it is unlikely to be improved through minor modifications. In summary, I think it is a borderline paper of ACL, or as a Findings paper. \nsummary_of_strengths\nHow to deal with negation semantic is one of the most fundamental and important issues in NLU, which is especially often ignored by existing models. This paper verifies the significance of the problem on multiple datasets, and in particular, proposes to divide the negations into important and unimportant types and analyzes them (Table 2). The work of the paper is comprehensive and solid. \nsummary_of_weaknesses\nHowever, I think the innovation of this paper is general. The influence of negation expressions on NLP\/NLU tasks has been widely proposed in many specialized studies, as well as in the case\/error analysis of many NLP\/NLU tasks. In my opinion, this paper is the only integration of these points of view and does not provide deeper insights to inspire audiences in related fields. \ncomments,_suggestions_and_typos\nNA ","rid":"4dd88ab5e2b781c47a44fea8fb474eea99fcc9e899ddc5770953faa4a2fcd1b8126ca5b649a34ca1fe3c1853be6b274a4edcffff0820a080b2af0c0e46e4373b","scores":{"overall":"3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.","best_paper":"No","replicability":"5 = They could easily reproduce the results.","datasets":"1 = No usable datasets submitted.","software":"2 = Documentary: The new software will be useful to study or replicate the reported research, although for other purposes it may have limited interest or limited usability. (Still a positive rating)"},"label":[[279,381,"Eval_pos_1"],[382,421,"Eval_neg_1"],[421,480,"Eval_neg_3"],[481,558,"Major_claim"],[729,909,"Jus_pos_2"],[921,971,"Eval_pos_2"],[994,1050,"Eval_neg_2"],[1051,1220,"Jus_neg_2"],[1222,1235,"Eval_neg_4"],[1236,1295,"Jus_neg_4"],[1295,1373,"Eval_neg_4"]],"Comments":["2;2"]}
{"id":1154,"text":"paper_summary\nThe paper defines a CBMI metric over the NMT source and a target word (given the target history) and then uses it to re-weight the NMT training loss. The definition is simplified to the quotient of NMT probability and the LM probability. Experiments shows that the training strategy improves the translation quality, over two training datasets, outperforming previous works. The paper further shows the method also improves the human evaluation. \nsummary_of_strengths\n- The proposed method appears to be simple, but works; -Paper appears to be well written; -Experiments comparison and analysis, human evaluation; Overall, paper did a good job in presenting and examining the effectiveness of a simple idea. \nsummary_of_weaknesses\nI think the paper (and related works) presented the works in a way that they presented a hypothesis (eg, importance of token reweighing), then conduct experiments and analysis showing the effectiveness of the method, then saying re-weighing the token importance works. After finishing reading, I felt the need to go back go re-examine the hypothesis to understand more and realized that I still don't understand the problem in a machine learning sense. The authors are encouraged to (at least) post some \"aha\" examples showing re-weighting this way indeed is the one that matters. Also, discussing and revealing the reason why NMT still needs this re-weighting even though the NMT model can in principle implicitly capture them would be really helpful. \ncomments,_suggestions_and_typos\nPlease see the weakness section. ","rid":"af37dc37dc2ac7787e56d29aff57ae37121220fd8223de86a710d9cbb5115a82019f8f0add5f5fe910c8d24b027b1959488ff86591e6799075f7f16066b66f07","scores":{"overall":"3.5 ","best_paper":"No","replicability":"5 = They could easily reproduce the results.","datasets":"1 = No usable datasets submitted.","software":"1 = No usable software released."},"label":[[482,535,"Eval_pos_1"],[537,571,"Eval_pos_2"],[628,721,"Eval_pos_3"],[1014,1197,"Eval_neg_1"],[1198,1325,"Jus_neg_1"]],"Comments":["1;1"]}
{"id":1155,"text":"paper_summary\nThis paper describes the development of a data set that can be used to develop a system that can generate automated feedback to learners' responses to short-answer questions.  The data set includes questions, their answers, and their feedback in the domain of computer networking, mostly in English but with a sizable German subset as well.  The paper describes the construction of the data set, including agreement and encountered challenges, as well as experimental results that can serve as a baseline for future work. \nsummary_of_strengths\nAlthough the domain is niche, since the authors do an extremely thorough job of thoughtfully constructing their data set with expert annotators and guidance, agreement-measurement, and validity evidence, this paper should serve as a model to the community with respect to how to compile similar data sets.\nWhile the authors mention that the data set is small -- 4,519 response\/feedback pairs covering 22 English and 8 German questions -- it's actually quite large for something that is completely human-compiled and human-reviewed.\nThis paper is very clear, easy to follow, and well-organized. \nsummary_of_weaknesses\nUnfortunately, the final data set contains imbalanced classes, something the authors aim to address in future versions of the data set.  I wouldn't use this as a reason to reject this paper, however.\nSome in our community may find this work, and its domain, rather niche; this paper would be a great fit for the BEA workshop. \ncomments,_suggestions_and_typos\nCan the authors mention the dates during which the data was collected?  Since this was such a big manual effort, I wouldn't be surprised if the bulk of the work was done in 2021 on data collected in 2020, for instance. This is also important since the domain is computer networking which changes fairly rapidly.\nOn line 005, insert \"many\" between \"by\" and \"Automatic\".\nOn line 040, change \"interesting\" to \"useful\".\nOn line 054, \"in the last decades\" should read \"over the past decades\".\nOn line 154, \"detrimental for\" should be \"detrimental to\".\nThe last sentence of section 2.2, beginning with \"Lastly, structured collections...\" seems out-of-place here.  Should this be a separate paragraph?  Or can you do more to tie it in with the preceding sentences?\nOn line 395, \"refined for multiple years\" should be \"refined over multiple years\".\nIn this field, it's typical to refer to learners' responses to questions as \"responses\" rather than \"submissions\".  Just a minor thing you may want to consider :) ","rid":"e3d637ae00e884f73e2173bc7a3275fc64e6b4cebfeb5ce730bf7a0f5a5700b431143167c7893ae4b373f928b4104531662f851ff665d1d2174c5bf8d5d95036","scores":{"overall":"4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.","best_paper":"Maybe","replicability":"4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.","datasets":"5 = Enabling: The newly released datasets should affect other people's choice of research or development projects to undertake.","software":"3 = Potentially useful: Someone might find the new software useful for their work."},"label":[[558,588,"Jus_pos_1"],[588,761,"Jus_pos_1"],[762,863,"Eval_pos_1"],[995,1089,"Eval_pos_2"],[1090,1152,"Eval_pos_3"],[1175,1310,"Eval_neg_1"],[1375,1446,"Eval_neg_2"],[1447,1500,"Major_claim"]],"Comments":["2;1"]}
{"id":1156,"text":"paper_summary\nThis paper presents a cross-lingual information retrieval approach using knowledge distillation. The underlying model is ColBERT with XLM-R as the pretained language model. The approach makes use of a teacher model based on query translation and monolingual IR in English. The student model is trained with two objectives. One is an IR objective to match the teacher model's query-passage relevance predictions. The second objective is to learn a representation of the non-english text that most closely matches the teacher's representation at the token level. This relies on a cross lingual token alignment based on greedily aligning tokens with the highest cosine similarity. The authors do abalations of their two objectives and find they are both useful and also compare against fine-tuning ColBERT directly on cross lingual data. On the XOR-TyDi leaderboard, one of this paper's models is the current best. \nsummary_of_strengths\n- Novel approach that does cross lingual IR where the resulting model does not use MT  -New cross lingual token alignment based on multilingual pretrained langauge model -Good abalations and comparisons with fine-tuning on cross lingual data -Strong performance on zero-shot settings as well -The paper has best performance on XOR-TyDi \nsummary_of_weaknesses\nNo major weaknesses \ncomments,_suggestions_and_typos\nline 62-64 asks whether a high performance CLIR model can be trained that can be operate without having to rely on MT. But the training process still relies on MT, so this approach does still rely on MT, right? I guess the point is that it only relies on MT at training time and not at evaluation \/ inference. It might be possible to try to make this clearer. ","rid":"0b700376f03b1c3df377fe0a191027d6dff597add85185cdf402bd12f05c6e6edbebdb6cfd2b502e3cb162776458a21933f543ef804eb6790056882b5e62d7ac","scores":{"overall":"4 = This paper represents solid work, and is of significant interest for the (broad or narrow) sub-communities that might build on it.","best_paper":"Maybe","datasets":"1 = No usable datasets submitted.","software":"1 = No usable software released."},"label":[[950,1033,"Eval_pos_1"],[1036,1117,"Eval_pos_4"],[1119,1189,"Eval_pos_2"],[1191,1239,"Eval_pos_3"],[1307,1327,"Major_claim"]],"Comments":["1;1"]}
{"id":1157,"text":"paper_summary\nThe paper investigates methods to automatically generate morally framed arguments (relying on a specific stance, on the given topic focusing on the given morals), and analyses the effect of these arguments on different audiences (namely, as liberals and conservatives). \nsummary_of_strengths\n- The topic of the paper is potentially interesting to the ACL audience in general, and extremely interesting in particular to the Argument Mining (and debating technology) research community. Investigating methods to inject morals into argument generation systems to make arguments more effective and convincing is a very valuable step in the field (opening at the same time ethical issues).\n-The paper is clear, well written and nicely structured -The experimental setting is well described and the applied methods are technically sound. It relies on the solid framework of the IBM Debater technology. \nsummary_of_weaknesses\n- very limited size of the user study  (6 people in total, 3 liberals and 3 conservatives). Moreover, a \"stereotypical\" hypothesis of their political vision is somehow assumed) -the Cohen’s κ agreement was 0.32 on the moral assignment -> while the authors claim that this value is in line with other subjective argument-related annotations, I still think it is pretty low and I wonder about the reliability of such annotation. \ncomments,_suggestions_and_typos\n[line 734] Ioana Hulpu? - > check reference ","rid":"48f9a3caf3ba774261a573d0f6d388287ca71ee6c2c330c03bb7a3743d9907c768fa31f99b02454079066176e7d9c25999903d361aae8b3f808f7164a1445fcb","scores":{"overall":"4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.","best_paper":"No","replicability":"4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.","datasets":"1 = No usable datasets submitted.","software":"4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work."},"label":[[308,498,"Eval_pos_1"],[499,698,"Eval_pos_2"],[699,754,"Eval_pos_3"],[755,845,"Eval_pos_4"],[846,910,"Jus_pos_4"],[935,970,"Eval_neg_1"],[971,1024,"Jus_neg_1"],[1024,1110,"Eval_neg_3"],[1274,1360,"Eval_neg_2"]],"Comments":["2;2"]}
{"id":1158,"text":"paper_summary\nThis paper introduces an approach to produce privacy-preserving document embeddings with the property that every single sentence of the document could be replaced by some other random one while obtaining a similar embedding for the document. The paper is way out of my area of expertise, and thus this review must remain an educated guess. \nThe proposed method replaces sentence embeddings from the private document for embeddings obtained from a set of public documents. The method uses embeddings that are close to the original ones by picking those with high \"Tukey Depth\" with respect to the set of candidate embeddings. Finally, the embeddings are spread apart by passing them through a network trained with an unsupervised clustering signal, and, if understand correctly, averaged together. \nsummary_of_strengths\n1. The method seems to be well grounded in a theoretical framework. \n2. The paper is very clear, and even though I lack the relevant background I could follow it (with quite some effort, which is also quite natural) 3. The empirical results seem good (but I am not familiar with the state of the art in this area) \nsummary_of_weaknesses\n1. As an outsider, I wonder whether some natural baselines could have been explored. For instance, given that the method uses sentence embeddings after a clustering transformation, I wonder what would happen if cluster centroids were directly used instead. Similarly, I wonder what's the need for computing the Tucker Depth, and not just using the closest vector in terms of cosine similarity. \ncomments,_suggestions_and_typos\nL404: It might be worth recalling the reader that k is the number of sentences at this point, given that this fact was introduced much earlier on and not used until then. ","rid":"3b7abb0934c42ac0f248c37cd980b1fd8cb472be2563c43775260b1a7d765728f8f85a513beba740c4a5c6a50e70cfcb2fa3fe097f7606711ae41ec14f73aa8e","scores":{"overall":"4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.","best_paper":"No","replicability":"3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and\/or the training\/evaluation data are not widely available.","datasets":"1 = No usable datasets submitted.","software":"1 = No usable software released."},"label":[[836,901,"Eval_pos_1"],[905,995,"Eval_pos_2"],[1052,1083,"Eval_pos_3"],[1173,1254,"Eval_neg_1"],[1255,1426,"Jus_neg_1"]],"Comments":["2;1"]}
{"id":1159,"text":"paper_summary\nThis paper is about improving the prosody of neural text-to-speech (NTTS) systems using the surrounding context of a given input text. The study introduced an extension to a well known NTTS system i.e., FastSpeech-2. The extension is a phoneme level conditional VAE. As cited in the current paper both FastSpeech-2 and conditional VAE are already proposed in the literature. The main novelty of this paper is representation of surrounding utterances using a pre-trained  BERT model and generation of prosodically varied samples with the help of learned contextual information. Authors followed standard TTS evaluation protocols to evaluate their proposed architecture, and evaluation results are in favor of the proposed architecture. \nsummary_of_strengths\n- This paper introduced a new component to FastSpeech-2, a well known non-autoregressive NTTS architecture, called as cross utterance conditional VAE (CUC-VAE).  -The CUC-VAE contains two main components 1) cross utterance (CU) embedding and 2) CU enhanced conditional VAE. \nsummary_of_weaknesses\n- As a reviewer, I found the paper slightly difficult to read -- some long sentences can be rewritten to improve the clarity of the paper reading.  -The subjective results are derived on a small set of utterances (11 audios) using a small number of listeners (23 subjects), this may not be substantial enough for statistical significance of the results published in the paper.\n-It is not clear why CUC-VAE TTS system with L=1 performed worse than baseline system -- an appropriate reason or further analysis may be required to validate this.  -In general, there are quite a few things missing -- details provided in comments section. \ncomments,_suggestions_and_typos\n**Typos:** -Background section: \"...high fidelity thank to…\" -> \"...high fidelity thanks to…\" -Background section: \" … Fang et al., 2019).Many…\" -> \" … Fang et al., 2019). Many…\" -Figure-1: \"...which integrated to into…\" -> \"...which integrated into…\" **Comments:** -Author did not mention how the initial durations of phonemes are obtained.\n-Are durations of phonemes predicted in frames or seconds?\n-Figure-1 did not mention how the proposed CUC-VAE TTS system works in the inference time. Moreover, it is hard to understand the color schema followed in the Figure-1, there is no legend.\n-There is no mentioning of train, valid and test set splits in the dataset section.\n-In Table-2 the baseline system received a better MOS score than the baseline + fine-grained VAE and baseline + CVAE, why is it? Whereas in Table-4 the baseline system show high MCD and FFE error than the baseline + fine-grained VAE and baseline + CVAE systems, why is it?\n-How do you represent the reference mel-spectrogram at phoneme level?\n-Did you use pre-trained HiFi-GAN to synthesize speech from the predicted mel-spectrograms? ","rid":"a331c73408dcc74e2e0cddb0cd29ec5d9d58a7772f24c2e2d84f8bcad13f7bd53b391ff113fa24c42e5dbf0a4d22f7f3d682bb72d24f7bb7bc1eabc0a3cfe765","scores":{"overall":"2 = Borderline: This paper has some merit, but also significant flaws. It does not warrant publication at top-tier venues, but might still be a good pick for workshops.","best_paper":"No","replicability":"3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and\/or the training\/evaluation data are not widely available.","datasets":"1 = No usable datasets submitted.","software":"3 = Potentially useful: Someone might find the new software useful for their work."},"label":[[1069,1129,"Eval_neg_1"],[1132,1214,"Jus_neg_1"],[1217,1340,"Jus_neg_2"],[1342,1444,"Eval_neg_2"],[1446,1530,"Eval_neg_3"],[1534,1609,"Jus_neg_3"],[1612,1660,"Eval_neg_4"],[1664,1702,"Jus_neg_4"]],"Comments":["3;2"]}
{"id":1160,"text":"paper_summary\nThis paper proposes a novel refinement method to synchronously refine the previously generated words and generate the next word for language generation models. The authors accomplish this goal with an interesting implementation without introducing additional parameters. Specifically, the authors reuses the context vectors at previous decoding steps (i.e., c_1, c_2, ..., c_{i-2}) to calculate the refined probabilities in a similar way to the standard generation probabilities (the only difference is that using c_{0<n<i-1} instead of c_{i-1}). A refinement operation will be conducted at a previous position, where the refinement probability is greater than the generation probability.\nTo reduce the computational cost and potential risk of \"over-refinement\", the authors design a local constraint that narrow the refinement span to the N nearest tokens. In model training, the authors randomly select future target words not greater than N to cover a variety of different future contexts as bleu parts. \nsummary_of_strengths\n1. A novel approach to accomplish the modeling of future context. \n2. Comprehensive experiments to validate the effectiveness of the proposed approach across different tasks (e.g., standard and simultaneous machine translation, storytelling, and text summarization). \n3. Detailed analyses to show how each component (e.g., the hyper parameter N, local constraints and refinement mask) works. \nsummary_of_weaknesses\nThe main concern is the measure of the inference speed. The authors claimed that \"the search complexity of decoding with refinement as consistent as that of the original decoding with beam search\" (line 202), and empirically validated that in Table 1 (i.e., #Speed2.).\nEven with local constraint, the model would conduct 5 (N=5) more softmax operations over the whole vocabulary (which is most time-consuming part in inference) to calculate the distribution of refinement probabilities for each target position. Why does such operations only marginally decrease the inference speed (e.g., form 3.7k to 3.5k tokens\/sec for Transformer-base model)?\nHow do we measure the inference speed? Do you follow Kasai et al., (2021) to measure inference speed when translating in mini-batches as large as the hardware allows. I guess you report the batch decoding speed since the number is relatively high. Please clarify the details and try to explain why the refinement model hardly affect the inference speed.\nThe score will be increased if the authors can address the concern.\n[1] Jungo Kasai, Nikolaos Pappas, Hao Peng, James Cross, and Noah Smith. Deep Encoder, Shallow Decoder: Reevaluating Non-autoregressive Machine Translation. ICLR 2021. \ncomments,_suggestions_and_typos\n1. Line118: SelfAtt_c => Cross Attention, the attention network over the encoder representations is generally called as cross attention.\n2. Ablation study in Section 4.1.3 should be conducted on validation sets instead of test sets (similar to Section 4.1.2). In addition, does the refinement mask in Table 2 denote that randomly selecting future target words no greater than N in model training (i.e., Line 254)?\n3. Is PPL a commonly-used metric for storytelling? ","rid":"32528ea92c39b9389b0f8f7f6bbf216f4e7af7362600c7ab871198c6eb3b2151b22039c196d0d61bf70af1db62ed229cd72e68140bcd3875e3d62240175aced5","scores":{"overall":"3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.","best_paper":"No","replicability":"5 = They could easily reproduce the results.","datasets":"1 = No usable datasets submitted.","software":"1 = No usable software released."},"label":[[174,241,"Eval_pos_5"],[242,284,"Jus_pos_5"],[1046,1109,"Eval_pos_1"],[1113,1216,"Eval_pos_2"],[1224,1307,"Jus_pos_2"],[1313,1435,"Eval_pos_3"],[1458,1514,"Eval_neg_1"],[2353,2458,"Jus_neg_1"]],"Comments":["2;3"]}
{"id":1161,"text":"paper_summary\nThe paper proposes 6 test corpora for vision and language captioning systems that target specific competency. For each competency, examples are generated semi-automatically from existing language + vision tasks, such QA in V7W, and are created in a FOIL style, where one example correctly describes the image, while another example makes a minimal change to caption and does not describe the image. Systems are then challenged to prefer captions that correctly identify the image. The competencies tested include existence, plurality\/counting, spatial reasoning (via prepositions), situational knowledge (via imSitu data), and coreference. The paper evaluates several recent pre-training based models, finding that many fail at their challenges, and that the multi-task model 12-in-1, works best. \nsummary_of_strengths\nProposes a fairly diverse set of challenges that could be a useful diagnostic going forward.\nThe paper evaluates currently relevant model on the diagnostic, establishing clear baselines for their dataset moving forward.  Because the paper encompasses essentially 5 independent datasets, it a very substantial body of work. It seems larger than a standard paper. \nsummary_of_weaknesses\n(being a previous reviewer R BWRg, I will respond to previously identified weakness) I still find the argument of what is and is not included in the diagnostic unclear. In many ways, this seems like a case of a subset of competencies that we have enough visual annotations to semi-automatically create data for. In my opinion, the paper should steer away from making arguments that these examples are deeply linguistic, beyond, involving nouns, counting, verbs, and coreference. As such, I find the title and some of the introduction over-claiming, but, this is really a matter of opinion, resting on what exactly 'linguistic' means.\nThe main body of the paper still lacks examples but I appreciate their inclusion in the appendix. It's very hard to imagine the foils from the descriptions alone. This may be asking a lot, but the paper would be significantly improved if the last page were almost entirely made of examples from the appendix. This is a CVPR style of presentation, and would require significant text trimming.  The examples were good overall, but the co-ref part of the benchmark stands out. It is essentially a QA task, which isn't really compatible with just caption based training that most of the evaluated most are setup to do (with the exception of 12-1). This isn't an issue, because its not really the benchmark's problem, but I am not sure the format of the foil is that sensible. I suspect this will be the least used of the new foils, but I don't have a concrete proposal how it could be improved to really be a captioning task. \ncomments,_suggestions_and_typos\n- ","rid":"a42c480628723affbe6a3d6044ee9416d717cb6c2075135233c3e2960e2cc480a0cd0b4faf5d44572bb15d6197bced37707077a2c1ced77ebaa05dd8c2b5e70a","scores":{"overall":"4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.","best_paper":"No","replicability":"3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and\/or the training\/evaluation data are not widely available.","datasets":"4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.","software":"1 = No usable software released."},"label":[[833,925,"Eval_pos_1"],[1054,1119,"Jus_pos_2"],[1120,1195,"Eval_pos_2"],[1302,1386,"Eval_neg_1"],[1387,1851,"Jus_neg_1"],[1852,1899,"Eval_neg_4"],[1950,2160,"Jus_neg_4"],[2245,2325,"Eval_neg_3"],[2325,2774,"Jus_neg_3"]],"Comments":["3;2"]}
{"id":1162,"text":"paper_summary\nMotivated by empirical findings that training models with Prompt Tuning can achieve the same performance as fully fine-tuning a model, but the training takes much longer to reach the same performance, they explore ways to exploit knowledge from already trained prompts.\nThey explore using already trained prompts to transfer knowledge between tasks (using the same frozen model) and also the transfer of prompt between _different_ frozen models. For between task transfer, they either directly re-use a prompt from a source task on the target task or they use the prompt learned from the source task as the initialization point for the target task.\nFor between model transfer, they uses these same methods but include a learnable `projector` (a small, 2 layer neural-network) that maps the prompts from one frozen model to another be using the projected prompt in one of the methods mentioned above. They have two methods for learning this `projector`. In the first method, which they call _Distance Minimization_, they minimize the $L_2$ distance between a projected source prompt (trained on the source frozen model) and a target prompt (a prompt trained on the same task using the target model). In the second method (_Task Tuning_) they learn the `projector` via backpropagation. In this case they take a prompt trained on a source task $P_s$, project it ($Proj(P_s)$) and then use that when prompt tuning the target model. Gradient updates are only applied to the projector.\nThey also look at several methods of prompt similarity and use them to predict prompt transferability. They main methods are Cosine and Euclidean distances between prompt tokens and their novel model activation similarity where prompts are fed into frozen models and the activations of the feed-forward layers are recorded. The call this method _ON_. ### Results Their first results look at the performance of directly re-using a prompt trained on a source task for a downstream task. They find that this can produce strong performance (measured in relative performance, the direct source to target prompt transfer performance divided by the performance researched from directly training on the target task) within clusters of similar tasks.\nTheir second results look at the performance of using a prompt learned on a source task to initialize the prompt for a target task and then doing Prompt Tuning. They find that this method can give consistent gains in terms of task performance as well as speed of convergence.\nTheir third set results examine transfer across models. They find that direct re-use of a prompt projected by the `projector` learned via the _Distance Minimization_ method results in poor performance, especially within the Sentiment tasks. They find that direct reuse of a prompt projected by a `projector` learned with their _Task Tuning_ method does better especially when the tasks are within the same cluster. They also look at how using a _Task Tuning_ prompt to initialize training of a new prompt performs and finds that it can lead to some improvements in task performance and small improvements in convergence speed.\nTheir final set of results examine use prompt similarity methods to predict prompt transferablity (in the context of direct prompt reuse). They find that all methods are able to distinguish between multiple prompts (created by training with different random seeds) trained for the same task from prompts trained for other tasks. They also find that _ON_ produces a ranking of similar prompts that best correlate with direct reuse performance (using Spearman's rank correlation scores). They also find that the correlation decreases as the size of the frozen model grows. \nsummary_of_strengths\nThe strengths of the paper include:  * Experiments on many different and diverse datasets, 17 with a good mixture of sentiment, NLI, EJ, Paraphrase detection, and Question answers. \n  * Experiments across many model sizes and architectures, including encoder-only models like RoBERTa instead of just the encoder-decoder and decoder-only models we see else where. \n  * The inclusion of small motivating experiments like the convergence speed are a great way to establish the importance of the work and the impact it would have. \n   * The use of the same methods (direct reuse of prompts and using prompts as initialization) in different settings (cross task transfer with the same model and cross model transfer with the same task) and similar results in each demonstrate the robustness of the method. \n   * Not only does their novel prompt similarity method (_ON_ based on model activations when processing the prompt) work great at predicting direct use similarity, it also captures the non-linear way the model interacts with the prompt in a way that simple methods like token similarity can. \nsummary_of_weaknesses\nThe majority of the weaknesses in the paper seem to stem from confusion and inconsistencies between some of the prose and the results.\n1. Figure 2, as it is, isn't totally convincing there is a gap in convergence times. The x-axis of the graph is time, when it would have been more convincing using steps. Without an efficient, factored attention for prompting implementation a la [He et al. (2022)](https:\/\/arxiv.org\/abs\/2110.04366) prompt tuning can cause slow downs from the increased sequence length. With time on the x-axis it is unclear if prompt tuning requires more steps or if each step just takes more time. Similarly, this work uses $0.001$ for the learning rate. This is a lot  smaller than the suggested learning rate of $0.3$ in [Lester et al (2021)](https:\/\/aclanthology.org\/2021.emnlp-main.243\/), it would have been better to see if a larger learning rate would have closed this gap. Finally, this gap with finetuning is used as a motivating examples but the faster convergence times of things like their initialization strategy is never compared to finetuning. \n2. Confusion around output space and label extraction. In the prose (and Appendix A.3) it is stated that labels are based on the predictions at `[MASK]` for RoBERTa Models and the T5 Decoder for generation. Scores in the paper, for example the random vector baseline for T5 in Table 2 suggest that the output space is restricted to only valid labels as a random vector of T5 generally produces nothing. Using this rank classification approach should be stated plainly as direct prompt reuse is unlikely to work for actual T5 generation. \n3. The `laptop` and `restaurant` datasets don't seem to match their descriptions in the appendix. It is stated that they have 3 labels but their random vector performance is about 20% suggesting they actually have 5 labels? \n4. Some relative performance numbers in Figure 3 are really surprising, things like $1$ for `MRPC` to `resturant` transfer seem far too low, `laptop` source to `laptop` target on T5 doesn't get 100, Are there errors in the figure or is where something going wrong with the datasets or implementation? \n5. Prompt similarities are evaluated based on correlation with zero-shot performance for direct prompt transfer. Given that very few direct prompt transfers yield gain in performance, what is actually important  when it comes to prompt transferability is how well the prompt works as an initialization and does that boost performance. Prompt similarity tracking zero-shot performance will be a good metric if that is in turn correlated with transfer performance.  The numbers from Table 1 generally support that this as a good proxy method as 76% of datasets show small improvements when using the best zero-shot performing prompt as initialization when using T5 (although only 54% of datasets show improvement for RoBERTa). However Table 2 suggests that this zero-shot performance isn't well correlated with transfer performance. In only 38% of datasets does the best zero-shot prompt match the best prompt to use for transfer (And of these 5 successes 3 of them are based on using MNLI, a dataset well known for giving strong transfer results [(Phang et al., 2017)](https:\/\/arxiv.org\/abs\/1811.01088)). Given that zero-shot performance doesn't seem to be correlated with transfer performance (and that zero-shot transfer is relatively easy to compute) it seems like _ON_'s strong correlation would not be very useful in practice. \n6. While recent enough that it is totally fair to call [Vu et al., (2021)](https:\/\/arxiv.org\/abs\/2110.07904) concurrent work, given the similarity of several approaches there should be a deeper discussion comparing the two works. Both the prompt transfer via initialization and the prompt similarity as a proxy for transferability are present in that work. Given the numerous differences (Vu et al transfer mostly focuses on large mixtures transferring to tasks and performance while this work focuses on task to task transfer with an eye towards speed. _ ON_ as an improvement over the Cosine similarities which are also present in Vu et al) it seems this section should be expanded considering how much overlap there is. \n7. The majority of Model  transfer results seem difficult to leverage. Compared to cross-task transfer, the gains are minimal and the convergence speed ups are small. Coupled with the extra time it takes to train the projector for _Task Tuning_ (which back propagation with the target model) it seems hard to imagine situations where this method is worth doing (that knowledge is useful). Similarly, the claim on line 109 that model transfer can significantly accelerate prompt tuning seems lie an over-claim. \n8. Line 118 claims `embedding distances of prompts do not well indicate prompt transferability` but Table 4 shows that C$_{\\text{average}}$ is not far behind _ON_. This claim seems over-reaching and should instead be something like \"our novel method of measuring prompt similarity via model activations is better correlated with transfer performance than embedding distance based measures\" \ncomments,_suggestions_and_typos\n1. Line 038: They state that GPT-3 showed extremely large LM can give remarkable improvements. I think it would be correct to have one of their later citations on continually developed LM as the one that showed that. GPT-3 mostly showed promise for Few-Shot evaluation, not that it get really good performance on downstream tasks. \n2. Line 148: I think it would make sense to make a distinction between hard prompt work updates the frozen model (Schick and Schütez, etc) from ones that don't. \n3. Line 153: I think it makes sense to include [_Learning How to Ask: Querying LMs with Mixtures of Soft Prompts_ (Qin and Eisner, 2021)](https:\/\/aclanthology.org\/2021.naacl-main.410.pdf) in the citation list for work on soft prompts. \n4. Figure 3: The coloring of the PI group makes the text very hard to read in Black and White. \n5. Table 1: Including the fact that the prompt used for initialization is the one that performed best in direct transfer in the caption as well as the prose would make the table more self contained. \n6. Table 2: Mentioning that the prompt used as cross model initialization is from _Task Tuning_ in the caption would make the table more self contained. \n7. Line 512: It is mentioned that _ON_ has a drop when applied to T$5_{\\text{XXL}}$ and it is suggested this has to do with redundancy as the models grow. I think this section could be improved by highlighting that the Cosine based metrics have a similar drop (suggesting this is a fact of the model rather than the fault of the _ON_ method). Similarly, Figure 4 shows the dropping correlation as the model grows. Pointing out the that the _ON_ correlation for RoBERTA$_{\\text{large}}$ would fit the tend of correlation vs model size (being between T5 Base and T5 Large) also strengths the argument but showing it isn't an artifact of _ON_ working poorly on encoder-decoder models. I think this section should also be reordered to show that this drop is correlated with model size. Then the section can be ended with hypothesizing and limited exploration of model redundancy. \n8. Figure 6. It would have been interesting to see how the unified label space worked for T5 rather than RoBERTAa as the generative nature of T5's decoding is probably more vulnerable to issue stemming from different labels. \n9. _ ON_ could be pushed farther. An advantage of prompt tuning is that the prompt is transformed by the models attention based on the value of the prompt. Without having an input to the model, the prompts activations are most likely dissimilar to the kind of activations one would expect when actually using the prompt. \n10. Line 074: This sentence is confusing. Perhaps something like \"Thus\" over \"Hence only\"? \n11. Line 165: Remove \"remedy,\" ","rid":"a6f82cf1b57c8583edc2f577a981c611737147123a0d7938a14b18effdeef61e7a661ded347656f6a6ddf9ab69c25c5e6a56f25a4df9c814a9ece60fc02ebcee","scores":{"overall":"3.5 ","best_paper":"No","datasets":"1 = No usable datasets submitted.","software":"2 = Documentary: The new software will be useful to study or replicate the reported research, although for other purposes it may have limited interest or limited usability. (Still a positive rating)"},"label":[[3771,3822,"Eval_pos_2"],[3823,3913,"Jus_pos_2"],[4100,4259,"Eval_pos_3"],[4539,4827,"Eval_pos_1"],[4850,4984,"Eval_neg_1"],[4985,6691,"Jus_neg_1"],[6692,9049,"Jus_neg_1"],[9050,9951,"Jus_neg_1"]],"Comments":["3;3"]}
{"id":1163,"text":"paper_summary\nThis paper focuses on using bandit learning to learn from user feedback for Extractive QA (EQA), the binary supervisory signals from user feedback serve as rewards pushing QA systems to evolve. The learning algorithm aims to maximise the rewards of all QA examples, which consists of online learning and offline learning, the online learning receives user feedback and updates model parameters after seeing one QA example, whereas offline learning updates model parameters after seeing all QA examples.  The experimental results on QA datasets from MRQA support the effectiveness of the proposed bandit learning approach, proving that the proposed approach can consistently improve model’s performance on SQuAD, HotpotQA and NQ in in-domain experiments under online learning especially when there are extremely little QA examples available for SQuAD. Besides, a set of experiments are conducted to investigate the difference between online learning and offline learning, and the importance of model initialisation in the proposed bandit learning approach. \nsummary_of_strengths\n1. The proposed bandit learning approach that learns from user feedback for EQA is novel, which simulates real deployment environment and provides insights for further exploration in bridging the gap between QA model training and deployment. \n2. Empirical results show the effectiveness of the proposed approach, especially the in-domain experimental results for online learning. \n3. Conducting extensive experiments studying the effect of domain transfer and model initialisation. \nsummary_of_weaknesses\n1. The binary reward from user feedback is weak due to the large search space for EQA, resulting in the incapability of providing precise supervisory signals. Need to design a more sophisticated reward. \n2. The proposed approach heavily relies on how accurate the initial model is, which means it is highly sensitive to model initialisation, limiting its usefullness. \n3. In in-domain experiments of online and offline learning, bandit learning approach hurts model’s performance under some scenarios especially for TriviaQA and SearchQA. \n4. Some other papers of learning from feedback for QA should be compared, such as Learning by Asking Questions, Misra et al. CVPR 2017. \ncomments,_suggestions_and_typos\nQuestions:      1. Why only use single-pass in online learning? ","rid":"6948377b128ce8c60c32a0fc8cf7ffe45ae2fc1ded70405ea0ede6d577ec7fa193db3011dc9221756d0eaa8b73003124d81f069f9da16fe2d894c05637eefab9","scores":{"overall":"4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.","best_paper":"No","replicability":"4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.","datasets":"1 = No usable datasets submitted.","software":"1 = No usable software released."},"label":[[1336,1472,"Eval_pos_1"],[1476,1573,"Eval_pos_2"],[1600,1755,"Jus_neg_1"],[1756,1800,"Eval_neg_1"],[1804,1878,"Eval_neg_2"],[1879,1938,"Jus_neg_2"],[1939,1965,"Eval_neg_2"]],"Comments":["2;1"]}
{"id":1164,"text":"paper_summary\nThis paper works on the problem of personalization in knowledge grounded conversation (KGC). To develop a benchmark, the authors collected a new KGC dataset based on Reddit containing personalized information (e.g. user profile and dialogue history). The authors propose a probabilistic model for utterance generation conditioned on both personalized profile (personal memory) and personal knowledge. Dual learning is employed to better learn the unconstrained relation between personal memory $Z^m$ and knowledge $Z^k$ , and variational method is proposed to approximately marginalize out $Z^m$ and $Z^k$ during inference. The results with automatic evaluation show promising improvement and human evaluation also validates this. Finally, various ablation studies are conducted to reveal the contribution of each model component. \nsummary_of_strengths\n- The problem of personalization in KGC is a relatively overlooked yet important problem. The authors developed a promising method and benchmark for this new challenge.\n- The idea of incorporating dual learning to link personalized sources (e.g. personal memory and knowledge) is very interesting and convincing. I’d like to see follow-up works comparing the ideas against this paper’s.\n- The improvement in automatic evaluation is significant (though not fully reliable, as the author’s acknowledge in line 522). Human evaluation also corroborates the proposed model’s superiority, though the improvement becomes less significant. \nsummary_of_weaknesses\n- The paper is generally well-written and easy to follow, but the definition of personal memory was quite ambiguous and not fully defined. For instance, does this concept include false beliefs (incorrect knowledge), subjective opinions (unsupported knowledge) or inferential knowledge? What would be the unit of personal memory in the context of visually grounded dialogues (line 134)? How can we extend the idea to inter-personal knowledge, i.e. common ground?\n- I understand the space is limited, but I think more information\/explanation on the collected dataset should be added (e.g. data collecting procedure and reviewing process). \ncomments,_suggestions_and_typos\n- In lines 198-220, explanation of $\\phi$, $\\psi$ and $\\pi$ is not clear. Can they be better explained or incorporated in Figure 2?\n- In Figure 2, should the distilled distribution of $Z^p$ not be conditioned on $Z^k$? In the text, $q_\\phi (Z^p | C, R)$ is not conditioned on $Z^k$ (lines 199, 207) - Typo: “the the” in line 278 - For Table 3, did you also evaluate against human answers (e.g. original response)? If available, it may be better to be incorporated.\n- What exactly is personal memory? How is this defined, esp. in other domains? I’d like to see more discussion on this in the updated paper. ","rid":"e98fb117fe41ef1ad9ab1de71467e6a101280857b649e488d537b1d56694d0fda758df2344e2c13b1cbfdccc3a1fac74b6f2b64d4f219d1f256c93f04702feb1","scores":{"overall":"4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.","best_paper":"No","replicability":"3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and\/or the training\/evaluation data are not widely available.","datasets":"4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.","software":"4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work."},"label":[[869,1035,"Eval_pos_1"],[1037,1179,"Eval_pos_2"],[1255,1310,"Eval_pos_3"],[1381,1499,"Eval_pos_3"],[1579,1660,"Eval_neg_1"],[1660,1983,"Jus_neg_1"],[2024,2102,"Eval_neg_2"],[2103,2159,"Jus_neg_2"]],"Comments":["2;2"]}
{"id":1165,"text":"paper_summary\nThis work tackles the automatic identification of human values in arguments. \nHuman values can provide the underlying motives behind arguments and can explain why someone takes a position or what goals the person sees as worth striving for. \nOpposing values or different ways of prioritizing these values lead to conflicts and disagreements. If the underlying values can be made explicit, valuable insights can be gained in the analysis of different discourses. In other contexts arguments can be automatically generated for a target audience based on appropriate matching values and thus be more convincing. This motivates the authors to tackle the task and initially address the following problems: they systematically evaluate existing taxonomies from social and psychological science. Based on this they develop a framework that integrates the values from the various theories and that provides different levels of granularity, thus making an automatic classification of these values more feasible. The framework is used to create a resource with arguments from different geographical regions annotated with human values. The authors conduct some classification experiments with transformer-based models and discuss the results in an analysis. \nsummary_of_strengths\nThe authors provide the following contributions: -a systematic literature review of existing theories about the classification of human values in argumentation -a theoretically-informed framework to annotate human values in arguments. Different levels of granularity provide flexibility for the annotation of new data and is useful to do automatic classification of human values, especially when data is scarce and the classes are imbalanced -a resource annotated with their new framework that is made accessible for the research community. The annotation process is described in detail. The resource contains arguments from different cultural regions.  -experiments for the automatic classification of human values using the new data set. These can serve as a baseline for future research and they provide some initial insights about the difficulty of the task and the robustness \nsummary_of_weaknesses\nSection 3 could benefit from a more clear structure and some insights from the background section that lead to the design of the proposed taxonomy. It took me some time to understand that the schemata introduced in section 2.1 are also considered when designing the new taxonomy (Table 1), making this more explicit (already in section 2.1) would help to make this more clear. To me it is not clear how the beginning of section 3 (claim 1: human values are implicit, claim 2: human values make arguments more persuasive, claim 3: the implicit connection has to me made explicit) provide justifications for the new design. To me the motivation for the proposed taxonomy is a) that it combines different theories therefore is more complete, missing values have been added and b) that is has different levels of granularity which is useful for both, quantitative analyses and machine learning. Although this was hinted at in the introduction, I am missing the clear motivation \/ advantages for the new taxonomy in this section. \nOn top of that there is a missing reference that is relevant, as it tackles the automatic classification of human values using the Schwartz taxonomy (though in social media and using feature-based approaches, no Deep Learning): A Societal Sentiment Analysis: Predicting the Values and Ethics of Individuals by Analysing Social Media Content (https:\/\/aclanthology.org\/E17-1069.pdf) \ncomments,_suggestions_and_typos\nThis paper has some weaknesses in section 2 \/ 3 but these can be resolved by some rewording \/ making things more clear \/ adding the missing reference. The theoretical review and development of the new taxonomy, the resource and the experiments are a strong contribution and will be valuable for the research community.  minor remarks: 212-218: unclear, what is meant by 'task'? The 'task' for the ML model? \n247-255: sounds more like future work or has to be rephrased to make clear that this is a motivation for using cross-cultural data 289: how was it translated? manually? \nWhy only 50 arguments from Africa but 100 from India and China? \ntypos: 155 (present*s*), 268 conclusion(s) ","rid":"37280326dd784256b0f392b38a62f06359a09c4159c5463d3d51d9ca70d2a69b91c3e0beb1b7816558eaf5174963d9bfdf4e72618996ec5dfba32675cf1c9bd4","scores":{"overall":"4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.","best_paper":"No","replicability":"4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.","datasets":"4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.","software":"3 = Potentially useful: Someone might find the new software useful for their work."},"label":[[1334,1443,"Eval_pos_1"],[1445,1518,"Eval_pos_2"],[1519,1726,"Jus_pos_2"],[2188,2335,"Eval_neg_1"],[2336,3213,"Jus_neg_1"],[3214,3275,"Eval_neg_3"],[3276,3362,"Jus_neg_3"],[3779,3946,"Major_claim"]],"Comments":["2;2"]}
{"id":1166,"text":"paper_summary\nThis paper proposes a solution for \"Contrastive Conflicts\". What exactly are “Contrastive Conflicts”? They occur when multiple questions are derived from a passage, each with different semantics. The questions are going to be close to the passage in representation space and by transitivity they are going to be close among themselves even though they are semantically different (Transitivity Conflict). In addition to this, if multiple questions derived from the same passage are in the same training batch, then the questions will see that passage as both positive and negative (In-Batch Conflict).\nThe solution proposed by the paper is to use smaller granularity units, i.e. contextualized sentences. Per sentence representations are computed by using per sentence special indicator tokens, then a similar approach to DPR is used to finetune sentence representations. Because different questions have answers in different sentences the contrastive conflict is generally resolved.\nImprovements are reported on NQ, TriviaQA and SQuAD, especially on SQuAD where conflicts are reported to be severe (i.e. often multiple different questions are extracted from the same passage). Extensive experiments show that the method does well even in transfer learning. \nsummary_of_strengths\nStrengths: -The paper obtains small but convincing improvements on NQ and TriviaQA, and large but a bit puzzling results on SQuAD (considering that one of the baselines does not match the DPR paper and that SQuAD can benefit dramatically from combining DPR with BM25, but it is not done in this paper).\n-The paper presents many interesting ablations and transfer learning experiments that help further convince the reader of the efficacy of the method. \nsummary_of_weaknesses\nWeaknesses: -Retrieving (avg # sentences) * 100 sentences (see section 3.3) instead of just 100 sentences seems to be a bit of a cheat. For a strict comparison to DPR, Top-20 and Top-100 performance should be reported with exactly those numbers of retrieved elements and without post-processing on larger sets of retrieved passages. One could argue that allowing for more expensive passage retrieval is what is giving the improvements in this paper, other than for SQuAD where the lower granularity does seem to be helping, except it doesn’t help as much as BM25.\n-The idea of having more granular representations for passage retrieval is far from new. The authors do cite DensePhrases (Lee et al. 2021), but don’t mention that it’s already at lower granularity than passage level. They could also cite for example ColBERT (Khattab et al. 2021).\n-The big improvement reported in Table 2 for SQuAD “Single” is a bit confusing since it relies on a Top-20 number that is much lower that what is reported on the DPR paper (although this seems to be a common problem). On the positive side, the number reported for SQuAD “Multi” matches the DPR paper. \ncomments,_suggestions_and_typos\nSuggestions: -Line 91: the authors claim that contrastive conflicts are *the* cause for bad performance on SQuAD, but the statement seems unjustified at that point. It might make sense to refer to later results in the paper. ","rid":"043aba43da4da9c0ab308268b8ace3bbc8c1ac766b70111814b13fb419cb7b1fdfe563dfa98d1263c3c060b2463526f656e0bddd2544c15ee19026645c76bcce","scores":{"overall":"3.5 ","best_paper":"No","replicability":"5 = They could easily reproduce the results.","datasets":"1 = No usable datasets submitted.","software":"1 = No usable software released."},"label":[[1305,1422,"Eval_pos_1"],[1597,1746,"Eval_pos_2"],[1782,1904,"Eval_neg_1"],[1905,2332,"Jus_neg_1"],[2333,2421,"Eval_neg_2"],[2423,2614,"Jus_neg_2"],[2616,2693,"Eval_neg_3"],[2694,2832,"Jus_neg_3"],[2972,3113,"Eval_neg_4"],[3114,3174,"Jus_neg_4"]],"Comments":["3;1"]}
{"id":1168,"text":"paper_summary\nThis paper explores the topic of automatic readability assessment, specifically reframing the task as one of pairwise ranking. The authors experiment with neural and other baseline models. \nsummary_of_strengths\n- A number of data-sets are used, some for testing only -Cross-corpus and cross-lingual experiments are done to show generalizability -A number of baseline systems are used, such as traditional classification- and regression-based systems (where possible) for comparison -A data-set is being released along with this work, which may lead to more development opportunities in the future \nsummary_of_weaknesses\n- For the regression-based systems, I see that OLS was used while for the classification-based systems, SVM was used. Why not use SVR for the former? It would seem to be a closer comparison and it might prove to be better baseline against NPRM, etc.\n-For the cross-linguistic analyses, multilingual BERT is used. I would assume French and Spanish are included here, so claiming that this does well cross-linguistically assumes that too, right? \ncomments,_suggestions_and_typos\n- Are \"slugs\" restricted to being only in either test\/train for the cross-validation experiments? I'm a little unclear on if this would be necessary or not. It might be good to address whether this is a concern.\n- \"In this background\" - This phrase is used a couple of times and it's unclear what is meant. Consider rephrasing, e.g. \"With this background\", \"Given this background\", etc.\n-2 Related Work - \"result in\" --> \"results in\" -4.4 Evaluation - \"w\" --> \"we\"? It's unclear what is meant.\n-Limitations of NPRM - \"while\" --> \"why\" (or just get rid of \"while\") ","rid":"d60ff492ac6bf5bdf7d28a9a93c1bb33a8484c0369070afcc4da9a0a87725aa38a8440a708841aa1603bcaeb528ff9730ffe1fa22f4231e5afbeb10efc8218d4","scores":{"overall":"3.5 ","best_paper":"No","replicability":"5 = They could easily reproduce the results.","datasets":"4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.","software":"3 = Potentially useful: Someone might find the new software useful for their work."},"label":[],"Comments":["1;3"]}
{"id":1169,"text":"paper_summary\nThe paper presents a novel approach to understanding math problems from their textual formulations. The approach builds on those from related work, choosing syntactic representations. The key novelties are (1) an internal graph representation of the operators and (2) a novel pretraining setting. The model achieves vast improvements over prior art. \nsummary_of_strengths\nThe new model addresses several key problems of previous work and appears to contribute a very logically motivated extension, modeling the structure of the required mathematical operations. The model description is clear and the experimental setup and results are reasonably clear and allow for an easy comparison with related work. There is also an ablation study to analyze the contribution of the individual components of the model.  The paper is easy to read. \nsummary_of_weaknesses\nThe model section seems to lack comparison with prior work. It is not entirely clear what is novel here and what is taken from prior work. It is also not entirely clear to me if pretraining is performed with data from all tasks and whether the same setup had been used previously. If this is different from prior work, that would be unfair and a major flaw. \ncomments,_suggestions_and_typos\nI'd like to see my doubt about the pretraining cleared up. ","rid":"616f1e9160debea4910d2e381d7793d0fdf92e90a76420060d99211d5c9fea7770d1ef76fde0ec0be8ad55122fb82e378ea9c27f5f8749cfe3c90de2e692badc","scores":{"overall":"3.5 ","best_paper":"No","replicability":"3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and\/or the training\/evaluation data are not widely available.","datasets":"1 = No usable datasets submitted.","software":"1 = No usable software released."},"label":[[310,364,"Eval_pos_1"],[386,575,"Eval_pos_2"],[576,606,"Eval_pos_3"],[611,717,"Eval_pos_6"],[823,850,"Eval_pos_5"],[873,932,"Eval_neg_1"],[933,1231,"Jus_neg_1"]],"Comments":["1;2"]}
{"id":1170,"text":"paper_summary\nThe paper presents an approach to supervised contrastive learning for NLU - classification task by anchoring CL losses using label embeddings. The overall CL loss comprises of three losses. First, the (Multihead) Instance-centered contrastive loss that aims to bring the instance embedding closer to the label embedding by anchoring on the instance and its label as positive example and instance-other labels as negative examples. Second, the label centered contrastive loss which uses labels to mine positive and negative labels from a batch. And finally, to promote better uniformity across learned embeddings - label embeddings are also regularized.  The authors conduct experiments on standard benchmark NLP tasks showing that their approach produces better\/competitive results to baselines. They also show the effectiveness of their approach in Few Shot and Data imbalance settings.  Ablation study provides the importance of each of the component \/ architectural choices. \nsummary_of_strengths\n1. Well written, well reasoned, easy to follow how model \/ architecture \/ experiments are setup up. \n2. Extensive experimentation \/ ablation with statistical significant testing. \n3. Effective use of label semantics in Supervised Contrastive Learning. \n4. The work seems well researched given their knowledge of CL field which is showcased not only in related work section but also the mention of alignment\/uniformity, preventing degradation in CL models, regularizing label embeddings etc. From a CL modeling perspective - the authors have crossed all the \"t\"s dotted all the \"i\"s. \n5. Results show improvements \/ competitiveness across standard benchmarks and their technique is easy to implement \/ replicate. \nsummary_of_weaknesses\n1. While the paper details a lot of \"How\"s - the \"Why\"s are missing from their design choices. E.g. why do they think the label semantics arent explored enough (line 70-72), what motivated the use of multi-head instance CL loss (135-137), why the need for a 3 layer projection head g (line 187-194).  Research papers should address the why if they wish the community to learn and extend their lessons.\n2. The author mention uniformity \/ alignment which is win in and off itself but they do not present any results based on these metrics (e.g. in https:\/\/arxiv.org\/pdf\/2104.08821.pdf)  3. Why do the authors use regularization for label embeddings but not for instance embeddings? Given that the goal is improving uniformity - they should have at least considered using (spread out) regularization for instance embeddings too (ref: eq 6 in https:\/\/arxiv.org\/pdf\/1708.06320.pdf) \ncomments,_suggestions_and_typos\n1. The claim they make in lines 225-228 are partially true. The equation aids in aligning instance with label embeddings but it does not contain any elements that encourages different instances to disperse. Such dispersion is only encouraged by considering hinge losses or using spread out regularization (e.g. by equation 6 in this paper). As such the claim of uniformity should be removed from lines 225-228.  2. Line 172 should read \"The input of LaCon contains two parts consisting of the text and all the labels for the task) ","rid":"4db1f42565b77cdf62fa416f06b563a7393032a712012b7bad26e3d05a4202b7e06673ed593490a7e25db57e7ae782ede2dd1d62f45785e1c3a77bdb2e5691f6","scores":{"overall":"3 = Good: This paper makes a reasonable contribution, and might be of interest for some (broad or narrow) sub-communities, possibly with minor revisions.","best_paper":"No","datasets":"1 = No usable datasets submitted.","software":"1 = No usable software released."},"label":[[1017,1114,"Eval_pos_1"],[1118,1193,"Eval_pos_2"],[1197,1266,"Eval_pos_3"],[1270,1504,"Eval_pos_4"],[1503,1597,"Jus_pos_4"],[1600,1726,"Eval_pos_5"],[1752,1844,"Eval_neg_1"],[1844,2150,"Jus_neg_1"],[2154,2285,"Eval_neg_2"],[2337,2428,"Eval_neg_3"],[2429,2573,"Jus_neg_3"],[2662,2718,"Eval_neg_4"],[2719,3190,"Jus_neg_4"]],"Comments":["3;2"]}
{"id":1171,"text":"paper_summary\nThis paper proposes to learn sentence embeddings with psudo tokens, aiming at addressing the sentence length bias issue of existing approaches that learn sentence embeddings based on contrastive loss. \nsummary_of_strengths\nThe idea of mapping the sentence onto a fixed number of pseudo tokens and then maps it back is very interesting. \nExperiments on STS task shows the proposed approach achieves good improvement over existing sentence embedding models. \nsummary_of_weaknesses\nRegarding sentence length feature:   -- The paper argues that the approach in SimCSE relies on sentence length features.  I don't agree with the argument since from the modelling point of view, there is no explicit inductive bias such that the SimCSE would prefer sentences that share the same number of tokens.     -- The authors stated that \"Through careful observation we find that all positive examples have the same length ..\".   I am not surprised since positive example pair in SimCSE is constructed by applying different dropout masks twice to the feedforward layer of the Transformer models, while the input to the Transformer model remains untouched.       -- I think sentence length side-effect is an issue of engineering\/implementation, and can also be addressed from engineering side.  For example, one could implement a sampling procedure where all examples in the same batch have similar number of tokens.   The paper argues that the SimCSE model also affected by \"syntactic structures\".  It would be great to have some examples or more descriptions on what does \"syntactic structures\" referring to.\nRegarding the proposed model:   -- If I understand correctly, Y_i in eq 1 is the sentence embedding induced by BERT model.  In such case, the attention in EQ1 would always output a weight matrix of shape [m, 1], i.e., a vector of all 1s.  Here m denotes the number of pseudo tokens. This is because there is only one key in the dictionary, i.e., the sentence embedding.  Thus all attention weights are put on that key.\nRegarding the experiments.  Although the proposed approach achieves good improvement, it is unclear whether the improvement is significantly better than the baseline systems. \ncomments,_suggestions_and_typos\nIs the proposed improvement also additive? E.g., if all systems are ALSO fine-tuned on supervised data such as NLI, does the improvement over the baseline system still hold.\nThis paper created a subset by filtering based on length of the positive\/negative example pairs. How big is this new subset? ","rid":"0d9b3952c1795a766a9ae820b322653dd683f4d0f5193a534d5fbd783821da175d66ae51dce6bcf5a111835f10c03338d24668f0c4132f25aef79b2b8f5f862d","scores":{"overall":"2 = Borderline: This paper has some merit, but also significant flaws. It does not warrant publication at top-tier venues, but might still be a good pick for workshops.","best_paper":"No","replicability":"3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and\/or the training\/evaluation data are not widely available.","datasets":"1 = No usable datasets submitted.","software":"1 = No usable software released."},"label":[[237,350,"Eval_pos_1"],[351,470,"Eval_pos_2"],[614,646,"Eval_neg_1"],[647,804,"Jus_neg_1"],[2055,2202,"Eval_neg_3"]],"Comments":["2;2"]}
{"id":1172,"text":"paper_summary\nSee the prior review for a summary. Based upon the author response I do raise my score slightly from 2.5 to 3.0 to reflect that the definitions referenced in the author response might be sufficient for a target audience that is intimately familiar with WSD. On the other hand, it remains open as to what the impact of the proposed approach would be on any of the noted downstream applications, or beyond English. While WSD can be considered part of the traditional NLP preprocessing pipeline, it's impact on modern end-to-end solution is likely small. Nevertheless there might be high-impact cases such as token-based retrieval (which is used widely), and investigating the impact of the proposed approach on such applications might provide a convincing data point that can provide evidence for the impact of the proposed work. \nsummary_of_strengths\nSee the prior review. \nsummary_of_weaknesses\nSee the prior review. \ncomments,_suggestions_and_typos\nSee the prior review. ","rid":"f4fd93843b665513df9be69b38341a6407312cea1e77fad05cc54c07c73031dab1459620984100d59aa8d1b90d9712d1ab1de2efdc7887b20ff3f33e13bff393","scores":{"overall":"3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.","best_paper":"No","replicability":"4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.","datasets":"3 = Potentially useful: Someone might find the new datasets useful for their work.","software":"4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work."},"label":[],"Comments":["1;1 nothing to annotate bc about previous authors review"]}
{"id":1173,"text":"paper_summary\nThe authors construct a corpus on suicide events. The token level annotations and event types are invaluable. This corpus enable quantification of the performance of the state-of-the-art modeling approaches. The performance scores show that we still need to work a lot on this task. \nsummary_of_strengths\nThe inter-annotator agreement and the use of Reddit data, which is user-generated text, make this work valuable. The token level annotations and event types are invaluable. \nsummary_of_weaknesses\nThe paper should contain discussion of some design decisions: 1- How did the documents are filtered? Is it a random sample or keyword based filtering? What did you do for the documents that do not contain any suicide related information? \n2- \"with more than 50 words are kept to increase\" -> What do the excluded documents contain? What is the ratio of the posts excluded in respect of this decision? \n3- \"Finally, we further fine tune the pre-trained BERT model over unlabeled Reddit posts (i.e., about 40K posts) using masked language modeling (Devlin et al., 2019).\" - > how did you select the 40k docs? The document selection matters: Caselli, T., Mutlu, O., Basile, A., & Hürriyetoğlu, A. (2021, August). PROTEST-ER: Retraining BERT for Protest Event Extraction. ... \ncomments,_suggestions_and_typos\n1- The ratio of RF-Other is too high (40%) in the corpus in comparison to other event types. I think this requires some elaboration. \n2- The text mention 2300 docs annotated. However, table 1 row #documents contains more documents 2214+130+109 3- How is the disagreements were resolved for the 20% that was co-annotated. \n4- The reddit corpus could be compared to other event detection datasets that utilize user-generated text. Comparing this corpus to MAVEN and CycSecED is not fair. \n5- Do you use sentences or whole posts as the unit to be annotated?\nEnglish language: \"pubic posts\" -> public posts \"a ED model must\" -> \"an ED model must\" ","rid":"42e1a1ffb6697cfc6e56e3907ec8da5ecbdec3559d597919138cf9de7718f899b458941998c44d697da2e3ed8a114f66da1bc6ced98d98d90b8c92f6a51721b4","scores":{"overall":"2.5 ","best_paper":"No","replicability":"3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and\/or the training\/evaluation data are not widely available.","datasets":"4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.","software":"2 = Documentary: The new software will be useful to study or replicate the reported research, although for other purposes it may have limited interest or limited usability. (Still a positive rating)"},"label":[[319,431,"Eval_pos_1"],[432,491,"Eval_pos_1"],[515,576,"Eval_neg_1"],[577,1287,"Jus_neg_1"],[1323,1412,"Jus_neg_2"],[1412,1453,"Eval_neg_2"]],"Comments":["2;2"]}
{"id":1174,"text":"paper_summary\nThe paper describes a new approach towards MeSH label prediction, utilizing the title abstract journal relative information. The proposed model combines BiLSTMs, Dilated CNNs and GCNNs to extract features from abstracts, titles and the mesh term hierarchy respectively. Limiting the search MeSH space with information extraction from metadata (such as other articles published in that journal) allows for a boost in performance by building dynamic attention masks. The final model shows good performance compared to related approaches, one of which uses the full article. \nsummary_of_strengths\n- Utilized information past the document itself to limit the MeSH search space -Introduces novel end-to-end architecture that can be used in other tasks involving scholarly articles -Achieves good performance compared to related approaches. \nsummary_of_weaknesses\n- Threshold is said to have a very big impact but is not discussed in detail with different ablations. How does threshold affect computational complexity (outside of performance)?  -Some of the design choices are not explained well (e.g. why IDF-weighting) -Training time (epochs) and computational complexity of the kNN and GCNN component is not discussed. \ncomments,_suggestions_and_typos\n- Equations 10 & 11 should be H_{abstract} instead of D_{abstract}? If not, when is H_{abstract} used?  -There is a significant drop in performance for MeSH terms when metadata are not available, leading to a worse performance than other methods (Ablations-d). In case of new journals or preprints, is this the expected performance?  -With the tuned threshold, how many MeSH terms are not selected during the dynamic masking on average in the different data splits? What is the hierarchical level of these terms?  -A few minor typos, proof reading should fix them. Nothing major. ","rid":"aa75cb5d1fcdad14f947645ab06b4db48b3cd30fa91947d3667b8a7637b43b2b1070fc11e4e9b623604de1ddb6198bc41812aa98ea7a58e4521a239374596edf","scores":{"overall":"3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.","best_paper":"No","replicability":"3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and\/or the training\/evaluation data are not widely available.","datasets":"1 = No usable datasets submitted.","software":"1 = No usable software released."},"label":[[688,789,"Eval_pos_2"],[791,849,"Eval_pos_1"],[874,1025,"Eval_neg_1"],[1054,1102,"Eval_neg_2"],[1104,1128,"Jus_neg_2"],[1130,1230,"Eval_neg_3"]],"Comments":["2;2"]}
{"id":1175,"text":"paper_summary\nThis paper discusses methods to improve MT for Interactive machine translation but not in a standard sense. In IMT, usually MT predicts the next set of tokens given the prefix that has been post-edited by translators. In this paper, the authors tackle another modified use case where translators are first provided a full translation and then they are asked to post-edit portions of the translated text. Usually constrained decoding is used for such a task where model has to regenerate the beam according to the post-edited content.  Overall, I liked reading the paper because everything was stitched and motivated well throughout the paper. It does look like the paper was proof-read multiple times because I did not really find any grammatical errors. Having said that, I do think there's scope of improvement in the paper which I laid down in Weakness section. \nsummary_of_strengths\nThe task of IMT is tough and authors did a good job at providing full context in the problem and how they tackle a modified IMT problem.  Latency is definitely a major component in adoption of this technology. I liked the way this paper outlined the metrics and provided motivation for it.  The approach is quite simple and very effective in terms of quality and latency. \nsummary_of_weaknesses\nUsage of data and experiment design: WMT data contain human generated references, any reason why post-edited content was not used in the paper for experiments? Post-edited content would have been much closer to what your task is and there's plenty of available data out there which is post-edited.   This task of sequence refinement has been studied well in  1. Relation to Neural Automatic Post editing models  2. Sequence Refinement part of Levenshtein Transformer (LevT) by Gu. et. al. 2019 is not discussed when it is almost the same thing although the use case is different (for Automatic Post Editing). \n3. Why was there no discussion on non-autoregressive models? Text infilling approach could indeed solved with a non-autoregressive model like LevT. What are the other methods of text infilling, why aren't those compared with your cross-lingual text-infilling approach?\nI am left with some other questions after reading the paper, I am writing them as weakness here but they are essentially points where this paper can improve.  L246: \"Then we combine the sampled data D and the trivial data D as the augmented data to train the model P in our experiments.\" what is the ratio of this combination, is it 1:1, if so are you not giving more weight text infilling problem than translation? An ablation study on this combination would have been good.  In the real-world scenario, given that the sample size is low (200), is 65.79 (BiTIIMT) significantly better than 64.05 (LCD)? Same question for edit distance cost? \ncomments,_suggestions_and_typos\nL273: \"In other words, we employ the standard beam search algorithm to yield Yb, which leads to a valid Y, without explicitly imposing the constraint during decoding\" Is this always the case in all languages? If not, can you mention the accuracy here? ","rid":"6e168e3515d5d2db9160c984c911780d69469fb193a879388c2c371182178f2cde272e0130f98ab0ed0996afd5484938b0bfd620a0ad5e5988e59fe539786640","scores":{"overall":"3.5 ","best_paper":"No","replicability":"5 = They could easily reproduce the results.","datasets":"1 = No usable datasets submitted.","software":"1 = No usable software released."},"label":[[548,656,"Major_claim"],[901,1038,"Eval_pos_1"],[1110,1191,"Eval_pos_2"],[1192,1273,"Eval_pos_3"]],"Comments":["2;3"]}
{"id":1176,"text":"paper_summary\nThis paper addresses and focuses on disambiguation resolution on task-oriented dialog systems. The authors augment the seminal databases (MultiWOZ and SGD) with synthetic disambiguation turns, and use it as a new dataset to make their dialog system understand the user's answer and do follow up clarification questions. \nsummary_of_strengths\nOverall this paper is complete and very well written. The ablation studies are convincing, and the results are promising. Furthermore the analysis presented in the paper is concise and complete as well. This paper has a potential to inspire other dialog researchers to work on enhancing universal dialog skills. \nsummary_of_weaknesses\nIt might be difficult for readers to understand the explanation in Sec 3.1. and 3.2., rephrasing the paragraph along with its pseudo-code can add clarity. \ncomments,_suggestions_and_typos\nIt would be really helpful for the dialog community if authors open their experiment script, especially their script to do automatic augmentation (Sec. 3.2.) ","rid":"8f644ad52cbea4b490c7523c970fb6c480f03b29b34a8685776a96b39e564b9cd475ec4883a236cea033b23df9795d8b15347a384842e3a0a6a261776dff0665","scores":{"overall":"3.5 ","best_paper":"No","datasets":"5 = Enabling: The newly released datasets should affect other people's choice of research or development projects to undertake.","software":"1 = No usable software released."},"label":[[356,409,"Eval_pos_1"],[410,477,"Eval_pos_2"],[478,558,"Eval_pos_3"],[559,668,"Eval_pos_4"],[691,776,"Eval_neg_1"]],"Comments":["1;1"]}
{"id":1177,"text":"paper_summary\nThe performance of structured prediction models can be greatly improved by scaling to larger state spaces, yet the inference complexity of these models scales poorly w.r.t. the size of the state space. The goal of this work is to reduce the inference complexity of structured models by factorizing the clique potentials using low-rank tensor decompositions and performing message passing in an induced rank space instead of the original state space.\nThis work makes three contributions: 1. Using the language of factor graph grammars, this work unifies previous low-rank tensor decomposition works such as Yang et al 2021b  and Chiu et al 2021. This work shows that those works are essentially performing message passing on a factor graph with two types of nodes: the original state nodes and auxiliary rank nodes induced by the low-rank tensor decomposition. \n2. On a sub-family of factor graph grammars which subsume most commonly-used structured prediction models such as HMMs, HSMMs, and PCFGs, this work proposes to marginalize the state nodes first and only perform inference in the induced rank nodes, which reduces the complexity by replacing a factor of the state size by a factor of the rank size which is usually smaller. \n3. Empirically this work scales HMMs and PCFGs to very large state spaces and achieves strong performance. \nsummary_of_strengths\n1. This work is insightful in pointing out that by performing message passing only in the rank space after marginalizing the original state nodes (which is a one-time cost), a factor of the number of states in the total complexity can be replaced by a factor of the rank size. This idea is generally applicable to a large family of factor graph grammars that have one external node per hypergraph fragment, and it might enable scaling many structured prediction models. \n2. This work gets strong empirical performance by scaling to very large state spaces when compared to previous structured prediction works. In particular, this work trains the largest-ever PCFG in the task of unsupervised parsing on PTB (to my knowledge) and establishes a new state-of-the-art performance in this particular task. \n3. This work confirms findings of previous works such as Chiu and Rush 2020 that scaling structured prediction models can improve performance. For example, Figure 6 (b) suggests that scaling PCFGs to beyond 10k pre-terminals might further improve modeling performance. \nsummary_of_weaknesses\nBy showing that there is an equivalent graph in the rank space on which message passing is equivalent to message passing in the original joint state and rank space, this work exposes the fact that these large structured prediction models with fully decomposable clique potentials (Chiu et al 2021 being an exception) are equivalent to a smaller structured prediction model (albeit with over-parameterized clique potentials).  For example, looking at Figure 5 (c), the original HMM is equivalent to a smaller MRF with state size being the rank size (which is the reason why inference complexity does not depend on the original number of states at all after calculating the equivalent transition and emission matrices). One naturally wonders why not simply train a smaller HMM, and where does the performance gain of this paper come from in Table 3.\nAs another example, looking at Figure 4 (a), the original PCFG is equivalent to a smaller PCFG (with fully decomposable potentials) with state size being the rank size. This smaller PCFG is over-parameterized though, e.g., its potential $H\\in \\mathcal{R}^{r \\times r}$ is parameterized as $V U^T$ where $U,V\\in \\mathcal{R}^{r \\times m}$ and $r < m$, instead of directly being parameterized as a learned matrix of $\\mathcal{R}^{r \\times r}$. That being said, I don't consider this a problem introduced by this paper since this should be a problem of many previous works as well, and it seems an intriguing question why large state spaces help despite the existence of these equivalent small models. Is it similar to why overparameterizing in neural models help? Is there an equivalent form of the lottery ticket hypothesis here? \ncomments,_suggestions_and_typos\nIn regard to weakness #1, I think this work would be strengthened by adding the following baselines: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared. \n2. For each HMM with rank r, add a baseline smaller HMM with state size being r. ","rid":"fa60f1a1f132578809b55d458098cf7c2899fc3d35febeb6386f1aa13aeebf1576a62ad3c3d55342cf4bf1db6f8e0ce24aaa678fdc064d0586ae23e1e6edd720","scores":{"overall":"5 = Top-Notch: This is one of the best papers I read recently, of great interest for the (broad or narrow) sub-communities that might build on it.","best_paper":"Maybe","datasets":"1 = No usable datasets submitted.","software":"3 = Potentially useful: Someone might find the new software useful for their work."},"label":[[1380,1403,"Eval_pos_1"],[1404,1653,"Jus_pos_1"],[1851,1987,"Eval_pos_2"],[1988,2179,"Jus_pos_2"],[4207,4281,"Eval_neg_1"],[4282,4707,"Jus_neg_1"]],"Comments":["3;2"]}
{"id":1178,"text":"paper_summary\nThis paper gives a comprehensive analysis of the Square One Bias in NLP. Through the statistics of ACL conference papers in recent years, the authors find that the current researchers are encouraged to go beyond the prototype experiment (optimizing accuracy\/F1 on an English dataset) in only a single dimension. The problems resulting from the Square One Bias, the recommendations to overcome this, the examples   (one-dimensional research) and counter-examples (multi-dimensional research) are also discussed in the paper. \nsummary_of_strengths\n1. The paper is well-written and gives a systematic discussion on the Square One Bias in NLP. \n2. I found the examples in the paper to demonstrate the Square One Biases (including architecture biases, annotation biases, selection biases, protocol biases, and organizational biases) pretty precise and helpful. \n3. From the perspective of this paper, one can easily notice unexplored areas of the research manifold, thus I believe this paper is helpful to the community to become more diverse. Especially, this paper would be inspiring for researchers who are interested in non-standard circumstances (e.g. non-English, non-standard architectures, non-accuracy\/F1 metrics, etc.). \nsummary_of_weaknesses\nAlthough this paper proposed many ideal recommendations for researchers and conference organizers to mitigate the Square One Biase issue, I might doubt the value in practice. As discussed in Section 6, doing multi-dimensional research would not only increase the workload of paper authors but also requires conference reviewers to be experts in multiple areas. As a result, the development of the field may be slowed down by the cumbersome evaluation. And besides, if we develop some standard multi-dimensional evaluation protocol, it may also introduce a new Square One Bias (e.g. some particular evaluation methods of fairness and efficiency can become dominant).  But overall, I agree that research that departs from prototypes in multiple dimensions should be encouraged by the reviewers and conference organizers. \ncomments,_suggestions_and_typos\nThe following papers, which consider the multi-dimensional evaluation in NLP, should be closely related to this work: 1. Dynabench: Rethinking Benchmarking in NLP (https:\/\/arxiv.org\/abs\/2104.14337) 2. Towards Efficient NLP: A Standard Evaluation and A Strong Baseline (https:\/\/arxiv.org\/abs\/2110.07038) It would be better to include and discuss them. ","rid":"da3c75eabf392377ac87f8a824fc74a96de1a193ae8b5b549decdf44cc11150c2340a91584f025dc2b71f02707dae76bb9a0192b85eeda5b980d5ac7271f74e4","scores":{"overall":"3.5 ","best_paper":"No","replicability":"4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.","datasets":"1 = No usable datasets submitted.","software":"1 = No usable software released."},"label":[[563,654,"Eval_pos_1"],[658,728,"Eval_pos_2"],[729,841,"Jus_pos_2"],[842,870,"Eval_pos_2"],[874,1052,"Eval_pos_3"],[1053,1239,"Jus_pos_3"],[1262,1435,"Eval_neg_1"],[1436,1837,"Jus_neg_1"]],"Comments":["2;2"]}
{"id":1179,"text":"paper_summary\nThis paper aims to train a fully unsupervised pretrained retriever for zero-shot text retrieval. Specifi- cally, this paper proposes a novel iterative contrastive learning to pretrain dual-tower dense retriever, then use lexical matching method to further enhance the pretrained dense retrieval model. Results on BEIR benchmark show that the proposed method achieves SOTA performance compared with supervised dense retrieval model. \nsummary_of_strengths\n1. The proposed unsupervised dense retrieval model achieves remarkable performance com- pared with supervised dense retrieval models on BEIR benchmark. \n2. Theproposedmodelisinitializedfrom6-layerDistilBERTandtheparametersofdualencoders are tied, so the proposed retrieval model is quite efficient. \nsummary_of_weaknesses\n1. The selling point of this paper is unsupervised pretrained dense retriever(LaPraDoR) can per- form on par with supervised dense retriever, but actually, LaPraDoR is a hybrid retriever rather than a pure dense retriever. In a way, it’s unfair to compare hybrid method to dense\/sparse method as shown in table 1, because it’s known that the dense retriever and sparse retriever are complementary. The comparable supervised models should also be hybrid retrievers. Besides, in table 3, it seems that without lexicon enhancement, the performance of proposed unsupervised model is not competitive either on in-domain MS-MARCO or cross domain BEIR benchmark compared with supervised model. \n2. In table 4, the combination of self-supervised tasks ICT and DaPI doesn’t seem to be com- plementary, the effectiveness of DaPI task, which will double the GPU memory usage, is not significant (0.434 -> 0.438) 3. ICoL is proposed to mitigate the insufficient memory on a single GPU and allow more neg- ative instances for better performance, but there are no corresponding experiments to show the influence of the number of negatives. As far as I know, the quality of negatives is more important than the quantity of negatives as shown in TAS-B. 4. It sounds unreasonable that increasing the model size can hurt the performance, as recent paper Ni et al. shows that the scaling law is also apply to dense retrieval model, so the preliminary experimental results on Wikipedia about model size should be provided in detail. \n5. Thepaperarguethattheproposedapproachistocomplementlexicalmatchingwithsemantic matching, while the training procedure of proposed model is totally independent with lexical matching. Therefore, the argument ”LEDR helps filter out such noise and allows the dense retriever to focus on fine-grained semantic matching” is confusing, because there is no suc- cession relationship between LEDR and dense retriever.\nReference: * Ni et al. 2021. https:\/\/arxiv.org\/abs\/2112.07899 \ncomments,_suggestions_and_typos\nthe proposed LaPraDoR achieves relative low performance on MS-MARCO while relative high per- formance on BEIR, the inductive bias of the proposed pretrain method is worth exploring. \nLine 300-304: q and d are confusing ","rid":"86ca1ba300de668d673f124abbe56095cf9c0bb9f5fe37005ee85b7120fca15216a1db9312acfd5ca37b43e218d77597258c1953c01adb538ea8be7c59ab7aac","scores":{"overall":"2.5 ","best_paper":"No","replicability":"2 = They would be hard pressed to reproduce the results: The contribution depends on data that are simply not available outside the author's institution or consortium and\/or not enough details are provided.","datasets":"1 = No usable datasets submitted.","software":"1 = No usable software released."},"label":[[470,620,"Eval_pos_1"],[624,714,"Jus_pos_2"],[715,767,"Eval_pos_2"],[1013,1103,"Eval_neg_1"],[1104,1254,"Jus_neg_1"],[1481,1581,"Eval_neg_2"],[1583,1690,"Jus_neg_2"],[2030,2108,"Eval_neg_3"],[2109,2303,"Jus_neg_3"],[2487,2635,"Eval_neg_4"],[2635,2714,"Jus_neg_4"]],"Comments":["3;2"]}
{"id":1180,"text":"paper_summary\nIn this work, the authors proposed a unified model of task-oriented dialogue understanding and response generation. The two major enhancements are adopting task-oriented dialogue pre-training on a data collection, and introducing the prompt-based learning for the multi-task capability via one model. From the experimental results, the pre-training strategy proved useful to improve the performance on the benchmark MultiWOZ. \nsummary_of_strengths\nWhile the idea of task-specific pre-training is not new, it is still interesting, and the proposed method proved effective in leveraging the language backbone T5, and can be potentially applied to other models and tasks. \nsummary_of_weaknesses\n1. There are some other contemporary state-of-the-art models, the authors can consider citing and including them for an extensive comparison. \n2. It will be good to see some analysis and insights on different combinations of pre-training datasets introduced in Table 1. \ncomments,_suggestions_and_typos\nHere are some questions: 1. Since some of the sub-tasks, like dialogue state tracking, require a fixed format of the output, if the model generation is incomplete or in an incorrect format, how can we tackle this issue? \n2. The dialogue multi-task pre-training introduced in this work is quite different from the original language modeling (LM) pre-training scheme of backbones like T5. Thus I was curious about why not pre-train the language backbone on the dialogue samples first with the LM scheme, then conduct the multi-task pre-training? Will this bring some further improvement? \n3. It will be good to see some results and analysis on the lengthy dialogue samples. For instance, will the performance drop on the lengthy dialogues? ","rid":"a0821b6cbd2b3bfbddc644f83e27e1bac50ac0153c23386ca20f5725418f812cc73f2220474452bd2b77d97410b6c4814451212581aec2b8fd71c84521db7a70","scores":{"overall":"3.5 ","best_paper":"No","replicability":"4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.","datasets":"3 = Potentially useful: Someone might find the new datasets useful for their work.","software":"4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work."},"label":[[462,683,"Eval_pos_1"],[708,848,"Eval_neg_2"],[851,975,"Eval_neg_1"]],"Comments":["2;2"]}
{"id":1181,"text":"paper_summary\nThe paper studies the benefits of introducing a Bayesian perspective to abstractive summarization. The authors run the MC dropout method on two pre-trained summarization models, sampling different summarization texts according to specific dropout filters. They use BLEUVarN as a metric of uncertainty for a possible summarization, showing the variations across the summary samples. The authors conduct experiments on three datasets on the correlation between the uncertainty and summarization performances, and show that the performance of the summarization can slightly improve by selecting the \"median\" summary across the pool of sampled ones. \nsummary_of_strengths\n- To the extent of my knowledge, it is the first work that study model uncertainty (in the particular form of the variability of generated summaries) in abstractive summarization.  - The paper provides an analyses on three collections, showing the (cor)relations between the metric of summarization uncertainty (or in fact summarization variability) and ROUGH. They observe that in general the higher the uncertainty score of a summary, the lower its ROUGH score.\n- The work shows that the performance of summarization can be slightly improved by selecting the summary that lays in the \"centroid\" of the pool of generated summaries. \nsummary_of_weaknesses\nMy main concerns are the lack of novelty and proper comparison with a previous study.\n- As correctly mentioned in the paper, the work of Xu et al. is not based on MC dropout. However, that work still provide a metric of uncertainty over a generated summary. In fact, the metric of Xu et al. (namely the entropy of the generation distributions) comes with no or little extra computational costs, while the MC dropout of 10 or 20 introduces considerably large feedforward overheads. I believe the method of Xu et al. can be compared against in the experiments of 5.1. This can let the reader know whether the extra cost of MC dropout method comes with considerable benefits.\n- There is no specific novelty in the method. The observation regarding the correlation between uncertainty and performance is in fact an expected one, and has already observed in several previous studies (also in the context of language generation), like: Not All Relevance Scores are Equal: Efficient Uncertainty and Calibration Modeling for Deep Retrieval Models Daniel Cohen, Bhaskar Mitra, Oleg Lesota, Navid Rekabsaz, Carsten Eickhoff In proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2021)- - The reported improvement is marginal, while achieved with the large overhead of MC sampling. My guess is that the improvement is only due to the effect of ensembling, inherent in MC droupout. \ncomments,_suggestions_and_typos\nAs mentioned above: - I believe the method of Xu et al. can be compared against in the experiments of 5.1. This can let the reader know whether the extra cost of MC dropout method comes with considerable benefits.\n- More evidences regarding the performance improvement, showing that it is not only due to the effect of ensembling.\n- Studying more efficient and recent Bayesian approaches, such as: Agustinus Kristiadi, Matthias Hein, and Philipp Hennig. 2020. Being Bayesian, Even Just a Bit, Fixes Overconfidence in ReLU Networks. In Proceedings of the 37th International Conference on Machine Learning (Proceedings of Machine Learning Research, Vol. 119), Hal Daumé III and Aarti Singh (Eds.). PMLR ","rid":"e226035b46a2b2286c61a62ba7d567c1de36ee507bc11d502316f5b531df65379be5e519183e6243575f237e0dcdedc1d0e8853ca7383aadc9f14c1af20e9d18","scores":{"overall":"2.5 ","best_paper":"No","replicability":"4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.","datasets":"1 = No usable datasets submitted.","software":"1 = No usable software released."},"label":[[684,764,"Eval_pos_1"],[833,862,"Eval_pos_1"],[1338,1423,"Eval_neg_1"],[1426,2775,"Jus_neg_1"]],"Comments":["2;2"]}
{"id":1182,"text":"paper_summary\nThis work has created a benchmark dataset for multi-task learning for biomedical datasets. Based on this new benchmark dataset, this work has proposed instruction learning based multi-task learning, which has shown to outperform single-task learning as well as vallina multi-task learning. \nsummary_of_strengths\n1. This work has newly aggregated more than 20 biomedical datasets in 9 categories into a new multi-task paradigm and formalize them into a text to text format so that we can build one unified model for all different tasks. \n2. This work has proposed using manually created instructions for multi-task learning so that the model can be instructed to perform each task without confusion. And this method has been shown to outperform a lot the vanilla multi-task learning and also outperform single-task learning in some cases. \nsummary_of_weaknesses\n1. In the proposed method, the BI would be concatenated with instances as the input to the BART model, and in the BI, examples are provided. Actually these examples are extracted from those instances, then why should we still have examples in BI? How about just having those instructions in the BI? \n2. One important baseline is missing: in those methods proposed for DecaNLP and UnifiedQA, etc., other types of tokens or phrases are used to indicate which task\/dataset each input instance belongs to, which is very important to let the model know what the input instance it is. However, in the baseline of vanilla multi-task learning (V-BB), no such kinds of special tokens are used at all, which forms a very unfair baseline to be compared with. The model are fed by so many instances from various kinds of tasks without any differentiation, which for sure would lead to deteriorate performance. For this reason, the effectiveness or the necessity of BI is questionable. \n3. More deep analysis over the impacts of different kinds of designs of the BI is needed, since such designs can vary a lot among different designers or writers. If so, the performance would be very unstable due to the variance of BI, which makes this type of method not applicable to real-world problems. \n4. Only Rouge-L is used for evaluation, which makes the evaluation not that reliable. Especially for some classification tasks, Rouge-L is not sensitive enough. \ncomments,_suggestions_and_typos\n1. In lines 382-384, it is mentioned that \"We have discarded long samples (>1024 token length) from validation and testing data as well.\". I think it is not appropriate to throw any examples from the test set. ","rid":"217a8d9fbdc29525938d0d7617a33310132b1cdea2c27638c8dac3ce9630355e12e631a78648efd9aeef02bef0a31d93dcacfc368754740b0522e182ed8caaff","scores":{"overall":"2.5 ","best_paper":"No","datasets":"4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.","software":"4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work."},"label":[[1178,1211,"Eval_neg_1"],[1212,1848,"Jus_neg_1"],[1852,1939,"Eval_neg_2"],[1939,2155,"Jus_neg_2"],[2159,2241,"Eval_neg_3"],[2241,2317,"Jus_neg_3"],[2488,2560,"Eval_neg_4"]],"Comments":["3;2"]}
{"id":1183,"text":"paper_summary\nThis paper presents a technique for numerical reasoning over tables by leveraging formulas based on the pre-trained model TUTA specifically designed for tables.  FORTAP, a FORmula-driven TAble pre-training model, is proposed in this paper, which incorporates numerical reasoning capabilities based on large-scale spreadsheet formulas.\nThe main contributions are two complementary\/auxiliary objective functions:   * Numerical Reference Prediction,   * Numerical Calculation Prediction.  They take relationships between cells into consideration and predict all operators using cell embeddings.  Experimental results show their method achieves better performance over several datasets when compared to both baseline and SOTA models. \nsummary_of_strengths\n- Show the potential of leveraging formulas in table pretraining.\n-Good results based on comprehensive experimental settings. Apart from that, the authors provide very detailed explanations and discussions.  -Very clear structure and good organization. \nsummary_of_weaknesses\n- Baselines chosen in this paper are not strong enough to show the performance brought by the introduced objectives. For example, under the TableQA settings, another model employing SQL named TaPEx (also mentioned in this paper) can be used to compare with the proposed model.\n-To further evaluate the effectiveness of the proposed method, experiments on other tasks (e.g. cell type classification, table type classification) compared to TUTA should be conducted. \ncomments,_suggestions_and_typos\n- More clarification and novelty is needed for contributions in Sec 1.\n-As mentioned in Sec 5, there is another model named TaPEx, which is also designed to improve reasoning skills in table pre-training. Why is there no comparison with TaPEx in the experiment section?\n-TUTA employs several datasets to evaluate the model performance on CTC (cell type classification), why is there only one dataset used in this paper?\n-Is there any deficiency\/limitation of introducing these objectives? Will it cause a performance drop on other tasks such as Table Type Classification and CTC? ","rid":"06b82c4720d419bb56d16f5272bfa123c4037299b66dd31b20e9cb8c4a7651de5fbe198e06b99a6276dc60d3f4189c857529134deb1b8b3651233850caacce54","scores":{"overall":"3.5 ","best_paper":"No","replicability":"4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.","datasets":"3 = Potentially useful: Someone might find the new datasets useful for their work.","software":"3 = Potentially useful: Someone might find the new software useful for their work."},"label":[[768,831,"Eval_pos_1"],[833,891,"Eval_pos_2"],[892,972,"Eval_pos_3"],[975,1019,"Eval_pos_4"],[1044,1158,"Eval_neg_1"],[1158,1318,"Jus_neg_1"],[1320,1380,"Jus_neg_2"],[1382,1506,"Eval_neg_2"],[1541,1609,"Eval_neg_3"]],"Comments":["2;2"]}
{"id":1184,"text":"paper_summary\nThis paper propose prefix-based models for controllable text generation. Similar to [1], prefixes are token embeddings of language models (e.g., GPT-2) used for learning attribute-specific information and steering the generation of the fixed language models. The authors further add a contrastive loss to enhance the models' controllability. In addition, an unsupervised learning method is introduced to handle scenarios where labels are not available. The authors evaluated the proposed models on multiple controllable text generation tasks, such as controlling sentiment and topics. The experimental results show that comparing to baselines like PPLM and GeDi, the proposed model can achieve a good balance between fluency and controllability.\n[1] Prefix-tuning: Optimizing continuous prompts for generation. ACL 2021 \nsummary_of_strengths\n- The proposed lightweight model achieved strong performance in multiple controllable text generation tasks.\n-The idea of controlling language models in unsupervised way is interesting and new. \nsummary_of_weaknesses\n- Missing human evaluation for the proposed unsupervised learning method. The major technical contribution (novelty) of the paper is controlling language models in unsupervised manner. Unfortunately, human evaluation is absent (in table 4) to demonstrate its effectiveness.\n-For the multi-aspect controlling experiments, CTRL[1] and PPLM[2] should be good baselines.\n[1] CTRL: A conditional transformer language model for controllable generation.  [2] Plug and play language models: A simple approach to controlled text generation. ICLR 2020 \ncomments,_suggestions_and_typos\nPlease consider adding new human evaluation results and baselines as mentioned in weaknesses. ","rid":"1aa7fa614cb1a09ab64c839bf6b5dd108114982fa670d578ff519ad94713573af7a859e5884c8519f77fc5b714d66e4c035e91acc4eca2f5809bf761ab834e41","scores":{"overall":"2.5 ","best_paper":"No","replicability":"3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and\/or the training\/evaluation data are not widely available.","datasets":"1 = No usable datasets submitted.","software":"1 = No usable software released."},"label":[[858,964,"Eval_pos_1"],[966,1050,"Eval_pos_2"],[1075,1146,"Eval_neg_1"],[1147,1257,"Jus_neg_1"],[1258,1346,"Eval_neg_1"]],"Comments":["2;1"]}
{"id":1185,"text":"paper_summary\nThe paper provides a benchmark dataset that can be used for training & evaluation of automated fact checking systems. The major contribution of this paper is that they provide a large collection of 33,697 claims with associated review articles and premise articles. In the experiment, this work presents a two-stage detection framework, including evidence sentence extraction and claim veracity inference.  LSTM-based baselines and RoBERTa-based baselines are included and compared. \nsummary_of_strengths\n1. The idea of using premises articles for claim inference in automated fact checking is interesting. \n2. The paper is overall well-structured and the methods are explained clearly. \nsummary_of_weaknesses\n1. The methods are not novel as they are largely borrowing from existing work. \n2. It would be nice to have more detailed descriptions of the data collection process, e.g., label mapping, and data statistics, (how many articles per claims? how many sentences per articles? sentence length?) If not enough space on the main text, these information could be added on appendix. \n3. It would be better if the authors evaluate more state-of-the-art methods on this benchmark dataset. \n4. In section 3.3, the authors claim that the disadvantage of using web search is indirect data leak. Can we eliminate the data leak through filtering with publishing time. \ncomments,_suggestions_and_typos\n1. The prequential evaluation is well-written.  It would be interesting to see more such analysis and discussion of the datasets. \n2. Did you try the combination of TF-IDF and dense retrieval for evidence sentence extraction? \n3. As your dataset is imbalanced, it would be better to see some analysis of the outputs. ","rid":"3c49820f93e3e8370d520d51b07e26d10427c0d1c93f60ce29a95bbf4f3aac2ea859a90b3903d6fd36f0e22c8a909a6e90a5342f89577d14cf7222d669d58497","scores":{"overall":"3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.","best_paper":"No","replicability":"4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.","datasets":"4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.","software":"1 = No usable software released."},"label":[[522,621,"Eval_pos_1"],[625,701,"Eval_pos_2"],[727,803,"Eval_neg_1"],[806,890,"Eval_neg_2"],[1413,1456,"Eval_pos_3"]],"Comments":["2;2"]}
{"id":1186,"text":"paper_summary\n*Note: I reviewed this paper in an earlier ARR cycle. There are no changes in the updated version that warrant a change in my score or the review. I’ve updated a summary of weaknesses to reflect the updates, and have listed a few suggestions on grammar.*\nThis work presents a method (X-GEAR) for zero-shot, cross-lingual event argument extraction. X-GEAR takes as input i) a passage, ii) a trigger word (a predicate, e.g., \"killed\"), and iii) a template indicating the desired roles (e.g., <Victim>NONE<\/Victim>). The output is the template filled with event arguments extracted from the passage (e.g., NONE might be replaced with civilian). X-GEAR is built using the standard Seq2seq framework with a copy mechanism, where the input is composed of the triplet (passage, template, trigger word) flattened as a sequence, and the output is the template filled with desired roles.\nThe method relies on recent advances in large, multilingual pre-trained language models (PTLM) such as MT5, which have been shown to perform robust cross-lingual reasoning. The key insight of the method is to use language-agnostic special tokens (e.g., <Victim>) for the template. Fine-tuning on the source language helps learn meaningful representations for templates, which allows their approach to work across target languages supported by the PTLM. \nsummary_of_strengths\n- The paper presents a simple but intuitive method for solving an important problem. The simplicity of the proposed method is a significant strength of this work. As the authors note, existing systems that perform structured extraction often rely on a pipeline of sub-modules. X-GEAR replaces that with a simple Seq2seq framework which is considerably easier to use and maintain.\n- The proposed method is clearly defined, the experiments are thorough and show considerable gains over the baselines.\n- The analysis provides several insights into the strengths and weaknesses of the proposed approach. \nsummary_of_weaknesses\nThe authors have addressed some of the weaknesses highlighted in the previous review. However, it would be great if the weakness of the proposed approach is also highlighted in the future version. Specifically, the method is not *truly* zero-shot as it can only work in cases where a PLTM for the target languages is available. I believe that this is an important point and should be highlighted in conclusion or related work. \ncomments,_suggestions_and_typos\n- L100 “Zero-shot cross-lingual learning *is an” -L104: Various structured prediction tasks have *been studied, -The footnote markers should be placed after the punctuation mark (e.g., L557). ","rid":"a28caf18ebbeb3bf9ec2265042966205e211f4515c2cb1824b93e9c9dbf8394f50d932b5bc5f9d0696e8c08bf2df858f1d60c9ceef54e27fce09b344dca0b6b3","scores":{"overall":"4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.","best_paper":"No","replicability":"4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.","datasets":"1 = No usable datasets submitted.","software":"1 = No usable software released."},"label":[[1369,1451,"Eval_pos_1"],[1452,1529,"Jus_pos_1"],[1749,1788,"Eval_pos_2"],[1789,1865,"Eval_pos_4"],[1868,1967,"Eval_pos_3"],[2076,2186,"Eval_neg_1"],[2187,2417,"Jus_neg_1"]],"Comments":["2;2"]}
{"id":1187,"text":"In the present paper, the authors describe the results of a quantitative analysis of various genres in terms of coreference. They analyse a number of coreference-related features and compare the genres from the point of view of their distribution. The aim is to find the differences between spoken and written genres. The work is interesting and useful, as the number of studies of coreference in spoken texts is limited. The paper address a number of important issues in both coreference analysis (e.g. how should the distance be measured) and the analysis of spoken language. As the authors use a number of existing resources, they also assure the comparability of the categories used. Here, it would be interesting to know, if there were any problems, e.g. if there were still some incompatible categories that the authors came across.\nSpecific comments I like the discussion about the variation in distance measured by different means at the beginning of section 2. Specifically, in a cross-lingual task, token-based measure is a problem. However, there could be differences across studies using various metrics. If measured in sentences or clauses, the distance may vary depending on a genre, if there is a variation in sentence length in terms of words (in spoken texts, there could be shorter sentences, etc.). The question is, if the distance should be measured in characters, but I believe that the decision depends on the conceptual background on what one wants to find out.\nAnother point in Section 2 in the discussion of the diverging results could be variation within spoken and written texts various authors use in their analysis. There could be further dimensions that have an impact on the choice of referring expressions in a language, e.g. narrativeness, if there are dialogues or monologues, etc.\nConcerning related work, Kunz et al. (2017) point out that coreference devices (especially personal pronouns) and some features of coreference chains (chain length and number of chains) contribute to the distinction between written and spoken registers.\nThere are several works concerned with lexicogrammar suggesting that distinction between written vs. spoken, and also between formal vs. colloquial are weak in English  (Mair 2006: 183).\nTable 1: the statistics on different parts of OntoNotes and the total number in OntoNotes are given in one table in the same column formatting, which is slightly misleading. \n  4.1: large NP spans vs. shot NP spans – sometimes only heads of  nouns or full NPs are considered.\nReferences to examples: 1→ (1), etc.\nPersonal pronouns: 1st and 2nd person pronouns are not considered in the analysis of coreference in some frameworks. The authors should verify which cases they include into their analysis.\nThe finding about NPs being more dominant is not surprising (and was also expected by the authors) and has also something to do with the fact that spoken texts reveal a reduced information density if compared to the written ones.\nThe discussion about the results on spoken vs. written is good and important. Even within written text, there could be a continuum, e.g. political speeches, which are written to be spoken or fictional texts that contain dialogues (as the authors point out themselves), could be closer to further spoken texts. At the same time, academic speeches or TED talks that contain less interaction with the audience (depending on a speaker’s style) could be closer to written texts, also in terms of referring expressions – we would expect them to contain more NPs, and probably complex Nps describing some notions.\nOverall, it is interesting to know if there are more dimensions than just the difference between spoken and written in the data, e.g.  narrativeness (narrative vs. non-narrative) or dialogicity (dialogic vs. monologic), etc. In fact, genre classification can and should be sometimes more fine-grained than just drawing a rigid line between texts that are considered to be spoken and those that are considered to be written.\nTextual problems: Page 2, Section 2: interfering mentions. - These → There are some typographical problems in the text.\nPage 3, Section 3: I am not sure if the abbreviation Sct. is allowed by the Coling style. \nIn the reference list, the authors should check spelling of the some entries, e.g. english→ English in Berfin et al. (2019), Zeldes (2018). There is an empty space in Godfrey et al. (1992).\nCited references: Kunz, Kerstin and Degaetano-Ortlieb, Stefania and Lapshinova-Koltunski, Ekaterina and Menzel, Katrin and Steiner, Erich (2017).  GECCo -- an empirically-based comparison of English-German cohesion. In De Sutter, Gert and Lefer, Marie-Aude and Delaere, Isabelle (eds.), Empirical Translation Studies: New Methodological and Theoretical Traditions. Mouton de Gruyter, pages 265–312.\n@BOOK{Mair2006,   title = {Twentieth-Century English: History, Variation and Standardization},   publisher = {Cambridge University Press},   year = {2006},   author = {Mair, Christian},   address = {Cambridge} } ","rid":"0","scores":{"overall":"4","Originality":"3","Readability_clarity":"5","Relevance":"5","Reproducibility":"5","Substance":"4"},"label":[[318,353,"Eval_pos_1"],[354,421,"Jus_pos_1"],[857,969,"Eval_pos_2"],[970,1042,"Jus_pos_2"],[2266,2400,"Jus_neg_1"],[2401,2429,"Eval_neg_1"],[2989,3066,"Eval_pos_3"],[3067,3595,"Jus_pos_3"]],"Comments":["3;2"]}
{"id":1188,"text":"Summary     - The paper studies the problem of under-translation common in auto-regressive neural machine translation. \n    - Two main pieces are introduced in this research work, random noise to the length constraint, output length prediction using BERT. \n    - The English-Japanese ASPEC dataset is used to evaluate the contribution of the two proposed improvements. \n    - A stronger or similar performance is shown for all the 4 length language groups using the proposed approach. Especially in the shorted range, the authors show more than 3 points improvement over the vanilla transformer. \n     - An interesting insight I got for the long sentences, vanilla transformer tend to produce shorter length sentences. The proposed approach generated translation close to the gold reference length, atleast for the dataset in use.\nStrenghts     - Ablation is performed for both the new component, random noise and BERT based output length prediction. \n    - Strong BLEU score for short sentence range and relatively close to gold reference length compared to the vanilla transformer.  Concerns     - The work of Lakew et al, uses English-Italian and English-German datasets for evaluation. These datasets should be used to have a consistent evaluation with the past work. \n    - Following from the last one, any specific reason why the English-Japanese dataset is a better choice for your proposed methods? Perhaps you can **motivate on linguistic grounds** why the language Japanese is a better testing ground for your method. \n    - Including an extra BERT-based-output-length prediction can incur additional computational overhead. The overhead of this computation should be stated in the work. \n    - In the introduction, you mention __However, the input sentence length is not a good estimator of the output length.__. I'm not sure why is this the case. ","rid":"1","scores":{"overall":"3","Originality":"3","Readability_clarity":"4","Relevance":"3","Reproducibility":"4","Substance":"3"},"label":[],"Comments":["1;1"]}
{"id":1189,"text":"Overviews: This paper focuses on Abusive Language Detection (ALD) and proposes a generic ALD model MACAS with multi-aspect embeddings for generalised characteristics of several types of ALD tasks across some domains.\nStrengths: The motivation of this paper is clear, i.e., to solve the problem that \"What would be the best generic ALD ...\", as described at the beginning of paragraph 2, section 1. The generic abusive language typology is categorised as two aspects, i.e., target aspect and content aspect, and multi-aspect embedding layer considers embeddings of both target and content, followed by cross-attention gate flow to refine four types of embeddings. The proposed model outperforms baselines on all the seven datasets. Detailed ablation studies have been given in section 5 as well.\nWeaknesses: For the structure of the paper, section 4 can be integrated into section 5 as a sub-section.\nthe description of baselines in section 4 is too detailed, which should be refined and shortened appropriately.\nThe paragraph 3, section 5.1 can be titled as a sub-section called \"case study\" since this paragraph analyzes some prediction examples in Table 3. ","rid":"0","scores":{"overall":"4","Originality":"4","Readability_clarity":"4","Relevance":"5","Reproducibility":"4","Substance":"4"},"label":[[228,267,"Eval_pos_1"],[900,958,"Eval_neg_1"],[959,1011,"Jus_neg_1"]],"Comments":["1;1"]}
{"id":1190,"text":"This work built a fake news prediction model using both news and user representation from user-generated texts. Experimental results showed that the user text information contributed to predicting the fake. Moreover, the paper showed linguistic analysis to show typical expressions by users in real and fake news. Cosine similarities between users are calculated using proposed user vectors to confirm the echo chamber effect. \n  Introducing vectors of news spreading users sounds an interesting idea. The paper's investigation, the user vector made from linguistic features contribute, is interesting and important. The results of active topics by users for both real and fake is also impressive. \n  There are some ways to build user vectors not only from their timeline and profiles but also from their tweets itself (e.g., Persona chat model). Does the proposed method have a clear advantage to such models? ","rid":"1","scores":{"overall":"4","Originality":"4","Readability_clarity":"5","Relevance":"5","Reproducibility":"4","Substance":"4"},"label":[[430,501,"Eval_pos_1"],[501,616,"Eval_pos_2"],[617,698,"Eval_pos_3"]],"Comments":["1;1"]}
{"id":1191,"text":"In this paper, the authors argue that using the topmost encoder output alone is problematic or suboptimal to neural machine translation. They propose multi-view learning, where the topmost encoding layer is regarded as the primary view and one intermediate encoding layer is used as an auxiliary view. Both views, as encoder outputs, are transferred to corresponding decoder steams, with shared model parameters except for the encoder-decoder attention. Prediction consistency loss is used to constrains these two streams. The authors claim that this method can improve the robustness of the encoding representations. Experiments on five translation tasks show better performance compared to vanilla baselines, and generalization to other neural architectures.\nOn one hand, the experiments conducted in this paper are rich, including five translation tasks, two NMT architectures, shallow and deep models, and many ablations and analysis.  On the other hand, I have several concerns regarding motivation, claims and experiments: -The authors pointed out two problems for using the topmost encoder output alone: 1) overfitting; 2) “It cannot make full use of representations extracted from lower encoder layers,”. I’m not convinced by the second one especially. For example, in PreNorm-based Transformer, the final encoder output is actually a direct addition of all previous encoding layers. Although there is a layer normalization, I believe this output carries critical information from lower layers.\n-The authors claim that “circumventing the necessity to change the model structure.”, but the proposed method requires to change the decoder, and manipulate the parameter sharing pattern. In my opinion, the method still requires structure modification.\n-The major ablations and analysis are performed on IWSLT De-En task, which is actually a low-resource task, where regularization is the main bottleneck. From Table 1, it seems like the proposed approach yields much smaller gains on large-scale WMT En-De task compared to low-resource tasks. Thus, it’s still questionable whether the conclusion from experiments on low-resource task can generalize to high-resource tasks.\n-Which WMT En-De test set did you use? WMT14 or WMT16? It seems like the authors used WMT16 for test, but the baseline (33.06 tokenized BLEU) is below standard (~34 BLEU).\n-Besides, some experiment has mixed results and is hard to draw convincing conclusions. For example, in Table 5, MV-3-6 (shared) achieves the best performance on De->En while MV-3-6 is the best on Ro->En. It seems like different tasks have different preferences (share or separate). In the paper, the author only highlights the superiority of separate settings on Ro->En task.\nOverall, I'm not convinced by the motivation and the analysis on low-resource tasks (In particular, this paper doesn't target at low-resource translation. Note that the authors claim that \"our method has a good generalization for the scale of data size.\"). I think the score of this paper is around 3.5 with several unclear questions to be solved. Since we don't have this option, I prefer to give the score of 3. ","rid":"0","scores":{"overall":"3","Originality":"2","Readability_clarity":"4","Relevance":"5","Reproducibility":"4","Substance":"3"},"label":[[774,822,"Eval_pos_1"],[823,938,"Eval_pos_1"],[958,1027,"Major_claim"],[1212,1260,"Eval_neg_1"],[1261,1502,"Jus_neg_1"],[1504,1689,"Jus_neg_2"],[1691,1755,"Eval_neg_2"],[1757,2047,"Jus_neg_3"],[2047,2176,"Eval_neg_3"],[2726,2809,"Eval_neg_4"],[2982,3140,"Major_claim"]],"Comments":["3;3"]}
{"id":1203,"text":"This paper describes a new deterministic dependency parsing algorithm and analyses its behaviour across a range of languages. \nThe core of the algorithm is a set of rules defining permitted dependencies based on POS tags. \nThe algorithm starts by ranking words using a slightly biased PageRank over a graph with edges defined by the permitted dependencies. \nStepping through the ranking, each word is linked to the closest word that will maintain a tree and is permitted by the head rules and a directionality constraint.\nOverall, the paper is interesting and clearly presented, though seems to differ only slightly from Sogaard (2012), \"Unsupervised Dependency Parsing without Training\". \nI have a few questions and suggestions: Head Rules (Table 1) - It would be good to have some analysis of these rules in relation to the corpus. \nFor example, in section 3.1 the fact that they do not always lead to a connected graph is mentioned, but not how frequently it occurs, or how large the components typically are.\nI was surprised that head direction was chosen using the test data rather than training or development data. \nGiven how fast the decision converges (10-15 sentences), this is not a major issue, but a surprising choice.\nHow does tie-breaking for words with the same PageRank score work? \nDoes it impact performance significantly, or are ties rare enough that it doesn't have an impact?\nThe various types of constraints (head rules, directionality, distance) will lead to upper bounds on possible performance of the system. \nIt would be informative to include oracle results for each constraint, to show how much they hurt the maximum possible score. \nThat would be particularly helpful for guiding future work in terms of where to try to modify this system.\nMinor: - 4.1, \"we obtain [the] rank\" - Table 5 and Table 7 have columns in different orders. I found the Table 7 arrangement clearer.\n- 6.1, \"isolate the [contribution] of both\" ","rid":0,"scores":{"overall":"3","PRESENTATION_FORMAT":"Poster","SUBSTANCE":"3","SOUNDNESS_CORRECTNESS":"4","APPROPRIATENESS":"5"},"label":[[522,578,"Eval_pos_1"],[580,689,"Eval_neg_1"],[753,834,"Eval_neg_2"],[835,1012,"Jus_neg_2"]],"Comments":["2;2"]}
