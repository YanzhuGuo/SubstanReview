{"id": 6, "review": "The paper proposes a task of selecting the most appropriate textual description for a given scene/image from a list of similar options. It also proposes couple of baseline models, an evaluation metrics and human evaluation score.  - Strengths: The paper is well-written and well-structured. \nIt is clear with its contributions and well supports them by empirical evidence. So the paper is very easy to read.  The paper is well motivated. A method of selecting the most appropriate caption given a list of misleading candidates will benefit other image-caption/understanding models, by acting as a post-generation re-ranking method.  - Weaknesses: I am not sure if the proposed algorithm for decoys generation is effective, which as a consequence puts the paper on questions.\nFor each target caption, the algorithm basically picks out those with similar representation and surface form but do not belong to the same image. But a fundamentally issue with this approach is: not belonging to the image-A does not mean not appropriate to describe image-A, especially when the representation and surface form are close. So the ground-truth labels might not be valid. As we can see in Figure-1, the generated decoys are either too far from the target to be a *good* decoy (*giraffe* vs *elephant*), or fair substitutes for the target (*small boy playing kites* vs *boy flies a kite*).\nThus, I am afraid that the dataset generated with this algorithm can not train a model to really *go beyond key word recognition*, which was claimed as contribution in this paper. As shown in Figure-1, most decoys can be filtered by key word mismatch---*giraffe vs elephant*, *pan vs bread*, *frisbee vs kite*, etc. And when they can not be separated by *key word match*, they look very tempting to be a correct option.\nFurthermore, it is interesting that humans only do correctly on 82.8% on a sampled test set. Does it mean that those examples are really too hard even for human to correctly classify? Or are some of the *decoys* in fact good enough to be the target's substitute (or even better) so that human choose them over ground-truth targets?\n- General Discussion: I think this is a well-written paper with clear motivation and substantial experiments. \nThe major issue is that the data-generating algorithm and the generated dataset do not seem helpful for the motivation. This in turn makes the experimental conclusions less convincing. So I tend to reject this paper unless my concerns can be fully addressed in rebuttal. ", "label": [[244, 291, "Eval_pos_1"], [292, 372, "Jus_pos_1"], [373, 408, "Eval_pos_1"], [409, 437, "Eval_pos_2"], [438, 631, "Jus_pos_2"], [647, 774, "Eval_neg_1"], [775, 1797, "Jus_neg_1"], [2152, 2240, "Eval_pos_3"], [2241, 2425, "Eval_neg_2"], [2426, 2512, "Major_claim"]]}
{"id": 16, "review": "paper_summary\nThis paper introduces an approach to produce privacy-preserving document embeddings with the property that every single sentence of the document could be replaced by some other random one while obtaining a similar embedding for the document. The paper is way out of my area of expertise, and thus this review must remain an educated guess. \nThe proposed method replaces sentence embeddings from the private document for embeddings obtained from a set of public documents. The method uses embeddings that are close to the original ones by picking those with high \"Tukey Depth\" with respect to the set of candidate embeddings. Finally, the embeddings are spread apart by passing them through a network trained with an unsupervised clustering signal, and, if understand correctly, averaged together. \nsummary_of_strengths\n1. The method seems to be well grounded in a theoretical framework. \n2. The paper is very clear, and even though I lack the relevant background I could follow it (with quite some effort, which is also quite natural) 3. The empirical results seem good (but I am not familiar with the state of the art in this area) \nsummary_of_weaknesses\n1. As an outsider, I wonder whether some natural baselines could have been explored. For instance, given that the method uses sentence embeddings after a clustering transformation, I wonder what would happen if cluster centroids were directly used instead. Similarly, I wonder what's the need for computing the Tucker Depth, and not just using the closest vector in terms of cosine similarity. \ncomments,_suggestions_and_typos\nL404: It might be worth recalling the reader that k is the number of sentences at this point, given that this fact was introduced much earlier on and not used until then. ", "label": [[836, 900, "Eval_pos_1"], [905, 928, "Eval_pos_2"], [1052, 1083, "Eval_pos_3"], [1173, 1254, "Eval_neg_1"], [1255, 1564, "Jus_neg_1"]]}
{"id": 23, "review": "paper_summary\nThis work tackles the automatic identification of human values in arguments. \nHuman values can provide the underlying motives behind arguments and can explain why someone takes a position or what goals the person sees as worth striving for. \nOpposing values or different ways of prioritizing these values lead to conflicts and disagreements. If the underlying values can be made explicit, valuable insights can be gained in the analysis of different discourses. In other contexts arguments can be automatically generated for a target audience based on appropriate matching values and thus be more convincing. This motivates the authors to tackle the task and initially address the following problems: they systematically evaluate existing taxonomies from social and psychological science. Based on this they develop a framework that integrates the values from the various theories and that provides different levels of granularity, thus making an automatic classification of these values more feasible. The framework is used to create a resource with arguments from different geographical regions annotated with human values. The authors conduct some classification experiments with transformer-based models and discuss the results in an analysis. \nsummary_of_strengths\nThe authors provide the following contributions: -a systematic literature review of existing theories about the classification of human values in argumentation -a theoretically-informed framework to annotate human values in arguments. Different levels of granularity provide flexibility for the annotation of new data and is useful to do automatic classification of human values, especially when data is scarce and the classes are imbalanced -a resource annotated with their new framework that is made accessible for the research community. The annotation process is described in detail. The resource contains arguments from different cultural regions.  -experiments for the automatic classification of human values using the new data set. These can serve as a baseline for future research and they provide some initial insights about the difficulty of the task and the robustness \nsummary_of_weaknesses\nSection 3 could benefit from a more clear structure and some insights from the background section that lead to the design of the proposed taxonomy. It took me some time to understand that the schemata introduced in section 2.1 are also considered when designing the new taxonomy (Table 1), making this more explicit (already in section 2.1) would help to make this more clear. To me it is not clear how the beginning of section 3 (claim 1: human values are implicit, claim 2: human values make arguments more persuasive, claim 3: the implicit connection has to me made explicit) provide justifications for the new design. To me the motivation for the proposed taxonomy is a) that it combines different theories therefore is more complete, missing values have been added and b) that is has different levels of granularity which is useful for both, quantitative analyses and machine learning. Although this was hinted at in the introduction, I am missing the clear motivation / advantages for the new taxonomy in this section. \nOn top of that there is a missing reference that is relevant, as it tackles the automatic classification of human values using the Schwartz taxonomy (though in social media and using feature-based approaches, no Deep Learning): A Societal Sentiment Analysis: Predicting the Values and Ethics of Individuals by Analysing Social Media Content (https://aclanthology.org/E17-1069.pdf) \ncomments,_suggestions_and_typos\nThis paper has some weaknesses in section 2 / 3 but these can be resolved by some rewording / making things more clear / adding the missing reference. The theoretical review and development of the new taxonomy, the resource and the experiments are a strong contribution and will be valuable for the research community.  minor remarks: 212-218: unclear, what is meant by 'task'? The 'task' for the ML model? \n247-255: sounds more like future work or has to be rephrased to make clear that this is a motivation for using cross-cultural data 289: how was it translated? manually? \nWhy only 50 arguments from Africa but 100 from India and China? \ntypos: 155 (present*s*), 268 conclusion(s) ", "label": [[1333, 1443, "Eval_pos_1"], [1444, 1518, "Eval_pos_2"], [1519, 1725, "Jus_pos_2"], [2188, 2334, "Eval_neg_1"], [2336, 3213, "Jus_neg_1"], [3214, 3274, "Eval_neg_2"], [3276, 3595, "Jus_neg_2"], [3779, 3947, "Major_claim"]]}
{"id": 25, "review": "paper_summary\nThis paper explores the topic of automatic readability assessment, specifically reframing the task as one of pairwise ranking. The authors experiment with neural and other baseline models. \nsummary_of_strengths\n- A number of data-sets are used, some for testing only -Cross-corpus and cross-lingual experiments are done to show generalizability -A number of baseline systems are used, such as traditional classification- and regression-based systems (where possible) for comparison -A data-set is being released along with this work, which may lead to more development opportunities in the future \nsummary_of_weaknesses\n- For the regression-based systems, I see that OLS was used while for the classification-based systems, SVM was used. Why not use SVR for the former? It would seem to be a closer comparison and it might prove to be better baseline against NPRM, etc.\n-For the cross-linguistic analyses, multilingual BERT is used. I would assume French and Spanish are included here, so claiming that this does well cross-linguistically assumes that too, right? \ncomments,_suggestions_and_typos\n- Are \"slugs\" restricted to being only in either test/train for the cross-validation experiments? I'm a little unclear on if this would be necessary or not. It might be good to address whether this is a concern.\n- \"In this background\" - This phrase is used a couple of times and it's unclear what is meant. Consider rephrasing, e.g. \"With this background\", \"Given this background\", etc.\n-2 Related Work - \"result in\" --> \"results in\" -4.4 Evaluation - \"w\" --> \"we\"? It's unclear what is meant.\n-Limitations of NPRM - \"while\" --> \"why\" (or just get rid of \"while\") ", "label": []}
{"id": 27, "review": "paper_summary\nThe paper presents an approach to supervised contrastive learning for NLU - classification task by anchoring CL losses using label embeddings. The overall CL loss comprises of three losses. First, the (Multihead) Instance-centered contrastive loss that aims to bring the instance embedding closer to the label embedding by anchoring on the instance and its label as positive example and instance-other labels as negative examples. Second, the label centered contrastive loss which uses labels to mine positive and negative labels from a batch. And finally, to promote better uniformity across learned embeddings - label embeddings are also regularized.  The authors conduct experiments on standard benchmark NLP tasks showing that their approach produces better/competitive results to baselines. They also show the effectiveness of their approach in Few Shot and Data imbalance settings.  Ablation study provides the importance of each of the component / architectural choices. \nsummary_of_strengths\n1. Well written, well reasoned, easy to follow how model / architecture / experiments are setup up. \n2. Extensive experimentation / ablation with statistical significant testing. \n3. Effective use of label semantics in Supervised Contrastive Learning. \n4. The work seems well researched given their knowledge of CL field which is showcased not only in related work section but also the mention of alignment/uniformity, preventing degradation in CL models, regularizing label embeddings etc. From a CL modeling perspective - the authors have crossed all the \"t\"s dotted all the \"i\"s. \n5. Results show improvements / competitiveness across standard benchmarks and their technique is easy to implement / replicate. \nsummary_of_weaknesses\n1. While the paper details a lot of \"How\"s - the \"Why\"s are missing from their design choices. E.g. why do they think the label semantics arent explored enough (line 70-72), what motivated the use of multi-head instance CL loss (135-137), why the need for a 3 layer projection head g (line 187-194).  Research papers should address the why if they wish the community to learn and extend their lessons.\n2. The author mention uniformity / alignment which is win in and off itself but they do not present any results based on these metrics (e.g. in https://arxiv.org/pdf/2104.08821.pdf)  3. Why do the authors use regularization for label embeddings but not for instance embeddings? Given that the goal is improving uniformity - they should have at least considered using (spread out) regularization for instance embeddings too (ref: eq 6 in https://arxiv.org/pdf/1708.06320.pdf) \ncomments,_suggestions_and_typos\n1. The claim they make in lines 225-228 are partially true. The equation aids in aligning instance with label embeddings but it does not contain any elements that encourages different instances to disperse. Such dispersion is only encouraged by considering hinge losses or using spread out regularization (e.g. by equation 6 in this paper). As such the claim of uniformity should be removed from lines 225-228.  2. Line 172 should read \"The input of LaCon contains two parts consisting of the text and all the labels for the task) ", "label": [[1017, 1114, "Eval_pos_1"], [1118, 1193, "Eval_pos_2"], [1197, 1266, "Eval_pos_3"], [1270, 1335, "Eval_pos_4"], [1335, 1597, "Jus_pos_4"], [1601, 1671, "Eval_pos_5"], [1676, 1724, "Eval_pos_6"], [1752, 1843, "Eval_neg_1"], [1844, 2150, "Jus_neg_1"], [2662, 2718, "Eval_neg_2"], [2719, 3070, "Jus_neg_2"]]}
{"id": 28, "review": "paper_summary\nThis paper proposes to learn sentence embeddings with psudo tokens, aiming at addressing the sentence length bias issue of existing approaches that learn sentence embeddings based on contrastive loss. \nsummary_of_strengths\nThe idea of mapping the sentence onto a fixed number of pseudo tokens and then maps it back is very interesting. \nExperiments on STS task shows the proposed approach achieves good improvement over existing sentence embedding models. \nsummary_of_weaknesses\nRegarding sentence length feature:   -- The paper argues that the approach in SimCSE relies on sentence length features.  I don't agree with the argument since from the modelling point of view, there is no explicit inductive bias such that the SimCSE would prefer sentences that share the same number of tokens.     -- The authors stated that \"Through careful observation we find that all positive examples have the same length ..\".   I am not surprised since positive example pair in SimCSE is constructed by applying different dropout masks twice to the feedforward layer of the Transformer models, while the input to the Transformer model remains untouched.       -- I think sentence length side-effect is an issue of engineering/implementation, and can also be addressed from engineering side.  For example, one could implement a sampling procedure where all examples in the same batch have similar number of tokens.   The paper argues that the SimCSE model also affected by \"syntactic structures\".  It would be great to have some examples or more descriptions on what does \"syntactic structures\" referring to.\nRegarding the proposed model:   -- If I understand correctly, Y_i in eq 1 is the sentence embedding induced by BERT model.  In such case, the attention in EQ1 would always output a weight matrix of shape [m, 1], i.e., a vector of all 1s.  Here m denotes the number of pseudo tokens. This is because there is only one key in the dictionary, i.e., the sentence embedding.  Thus all attention weights are put on that key.\nRegarding the experiments.  Although the proposed approach achieves good improvement, it is unclear whether the improvement is significantly better than the baseline systems. \ncomments,_suggestions_and_typos\nIs the proposed improvement also additive? E.g., if all systems are ALSO fine-tuned on supervised data such as NLI, does the improvement over the baseline system still hold.\nThis paper created a subset by filtering based on length of the positive/negative example pairs. How big is this new subset? ", "label": [[237, 350, "Eval_pos_1"], [351, 470, "Eval_pos_2"], [615, 687, "Eval_neg_1"], [687, 804, "Jus_neg_1"], [2055, 2202, "Eval_neg_2"]]}
{"id": 30, "review": "paper_summary\nThe authors construct a corpus on suicide events. The token level annotations and event types are invaluable. This corpus enable quantification of the performance of the state-of-the-art modeling approaches. The performance scores show that we still need to work a lot on this task. \nsummary_of_strengths\nThe inter-annotator agreement and the use of Reddit data, which is user-generated text, make this work valuable. The token level annotations and event types are invaluable. \nsummary_of_weaknesses\nThe paper should contain discussion of some design decisions: 1- How did the documents are filtered? Is it a random sample or keyword based filtering? What did you do for the documents that do not contain any suicide related information? \n2- \"with more than 50 words are kept to increase\" -> What do the excluded documents contain? What is the ratio of the posts excluded in respect of this decision? \n3- \"Finally, we further fine tune the pre-trained BERT model over unlabeled Reddit posts (i.e., about 40K posts) using masked language modeling (Devlin et al., 2019).\" - > how did you select the 40k docs? The document selection matters: Caselli, T., Mutlu, O., Basile, A., & H\u00fcrriyeto\u011flu, A. (2021, August). PROTEST-ER: Retraining BERT for Protest Event Extraction. ... \ncomments,_suggestions_and_typos\n1- The ratio of RF-Other is too high (40%) in the corpus in comparison to other event types. I think this requires some elaboration. \n2- The text mention 2300 docs annotated. However, table 1 row #documents contains more documents 2214+130+109 3- How is the disagreements were resolved for the 20% that was co-annotated. \n4- The reddit corpus could be compared to other event detection datasets that utilize user-generated text. Comparing this corpus to MAVEN and CycSecED is not fair. \n5- Do you use sentences or whole posts as the unit to be annotated?\nEnglish language: \"pubic posts\" -> public posts \"a ED model must\" -> \"an ED model must\" ", "label": [[319, 431, "Eval_pos_1"], [432, 492, "Eval_pos_2"], [515, 575, "Eval_neg_1"], [577, 1287, "Jus_neg_1"]]}
{"id": 32, "review": "paper_summary\nThis paper discusses methods to improve MT for Interactive machine translation but not in a standard sense. In IMT, usually MT predicts the next set of tokens given the prefix that has been post-edited by translators. In this paper, the authors tackle another modified use case where translators are first provided a full translation and then they are asked to post-edit portions of the translated text. Usually constrained decoding is used for such a task where model has to regenerate the beam according to the post-edited content.  Overall, I liked reading the paper because everything was stitched and motivated well throughout the paper. It does look like the paper was proof-read multiple times because I did not really find any grammatical errors. Having said that, I do think there's scope of improvement in the paper which I laid down in Weakness section. \nsummary_of_strengths\nThe task of IMT is tough and authors did a good job at providing full context in the problem and how they tackle a modified IMT problem.  Latency is definitely a major component in adoption of this technology. I liked the way this paper outlined the metrics and provided motivation for it.  The approach is quite simple and very effective in terms of quality and latency. \nsummary_of_weaknesses\nUsage of data and experiment design: WMT data contain human generated references, any reason why post-edited content was not used in the paper for experiments? Post-edited content would have been much closer to what your task is and there's plenty of available data out there which is post-edited.   This task of sequence refinement has been studied well in  1. Relation to Neural Automatic Post editing models  2. Sequence Refinement part of Levenshtein Transformer (LevT) by Gu. et. al. 2019 is not discussed when it is almost the same thing although the use case is different (for Automatic Post Editing). \n3. Why was there no discussion on non-autoregressive models? Text infilling approach could indeed solved with a non-autoregressive model like LevT. What are the other methods of text infilling, why aren't those compared with your cross-lingual text-infilling approach?\nI am left with some other questions after reading the paper, I am writing them as weakness here but they are essentially points where this paper can improve.  L246: \"Then we combine the sampled data D and the trivial data D as the augmented data to train the model P in our experiments.\" what is the ratio of this combination, is it 1:1, if so are you not giving more weight text infilling problem than translation? An ablation study on this combination would have been good.  In the real-world scenario, given that the sample size is low (200), is 65.79 (BiTIIMT) significantly better than 64.05 (LCD)? Same question for edit distance cost? \ncomments,_suggestions_and_typos\nL273: \"In other words, we employ the standard beam search algorithm to yield Yb, which leads to a valid Y, without explicitly imposing the constraint during decoding\" Is this always the case in all languages? If not, can you mention the accuracy here? ", "label": [[549, 656, "Major_claim"], [901, 1037, "Eval_pos_1"], [1111, 1190, "Eval_pos_2"], [1192, 1273, "Eval_pos_3"]]}
{"id": 33, "review": "paper_summary\nThis paper addresses and focuses on disambiguation resolution on task-oriented dialog systems. The authors augment the seminal databases (MultiWOZ and SGD) with synthetic disambiguation turns, and use it as a new dataset to make their dialog system understand the user's answer and do follow up clarification questions. \nsummary_of_strengths\nOverall this paper is complete and very well written. The ablation studies are convincing, and the results are promising. Furthermore the analysis presented in the paper is concise and complete as well. This paper has a potential to inspire other dialog researchers to work on enhancing universal dialog skills. \nsummary_of_weaknesses\nIt might be difficult for readers to understand the explanation in Sec 3.1. and 3.2., rephrasing the paragraph along with its pseudo-code can add clarity. \ncomments,_suggestions_and_typos\nIt would be really helpful for the dialog community if authors open their experiment script, especially their script to do automatic augmentation (Sec. 3.2.) ", "label": [[356, 409, "Eval_pos_1"], [410, 446, "Eval_pos_2"], [451, 477, "Eval_pos_3"], [478, 558, "Eval_pos_4"], [691, 776, "Eval_neg_1"], [777, 846, "Jus_neg_1"]]}
{"id": 35, "review": "paper_summary\nThis paper gives a comprehensive analysis of the Square One Bias in NLP. Through the statistics of ACL conference papers in recent years, the authors find that the current researchers are encouraged to go beyond the prototype experiment (optimizing accuracy/F1 on an English dataset) in only a single dimension. The problems resulting from the Square One Bias, the recommendations to overcome this, the examples   (one-dimensional research) and counter-examples (multi-dimensional research) are also discussed in the paper. \nsummary_of_strengths\n1. The paper is well-written and gives a systematic discussion on the Square One Bias in NLP. \n2. I found the examples in the paper to demonstrate the Square One Biases (including architecture biases, annotation biases, selection biases, protocol biases, and organizational biases) pretty precise and helpful. \n3. From the perspective of this paper, one can easily notice unexplored areas of the research manifold, thus I believe this paper is helpful to the community to become more diverse. Especially, this paper would be inspiring for researchers who are interested in non-standard circumstances (e.g. non-English, non-standard architectures, non-accuracy/F1 metrics, etc.). \nsummary_of_weaknesses\nAlthough this paper proposed many ideal recommendations for researchers and conference organizers to mitigate the Square One Biase issue, I might doubt the value in practice. As discussed in Section 6, doing multi-dimensional research would not only increase the workload of paper authors but also requires conference reviewers to be experts in multiple areas. As a result, the development of the field may be slowed down by the cumbersome evaluation. And besides, if we develop some standard multi-dimensional evaluation protocol, it may also introduce a new Square One Bias (e.g. some particular evaluation methods of fairness and efficiency can become dominant).  But overall, I agree that research that departs from prototypes in multiple dimensions should be encouraged by the reviewers and conference organizers. \ncomments,_suggestions_and_typos\nThe following papers, which consider the multi-dimensional evaluation in NLP, should be closely related to this work: 1. Dynabench: Rethinking Benchmarking in NLP (https://arxiv.org/abs/2104.14337) 2. Towards Efficient NLP: A Standard Evaluation and A Strong Baseline (https://arxiv.org/abs/2110.07038) It would be better to include and discuss them. ", "label": [[563, 654, "Eval_pos_1"], [658, 728, "Eval_pos_2"], [729, 841, "Jus_pos_2"], [842, 868, "Eval_pos_2"], [874, 974, "Jus_pos_3"], [980, 1051, "Eval_pos_3"], [1053, 1159, "Eval_pos_4"], [1160, 1237, "Jus_pos_4"], [1400, 1436, "Eval_neg_1"], [1437, 1928, "Jus_neg_1"]]}
{"id": 36, "review": "paper_summary\nThis paper aims to train a fully unsupervised pretrained retriever for zero-shot text retrieval. Specifi- cally, this paper proposes a novel iterative contrastive learning to pretrain dual-tower dense retriever, then use lexical matching method to further enhance the pretrained dense retrieval model. Results on BEIR benchmark show that the proposed method achieves SOTA performance compared with supervised dense retrieval model. \nsummary_of_strengths\n1. The proposed unsupervised dense retrieval model achieves remarkable performance com- pared with supervised dense retrieval models on BEIR benchmark. \n2. Theproposedmodelisinitializedfrom6-layerDistilBERTandtheparametersofdualencoders are tied, so the proposed retrieval model is quite efficient. \nsummary_of_weaknesses\n1. The selling point of this paper is unsupervised pretrained dense retriever(LaPraDoR) can per- form on par with supervised dense retriever, but actually, LaPraDoR is a hybrid retriever rather than a pure dense retriever. In a way, it\u2019s unfair to compare hybrid method to dense/sparse method as shown in table 1, because it\u2019s known that the dense retriever and sparse retriever are complementary. The comparable supervised models should also be hybrid retrievers. Besides, in table 3, it seems that without lexicon enhancement, the performance of proposed unsupervised model is not competitive either on in-domain MS-MARCO or cross domain BEIR benchmark compared with supervised model. \n2. In table 4, the combination of self-supervised tasks ICT and DaPI doesn\u2019t seem to be com- plementary, the effectiveness of DaPI task, which will double the GPU memory usage, is not significant (0.434 -> 0.438) 3. ICoL is proposed to mitigate the insufficient memory on a single GPU and allow more neg- ative instances for better performance, but there are no corresponding experiments to show the influence of the number of negatives. As far as I know, the quality of negatives is more important than the quantity of negatives as shown in TAS-B. 4. It sounds unreasonable that increasing the model size can hurt the performance, as recent paper Ni et al. shows that the scaling law is also apply to dense retrieval model, so the preliminary experimental results on Wikipedia about model size should be provided in detail. \n5. Thepaperarguethattheproposedapproachistocomplementlexicalmatchingwithsemantic matching, while the training procedure of proposed model is totally independent with lexical matching. Therefore, the argument \u201dLEDR helps filter out such noise and allows the dense retriever to focus on fine-grained semantic matching\u201d is confusing, because there is no suc- cession relationship between LEDR and dense retriever.\nReference: * Ni et al. 2021. https://arxiv.org/abs/2112.07899 \ncomments,_suggestions_and_typos\nthe proposed LaPraDoR achieves relative low performance on MS-MARCO while relative high per- formance on BEIR, the inductive bias of the proposed pretrain method is worth exploring. \nLine 300-304: q and d are confusing ", "label": [[471, 618, "Eval_pos_1"], [624, 714, "Jus_pos_2"], [715, 765, "Eval_pos_2"], [1013, 1102, "Eval_neg_1"], [1104, 1186, "Jus_neg_1"], [1481, 1582, "Eval_neg_2"], [1583, 1690, "Jus_neg_2"], [2499, 2633, "Eval_neg_3"], [2635, 2714, "Jus_neg_3"]]}
{"id": 40, "review": "paper_summary\nThis paper presents a technique for numerical reasoning over tables by leveraging formulas based on the pre-trained model TUTA specifically designed for tables.  FORTAP, a FORmula-driven TAble pre-training model, is proposed in this paper, which incorporates numerical reasoning capabilities based on large-scale spreadsheet formulas.\nThe main contributions are two complementary/auxiliary objective functions:   * Numerical Reference Prediction,   * Numerical Calculation Prediction.  They take relationships between cells into consideration and predict all operators using cell embeddings.  Experimental results show their method achieves better performance over several datasets when compared to both baseline and SOTA models. \nsummary_of_strengths\n- Show the potential of leveraging formulas in table pretraining.\n-Good results based on comprehensive experimental settings. Apart from that, the authors provide very detailed explanations and discussions.  -Very clear structure and good organization. \nsummary_of_weaknesses\n- Baselines chosen in this paper are not strong enough to show the performance brought by the introduced objectives. For example, under the TableQA settings, another model employing SQL named TaPEx (also mentioned in this paper) can be used to compare with the proposed model.\n-To further evaluate the effectiveness of the proposed method, experiments on other tasks (e.g. cell type classification, table type classification) compared to TUTA should be conducted. \ncomments,_suggestions_and_typos\n- More clarification and novelty is needed for contributions in Sec 1.\n-As mentioned in Sec 5, there is another model named TaPEx, which is also designed to improve reasoning skills in table pre-training. Why is there no comparison with TaPEx in the experiment section?\n-TUTA employs several datasets to evaluate the model performance on CTC (cell type classification), why is there only one dataset used in this paper?\n-Is there any deficiency/limitation of introducing these objectives? Will it cause a performance drop on other tasks such as Table Type Classification and CTC? ", "label": [[833, 891, "Eval_pos_1"], [908, 971, "Eval_pos_2"], [975, 1019, "Eval_pos_3"], [1044, 1158, "Eval_neg_1"], [1159, 1318, "Jus_neg_1"], [1320, 1408, "Eval_neg_2"], [1409, 1467, "Jus_neg_2"], [1468, 1506, "Eval_neg_2"], [1541, 1609, "Eval_neg_3"]]}
{"id": 44, "review": "This paper describes a new deterministic dependency parsing algorithm and analyses its behaviour across a range of languages. \nThe core of the algorithm is a set of rules defining permitted dependencies based on POS tags. \nThe algorithm starts by ranking words using a slightly biased PageRank over a graph with edges defined by the permitted dependencies. \nStepping through the ranking, each word is linked to the closest word that will maintain a tree and is permitted by the head rules and a directionality constraint.\nOverall, the paper is interesting and clearly presented, though seems to differ only slightly from Sogaard (2012), \"Unsupervised Dependency Parsing without Training\". \nI have a few questions and suggestions: Head Rules (Table 1) - It would be good to have some analysis of these rules in relation to the corpus. \nFor example, in section 3.1 the fact that they do not always lead to a connected graph is mentioned, but not how frequently it occurs, or how large the components typically are.\nI was surprised that head direction was chosen using the test data rather than training or development data. \nGiven how fast the decision converges (10-15 sentences), this is not a major issue, but a surprising choice.\nHow does tie-breaking for words with the same PageRank score work? \nDoes it impact performance significantly, or are ties rare enough that it doesn't have an impact?\nThe various types of constraints (head rules, directionality, distance) will lead to upper bounds on possible performance of the system. \nIt would be informative to include oracle results for each constraint, to show how much they hurt the maximum possible score. \nThat would be particularly helpful for guiding future work in terms of where to try to modify this system.\nMinor: - 4.1, \"we obtain [the] rank\" - Table 5 and Table 7 have columns in different orders. I found the Table 7 arrangement clearer.\n- 6.1, \"isolate the [contribution] of both\" ", "label": [[522, 578, "Eval_pos_1"], [579, 637, "Eval_neg_1"], [753, 834, "Eval_neg_2"], [835, 1012, "Jus_neg_2"]]}
{"id": 51, "review": "paper_summary\nThe authors compress generalized models using established quantization methods. Finding these fail, they evaluate the failure and to rectify it introduce a novel quantization framework that offers improvements leaps and bounds better than prior methods. Their token-level contrastive distillation approach is elegant, simple, and easy to comprehend. \nsummary_of_strengths\nThe authors introduce a well-researched, effective, and scalable method for quantizing generalized models. They implement competitive baselines,  evaluate the failure cases. Using the discovery that quantization produces homogenous embeddings they create their contrastive token distillation method which is shown to improve the generator model performance heavily. \nsummary_of_weaknesses\nThe only weakness I see in this paper is a lack of description of the incurred cost for token-level conservative loss. Without being able to see the associated code and run the experiments it is difficult to understand what negative impact using token level contrastive learning has. How much slower does it make the training? How does it affect GPU memory usage? \ncomments,_suggestions_and_typos\nIt would be beneficial to the community to release the software and associated models along with exporting the models to an engine that can take advantage of lower precision for increased inference speed. ", "label": [[386, 492, "Eval_pos_1"], [493, 559, "Eval_pos_2"], [560, 752, "Eval_pos_3"], [775, 893, "Eval_neg_1"], [894, 1139, "Jus_neg_1"]]}
{"id": 83, "review": "paper_summary\nThe paper presents NAKDIMON, a character-LSTM Hebrew diacritizer that does not use a morphological analyzer/dictionary.  It compares its performance to SOTA on the task; and presents a new test set for the task that is more diverse that previous sets. \nsummary_of_strengths\nThe paper is well written and clear. The work is well motivated and enough related work is presented.  The error analysis is clear and helpful. This is a good short paper with a negative result. \nsummary_of_weaknesses\nThe paper presents inconclusive negative results: it is unclear what the results would look like had millions of additional words that are *automatically diacritized* using Dicta are added in Nakdimon's training. The authors added 1.3M such words. I think an additional experiment that uses more words (automatically diacritized) or just generated word forms from a morphological dictionary can make the result more convincing (negative or positive it may be). \ncomments,_suggestions_and_typos\n- it would help to report OOV rates; and performance on in-vocabulary vs OOV for your system.\n- the discussion of Arabic use of diacritization is not accurate. Arabic \"dots\" are not optional in common use of Arabic; diacritical marks (vowels, nunation, gemination) are.  Check out  https://aclanthology.org/N07-2014.pdf https://aclanthology.org/2007.mtsummit-papers.20.pdf - I am puzzled by a difference between the Dicta test set and the new test set: the difference between CHA and WOR for the Dicta test set is much bigger that the respective difference in the new test set (between 1.4 and 2.4 times bigger).  Any thoughts on that? ", "label": [[288, 324, "Eval_pos_1"], [325, 390, "Eval_pos_2"], [391, 431, "Eval_pos_3"], [432, 483, "Major_claim"], [506, 554, "Eval_neg_1"], [556, 967, "Jus_neg_1"], [1096, 1159, "Eval_neg_2"], [1159, 1372, "Jus_neg_2"], [1375, 1451, "Eval_neg_3"], [1453, 1636, "Jus_neg_3"]]}
{"id": 87, "review": "- Strengths: A well written paper, examining the use of context in lexical entailment task is a great idea, a well defined approach and experimental set-up and good analysis of the results  - Weaknesses: Some information is missing or insufficient, e.g., the table captions should be more descriptive, a clear description for each of the word type features should be given.\nGeneral Discussion:  The paper presents a proposal of consideration of context in lexical entailment task. The results from the experiments demonstrate that context-informed models do better than context-agnostic models on the entailment task.  I liked the idea of creating negative examples to get negative annotations automatically in the two ways described in the paper based on WordNet positive examples. ( new dataset; an interesting method to develop dataset) I also liked the idea of transforming already-used context-agnostic representations into contextualized representations, experimenting with different ways to get contextualized representations (i.e., mask vs contetx2vec), and testing the model on 3 different datasets (generalizability not just across different datasets but also cross-linguistically).\nMotivations for various decisions in the experimental design were good to see, e.g., why authors used the split they used for CONTEXT-PPDB (it showed that they thought out clearly what exactly they were doing and why).\nLines 431-434: authors might want to state briefly how the class weights were determined and added to account for the unbalanced data in the CONTEXT-WN experiments. Would it affect direct comparisons with previous work, in what ways?  Change in Line 589: directionality 4 --> directionality, as in Table 4 Suggested change in Line 696-697: is-a hierarchy of WordNet --> \"is-a\" hierarchy of WordNet  For the sake of completeness, represent \"mask\" also in Figure 1.\nI have read the author response. ", "label": [[13, 34, "Eval_pos_1"], [35, 106, "Eval_pos_2"], [107, 155, "Eval_pos_3"], [160, 188, "Eval_pos_4"], [204, 247, "Eval_neg_1"], [249, 373, "Jus_neg_1"], [619, 782, "Eval_pos_5"], [783, 839, "Jus_pos_5"], [840, 1107, "Eval_pos_6"], [1108, 1191, "Jus_pos_6"], [1193, 1270, "Eval_pos_7"], [1272, 1411, "Jus_pos_7"]]}
{"id": 89, "review": "### Strengths: -Well-written, well-organized -Incorporate topical segmentation to copula LDA to enable the joint learning of segmentation and latent models -Experimental setting is well-designed and show the superiority of the proposed method from several different indicators and datasets ### Weaknesses: -No comparison with \"novel\" segmentation methods ### General Discussion: This paper presents segLDAcop, a joint latent model for topics and segments. \nThis model is based on the copula LDA and incorporates the topical segmentation to the copula LDA. The authors conduct comprehensive experiments by using several different datasets and evaluation metrics to show the superiority of their model.\nThis paper is well-written and well-organized. The proposed model is a reasonable extension of the copula LDA to enable the joint inference of segmentations and topics. Experimental setting is carefully designed and the superiority of the proposed model is fairly validated. \nOne concern is that the authors only use the simple NP segmentation and single word segmentation as segments of the previous method. As noted in the paper, there are many work to smartly generate segments before running LDA though it is largely affected by the bias of statistical or linguistic tools used. The comparison with more novel (state-of-the-art) segments would be preferable to precisely show the validity of the proposed method.\n### Minor comment -In line 105, \"latent radom topics\" -> \"latent random topics\" ", "label": [[16, 45, "Eval_pos_1"], [157, 289, "Eval_pos_2"], [307, 354, "Eval_neg_1"], [556, 700, "Eval_pos_3"], [701, 747, "Eval_pos_4"], [869, 976, "Eval_pos_5"], [977, 1109, "Eval_neg_2"], [1110, 1417, "Jus_neg_2"]]}
{"id": 94, "review": "paper_summary\nIn this paper, the authors proposes a new intermediate vector selection algorithm for BERT models, attempting to reduce the inference latency. The proposed core-set selection algorithm works right before the FF layer, which reduces the number of vectors in the input set according to a pre-defined length configuration. The intuition of of the algorithm is similar to DBSCAN with iterations, it starts with a core set containing only CLS embedding. Then it expands the core set by selecting m closest vectors to the core set. The expansion ends when the number of core-set reaching a certain number.\nIn experiments, the authors compare with the attention based method and first K selection method. The experiment results show that by fixing the speedup ratio at 3x, the proposed core-set selection algorithm shows advantage over other competitors.\nOverall, my opinion leans toward an acception. \nsummary_of_strengths\n- This authors put significant amount of content on theoretical justification for the proposed core-set selection algorithm -Results on LRA test set demonstrate that the proposed method can be used to further reduce the space complexity with big bird or performers \nsummary_of_weaknesses\n- Given this research topic, my first question would be: what is the upper bound / oracle performance for \"Select\", say, at the 3X speed up. If we have an oracle selection algorithm, can the performance be fully recovered?  In Table 1, the base average performance is 81.5%, and CS-opt achieves 80%. If the oracle performance is 80%, then we know a performance drop is inevitable, and this \"Select\" problem is fully solved by this paper. If the oracle performance is still 81.5%, then we can think there is still potential for further improvement. To me, such an study gives most valuable insights for understanding the evaluation results.\n- In table 1 and table 2, we can see CS-k-1 has a very close performance comparing to CS-opt. The only evaluation datapoints where CS-k-1 is significantly behind CS-opt is the SST-2 task in Table 1. However, if we go to Table 2, there is no difference between CS-k-1 and CS-opt results on SST-2. If the drop in Table 1 is caused by noise, then indeed CS-k-1 can be a much easier and simpler algorithm in terms of implementation.\n- In Table 4, the dataset that we can see significant gap between CS-opt and CS-k-1 is CIFAR-10 sequence-based classification task. However, I don't think it's proper to draw any key conclusion based on such a non-standard task. It itself is still a good toy task.\n- Overall, the intuition of the algorithm is to keep vectors close to the CLS embedding. I'm worrying that in some complex problems (e.g, QA domain), it might be really difficult for the model to determine the CLS embedding until the last layer. Selection in the first layer based on a noisy CLS embedding may cause the performance to degrade significantly. \ncomments,_suggestions_and_typos\nLine 247 \"However, if two or more tokens are exact duplicates of each other, then one can easily remove the duplicates from the input and modify the self-attention appropriately to get the same CLS embedding at the top layer, and hence the same prediction.\"\nIs this true? As the transformer has positional embedding and the self-attention layer is multi-head. ", "label": [[862, 908, "Major_claim"], [2420, 2516, "Jus_neg_1"]]}
{"id": 95, "review": "This paper proposes a method for building dialogue agents involved in a symmetric collaborative task, in which the agents need to strategically communicate to achieve a common goal.   I do like this paper.  I am very interested in how much data-driven techniques can be used for dialogue management.  However, I am concerned that the approach that this paper proposes, is actually not specific to symmetric collaborative tasks, but to tasks that can be represented as graph operations, such as finding an intersection between objects that the two people know about.\nIn Section 2.1, the authors introduce symmetric collaborative dialogue setting. \n However, such dialogs have been studied before, such as Clark and Wilkes-Gibbs explored (Cognition '86), and Walker's furniture layout task (Journal of Artificial Research '00).\nOn line 229, the authors say that this domain is too rich for slot-value semantics.  However, their domain is based on attribute value pairs, so their domain could use a semantics represenation based on attribute value-pairs, such as first order logic.\nSection 3.2 is hard to follow.        The authors often refer to Figure 2, but I didn't find this example that helpful.        For example, for section 3.1, at what point of the dialogue does this represent?  Is this the same after `anyone went to columbia?' ", "label": [[184, 205, "Major_claim"], [207, 299, "Eval_pos_1"], [301, 427, "Eval_neg_1"], [428, 565, "Jus_neg_1"], [648, 694, "Eval_neg_2"], [696, 825, "Jus_neg_2"], [1079, 1109, "Eval_neg_3"], [1117, 1338, "Jus_neg_3"]]}
{"id": 98, "review": "This paper proposes to present a more comprehensive evaluation methodology for the assessment of automatically generated rap lyrics (as being similar to a target artist).  While the assessment of the generation of creative work is very challenging and of great interest to the community, this effort falls short of its claims of a comprehensive solution to this problem.\nAll assessment of this nature ultimately falls to a subjective measure -- can the generated sample convince an expert that the generated sample was produced by the true artist rather than an automated preocess?  This is essentially a more specific version of a Turing Test.   The effort to automate some parts of the evaluation to aid in optimization and to understand how humans assess artistic similarity is valuable.  However, the specific findings reported in this work do not encourage a belief that these have been reliably identified.\nSpecifically -- Consider the central question: Was a sample generated by a target artist?        The human annotators who were asked this were not able to consistently respond to this question.        This means either 1) the annotators did not have sufficient expertise to perform the task, or 2) the task was too challenging, or some combination of the two.   The proposed automatic measures also failed to show a reliable agreement to human raters performing the same task.        This dramatically limits their efficacy in providing a proxy for human assessment.   The low interannotator agreement may be \"expected\" because the task is subjective, but the idea of decomposing the evaluation into fluency and coherence components is meant to make it more tractable, and thereby improve the consistency of rater scores.  A low IAA for an evaluation metric is a cause for concern and limits its viability as a general purpose tool.   Specific questions/comments: - Why is a line-by-line level evaluation prefered to a verse level analysis. \nSpecifically for \"coherence\", a line by line analysis limits the scope of coherence to consequtive lines.\n- Style matching -- This term assumes that these 13 artists each have a distinct style, and always operate in that style. I would argue that some of these artists (kanye west, eminem, jay z, drake, tupac and notorious big) have produced work in multiple styles.  A more accurate term for this might be \"artist matching\".\n- In Section 4.2 The central automated component of the evaluation is low tf*idf with existing verses, and similar rhyme density.  Given the limitations of rhyme density -- how well does this work.  Even with the manual intervention described?\n- In Section 6.2 -- This description should include how many judges were used in this study? In how many cases did the judges already know the verse they were judging?  In this case the test will not assess how easy it is to match style, but rather, the judges recall and rap knowledge. ", "label": [[288, 370, "Major_claim"], [647, 790, "Eval_pos_1"], [792, 925, "Eval_neg_1"], [929, 1847, "Jus_neg_1"]]}
{"id": 99, "review": "paper_summary\nThis paper considers structural bias to be cases where the label can be predicted based on a specific feature (the authors motivate this with the example of hypothesis-only bias in NLI). They quantify this as a delta measure: the difference between accuracy on the standard test set and a \"hard\" test set. The latter is composed of all the wrongly predicted examples by a biased model. It then proposes to frame classification problems as generation problems and claims that a generative model with a uniform prior can overcome this bias. However, empirical results show that this is achieved at the expense of classification accuracy. The authors demonstrate  by finetuning the generative model as a classifier, however this reintroduces bias according to their delta metric. \nsummary_of_strengths\n1. Interesting idea with theoretical grounding. \n2. Clear and well-written. \n3. Strong empirical results: 5-10 point reduction in the metric used to measure bias. Is there a difference between in-domain and out-of-domain performance for MNLI? \nsummary_of_weaknesses\n1. The authors claim that this is a novel formulation of the NLI task but cite a prior paper that formulates this task the same way. The novelty is in the generative implementation and the scope of the claim should be reduced. \n2. I might have missed this, but the actual implementation of how p(y|B) is set to a uniform distribution during inference is not described. I would be happy to retract this point if the other reviewers saw this described somewhere. Additionally, how is training with a uniform prior actually implemented (Table 3)? \n3. One experiment I would like to see (and is missing at the moment) is a performance comparison of all models on challenge sets such as ANLI since they were constructed specifically to weed out models that perform well on the original test set using spurious features. \n4. Is there a reason for using only BERT over RoBERTa since the latter is generally a stronger baseline? Could a model pretrained on a larger and more diverse dataset be less prone to structural bias? \n5. The authors do not state if the code will be released. \ncomments,_suggestions_and_typos\nL50: \"Furthermore, models that contain such biases may make surprising predictions when the bias is present, causing problems in critical systems.\": Such as?\nL291: Is BERT trained with forward masks when using the autoregressive loss? ", "label": [[816, 861, "Eval_pos_1"], [865, 889, "Eval_pos_2"], [893, 918, "Eval_pos_3"], [919, 975, "Jus_pos_3"], [1212, 1306, "Eval_pos_1"]]}
{"id": 104, "review": "paper_summary\nThe paper tackles the challenge of long documents/dialogs summarizations. \nIt presents a simple but efficient approach \"split-then-summarize\" using a greedy algorithm for choosing the good partition.\nThe experiments in the second part of the paper show a real improvement using this generic algorithm (agnostic to the used model)  over the naive baseline. \nsummary_of_strengths\n- The challenge that this paper tends to solve is a painful problem and the proposed solution is simple to implement and yields to good results.\n- The paper is well written and easy to follow and understand.\n- The set of experiments proving the efficiency of the method is good. ( not optimal) \nsummary_of_weaknesses\n- In the related work, you cite several approaches - It would be fair to compare the SummN results with the implementation of the reported approaches. \nComparing SummN to the naive Baseline (Bart) is obviously better, but what is the benefit of SummN over the previous solutions ? \ncomments,_suggestions_and_typos\nIn the Target segmentation, it is not clear, what can we do when the number of sentences is lower than K ? \nIn section 5.6, could you please provide the Kappa ? ", "label": [[394, 460, "Eval_pos_1"], [464, 536, "Eval_pos_2"], [539, 599, "Eval_pos_3"], [602, 686, "Eval_pos_4"]]}
{"id": 107, "review": "The paper investigates different pre-trained language models for 'extreme' multilabel classification. The authors also compare against a baseline logistic regression model. The experiments are carried out on the American National Election Study (ANES) dataset, with the task of automatically coding (i.e. classification) answers to open-ended survey responses. The classification task is interesting and challenging, and I think a good example of important computational social science work. Overall the paper was well written and I appreciate the thoughtfulness that has gone in the data processing.\nMy main reasons for recommending a rejection are the following: -Framing: I think the paper sets the wrong expectations, and it tries to do too much by trying to sell the paper as being about multi-label classification in general.  -Not enough analyses to really understand when/why pre-trained language models (don\u2019t) work for this task.\nMore specific comments are below: GOAL OF THE PAPER I would advise not to try to sell the paper as being a general paper about multi-label classification. I think the current title \"Multi-label classification with pre-trained language models\" is too general and sets the wrong expectations. In fact, I wish that the authors had focused more specifically on this particular task (which could have made a very strong paper if done well), rather than trying to sell their paper as a general paper about multi-label classification, as I think it doesn't meet the expectations that the paper is setting.\nIf the paper is really about investigating pre-trained language models for multi-label classification, I would expect multiple datasets and in-depth analyses of when/why pre-trained language models work for multi-label classification.\nThe authors aim to sell the ANES dataset as a useful dataset for NLP researchers to look at. But this motivation could be stronger by explaining more clearly how this dataset differs from current multi-label datasets (including the ones that are out there but not included in public leaderboards). What makes this particular dataset, or the goal of using it for social science, challenging? \u201c Because of the resulting structure (a text + multiple labels), this type of data presents an interesting type of task (multi-label classification) for NLP models\u201d isn\u2019t really convincing.\nANALYSES The technical contribution is limited. The overall setup is very similar to Card and Smith (2015), which they cite heavily, but the authors investigate various pre-trained language models (using default parameters). The results aren\u2019t great (often outperformed by the baseline). This doesn't have to be a reasons to reject the paper, as it can be a very interesting data point to understand when to choose which model. However, the paper was lacking in-depth analyses to understand -why- the results weren\u2019t good. The main take away I got was that the limited dataset sizes were a challenge, but this could have been investigated more systematically (e.g. by varying the amount of training data for the same dataset). An in-depth error analysis could have also shed more light on this.\nADDITIONAL COMMENTS -Pay attention to (opening) quotation marks -Why the choice for logistic regression without regularization? Even as a baseline, regularization (e.g. L1, L2 or elastic net) is very common and usually much more effective.\n-It was really good to see that the authors have been careful with all the data processing steps. The steps were very well documented. However, I think at the moment that the balance is a bit off in terms of space (3 pages dedicated to data processing of the ANES dataset). I would suggest moving some of this information (e.g. about the file structure of the dataset, and how to go from numeric codes to a binary vector) to an appendix/supplementary material, to allow for more space to discuss the experiments.\n-\"code sets\" could have been defined more explicitly (I think these are just the set of possible labels/codes? and so all questions with the same possible labels/codes are grouped into one dataset?)\n-I didn't understand \"The latter has been assigned multiple times, as there were many more code columns available then needed\" Why not leave the remaining columns empty?\n-As the idea is to make the dataset available to other researchers, I'd recommend also having a development split (the current dataset only contains fixed train/test splits), to ensure that model development and parameter tuning in future research isn't done on the test set.  -\"Verbatim-level averaged \" what do you mean with this?\n-Section 5: are the differences significant? ", "label": [[361, 415, "Eval_pos_1"], [421, 490, "Eval_pos_2"], [492, 526, "Eval_pos_3"], [531, 599, "Eval_pos_4"], [601, 663, "Major_claim"], [666, 721, "Eval_neg_1"], [726, 830, "Jus_neg_1"], [834, 939, "Eval_neg_2"], [1095, 1230, "Eval_neg_3"], [1231, 1773, "Jus_neg_3"], [1774, 2071, "Eval_neg_4"], [2072, 2354, "Jus_neg_4"], [2364, 2402, "Eval_neg_5"], [2403, 2642, "Jus_neg_5"], [2783, 2877, "Eval_neg_6"], [2878, 3149, "Jus_neg_6"], [3391, 3524, "Eval_pos_5"], [3525, 3663, "Eval_pos_6"], [3664, 3902, "Jus_pos_6"], [3904, 3955, "Eval_neg_7"], [3956, 4101, "Jus_neg_7"]]}
{"id": 108, "review": "paper_summary\nThis paper proposes a method based on Optimal Transport (OT) to simultaneously incorporate the interaction between syntax and semantics as part of the content preservation objective for Text Style Transfer (TST) training. The intuition is based upon the hypothesis that semantically related words in both sentences should have the same syntactic importance too. Since two differently styled sentences can have different number of words causing similar words to appear at another levels, the authors propose to use OT for computing the optimal method for transitioning from one distribution to the other.\nThe authors evaluate the proposed method on three benchmark datasets using supervised and unsupervised settings and claim to achieve new SoTA results. \nsummary_of_strengths\n1. SoTA results on all four experimental directions : Informal <-> Formal, Informal -> Formal, Informal <-> Formal & Combined Domains, BLEU evaluated against the first reference. \n2. Ablation experiments for proving the efficacy of the proposed OT based method. \n3. Corroboration of the results with human evaluation \nsummary_of_weaknesses\n1. Lack of analysis to test the statistical stability of the proposed method (i.e. considering multiple generated hypotheses and comparing the score variability). \ncomments,_suggestions_and_typos\nTypos: L504: Compared to* the baselines ", "label": [[1134, 1207, "Eval_neg_1"], [1209, 1291, "Jus_neg_1"]]}
{"id": 112, "review": "paper_summary\nThis paper mainly investigates prompt-based finetuning in few-shot learning. It demonstrates two strategies that can reduce the requirement for prompts design and memory cost: null prompts tuning and that it can be made memory efficient by finetuning only the bias terms. \nsummary_of_strengths\n1. It demonstrates that prompt-based finetuning can be achieved via null prompts, that alleviates the effort of designing prompt patterns across different tasks/datasets, which is more generalizable, and questions the requirement of well designed prompt patterns. \n2. It demonstrates that by only finetuning the bias term, the finetuning process can be highly memory efficient, with only updating 0.1% of the parameters. \nsummary_of_weaknesses\n1. From the results in Table 2, it shows that in ALBERT, with null prompt, the performance largely decreases (from 8 #Wins to 1), which raises the question of the necessity and robustness of null prompting. Moreover, it shows that prompt-based finetuning with BitFit can achieve comparable or even better performance, with memory efficiency introduced. It suggests that only using BitFit is a more wise choice. \ncomments,_suggestions_and_typos\n1. It may be better to conduct more experiments across different models. \n2. It may be better to analyze the principle of the success of null prompts instead of only experimental evaluations. ", "label": [[1199, 1269, "Eval_neg_1"], [1273, 1388, "Eval_neg_2"]]}
{"id": 114, "review": "paper_summary\nThis paper introduces a large human-annotated Chinese predicate-argument dataset MuPAD that covers six domains. Different from the previous Chinese SRL dataset, MuPAD annotates hidden arguments (subject and object), which is a common phenomenon in Chinese and is useful for understanding semantics. The corpus construction contains a detailed annotation guideline and the annotation process includes double annotations and additional revision from the third expert annotator, which ensures the annotation quality. Additionally, this paper applies a frameless annotation strategy. To investigate the domain adaptation performance, the paper conducts a biaffine baseline and a multi-task learning model on top of the baseline model, which are the first scores on this new benchmark. \nsummary_of_strengths\n1. This paper provides a clear, high-quality annotation process for building up a predicate-argument dataset and it firstly introduces non-canonical texts, e.g., ZX, PB, PC, into the SRL task. \n2. The paper presents the model performance on this new cross-domain benchmark and provides a reasonable analysis of the results. \nsummary_of_weaknesses\n1. Though experts have higher annotator agreement, they are not guaranteed to perform 100% accuracy. What is the consistency score between expert annotators? \n2. The consistency and accuracy are confusing. What is the difference between arg-wise consistency and accuracy? The intuition is that given the low pred-wise consistency, the label annotation agreement should be even lower because all labels related to inconsistent predicates are different. \n3. Are the consistency and accuracy scores calculated before or after experts' revision? \ncomments,_suggestions_and_typos\n1. L70-71, researches ... focus ... make -> research ... focuses ... makes 2. L389, augment -> argument ", "label": [[820, 925, "Eval_pos_1"], [1014, 1139, "Eval_pos_2"], [1326, 1369, "Eval_neg_1"], [1370, 1616, "Jus_neg_1"]]}
{"id": 116, "review": "paper_summary\nThis paper proposes an approach for few-shot text classification. The authors propose a Siamese Network to encode the input text and a textual representation of a label. Once encoded, the cosine similarity of the two encodings is computed and it's used in a Textual Entailment like task to decide the label of the input. The main contribution of the paper is in the Label Tuning technique, which allows to 1) re-use the architecture for multiple tasks and ii) to save computational cost at inference time. \nsummary_of_strengths\nThe paper is in a well-know area in Computational Linguistics which is gaining attention in the past years. The paper introduces an interesting perspective on the usage of Cross Attention models for few-shot settings, demonstrating that a much simpler (and faster approach) can achieve a comparable accuracy in many different tasks/datasets. The authors show multiple results in English and multi-lingual settings that seems convincing. \nsummary_of_weaknesses\nThe main weak point of the paper is that at it is not super clear. There are many parts in which I believe the authors should spend some time in providing either more explanations or re-structure a bit the discussion (see the comments section). \ncomments,_suggestions_and_typos\n- I suggest to revise a bit the discussion, especially in the modeling section, which in its current form is not clear enough. For example, in section 2 it would be nice to see a better formalization of the architecture. If I understood correctly, the Label Embeddings are external parameters; instead, the figure is a bit misleading, as it seems that the Label Embeddings are the output of the encoder.\n-Also, when describing the contribution in the Introduction, using the word hypothesis/null hypothesis really made me think about statistical significance. For example, in lines 87-90 the authors introduce the hypothesis patterns (in contrast to the null hypothesis) referring to the way of representing the input labels, and they mention \"no significant\" difference, which instead is referring to the statistical significance. I would suggest to revise this part.\n-It is not clear the role of the dropout, as there is not specific experiment or comment on the impact of such technique. Can you add some details?\n-On which data is fine-tuned the model for the \"Knowledge Distillation\" -Please, add an intro paragraph to section 4.\n-The baseline with CharSVM seems disadvantaged. In fact, a SVM model in a few-shot setting with up to 5-grams risks to have huge data-sparsity and overfitting problems. Can the authors explain why they selected this baseline? Is a better (fair) baseline available?\n-In line 303 the authors mention \"sentence transformers\". Why are the authors mentioning this? Is it possible to add a citation?\n-There are a couple of footnotes referring to wikipedia. It is fine, but I think the authors can find a better citation for the Frobenius norm and the Welch test.\n-I suggest to make a change to the tables. Now the authors are reporting in bold the results whose difference is statistical significant. Would it be possible to highlight in bold the best result in each group (0, 8, 64, 512) and with another symbol (maybe underline) the statistical significant ones? \n- ", "label": [[650, 759, "Eval_pos_1"], [760, 882, "Jus_pos_1"], [884, 978, "Eval_pos_2"], [1002, 1068, "Eval_neg_1"], [1069, 1247, "Jus_neg_1"], [1282, 1406, "Eval_neg_2"], [1407, 1683, "Jus_neg_2"], [2150, 2190, "Eval_neg_3"], [2191, 2296, "Jus_neg_3"], [2416, 2462, "Eval_neg_4"], [2463, 2679, "Jus_neg_4"]]}
{"id": 117, "review": "paper_summary\nIn this work, the authors propose a method to improve the prediction of clinical outcomes from clinical notes text by finding relevant evidence from documents in a database and augmenting the input. They mainly experiment with three prediction tasks: ventilation prediction, mortality prediction, and length of stay prediction and propose different ways to combine and fuse the evidence information from documents. Results show some improvements over simple baseline models that use only clinical text.\nThe central architecture of the paper's proposed approaches consists of a sparse-dense retriever and reranker to select relevant documents, and an aggregation strategy to encode the selected documents along with the clinical notes. While the model components are all previously explored, it is interesting to see their application in a clinical outcome prediction setting with notes.\nThe paper is clear in terms of the problem motivation and goals. However, there are some missing details from the explanation of the task setup and the results are slightly unconvincing due to there not being one clear approach (out of four averaging and voting strategies to combine documents). Furthermore, the performance scores are not as high as some other approaches in literature (Rajkomar et al.) that use structured clinical data such as ICD codes in addition to clinical notes. So the much higher additional effort to obtain evidence instead of relying on other structured data in a patient's EHR is not justified. \nsummary_of_strengths\n1. The paper is clear in terms of problem motivation and goals. \n2. The experimental setup in terms of the variety of tasks chosen and the analysis is good. \n3. I like the human evaluation of the evidence analysis. \nsummary_of_weaknesses\n1. The task setup is not described clearly. For example, which notes in the EHR (only the current admission or all previous admissions) do you use as input and how far away are the outcomes from the last note date? \n2. There isn't one clear aggregation strategy that gives consistent performance gains across all tasks. So it is hard for someone to implement this approach in practice. \ncomments,_suggestions_and_typos\n1. Experimental setup details: Can you explain how you pick which notes from the patient's EHR do you use an input and how far away are the outcomes from the last note date? Also, how do you select the patient population for the experiments? Do you use all patients and their admissions for prediction? Is the test set temporally split or split according to different patients? \n2. Is precision more important or recall? You seem to consider precision more important in order to not raise false alarms. But isn't recall also important since you would otherwise miss out on reporting at-risk patients? \n3. You cannot refer to appendix figures in the main paper (line 497). You should either move the whole analysis to appendix or move up the figures. \n4. How do you think your approach would compare to/work in line with other inputs such as structured information? AUCROC seems pretty high in other models in literature. \n5. Consider explaining the tasks and performance metrics when you call them out in the abstract in a little more detail. It's a little confusing now since you mention mortality prediction and say precision@topK, which isn't a regular binary classification metric. ", "label": [[749, 900, "Eval_pos_1"], [901, 965, "Eval_pos_2"], [966, 1044, "Eval_neg_1"], [1049, 1128, "Eval_neg_2"], [1129, 1195, "Jus_neg_2"], [1197, 1388, "Jus_neg_3"], [1389, 1526, "Eval_neg_3"], [1551, 1611, "Eval_pos_3"], [1616, 1704, "Eval_pos_4"], [1709, 1762, "Eval_pos_5"], [1789, 1829, "Eval_neg_4"], [1830, 2001, "Jus_neg_4"], [2005, 2105, "Jus_neg_5"], [2106, 2172, "Eval_neg_5"]]}
{"id": 119, "review": "- Strengths: 1. Interesting research problem 2. The method in this paper looks quite formal. \n3. The authors have released their dataset with the submission. \n4. The design of experiments is good.\n- Weaknesses: 1. The advantage and disadvantage of the transductive learning has not yet discussed.\n- General Discussion: In this paper, the authors introduce a transductive learning approach for Chinese hypernym prediction, which is quite interesting problem. The authors establish mappings from entities to hypernyms in the embedding space directly, which sounds also quite novel. This paper is well written and easy to follow. \nThe first part of their method, preprocessing using embeddings, is widely used method for the initial stage. But it's still a normal way to preprocess the input data. The transductive model is an optimization framework for non-linear mapping utilizing both labeled and unlabeled data. The attached supplementary notes about the method makes it more clear. The experimental results have shown the effectiveness of the proposed method in this paper. The authors also released dataset, which contributes to similar research for other researchers in future. ", "label": [[16, 44, "Eval_pos_1"], [48, 92, "Eval_pos_2"], [162, 196, "Eval_pos_3"], [214, 296, "Eval_neg_1"], [458, 579, "Eval_pos_4"], [580, 627, "Eval_pos_5"], [913, 983, "Eval_pos_6"], [984, 1075, "Eval_pos_7"]]}
{"id": 125, "review": "This paper describes four methods of obtaining multilingual word embeddings and a modified QVEC metric for evaluating the efficacy of these embeddings. The embedding methods are:  (1) multiCluster : Uses a dictionary to map words to multilingual clusters. \nCluster embeddings are then obtained which serve as embeddings for the words that reside in each cluster.  (2) multiCCA : Extends the approach presented by Faruqui and Dyer (2014) for embedding bilingual words, to multilingual words by using English embeddings as the anchor space. Bilingual dictionaries (other_language -> English) are then used to obtain projections from other monolingual embeddings for words in other languages to the anchor space.  (3) multiSkip : Extends the approach presented by Luong et al. (2015b) for embedding using source and target context (via alignment), to the multilingual case by extending the objective function to include components for all available parallel corpora.  (4) Translation invariance : Uses a low rank decomposition of the word PMI matrix with an objective with includes bilingual alignment frequency components. May only work for  bilingual embeddings.  The evaluation method uses CCA to maximize the correlation between the word embeddings and possibly hand crafted linguistic data. Basis vectors are obtained for the aligned dimensions which produce a score which is invariant to rotation and linear transformations. The proposed method also extends this to multilingual evaluations.  In general, the paper is well written and describes the work clearly. A few major issues: (1) What is the new contribution with respect to the translation invariance embedding approach of Gardner et al.? If it is the extension to multilingual embeddings, a few lines explaining the novelty would help.  (2) The use of super-sense annotations across multiple languages is a problem. \nThe number of features in the intersection of multiple languages may become really small. How do the authors propose to address this problem (beyond footnote 9)?\n(3) How much does coverage affect the score in table 2? For example, for dependency parsing, multi cluster and multiCCA have significantly different coverage numbers with scores that are close.  (4) In general, the results in table 3 do not tell a consistent story. Mainly, for most of the intrinsic metrics, the multilingual embedding techniques do not seem to perform the best.  Given that one of the primary goals of this paper was to create embeddings that perform well under the word translation metric (intra-language), it is disappointing that the method that performs best (by far) is the invariance approach. It is also strange that the multi-cluster approach, which discards inter-cluster (word and language) semantic information performs the best with respect to the extrinsic metrics.\nOther questions for the authors: (1) What is the loss in performance by fixing the word embeddings in the dependency parsing task? What was the gain by simply using these embeddings as alternatives to the random embeddings in the LSTM stack parser?  (2) Is table 1 an average over the 17 embeddings described in section 5.1?  (3) Are there any advantages of using the multi-Skip approach instead of learning bilingual embeddings and performing multi-CCA to learning projections across the distinct spaces?\n(4) The dictionary extraction approach (from parallel corpora via alignments or from google translate) may not reflect the challenges of using real lexicons. \nDid you explore the use of any real multi-lingual dictionaries? ", "label": [[1496, 1565, "Eval_pos_1"], [1803, 1877, "Eval_neg_1"], [1879, 2040, "Jus_neg_1"], [2240, 2306, "Eval_neg_2"], [2307, 2837, "Jus_neg_2"]]}
{"id": 127, "review": "paper_summary\nThis paper presents a method for identifying event types and their arguments in a low-resource scenario, i.e., with a limited amount of training examples. The method uses distant supervision signals given in the form of templates that define event types and their arguments that were manually curated based on the definition of events in the ACE dataset. \nThe authors demonstrate the impressive ability of their model to outperform the current sota and baselines in the low-resource scenario and comparable ability in the high-resource scenario. They also perform ablation studies to emphasize the contribution of every suggested component.  Although this is the paper summary section and criticism should not appear here - I feel obligated to mention here that the authors did not report any statistical analysis of their result! This makes the entire result section meaningless and I implore the authors to add the significance testing results to their paper before publication. \nsummary_of_strengths\n- A new model for event and argument detection and typing that uses distant supervision signals in the form of additional prompt templates.\n-The model presents impressive performance (though its impressiveness is questionable until statistical analysis results will be presented) and comparable scores to sota and strong baseline models.\n-The paper is well written and detailed. \nsummary_of_weaknesses\nThe main weakness is that this method can only be applied to the event types and argument roles defined in the ACE dataset (which are not a lot). There are newer event-type datasets such as MAVEN (https://aclanthology.org/2020.emnlp-main.129/) that include many more events but cannot be used in this framework because they don't include the type definitions that are required for the prompting design.\nNo statistical analysis of the results is reported although a comparison between models is made. Sentences like \"The performance gap becomes more significant for the extremely low-resource situation.\" should not be written without statistical proof of significance. \ncomments,_suggestions_and_typos\nI would suggest trying to come up with templates based on MAVEN types too, this will cover many more event types. ", "label": [[1158, 1354, "Eval_pos_1"], [1356, 1395, "Eval_pos_2"], [1419, 1564, "Eval_neg_1"], [1565, 1821, "Jus_neg_1"], [1822, 1918, "Eval_neg_2"], [1919, 2087, "Jus_neg_2"]]}
{"id": 136, "review": "- Strengths: - Paper is very well-written and every aspect of the model is well-motivated and clearly explained.\n-The authors have extensively covered the previous work in the area.\n-The approach achieves state-of-the-art results across several text comprehension data sets. In addition, the experimental evaluation is very thorough.\n- Weaknesses: - Different variants of the model achieve state-of-the-art performance across various data sets. However, the authors do provide an explanation for this (i.e. size of data set and text anonymization patterns).\n- General Discussion: The paper describes an approach to text comprehension which uses gated attention modules to achieve state-of-the-art performance. Compared to previous attention mechanisms, the gated attention reader uses the query embedding and makes multiple passes (multi-hop architecture) over the document and applies multiplicative updates to the document token vectors before finally producing a classification output regarding the answer. This technique somewhat mirrors how humans solve text comprehension problems. Results show that the approach performs well on large data sets such as CNN and Daily Mail. For the CBT data set, some additional feature engineering is needed to achieve state-of-the-art performance.  Overall, the paper is very well-written and model is novel and well-motivated. \nFurthermore, the approach achieves state-of-the-art performance on several data sets.  I had only minor issues with the evaluation. The experimental results section does not mention whether the improvements (e.g. in Table 3) are statistically significant and if so, which test was used and what was the p-value. Also I couldn't find an explanation for the performance on CBT-CN data set where the validation performance is superior to NSE but test performance is significantly worse. ", "label": [[15, 112, "Eval_pos_1"], [114, 181, "Eval_pos_2"], [183, 274, "Eval_pos_3"], [275, 333, "Eval_pos_4"], [1299, 1329, "Eval_pos_5"], [1334, 1367, "Eval_pos_6"], [1457, 1501, "Eval_neg_1"], [1502, 1854, "Jus_neg_1"]]}
{"id": 137, "review": "paper_summary\nThe authors propose a multiple choice task posed over 10 images. These images are quite similar (either frames from the same video, or static images selected to be similar). Given a fine-grained visual description that applies to only one of the images, the task is to predict the correct one. Compared to human accuracy which hovers at ~90% for humans, the CLIP and ViLBERT get only roughly 30% accuracy. \nsummary_of_strengths\nThe task is well-posed and well-motivated: it is essentially a harder version of NLVR2. While NLVR2 is not saturated (SoTA = high 80s, human = high 90s), there's no reason why both of these tasks can't be considered in parallel. The paper is well-written and easy to follow. The experimental sections clearly illustrate the importance of selecting negative examples that are semantically similar during training. The filtration scheme that the authors use to construct their corpus ensures that only pairs humans (generally) agree upon make it into the final train/val/test sets. The error analyses show that questions with more similar images are harder, and shorter sentences are easier. \nsummary_of_weaknesses\nIn my view, there aren't too many shortcomings of this work. One could make the argument that this task is inspired pretty directly by NLVR2, and doesn't really add much beyond that beyond just being a harder version of that corpus. However... That's okay! Not every V+L task needs to test for something completely different than all others before it. One could also make the argument that the language itself is fairly artificial in the sense that these are not the sort of descriptions that would appear in any use case directly. But: the value as a benchmark is nonetheless clear.\nI had a few small technical/presentation suggestions: - It would be nice in table 5 could present the human results   directly, instead of having the reader need to flip back to compare - It would be nice if figure 4 could include human accuracy on these   subsets. \ncomments,_suggestions_and_typos\nOverall, the work straightforwardly builds upon prior efforts like NLVR2 and represents a fair and interesting test for vision+language models. Clearly, given the large gap between human agreement and model performance, there's a long way to go with this task. ", "label": [[442, 484, "Eval_pos_1"], [485, 670, "Jus_pos_1"], [671, 716, "Eval_pos_2"], [717, 854, "Jus_pos_2"], [1155, 1215, "Major_claim"], [1692, 1738, "Eval_pos_3"], [2038, 2181, "Eval_pos_4"]]}
{"id": 138, "review": "This paper presents a method for semantic role labelling based on the integration of syntactic information coming from heterogenous syntactic representations. More precisely, the proposal consists in extracting dependency information from treebanks using what authors call different annotation guidelines. It seems to me that there is at this stage a misunderstanding: the difference between the different treebanks mentioned in the paper relies on different formalisms, some are phrase-structure, others are dependency based. Moreover, there is no precise motivation of the reasons for doing that. There exists for example algorithms for generating homogeneous dependency-based representations from other formalisms. Nothing is said about the type of representation that can be extracted with this approach. The authors propose to join explicit and implicit methods for heterogenous parsing. The same type of problem appears there: it is not clear how heterogeneity is taken into account, the nature of the nodes being totally different depending on the representation. An experiment is presented, but the discussion does not propose an analysis of the observed improvements. The paper is not well organized and remains often obscure. It is not well motivated and does not really explains the impact of the method. ", "label": [[306, 368, "Eval_neg_1"], [369, 808, "Jus_neg_1"], [893, 932, "Eval_neg_2"], [933, 1176, "Jus_neg_2"], [1177, 1235, "Eval_neg_3"], [1236, 1315, "Eval_neg_4"]]}
{"id": 142, "review": "paper_summary\nThe authors propose Prix-LM, a unified multilingual representation model that can capture, propagate and enrich knowledge in and from multilingual KBs. Prix-LM is trained via a casual LM objective, utilizing monolingual knowledge triples and cross-lingual links. It embeds knowledge from the KB in different languages into a shared representation space, which benefits transferring complementary knowledge between languages. Comprehensive experiments demonstrate the effectiveness and robustness of Prix-LM for automatic KB construction in multilingual setups. \nsummary_of_strengths\n1. The authors leverage two types of knowledge, monolingual triples and cross-lingual links, extracted from existing multilingual KBs, and tune a multilingual language encoder XLM-R via a causal language modeling objective. Prix-LM integrates useful multilingual and KB-based factual knowledge into a single model. \n2. The authors evaluate Prix-LM model on four different tasks essential for automatic KB construction, covering both high-resource and low-resource languages. The main results across all tasks indicate that Prix-LM brings consistent and substantial gains over various state-of-the-art methods, demonstrating its effectiveness. \nsummary_of_weaknesses\nThe languages in this paper are some rich-resourced languages indeed, the authors need to test Prix-LM model on some low-resourced languages. \ncomments,_suggestions_and_typos\nSee Weaknesses ", "label": [[1072, 1239, "Eval_pos_1"]]}
{"id": 148, "review": "The paper presents what the authors call Definition Frames (DF), representations that combine word embeddings with explicit relations between concepts. Differently from previous approaches that integrate external knowledge into word embeddings (e.g. retrofitting word embeddings to a semantic network), DF retains the explicit relations that characterize concept. The authors follow the ideas brought forward by Pustejovsky and his work on qualia structures. Given a concept, Pustejovsky theorizes that such concept can be fully described by a set of attributes/properties. While in traditional word embedding techniques each word is represented by a single vector, DF represents each word with a matrix where each row is a dense representation of one of its attributes/properties (e.g. a row for \"IsA\", another one for \"PartOf\", etc.). The authors show the effectiveness of their proposed approach in word similarity and word relatedness.\nThe paper is well-written and very easy to follow. I really enjoyed the work: it is a rather simple approach backed up by very interesting linguistic theories, a combination that is not easy to find in most of the NLP papers today. In general, I appreciated this work and I think it deserves a place in a conference such as COLING.\nSaid that, the fact that I appreciated this work is one of the reasons I expected more and was let down by some parts of the paper. First, I think that a paper on word embeddings in 2020 must include some kind of comparison with current contextualized word embedding techniques, say BERT or ELMo. Of course, word similarity is not the best task to compare DF against BERT-like embeddings. However, I still believe that BERT could have been easily integrated into DF itself to obtain a BERT-based DF. For example, the Basis row/vector of a word w could be the BERT state corresponding to the CLS token of the WordNet definition of w, while the qualia dense representation could be the average BERT state of the qualia in the ConceptNet sentences that define the relations of w. Since the authors already make use of WordNet and ConceptNet, I think this would have cost little extra work for a much stronger and up-to-date paper.\nAnother issue of the paper is the evaluation. Word similarity and relatedness, while traditional NLP tasks, are still intrinsic evaluations. I understand this is a short paper, but I feel like one of the two tasks (similarity or relatedness) could have been sacrificed for a downstream task. I would also suggest not to show just the average Spearman correlation coefficient of several similarity datasets, as this makes it more difficult for the reader to compare DF to other word embedding techniques.\nReasons to accept: -DF is a perfect fit for COLING as it combines strong linguistic theories with existing distributional approaches.\n-The authors present a clever way to take advantage of knowledge resources (WordNet and ConceptNet) and word embeddings.\n-Since the resulting representations retain the qualia structure of the represented words, they are more interpretable than traditional word embeddings.\nReasons to reject: -The evaluation is not strong as it only comprises word similarity and relatedness, two intrinsic tasks.\n-The authors do not include BERT or BERT-like contextualized embeddings. As time passes, this may become a stronger reason to reject this work.\nOther (minor) comments: -In case of accept, please include in the appendix the individual results on each word similarity/relatedness dataset (helps reproducibility).\n-The font in Figures 2 and 3 in Appendix D is too small. ", "label": [[940, 990, "Eval_pos_1"], [991, 1016, "Eval_pos_2"], [1018, 1171, "Jus_pos_2"], [1172, 1271, "Major_claim"], [1404, 1568, "Eval_neg_1"], [1569, 2199, "Jus_neg_1"], [2200, 2245, "Eval_neg_2"], [2246, 2703, "Jus_neg_2"], [2724, 2754, "Eval_pos_3"], [2755, 2837, "Jus_pos_3"], [2839, 2958, "Eval_pos_4"], [2960, 3049, "Jus_pos_5"], [3049, 3111, "Eval_pos_5"], [3132, 3160, "Eval_neg_3"], [3161, 3235, "Jus_neg_3"]]}
{"id": 150, "review": "The paper presents a research aiming to create larger training and evaluation sets of data in translation enterprises that have English on one side and dialects of Arabic (Egyptian and Levantine), known as resource-poor, on the other. Supplementary parallel data are generated by a bootstrapping technique applied to an original set of English sentences, acquired from more sources, and their human translations in Modern Standard Arabic (MSA) and two of its dialects. To this original set, a Byte-Pair Encoding with a carefully chosen fixed vocabulary is applied and then the training corpus is augmented via a bootstrapping approach. The paper is important as an example of successful adaptation of known MT techniques for pairs of languages in which one is resource-poor and for offering to the research community a 4-way benchmark dataset between Egyptian, Levantine, MSA and English.\nTechnical details are clean and prove an intimate knowledge of the fields of NN and MT. English is accurate.  The plot displaying the correlation between the size of the vocabulary and the BLEU score of translations, combined with the authors\u2019 comments related to the dimension of the training set, suggest the use of a 3-dimensional plot.  As far as I understand, bootstrapping original texts can occasionally lead to syntactically incorrect sentences. If this is the case, care should be taken in using the BLUE measure, because comparing syntactically incorrect data with their translations is not what we usually want, since it may result in slightly too optimistic conclusions.  Incomplete reference with wrong year: \u201cRico Sennrich, Barry Haddow, and Alexandra Birch. 2015. Neural machine translation of rare words with subword units.\u201d The correct reference is the following: \u201cRico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural Machine Translation of Rare Words with Subword Units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016), Berlin, Germany.\u201d\nOther incomplete references:  -\u201cYinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, and Luke Zettlemoyer. 2020. Multilingual denoising pre-training for neural machine translation.\u201d\n-\u201cAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need.\u201d\nAlso, pages are missing for a number of references. ", "label": [[636, 658, "Eval_pos_1"], [659, 888, "Jus_pos_1"], [889, 976, "Eval_pos_2"], [977, 997, "Eval_pos_3"], [1573, 1609, "Eval_neg_1"], [1611, 2008, "Jus_neg_1"], [2009, 2036, "Eval_neg_2"], [2040, 2393, "Jus_neg_2"]]}
{"id": 159, "review": "paper_summary\nThe paper focuses on exploring the role of context in determining the humans\u2019 (annotators) perception of hate or counter-hate in social media comments. Further, the authors also investigate if the context helps improve the models that detect hate speech or counter hate. The authors aggregate a corpus of 6,846 text pairs containing \u2014 (a) the context referred as \u201cparent\u201d and the comment to be classified referred to as \u201ctarget\u201d, and (b) annotations indicating whether each target is hate/counter-hate/neutral. Experiments demonstrate the benefits of incorporating context and training context-aware neural models towards better detection of hate speech in text. \nsummary_of_strengths\nThe paper is written well. \nThe paper introduces an annotated dataset which could be useful for future research. \nThe authors perform a detailed analysis of the collected data and the results as well. \nsummary_of_weaknesses\n1. The importance of context is well known and well-established in several prior work related to hate speech. While the paper cites works such as Gao and Huang, 2017 and Vidgen, et al., it just mentions that they don\u2019t identify the role of context in annotation or modeling. The former definitely considers its role for modeling and the latter incorporates it in the annotation phase. Though this work performs analysis of corpus and model to study the role of the context, the claim of being the first work to establish the importance of context may be a little stretched. \n2. Table 2 includes several work but drops out Vidgen et al, 2021, which might be really similar to the dataset presented in this work though the size varies significantly here. So, why is this dataset not used as a potential benchmark for evaluation (for investigating the role of context in detection of hate) as well? \n3. Though MACE can be used to assess the competent annotators and eliminate redundant annotators, it could be challenging to use when it involves most ambiguous content. \n4.  Some of the analysis and results discussed  (for eg. section 6) might be specific to the tested Roberta model. More experiments using different architectures are needed to determine if the findings and errors that arise are consistent across different models and different settings. \ncomments,_suggestions_and_typos\nClaim of being the first one to recognize the importance of context might be too stretched. \nMore experiments with multiple runs with different random seeds for the dataset split will help report the mean score and the standard deviation. This will help us understand the sensitivity of the model to the data split or order of training etc. ", "label": [[699, 725, "Eval_pos_1"], [727, 811, "Eval_pos_2"], [813, 899, "Eval_pos_3"], [926, 1307, "Jus_neg_1"], [1308, 1496, "Eval_neg_1"], [1995, 2105, "Eval_neg_2"], [2106, 2277, "Jus_neg_2"], [2311, 2402, "Eval_neg_3"]]}
{"id": 161, "review": "The paper explores the application of active learning to modern NLP architectures based on the BERT Transformer paradigm. The goal of AL in this context is to validate if AL pooling strategies allow training those models in use cases with scarce availability of labelled examples. \nTo demonstrate that AL helps on training in such scenarios in terms of accuracy and stability, they implement two architectures: one bare BERT and another more complex that includes stacking all inner representations of BERT and applying a CNN before the output layers. \nThe paper includes experiments applied to some GLUE tasks, including text entailment or sentiment analysis. They compare two strategies for acquiring new data to label: random selection and BALD, an acquisition function based on MC dropout that explores model uncertainty associated with each unlabelled sample.\nBeyond analysing the role of AL, the article analyses if freezing some layers of the pre-trained model during the fine-tuning process allows improving the accuracy. By freezing some layers at lower and higher stages of the BERT model, they compare the accuracies obtained for different GLUE tasks.\nEven though it is true that it might novel to study it when applied to BERT models, I recommend the authors to take a look to \"Practical Obstacles to Deploying Active Learning\", by David Lowell et al, where they analyse the role of AL in NLP tasks. In this paper, they complain about the lack of robustness of AL methods in NLP tasks. In the present article, I miss the most basic acquisition function used in AL: uncertainty sampling, that does not need any further forward pass. It would be interesting to compare the performance of such a simple heuristic for selecting the elements to query. It might provide an explanation for the qualitative analysis included based on the ambiguity of the neutral examples.\nBeyond this metric, I would also encourage the authors to review other methods of AL that take into account the diversity of the selected elements, especially in methods like BERT that are batch trained. In those methods, they try to ensure that elements included in each batch are diverse enough to enrich the training process: Zhdanov, Fedor. \u201c Diverse mini-batch Active Learning.\u201d arXiv preprint arXiv:1901.05954 (2019). \n[1] Du, Bo, et al. \u201cExploring representativeness and informativeness for active learning.\u201d IEEE transactions on cybernetics 47.1 (2015): 14\u201326. \nOne thing that is quite confusing is the analysis of AL results in section 4.2. It is unclear if the size of the subset U was optimized for obtaining the best results when applying BALD as the acquisition function. How did you perform this optimization?\nIn general terms, results for both the AL and the layer freezing experiments present a high variance and depend a lot on the task.  When looking at NLI tasks, it is hard to tell that using BALD is beneficial enough for the higher complexity that it adds to the training process. Why did you not include more tasks to show that the conclusions extracted truly generalize? ", "label": [[1498, 1575, "Eval_neg_1"], [1577, 1876, "Jus_neg_1"], [1877, 2024, "Eval_neg_2"], [2025, 2301, "Jus_neg_2"], [2447, 2526, "Eval_neg_3"], [2527, 2700, "Jus_neg_3"], [2701, 2831, "Eval_neg_4"], [2833, 3071, "Jus_neg_4"]]}
{"id": 163, "review": "This paper presents the gated self-matching network for reading comprehension style question answering. There are three key components in the solution:  (a) The paper introduces the gated attention-based recurrent network to obtain the question-aware representation for the passage. Here, the paper adds an additional gate to attention-based recurrent networks to determine the importance of passage parts and attend to the ones relevant to the question. \nHere they use word as well as character embeddings to handle OOV words. \nOverall, this component is inspired from Wang and Jiang 2016.\n(b) Then the paper proposes a self-matching attention mechanism to improve the representation for the question and passage by looking at wider passage context necessary to infer the answer. This component is completely novel in the paper.\n(c) At the output layer, the paper uses pointer networks to locate answer boundaries. This is also inspired from Wang and Jiang 2016 Overall, I like the paper and think that it makes a nice contribution.\n- Strengths: The paper clearly breaks the network into three component for descriptive purposes, relates each of them to prior work and mentions its novelties with respect to them. It does a sound empirical analysis by describing the impact of each component by doing an ablation study. This is appreciated.\nThe results are impressive!\n- Weaknesses: The paper describes the results on a single model and an ensemble model. I could not find any details of the ensemble and how was it created. I believe it might be the ensemble of the character based and word based model. Can the authors please describe this in the rebuttal and the paper.\n- General Discussion: Along with the ablation study, it would be nice if we can have a qualitative analysis describing some example cases where the components of gating, character embedding, self embedding, etc. become crucial ... where a simple model doesn't get the question right but adding one or more of these components helps. This can go in some form of appendix or supplementary. ", "label": [[963, 1033, "Major_claim"], [1215, 1249, "Eval_pos_2"], [1250, 1320, "Jus_pos_2"], [1321, 1341, "Eval_pos_2"], [1342, 1369, "Eval_pos_3"], [1384, 1525, "Eval_neg_1"], [1527, 1673, "Jus_neg_1"]]}
{"id": 166, "review": "paper_summary\nThis work improves the span-based methods of nested NER.  The authors propose to use the triaffine scoring function to fuse heterogeneous factors, i.e. tokens, boundaries, labels, other spans, to obtain span representations and label scores.  Experiments and ablation studies confirm the effectiveness. \nsummary_of_strengths\n- The paper is well-written and easy to follow.\n- Ablation studies and analyses are thorough. \nsummary_of_weaknesses\n- The authors claimed SOTA in L114-115, however, as acknowledged by authors in L388-389, the proposed method underperforms several SOTA when both using BERT as the encoder.  Other approaches may obtain higher results when using ALBERT,  so the comparison is not fair and I think the claim is somewhat exaggerated.\n-  I do not think Eq. (11) and Eq. (13) are equivalent. Note that in Eq(1-3), every input of the Triaffine function would be fed into an MLP layer firstly, which is NONLINEAR.  Hence the coefficients \\beta_{i, j, g, r} cannot be extracted to the front (in Eq. 13).  The authors said that Eq. (11) can be decomposed to Eq. (12-13) in L277-278, which is WRONG, and so do Eq.(23-28) in Appendix B.  As a consequence, some ablation studies may not faithfully reflect the advantage of the Triaffine function as the decomposition does not hold mathematically - Triaffine mechanisms are widely used. Extending them to heterogeneous factors is somewhat trivial.  Hence technical contributions and novelties are limited.  First, I do not think boundary tokens are heterogeneous to inside tokens. Using an attention mechanism to compute the weight of each token within the span and using their weighted sum to represent the span, known as attention-pooling [1], has been explored before. [ 2] systematically compare different span representations in different tasks. They find that attention-pooling span representation performs the best in NER ( better than end-point representation, i.e. the baseline model of this work), but has a lower performance in many other tasks. So the result of this work is not surprising, since the authors also use attention-pooling to tackle (nested) NER, except that they replace the naive attention with the triaffine attention mechanism. After obtaining the span representation, it is often to concatenate the boundary token representations to the span representation and feed them to an MLP classifier for labeling. The authors use another triaffine instead. The really exciting part of this work is to use triaffine to fuse information from other related spans. To overcome the large computational complexity, the authors propose to (1) prune span, which is often used. ( 2) use decomposition, but the derivation is wrong as I discussed before. Finally, as other reviewers mentioned, this paper does not raise novel issues.\n[1] What do you learn from context? Probing for sentence structure in contextualized word representations. In ICLR 2019.\n[2] A Cross-Task Analysis of Text Span Representations. \ncomments,_suggestions_and_typos\nObtaining powerful span representations is crucial to many span-relation tasks [1, 2]. The authors could evaluate the proposed Triaffine mechanism on these tasks to make it more influential [1] Jiang, et al \"Generalizing Natural Language Analysis through Span-relation Representations\" In ACL2020. \n[2] Toshniwal, et al \"A Cross-Task Analysis of Text Span Representations\" In RepL4NLP 2020 ", "label": [[341, 386, "Eval_pos_1"], [389, 432, "Eval_pos_2"], [458, 691, "Jus_neg_1"], [693, 769, "Eval_neg_1"], [773, 1164, "Jus_neg_2"], [1166, 1322, "Eval_neg_2"], [1325, 1423, "Jus_neg_3"], [1425, 1481, "Eval_neg_3"], [1483, 2032, "Jus_neg_4"], [2033, 2232, "Eval_neg_4"], [2455, 2558, "Eval_pos_3"], [2695, 2740, "Eval_neg_5"], [2742, 2820, "Eval_neg_6"]]}
{"id": 181, "review": "-- Summary The article presents a new method for injecting both synonymy and antonymy relations into static word embeddings, comparably to methods such as Attract-Repel, which is used as a reference in this work. This method first turns the input embeddings into a kind of similarity matrix (more precisely a Gram matrix) which is combined with matrices modulating the initial value according to the kind of relations (synonymy or antonymy) existing in a reference resource between each pair of words. The resulting matrix is finally decomposed into a product of vectors similar to the initial product for building the similarity matrix for obtaining the specialized embeddings. The method is evaluated both intrinsically and extrinsically. The intrinsic evaluation is performed on two recent and large-scale reference datasets - Simlex-999 and SimVerb-3500 - while the extrinsic evaluations focus on two lexical semantics tasks: synonym/antonym classification and lexical simplification. Finally, experiments are presented concerning the robustness of both the proposed method, HRSWE, and Attract-Repel for the word similarity and the synonym/antonym classification tasks, robustness being defined as the impact on results of switching a certain proportion of synonyms into antonyms or the opposite. In all these experiments, results of HRSWE and Attract-Repel are close, sometimes with an advantage for the former, sometimes for the latter.\n-- Strengths - as far as I know, the proposed method is a new approach for solving the target problem whereas all the previous post-processing methods (retrofitting, counter-fitting, attract-repel) are more or less built on the same schema.\n- this new method offers the possibility to control more directly the injection of semantic knowledge into the embeddings by applying linear algebra operations while existing approaches are based on the definition of objective functions between opposite terms that are optimized through SGD, which does not offer a clear view of the applied changes.\n- the article proposes a simple but interesting way, called thesauri contagion, to extent the initial set of synonyms and antonyms.\n- HRSWE is able to outperform Attract-Repel in some experiments and seems to be globally more robust than Attract-Repel. However, the main difference between the two method concerns their speed: HRSWE is much faster than Attract-Repel.\n-- Weaknesses - the main weakness of the paper concerns the results of the proposed method, compared to those of the reference method, Attract-Repel. This weakness is twofold. First, HRSWE is far from outperforming Attract-Repel in all the experiments and moreover, the differences between the two methods are always small. Since no indication is given about the application of statistical tests, we do not know if these differences are significant or not. For instance, a difference of 1.4 points for Spearman correlation on SimVerb-3500 is not necessary a significant difference. Hence, it is difficult from these results to state that one method is actually better than the other. Moreover, they have the same number of hyperparameters. It should also be noted that while HRSWE-3 generally obtains the best results among the different versions of HRSWE, this is not true in the case of lexical simplification, which also contributes a little bit to blur the interpretation of results.\nBut the main issue concerning the evaluations relies on the use of thesauri contagion. As mentioned above, this is an interesting way to expand the reference resource but it could also be applied to Attract-Repel for enlarging the number of the relations it exploits. As a consequence, comparing Attract-Repel without thesauri contagion with HRSWE with thesauri contagion is not actually fair. The paper gives the results of HRSWE without thesauri contagion but that case, except for the lexical simplification task, Attract-Repel is always the best method (we do not know for robustness).\n- the idea of comparing HRSWE and Attract-Repel in terms of robustness is interesting as this kind of aspect is important but rarely addressed. However, the global objective of this evaluation is not very clear since the target perturbation concerns possible errors about synonyms that are falsely considered as antonyms and vice versa. This case is not likely to happen with reference resources since synonyms and antonyms are generally not ambiguous for humans. In that case, ambiguities between synonyms and hypernyms for instance could be more interesting to consider but are out of the scope of the proposed method. The situation would be different with relations automatically extracted from text since distributional approaches for instance are known to have difficulties for distinguishing synonyms and antonyms. But in that case, words that are neither synonyms nor antonyms should also be taken into account as possible perturbations. This is not the experimented settings for antonyms, which come from BabelNet, something that should be mentioned together with the main characteristics of the reference resource (even if it is described in (Mrks\u030cic\u0301 et al., 2017)). The synonyms are more likely to raise such issue since they come from PPDB but the acquisition method in that case is not prone to confuse synonyms and antonyms. Finally, the robustness evaluation could also be justified by the results of thesauri contagion but nothing is said about whether this mechanism produces false relations and how many.\n- HRSWE is much faster that Attract-Repel in the experiments of the paper but since these experiments only focus on the relations involving the vocabulary of the datasets, the number of relations is not very high for each experiment (this number should be indicated). But as HRSWE is based on operations such as eigen decomposition, it is probably less scalable for dealing with large vocabularies, even with the use of randomized algorithms, than methods such as Attract-Repel. Finally, even if HRSWE is faster than Attract-Repel, the durations in both cases are not very high and are not actually problematic.\n-- Specific remarks - related work: the part about post-processing methods is fairly limited, with missing references.\nOf course, the article about retrofitting, which is not cited whereas the word \"retrofitting\" is used: Manaal Faruqui, Jesse Dodge, Sujay Kumar Jauhar, Chris Dyer, Eduard Hovy, and Noah A. Smith. 2015. Retrofitting Word Vectors to Semantic Lexicons. In 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL HLT 2015), pages 1606\u20131615, Denver, Colorado.\nCounter-fitting is a previous work of the authors of Attract-Repel that takes into account both synonyms and antonyms: Nikola Mrk\u0161i\u0107, Diarmuid \u00d3 S\u00e9aghdha, Blaise Thomson, Milica Ga\u0161i\u0107, Lina M. Rojas-Barahona, PeiHao Su, David Vandyke, Tsung-Hsien Wen, and Steve Young. 2016. Counter-fitting Word Vectors to Linguistic Constraints. In 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL HLT 2016), pages 142\u2013148, San Diego, California.\nIt also could have been a method to test in the evaluations.\nParagram is the method that was extended by Attract-Repel for integrating antonyms: John Wieting, Mohit Bansal, Kevin Gimpel, and Karen Livescu. 2015. From Paraphrase Database to Compositional Paraphrase Model and Back. Transactions of the Association for Computational Linguistics, 3:345\u2013358 - hyperparameters for Attract-Repel Intervals of tested values are given for each parameter but the best value is never provided. Moreover, Mrks\u030cic\u0301 et al. (2017) adopted a batch size of 50 while the minimal value is equal here to 64, with the exception of a value of 32 for lexical simplification. The upper bound for the number of epochs could also be higher.\n- hyperparameters for HRSWE The best values of hyperparameters are not given for the lexical simplification task. ", "label": [[1458, 1544, "Eval_pos_1"], [1545, 1682, "Jus_pos_1"], [1686, 1843, "Eval_pos_2"], [1844, 2032, "Jus_pos_2"], [2036, 2165, "Eval_pos_3"], [2167, 2286, "Eval_pos_3"], [2287, 2401, "Eval_pos_4"], [2418, 2551, "Eval_neg_1"], [2552, 3979, "Jus_neg_1"], [3982, 4123, "Eval_pos_4"], [4124, 4190, "Eval_neg_4"], [4191, 5502, "Jus_neg_4"], [6137, 6233, "Eval_neg_5"], [6234, 7893, "Jus_neg_5"]]}
{"id": 185, "review": "- Strengths: The paper offers a natural and useful extension to recent efforts in interactive topic modeling, namely by allowing human annotators to provide multiple \"anchor words\" to machine-induced topics. The paper is well-organized and the combination of synthetic and user experiments make for a strong paper.\n- Weaknesses: The paper is fairly limited in scope in terms of the interactive topic model approaches it compares against. I am willing to accept this, since they do make reference to most of them and explain that these other approaches are not necessarily fast enough for interactive experimentation or not conducive to the types of interaction being considered with an \"anchoring\" interface. Some level of empirical support for these claims would have been nice, though.\nIt would also have been nice to see experiments on more than one data set (20 newsgroups, which is now sort of beaten-to-death).\n- General Discussion: In general, this is a strong paper that appears to offer an incremental but novel and practical contribution to interactive topic modeling. The authors made the effort to vet several variants of the approach in simulated experiments, and to conduct fairly exhaustive quantitative analyses of both simulated and user experiments using a variety of metrics that measure different facets of topic quality. ", "label": [[13, 108, "Eval_pos_1"], [110, 207, "Jus_pos_1"], [208, 235, "Eval_pos_2"], [240, 313, "Eval_pos_3"], [329, 437, "Eval_neg_1"], [438, 787, "Jus_neg_1"], [939, 1078, "Major_claim"]]}
{"id": 186, "review": "paper_summary\nThis paper proposes an explicit future information utilization framework to improve monotonic attention mechanisms in simultaneous machine translation. Specifically, the authors try several approaches to integrate the future information from language modeling into the multi-head monotonic attention. Instead of using future information from language modeling directly in the output layer of the attention model, they transform the monotonic attention mechanism to explicitly use the future information. Several experiments show some improvement over the SoTA multi-head monotonic attention models. \nsummary_of_strengths\nThis paper proposes an approach to incorporate information from language modeling models into simultaneous neural machine translation. The ideas are novel and the experiments show some improvement over the baseline. \nsummary_of_weaknesses\nWe can have more baselines to compare with the proposed models. It should be better if authors compare with some simple combinations of multi-head monotonic attention model and language ( joint training in target sides) or LMs as the re-rank metrics to the predicted target output. \ncomments,_suggestions_and_typos\nIf you finetune XLM in the target language, do we get better results for MMA-XLM? The high accuracy of SLM is because of its training on in-domain data. Therefore we should expect the same thing when fine-tuning XLM in the target language. ", "label": [[770, 789, "Eval_pos_1"], [794, 849, "Eval_pos_2"], [874, 937, "Eval_neg_1"], [938, 1156, "Jus_neg_1"]]}
{"id": 189, "review": "paper_summary\nThe paper \"CBLUE: A Chinese Biomedical Language Understanding Evaluation Benchmark\" presents a benchmark of eight diverse tasks to benchmark models for BioNLP in Chinese. \nThe authors report results of 11 different PLMs and compare them to human performance. \nThe results show that there is still a performance gap between humans and NLP models. \nsummary_of_strengths\n- The authors present a diverse benchmark for BioNLP in Chinese, comprising eight different data sets.  -The benchmark comes with a rich ecosystem of baselines, a submission system and a leaderboard.\n-The authors present an extensive evaluation of different PLMs and give human baseline results. \nsummary_of_weaknesses\n- I am not quite clear on which datasets were produced by the authors and which were prior work. I feel this should be described in more detail in the paper.  -What is the source on the amount of Chinese speakers? statista claims it's 16% of the world's population (https://www.statista.com/chart/12868/the-worlds-most-spoken-languages/).\n-References to the supplementary material/appendix are missing in the main text and should be inserted in the appropriate locations. This is especially important for the data set details, as these are missing from the main text. \ncomments,_suggestions_and_typos\n- L236: Acronym IRB was not introduced -L460: specficanti-toxin -> specific anti-toxin -Alibaba QUAKE is sometimes spelled Alibaba KUAKE, is this on purpose? ", "label": [[384, 484, "Eval_pos_1"], [487, 581, "Eval_pos_2"], [583, 677, "Eval_pos_3"], [703, 797, "Eval_neg_1"], [798, 858, "Jus_neg_1"], [1040, 1172, "Eval_neg_2"], [1173, 1268, "Jus_neg_2"]]}
{"id": 193, "review": "paper_summary\nThis paper argues that analyze contextual embeddings is particularly effective with paraphrases, because it allows experiments to control for meaning which is presumably the same in paraphrases. The authors draw several key conclusions about BERT via this paraphrase-based analysis. \nsummary_of_strengths\nThe idea of controlling for meaning with paraphrases is unique and promising. It is clever in that instead of focusing on the ways in which context should change the meaning of words, it focuses on the specific contexts which should cause the meanings to converge. This allows for more precise control over what we expect should happen. \nsummary_of_weaknesses\nThe meaningfulness of these conclusions strongly depend on PPDB being true paraphrases. If they aren't paraphrases in all dimensions and in the strictness sense, it's unclear that one is truly controlling for meaning. In many example paraphrases shown in the paper, it's clear that one wouldn't not expect representation to be the same for even an ideal representation. For example, \"are mad\" and \"'re out of your mind\" can and often do have different meanings. Even for the highest rated paraphrases such as \"where did they come from?\" vs \"where are they from\" can mean drastically different things, and it's unclear whether a representation that has an identical interpretation of those two phrases is what we really want. \ncomments,_suggestions_and_typos\nI would recommend revisiting strong assumptions being made in this paper. No two sentences mean exactly the same thing, even before discounting noise from PPDB. How these analyses are affected by differences in sentence or phrase meanings should be a big part of the discussion. ", "label": [[319, 396, "Eval_pos_1"], [397, 655, "Jus_pos_1"], [679, 766, "Eval_neg_1"], [767, 1403, "Jus_neg_1"], [1437, 1510, "Eval_neg_2"], [1511, 1715, "Jus_neg_2"]]}
{"id": 194, "review": "- Strengths: *The paper is very well written *It shows how stylometric analysis can help in reasoning-like text classification *The results have important implications for design on NLP datasets *The results may have important implications for many text classification tasks - Weaknesses: *I see few weaknesses in this paper. The only true one is the absence of a definition of style, which is a key concept in the paper - General Discussion: This paper describes two experiments that explore the relationship between writing task and writing style. In particular, controlling for vocabulary and topic, the authors show that features used in authorship attribution/style analysis can go a long way towards distinguishing between 1) a natural ending of a story 2) an ending added by a different author and 3) a purposefully incoherent ending added by a different author.\nThis is a great and fun paper to read and it definitely merits being accepted. \nThe paper is lucidly written and clearly explains what was done and why. The authors use well-known simple features and a simple classifier to prove a non-obvious hypothesis. Intuitively, it is obvious that a writing task greatly constraints style. However, proven in such a clear manner, in such a controlled setting, the findings are impressive.\nI particularly like Section 8 and the discussion about the implications on design of NLP tasks. I think this will be an influential and very well cited paper. Great work.   The paper is a very good one as is. One minor suggestion I have is defining what the authors mean by \u201cstyle\u201d early on. The authors seem to mean \u201ca set of low-level easily computable lexical and syntactic features\u201d.  As is, the usage is somewhat misleading for anyone outside of computational stylometrics.  The set of chosen stylistic features makes sense. However, were there no other options? Were other features tried and they did not work? I think a short discussion of the choice of features would be informative. ", "label": [[14, 44, "Eval_pos_1"], [128, 194, "Eval_pos_2"], [196, 274, "Eval_pos_3"], [290, 420, "Eval_neg_1"], [870, 948, "Major_claim"], [950, 1022, "Eval_pos_4"], [1023, 1124, "Eval_pos_5"], [1125, 1297, "Jus_pos_5"], [1298, 1393, "Eval_pos_6"], [1394, 1506, "Major_claim"], [1507, 1589, "Eval_neg_2"], [1590, 1776, "Jus_neg_2"], [1778, 1914, "Jus_neg_3"], [1915, 1989, "Eval_neg_3"]]}
{"id": 203, "review": "# Paper summary This paper presents a method for learning well-partitioned shared and task-specific feature spaces for LSTM text classifiers. Multiclass adversarial training encourages shared space representations from which a discriminative classifier cannot identify the task source (and are thus generic). The models evaluates are a fully-shared, shared-private and adversarial shared-private -- the lattermost ASP model is one of the main contributions. They also use orthogonality constraints to help reward shared and private spaces that are distinct. The ASP model has lower error rate than single-task and other multi-task neural models. They also experiment with a task-level cross validation to explore whether the shared representation can transfer across tasks, and it seems to favourably. Finally, there is some analysis of shared layer activations suggesting that the ASP model is not being misled by strong weights learned on a specific (inappropriate) task.\n# Review summary Good ideas, well expressed and tested. Some minor comments.\n# Strengths - This is a nice set of ideas working well together. I particularly like the focus on explicitly trying to create useful shared representations. These have been quite successful in the CV community, but it appears that one needs to work quite hard to create them for NLP.\n-Sections 2, 3 and 4 are very clearly expressed.\n-The task-level cross-validation in Section 5.5 is a good way to evaluate the transfer.\n-There is an implementation and data.\n# Weaknesses - There are a few minor typographic and phrasing errors. Individually, these are fine, but there are enough of them to warrant fixing: ** l:84 the \u201cinfantile cart\u201d is slightly odd -- was this a real example from the data? \n** l:233 \u201care different in\u201d -> \u201cdiffer in\u201d ** l:341 \u201cworking adversarially towards\u201d -> \u201cworking against\u201d or \u201ccompeting with\u201d? \n** l:434 \u201ctwo matrics\u201d -> \u201ctwo matrices\u201d ** l:445 \u201care hyperparameter\u201d -> \u201care hyperparameters\u201d ** Section 6 has a number of number agreement errors (l:745/746/765/766/767/770/784) and should be closely re-edited. \n** The shading on the final row of Tables 2 and 3 prints strangely\u2026 -There is mention of unlabelled data in Table 1 and semi-supervised learning in Section 4.2, but I didn\u2019t see any results on these experiments. Were they omitted, or have I misunderstood?\n-The error rate differences are promising in Tables 2 and 3, but statistical significance testing would help make them really convincing. Especially between SP-MLT and ASP-MTL results to highlight the benefit of adversarial training. It should be pretty straightforward to adapt the non-parametric approximate randomisation test (see http://www.lr.pi.titech.ac.jp/~takamura/pubs/randtest.pdf for promising notes a reference to the Chinchor paper) to produce these.\n-The colours are inconsistent in the caption of Figure 5 (b). In 5 (a), blue is used for \u201cOurs\u201d, but this seems to have swapped for 5 (b). This is worth checking, or I may have misunderstood the caption.\n# General Discussion - I wonder if there\u2019s some connection with regularisation here, as the effect of the adversarial training with orthogonal training is to help limit the shared feature space. It might be worth drawing that connection to other regularisation literature. ", "label": [[991, 1029, "Eval_pos_1"], [1065, 1115, "Eval_pos_2"], [1116, 1334, "Jus_pos_2"], [1336, 1383, "Eval_pos_3"], [1385, 1471, "Eval_pos_4"], [1525, 1656, "Eval_neg_5"], [1658, 3012, "Jus_neg_5"]]}
{"id": 205, "review": "This paper introduces new configurations and training objectives for neural sequence models in a multi-task setting. As the authors describe well, the multi-task setting is important because some tasks have shared information and in some scenarios learning many tasks can improve overall performance.\nThe methods section is relatively clear and logical, and I like where it ended up, though it could be slightly better organized. The organization that I realized after reading is that there are two problems: 1) shared features end up in the private feature space, and 2) private features end up in the  shared space. There is one novel method for each problem. That organization up front would make the methods more cohesive. In any case, they introduce one  method that keeps task-specific features out of shared representation (adversarial loss) and another to keep shared features out of task-specific representations (orthogonality constraints). My only point of confusion is the adversarial system. \nAfter LSTM output there is another layer, D(s^k_T, \\theta_D), relying on parameters U and b. This output is considered a probability distribution which is compared against the actual. This means it is possible it will just learn U and b that effectively mask task-specific information from  the LSTM outputs, and doesn't  seem like it can guarantee task-specific information is removed.\nBefore I read the evaluation section I wrote down what I hoped the experiments would look like and it did most of it. This is an interesting idea and there are  a lot more experiments one can imagine but I think here they have the basics to show the validity of their methods. It would be helpful to have best known results on these tasks.\nMy primary concern with this paper is the lack of deeper motivation for the  approach. I think it is easy to understand that in a totally shared model there will be problems due to conflicts in feature space. The extension to  partially shared features seems like a reaction to that issue -- one would  expect that the useful shared information is in the shared latent space and  each task-specific space would learn features for that space. Maybe this works and maybe it doesn't, but the logic is clear to me. In contrast, the authors seem to start from the assumption that this \"shared-private\" model has this issue. I expected the argument flow to be 1) Fully-shared obviously has this problem; 2) shared-private seems to address this; 3) in practice shared-private does not fully address this issue for reasons a,b,c.; 4) we introduce a method that more effectively constrains the spaces. \nTable 4 helped me to partially understand what's going wrong with shared-private and what your methods do; some terms are _usually_ one connotation or another, and that general trend can probably get them into the shared feature space. This simple explanation, an example, and a more logical argument flow would help the introduction and make this a really nice reading paper.\nFinally, I think this research ties into some other uncited MTL work [1], which does deep hierarchical MTL - supervised POS tagging at a lower level, chunking at the next level up, ccg tagging higher, etc. They then discuss at the end some of the qualities that make MTL possible and conclude that MTL only works \"when tasks are sufficiently similar.\" The ASP-MTL paper made me think of this previous work because potentially this model could learn what sufficiently similar is -- i.e., if two tasks are not sufficiently similar the shared model would learn nothing and it would fall back to learning two independent systems, as compared to a shared-private model baseline that might overfit and perform poorly.\n[1] @inproceedings{sogaard2016deep,   title={Deep multi-task learning with low level tasks supervised at lower layers},   author={S{\\o}gaard, Anders and Goldberg, Yoav},   booktitle={Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics},   volume={2},   pages={231--235},   year={2016},   organization={Association for Computational Linguistics} } ", "label": [[301, 383, "Eval_pos_1"], [384, 429, "Eval_neg_1"], [430, 1004, "Jus_neg_1"], [1393, 1510, "Eval_pos_2"], [1511, 1669, "Jus_pos_2"], [1733, 1819, "Eval_neg_2"], [1820, 3003, "Jus_neg_2"], [3004, 3110, "Eval_neg_3"], [3111, 4099, "Jus_neg_3"]]}
{"id": 220, "review": "paper_summary\nThis paper presents two approaches for reducing the storage costs in late interaction ranking models. First, the term representations are auto-encoded using \"side information\" (representing the static embedding for the term). A quantization process further reduces the size of the representations on disk. The approach is evaluated on MS MARCO, TREC DL, and CAR and shows that it can effectively reduce the storage burden while having a minimal impact on ranking effectiveness. \nsummary_of_strengths\n- The auto-encoding strategy of using the static embedding as \"side information\" to reduce the size of the representations is clever, novel, and practical. I could easily see this strategy becoming common practice when using these models. \n - The approach can dramatically reduce the storage requirements of late interaction models while having minimal impact on ranking effectivness. \n - The approach is parameterized, allowing it to easily scale to various operating points (trading off storage for performance). \nsummary_of_weaknesses\n- Generally, the evaluation is rather \"narrow\" -- it compares only with variations of the same model (either full cross or split at the same level). It may be helpful to include more baselines to put these results in more context. The baselines presented, however, are rather strong -- with DistilBERT achieving 0.390 MRR@10 on MS MARCO (compared to Nogueria & Cho's 0.365 using BERT-large). \n - The evaluation on CAR appears to have been conducted using \"automatic\" labels (based on the reported number of positive passages per query), even though human-provided labels are available. In general, further details about which version of CAR was used should be provided. \n - Equivalent performance is determined statistically via the failure of a one-sided t-test. Equivalence testing (e.g., TOST) could be used instead to strengthen the statistical claims. \n - It would be nice to have seen this work on other base models-- particularly a T5-based model. \ncomments,_suggestions_and_typos\n- This work may feel more \"at home\" at an IR conference -- I'm not sure how interesting this will be to a broader *CL audience. That being said, I like this work and would be happy to see it presented at an ARR venue as well. \n - Can this approach be used with the ColBERT model, where the ranking scores are based only on the similarities within the last layer? ", "label": [[516, 669, "Eval_pos_1"], [669, 753, "Eval_pos_1"], [1054, 1098, "Eval_neg_1"], [1102, 1282, "Jus_neg_1"], [1912, 1973, "Eval_neg_2"], [2042, 2265, "Major_claim"]]}
{"id": 223, "review": "paper_summary\nThe paper discusses the use of character-level machine translation: although recent work on this direction shows advantages over subword-level models, character-level models are rarely used in practice in the WMT shared tasks, and the authors attribute it to higher higher computational costs. The paper presents and extensive literature survey and an empirical evaluation of three character-level architectures, two of which have not been used in MT before. Experimental findings include: character-level models are less susceptible to the beam search curse; character-level models are more robust to source noise but generally underperform the subword models; sampling-based decoding methods produce poor quality outputs with character-level models. The paper also presents a two-step decoding method which is comparable to subword-level decoders in terms of efficiency, but reports poor translation quality. \nsummary_of_strengths\nThe paper is very well-written and accessible to the NLP audience beyond the MT community. It raises an important question of the practical advantages and disadvantages of character-level approaches in MT, and presents a thorough literature survey, analysis of recent shared task submissions, and extensive empirical evaluation. My expertise in MT is limited, but the methodology looks well-thought-out to me and the experimental setup seems robust. \nsummary_of_weaknesses\n1. I would have liked to see more discussion and empirical analysis of how typological differences between languages affect the relative performance of subword- and character-level models. The claims of the paper are quite broad, but there is not enough evidence to convince me that the conclusions hold generally across languages. More specifically:    * I assume that character-level MT would be more beneficial for languages with logographic writing systems (e.g. Chinese) than the ones that use alphabets. My understanding is that the WMT shared task language pairs are skewed towards European languages: if that is the case, I think the shared task analysis could have considered more venues, e.g. CWMT, The Workshop on Asian Translation, or IWSLT 2020 Open Domain Translation task (ja-zh, 20M training sentences). In any case, I think the paper should have listed the WMT languages and discussed if the use/mention of character-level methods varies by language, script, or morphological patterns.\n    * The empirical evaluation on the WMT dataset is also only performed on European languages (English, German, and Czech). In the smaller experiments, which were used for model selection, character-level models outperform subword-level ones on English-Arabic translation; this aligns with the findings of Shaham and Levy (2020), who attribute it to non-concatenative morphology of Semitic languages. So I am left wondering if the conclusions would have been different if the larger-data evaluation included languages with non-concatenative morphologies.\n2. This might be beyond the scope of the paper, but I am also wondering if the same conclusions extend to unsupervised and semi-supervised cases (for details, see the following section). \ncomments,_suggestions_and_typos\n- I am wondering if the conclusions about the performance gap between character-level and subword-level models apply to unsupervised MT. For example, character-level FSTs for unsupervised translation between closely related languages (e.g. Serbian and Bosnian) perform very well [1]. An unsupervised subword-level neural MT model for the same task tested demonstrates very poor results [2], but training the same model with character-level tokenization yields a great improvement [3].\n-I think that the process of selecting the best-performing model in a smaller experiment and then evaluating it extensively is reasonable, but I am wondering if evaluating the two rejected models, CANINE and Charformer, in terms of robustness could have given other interesting insights. I understand that evaluating all types of models on the larger dataset might not be feasible, but maybe the robustness testing could somehow be included in the smaller-data experiments too?\n-I might be missing something, but I could not figure out if the results for the proposed two-step decoding method are reported anywhere in the paper. Are the results in Table 1 with decoder downsampling obtained with or without the two-step method? If without, then how much worse does the two-step method perform?\n-Table 2 is not colorblind-friendly (red-green is the most common type of colorblindness). I would suggest changing the colors and/or specifying in the caption that the top number in each cell corresponds to chrF and the bottom one to COMET.\n-In Table 3, en-cs BPE to character results are missing the recall on lemmas/forms seen in training.\n[1] Pourdamghani, Nima, and Kevin Knight. \" Deciphering related languages.\" EMNLP 2017.\n[2] He, Junxian, et al. \"A Probabilistic Formulation of Unsupervised Text Style Transfer.\" ICLR 2019.\n[3] Ryskina, Maria, et al. \"Comparative Error Analysis in Neural and Finite-state Models for Unsupervised Character-level Transduction.\" SIGMORPHON 2021. ", "label": [[947, 1037, "Eval_pos_1"], [1038, 1275, "Jus_pos_1"], [1307, 1349, "Eval_pos_2"], [1360, 1397, "Eval_pos_3"], [1422, 1608, "Eval_neg_1"], [1654, 1751, "Jus_neg_1"], [1773, 2978, "Jus_neg_1"], [2982, 3123, "Eval_neg_2"], [3200, 3683, "Jus_neg_2"], [4163, 4312, "Eval_neg_3"]]}
{"id": 225, "review": "paper_summary\nThis paper use methods relying on spelling, using character embeddings, to detect sound change across time. They rely on three datasets to test the effectiveness of their methods: a synthetic one in a language having a simplified phonotactic system, one in Danish with synthetically generated sound change, and a real one in Danish, with known sound change. They show that they can successfully detect the selected sound changes in the datasets, and identify precisely the context of the change. \nsummary_of_strengths\nThe paper is globally nice to read and introduces an interesting idea. The related works section is particularly well-written, with the right level of detail, even though some references are missing. \nThe experimental setup is sound and clear; in particular, the use of a control dataset is appreciated. \nsummary_of_weaknesses\n1) The main issue in this paper is that you only show that you can successfully detect known sound changes, and that you don\u2019t spuriously detect them when they\u2019re absent, thanks to the control corpus. But you might detect a lot of changes in characters embeddings using your methods, that are not related to sound change. Using your system in an exploratory fashion, to detect all possible changes and categorize them, would greatly strengthen the paper. As it is, to my understanding, there is no way to be sure that the character embeddings do not lead to detecting a lot of changes independent from phonology, and this is a major issue. Moreover, the link between spelling and sound change is not straightforward to me and should be more clearly justified.\n2) Semantic and phonological change across time should indeed take a logistic shape, as you describe in your description; your experiments also seem to show that the change in the Geo corpus is not linear. Why did you stick to a linear shape? Shoemark et al (2019) that you cite also tried a logarithmic shape, you could try it too.\n3) There is a large amount of work on semantic change using contextualized embeddings and pre-trained language models, that you do not evoke in your related works (from Mario Giulianelli, Matej Martinc\u2026).  4) The formalism you use, a --> b / c, should be introduced more clearly from the beginning. Especially since you use it as early as in the summary, where it can\u2019t be comprehended without prior knowledge of this formalism. The explanation at line 190 might benefit from a scheme. Similarly, some words like \u201cplosive\u201d might benefit from a short definition (even in a footnote) for readers not familiar with the domain. \ncomments,_suggestions_and_typos\nNor clear how the Bin:Control effect is computed. Globally, all metrics described in Section 5 would benefit from equations.\nOverall, the localization of Tables and Figures in the paper would be more optimal to ease the reading. \nTables 2 to 4 look strange to me without lines.\nL.32: Choose only one between \u201csince\u201d and \u201chowever\u201d L59-62: Add references here.\nL. 362 and 364: add \u201cthe\u201d distance, \u201cthe\u201d main effect.\nL. 390: in both corpora (no \u201cthe\u201d) L. 480: repetition of \u201clanguage\u201d\u2019. ", "label": [[532, 602, "Eval_pos_1"], [603, 836, "Jus_pos_1"], [1181, 1313, "Eval_neg_1"], [1314, 1618, "Jus_neg_1"], [1954, 2115, "Eval_neg_2"], [2161, 2250, "Eval_neg_3"], [2251, 2576, "Jus_neg_3"]]}
{"id": 228, "review": "paper_summary\nThis paper presents the residue detection approach, which is specific to NLP adversarial attacks. The method leverages a linear classifier operating on the residue of sentence embedding to detect the adversary.\nIn the empirical evaluation, they detect substitution attacks on four classification datasets, and a targeted universal concatenation attack on a regression dataset. Results show the proposed method can outperform state-of-the-art detection approaches, such as FGWS, on most datasets. \nsummary_of_strengths\nThe proposed residue detection method is novel, which leverages the residue of sentence embeddings and feeds the residue into a linear classifier to identify adversarial attacks.  The study proposes two hypotheses of the residue properties in NLP adversarial samples. It further provides the rationale analysis and empirical evaluations to validate these properties.\nThe evaluations cover two types of adversarial attacks on five datasets. Besides, they conducted experiments on both NLP and image tasks. \nsummary_of_weaknesses\nIn section 3, some contents are basically introducing previous work, such as PGD, which are not closely related to the proposed method. I would suggest putting these parts into related work section.  Figure2 provides insightful findings on the residue properties. However, they are evaluated only on one dataset. It would be great to apply it to more datasets. \ncomments,_suggestions_and_typos\nPlease see the suggestions and comments in the summary of weaknesses. ", "label": [[532, 579, "Eval_pos_1"], [580, 710, "Jus_pos_1"]]}
{"id": 232, "review": "paper_summary\nThis paper proposes LaCon, a new supervised contrastive learning approach, which is based on learning label embeddings jointly. Specifically, three losses are proposed: 1) multi-head instance-centered contrastive loss, 2) label-centered contrastive loss, and 3) label embedding regularization loss. The authors shows that combining all three losses improves the fine-tuning results of BERT. \nsummary_of_strengths\n- LaCon achieves strong results in 8 datasets and outperforms several prior methods.\n-Table 3 demonstrates the gain of each component of the proposed method. The authors show that all three proposed losses help.\n-It is good to see that the authors provide the mean and standard deviation out of 10 runs in their experiments and also bold the ones with p-value < 0.05.\n-Overall, the writing is clear and easy to follow. I like how the authors visualize the method in Figure 1. \nsummary_of_weaknesses\n- It is surprising to see that unlike conventional NCE or InfoNCE, the authors omit the positive term in the denominator in equations (3), (4), and (5). In NCE or InfoNCE, adding the positive term in the denominator is intuitive since it is optimizing the log probability of selecting the positive example. However, the authors do not explain the reason behind their counter-intuitive change nor provide an ablation study on this modification. I would like to see the justification of this change and experimental results that support it.\n-In subsection 2.6, the authors introduce LaCon with label fusion which requires additional computation compared to LaCon-vanilla and CE baseline. It would be better if the authors can quantify the computation overhead by providing the inference time of the methods.\n-The Instance-centered Contrastive Loss (ICL) is not very novel given that it is just a different view of the softmax classifier. By treating the weights of the last linear layer as the embedding vectors of each class, optimizing the cross-entropy is almost identical to ICL (except that the vectors should be L2 normalized which is basically applying weight normalization to the last linear layer and normalize the features right before the last layer). \ncomments,_suggestions_and_typos\n- In line 211, \"we modify the InfoNCE (Gutmann and Hyv\u00e4rinen, 2010) to calculate the loss\", here the authors cite the wrong paper. Gutmann and Hyv\u00e4rinen (2010) propose NCE, but InfoNCE is proposed by Oord et al. (2018). The proposed loss is similar to both NCE and InfoNCE, so changing either the citation or name of the method is fine. The same error also appears in line 94.\n-The proposed method lies in the metric learning field; however, the authors do mention the related metric learning work in their section 5 (related work). For example, the contrastive loss was first proposed by Chopra et al. (2005) and triplet loss (Weinberger et al., 2005; Schroff et al., 2015) is also quite relevant to this paper.\n-It would be great if we can see that the proposed method works for other pretrained models besides BERT as well.\nReferences -Oord, A\u00e4ron van den et al. \u201cRepresentation Learning with Contrastive Predictive Coding.\u201d ArXiv abs/1807.03748 (2018): n. pag.\n-Chopra, Sumit, Raia Hadsell, and Yann LeCun. \" Learning a similarity metric discriminatively, with application to face verification.\" 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05). Vol. 1. IEEE, 2005.\n-Weinberger, Kilian Q., John Blitzer, and Lawrence Saul. \" Distance metric learning for large margin nearest neighbor classification.\" Advances in neural information processing systems 18 (2005).\n-Schroff, Florian, Dmitry Kalenichenko, and James Philbin. \" Facenet: A unified embedding for face recognition and clustering.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2015. ", "label": [[795, 901, "Eval_pos_1"], [1232, 1464, "Eval_neg_1"], [1732, 1795, "Eval_neg_2"], [1795, 1861, "Jus_neg_2"]]}
{"id": 236, "review": "paper_summary\nThis paper proposed to use a single dialog example(for per service or slot) to form an example-based prompt for schema-guided dialog state tracking. Based on T5 seq2seq fine-tuning, they compared the example-based prompt with the previous description-based prompt. The evaluation is based on two schema-guided datasets: SGD and MultiWOZ-leave-one-out, and two SOTA models for schema-guide dialog: T5-ind and T5-seq. The results showed that although the example-based prompt will take more annotation time, using example-based prompt, the fine-tuned seq2seq model will lead to better generalization to new APIs, more data-efficient, and robust to schema variations. However, some details are missing and some ablation studies are not well-designed. \nsummary_of_strengths\nOn the specific schema-guided dialog state tracking task, compared to the description-based prompt, the proposed example-based prompt can generalize better to new APIs, more data-efficient and more robust to schema variation. \nThe idea of using example-based prompt for seq2seq fine-tuning seems simply focused and promising, which can be used for other seq2seq dialog state tracking settings, and may also be extended to other dialog tasks. \nsummary_of_weaknesses\n1. Some of the details and analysis are not clear, please see comments 1,2,3,4 2. Some of the ablation studies are unclear or problematic. See comments 5,6 \ncomments,_suggestions_and_typos\n1. line 130, It is confusing why SGD dataset requires manually created dialog examples for prompts; But for MultiWOZ, it just chooses the example from the training set. please explain why. \n2. line 175 seems lacking of experimental results to support it. \n3. line 121, the paper lacks details for how to design and use prompts for dialog with multi-domains/services. According to line 140,  I only can guess it split the dialog state with multi-domain/service into separate examples, correct? please explain more. If possible, please show some examples in the Appendix for easy understanding. \n4. Error analysis in 5.2 is crappy, more quantitive error analysis will help. \n5. Table 5, the models above the line are using slot descriptions, while the SDT-seq is using name only. It seems they are not directly comparable. Although we may guess that description is more robust than name or slot-IDs, as shown in previous works[1,2]. Please explain more, and also list all name-based results on SGD-X. 6. line 165, finetuning T5-seq with prompt examples is meaningless given there are only #services new examples in prompt. Instead, for the combination of using description + example, could you simply append those example-based prompt after description-based prompt.  This new experiment will help to show how the interesting combination will help.   [1]Cao, Jie, and Yi Zhang. \" A Comparative Study on Schema-Guided Dialogue State Tracking.\" Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021. \n[2]Lee, H., Gupta, R., Rastogi, A., Cao, Y., Zhang, B. and Wu, Y., 2021. SGD-X: A Benchmark for Robust Generalization in Schema-Guided Dialogue Systems. arXiv preprint arXiv:2110.06800.\n7. The model names T5-ind, T5-seq are misleading. Maybe using Desc-ind/seq, Eg-ind,Demo-ind/seq? ", "label": [[1252, 1319, "Eval_neg_1"], [1319, 1331, "Eval_neg_1"], [1331, 1405, "Eval_neg_2"], [1438, 2110, "Jus_neg_1"], [2111, 3215, "Jus_neg_2"]]}
{"id": 238, "review": "paper_summary\nThis paper adopts a multi-task learning method from (Zhang et al 2020) in the zero-shot multilingual dependency parsing scenario. The method aims to use curriculum learning to optimize the worst-case-aware loss rather than averaged loss in common multi-task learning settings.  The experimental results show that this method outperforms general sampling method baselines in zero-shot learning and achieves state-of-the-art when using mBERT and XLM-R. They also explore the effect of varying the homogeneity of training languages by comparing models trained on homogeneous languages (e.g. GERMANIC, ROMANCE, SLAVIC) and skewed samples (e.g. ROMANCE+EU/AR/TR/ZH). Their results performs consistently better than baseline on all training samples but there is no clear pattern to show in which case this method works best and why. \nsummary_of_strengths\nThis paper shows that the worst-case-aware loss optimization is helpful for zero-shot multilingual dependency parsing. Although the method is model-agnostic and simple, it achieves the state-of-the-art without any external resource, which is not always available for low-resource languages. It would be interesting to also explore the effect of this method in other multilingual tasks.  The paper empirically investigates how the method performs with different training languages settings and gives an interesting analysis. \nsummary_of_weaknesses\nAlthough the method performs consistently well with different training languages, there is less discussion on why and how this method helps for the test languages. For example, it is not clear why both the highest and lowest gain come from the skewed samples group (e.g. ROM+EU and ROM+AR). \ncomments,_suggestions_and_typos\n1. There is less evidence on how the method helps target languages, so it would be interesting to do some qualitative analysis to see how this method helps a specific language compared with the baseline sampling method. For example, it may be possible to compare the baseline with the proposed model trained on ROM+EU to see where the benefit comes from.\n2. The paper mainly experiments with the biaffine dependency parser. It will also be interesting to experiments with other recently proposed parsers to see if the method is consistently better or not.  3. The paper does not refer to the table 3 when discussing the results in section 4. ", "label": []}
{"id": 250, "review": "paper_summary\nThis paper is an extension of the SUPERB benchmark for assessing pre-trained upstream speech models acquired through Self-supervised learning. The authors argue that SUPERB is limited to 10 tasks and that 5 new tasks, here AST, Out-of-Domain ASR, Voice Conversion, Speech separation and Speech enhancement would permit to better evaluate the semantic and generative capability of such SSL models. The authors compare the same SSL models used in the SUPERB paper on the 5 tasks. All SSL models were used as-is without fine-tuning on the task (frozen). Overall results show some superiority of Wav2vec and Hubert models on AST and OOD ASR tasks, while VC SE and SS tasks lead to less obvious superiority of models. The authors then performed experiments with varying data training for some SSL models or downstream tasks and showed that the ranking of ssl models stay consistent which should prove the robustness of these new evaluation tasks. \nsummary_of_strengths\n- Setting up benchmarks is highly important to assess SSL speech models -The proposed tasks provide a complementary view to the 10 already existing task of SUPERB  -The authors have shown that the proposed tasks leads to consistent results according to the upstream and downstream training data -Although the authors did not trained the SSL models, this stay an impressive and useful amount of work, that according to what I understood from the text, should be made available to the community (when?) \nsummary_of_weaknesses\n- If the 5 new tasks are indeed assessing different properties than the 10 superb tasks do, they are not all well justified. AST is tested on only one couple of language (EN->De) while OODASR on 3 (+spontaneous). Regarding SE and SS, given that SSL models are trained only on clean read speech it is not properly justified why such tasks are relevant.  -The 5 tasks are indeed an interesting addition to SUPERB but : AST is not new since it has already been used in SSL benchmarks (cf. LeBenchmark). A more realistic OODASR would arguably be out-of-domain data (cf. Hsu et al 2021)  rather than out of language data for which XLSR and XLS-R are far more adequate. Finally, the SE and SS tasks are difficult to justify and probably far less relevant for an ACL venue than more speech oriented events.   -Such contribution is really useful to assess to which extent models can improve downstream tasks but it does not reveal why they do it. At the end we don't learn much about the models.  -Finally, it is rare that models are used without fine-tuning so it is unclear why such fine-tuning was not considered in the study (adaptability is not an interesting feature?) \ncomments,_suggestions_and_typos\nMost of my comments are given above.  Details : some references have no venue -> please correct this.     misc{hsu2021robust,       title={Robust wav2vec 2.0: Analyzing Domain Shift in Self-Supervised Pre-Training},        author={Wei-Ning Hsu and Anuroop Sriram and Alexei Baevski and Tatiana Likhomanenko and Qiantong Xu and Vineel Pratap and Jacob Kahn and Ann Lee and Ronan Collobert and Gabriel Synnaeve and Michael Auli},       year={2021},       eprint={2104.01027},       archivePrefix={arXiv},       primaryClass={cs. SD} } ", "label": [[1143, 1272, "Eval_pos_1"], [1274, 1470, "Eval_pos_2"], [1504, 1626, "Eval_neg_1"], [1627, 1852, "Jus_neg_1"], [2199, 2301, "Major_claim"], [2305, 2489, "Eval_neg_2"], [2492, 2623, "Eval_neg_3"]]}
{"id": 270, "review": "paper_summary\nMotivated by the shortcomings and relative simplicity of existing natural language understanding (NLU) benchmarks for task-oriented dialogue (ToD), the authors propose NLU++, a novel dataset created by NLU experts that provides fine-grained domain ontologies, contains a large amount of multi-intent sentences, differentiates between domain-specific and generic intents and slots, and is tailored to address problems observed in industrial ToD applications. NLU++ is intended to be challenging to state-of-the-art ToD systems and emphasizes sample quality over quantity. It includes two domains (\"Hotel\" and \"Bank\"), with 1k and 2k samples, respectively.\nThe authors perform a series of intent detection (ID) and slot labeling (SL) experiments based on this new resource, with a particular focus on low-resource settings. For intent detection, MLP classifiers are compared with models based on the question answering (QA) paradigm. The presented findings demonstrate that QA models generally outperform MLP models on the ID task, achieving F1 scores of up to 93 points. SL proves to be more challenging, with the evaluated models obtaining F1 scores of up to 72 points. Unsurprisingly, F1 is substantially lower in the low-resource setting for both tasks. The authors also demonstrate that generic intents transfer across domains, with ID models trained on the one of the two domains being able to detect generic intents in the other domain with substantial accuracy. \nsummary_of_strengths\n- NLU++ represents a high-quality, richly annotated TOD resource that captures the requirements of industrial TOD systems and, as such, is likely to facilitate the development of improved dialogue models that are effective in the low-resource setting (assuming the dateset will be publicly available).\n-The experimental section is extensive enough to showcase the properties of NLU++ and illustrates well as to why it may be of interest to subsequent studies.\n-Cross-domain experiments that focus on domain-general intents and slots are interesting and insightful. \nsummary_of_weaknesses\n- The paper would benefit from a more thorough discussion of the data collection / selection process, as well as from an evaluation of the annotation consistency between the four enlisted NLU experts (e.g. how high is the inter-annotator agreement, does having only four annotators introduce unwanted annotation artifacts or biases, were the annotations validated in any way?).\n-Line 296: A detailed explanation of how the new samples were created should be added.\n-Table 4: It is unclear why MultiWoZ was not included in the comparison. \ncomments,_suggestions_and_typos\nNone. ", "label": [[1506, 1626, "Eval_pos_1"], [1640, 1754, "Jus_pos_1"], [1807, 1963, "Eval_pos_2"], [1965, 2069, "Eval_pos_3"], [2094, 2291, "Eval_neg_1"], [2293, 2467, "Jus_neg_1"]]}
{"id": 273, "review": "paper_summary\nThis paper targets a source of ambiguity in user interactions with task-oriented dialog systems: when a user asks for an action (e.g. booking) to be taken with regards to, or asks a question about an ambiguous/underspecified entity. This paper proposes that existing datasets are unable to properly model this ambiguity, and thus provide an extended version of the popular MultiWOZ and SGD task-oriented dialog datasets containing disambiguation questions and responses. The authors create disambiguation annotations by applying templates from a limited-domain dataset and paraphrase a subset. The authors test a baseline GPT2-based model on named entity recognition when trained with the original data compared to different ways to use their augmented datasets, showing that training with augmented data improves named entity recognition/disambiguation on their test split. \nsummary_of_strengths\n1. The authors identify an important challenge with existing popular benchmark datasets/tasks for task-oriented dialog and propose a way to solve this challenge via data augmentation 2. Different addressing/reference methods are tested, which mirror common modes of user interaction (e.g. typos, positional referencing, typos or multiple selections). The analysis of each form of augmentation is of particular interest and should be put in the main paper body rather than put in a table in the supplementary section. \n3. There is an interesting discussion on how to best use the augmented data - whether to augment the target dataset and a pre-training dataset, to perform joint augmentation, as compared to using the non-augmented data. They show that pre-fine-tuning with augmented data can lead to improvements with a lower training budget compared to full augmentation. \nsummary_of_weaknesses\n1. It would be more relevant and important to remark on how much the degree of ambiguity (number of possible options) varies across dataset examples and whether that was sampled uniformly, by difficulty, or otherwise during data augmentation. It would also be helpful to include whether this degree of ambiguity is reflected in real-world dialog systems as opposed to benchmark WoZ-collected datasets like MultiWOZ and SGD --- essentially, the problem seems to be intuitive, but the paper lacks some context on how widespread the issue actually is. \n2. The results only support the assertions that \"augmentation helps resolve ambiguity\" and \"augmentation brings no harm\" for the specific model used; however, the JGA numbers in Table 3 are a fair degree below state of the art for the original data (~60 for MultiWOZ, ~80-90 for SGD) which raises questions about whether the augmented data and disambiguation task are applicable to state of the art modeling techniques for task oriented dialog. It would have been important to either experiment with additional models or at least include baseline numbers for state of the art models applied to each dataset (and augmented dataset). \n3. The named entity recognition accuracy metric seems to be weak compared to the proposed scale of the problem. If ambiguity is supposed to cause issues in task-oriented dialog models it would be important to measure end-to-end metrics like success rate, inform rate, as well as preference tracking and dialog policy learning (all metrics used in the base MultiWOZ/SGD settings) to demonstrate whether the proposed database result disambiguation task helps in the downstream dialog task and the end goal of the system. There are no data/experiments generalizing this to usage in end-to-end systems. Similarly, the assertion of \"augmentation brings no harm\" is not supported for general end-to-end task oriented dialog. \ncomments,_suggestions_and_typos\nComments incorporated into strengths/weaknesses above ", "label": [[914, 1093, "Eval_pos_1"], [1262, 1329, "Eval_pos_2"], [1432, 1504, "Eval_pos_3"], [1507, 1785, "Jus_pos_3"], [1811, 2050, "Eval_neg_1"], [2361, 2506, "Eval_neg_2"], [2508, 2802, "Jus_neg_2"], [2803, 2964, "Eval_neg_2"], [2994, 3102, "Eval_neg_3"], [3103, 3589, "Jus_neg_3"], [3589, 3710, "Jus_neg_3"]]}
{"id": 281, "review": "paper_summary\nThe paper presents a novel dataset for the task of stereotype detection. The authors also propose a multi-task model that uses neighbor tasks such as hate speech detection and offensive language detection to improve the classification on stereotypes. They further introduce a reinforcement learning agent that can identify the most informative examples in multi-task learning setup. An ablation study analyzes the importance of different factors in the classification process. \nsummary_of_strengths\n- detecting stereotypes and biases in the pre-trained language model is an important problem in contemporary NLP -the proposed dataset appears to be of better quality than existing resources and contain real-world examples rather than artificially constructed ones -using multi-task approach is justified in the presented situation -various experiments and ablation study give a good overview of the advantages of the model \nsummary_of_weaknesses\n- the writing of the paper can be improved in some sections (Introduction/Related work) to accommodate readers that are not experts on the task -missing details on corpus creation (instructions, agreement) \ncomments,_suggestions_and_typos\n- the paragraph from l90 to l116 is very unclear. It is explained in later paragraphs, but on first reading it is somewhat confusing -l121 - ``consists'' -> ``consists of'' or ``contains'' -can you include more details on the selection criteria for candidate examples from reddit (section 3.1). E.g.: what are the topics, what are the stereotype targets, what kind of filtering do you apply?\n-can you include agreement statistics for the corpus in section 3.1 or 3.2?\n-while the contribution of the reinforcement agent is clear, it would be good to include a different baseline for \"selecting relevant examples\", in order to justify the use of RL in that setup over a simpler selection method ", "label": [[627, 703, "Eval_pos_1"], [708, 778, "Jus_pos_1"], [846, 937, "Eval_pos_2"], [962, 1020, "Eval_neg_1"], [1048, 1104, "Jus_neg_1"], [1105, 1139, "Eval_neg_2"], [1141, 1164, "Jus_neg_2"], [1201, 1248, "Eval_neg_3"], [1249, 1590, "Jus_neg_3"]]}
{"id": 282, "review": "paper_summary\nThis paper presents a knowledge distillation method based on MAML. The paper's main aim is to address two drawbacks of existing knowledge distillation methods: (1) teacher being unaware of the capacity of the student, and (2) teacher not optimised for distillation.\nThe paper assumes the teacher network is trainable and proposes a simple technique that, in the MAML framework, applies \"pilot updates\" in the inner loop to make the teacher and student networks more compatible. Pilot updates are essentially MAML's original inner loop, but applied twice, the first time to update the teacher and the second time to update the student.\nExperiments are performed on distilling BERT and evaluated on a number of tasks/datasets in the GLUE benchmark. The proposed method consistently outperforms the compared baselines on almost all tasks.\nOn the CV side, the proposed method is evaluated on distilling ResNet and VGG. The method achieves comparable performance with SOTA model CRD. \nsummary_of_strengths\n- Addresses knowledge distillation, an important technique with wide application in machine learning, natural language processing and computer vision.\n- The proposed method achieves SOTA performance on a large number of tasks in NLP, in distilling BERT.\n- The presentation is clear, logical and easy to follow.\n- A comparative analysis on the tradeoff between model performance and computational costs. \nsummary_of_weaknesses\n- The proposed method is quite simple. Its essence is the \"pilot update\" mechanism, which basically applies the inner loop of MAML twice, once to update the teacher, and once to update the student.\n- The achieved performance boost is moderate.\n- The comparative analysis in Table 2 shows that, with significantly more computational costs than the two compared baselines (PKD and ProKT), the model achieves a modest performance gain of about 0.5 absolute F1 points. This analysis, though very welcome, demonstrates that the proposed method suffers a large computational penalty for a small accuracy/F1 gain. \ncomments,_suggestions_and_typos\nThis paper is a resubmission from ARR August, and it has not been significantly improved. The only major addition is Table 2 that describes the tradeoff between computational cost and model performance. Therefore, the majority of reviewers' comments from ARR August still holds (I was one of the original reviewers). ", "label": [[1017, 1165, "Eval_pos_1"], [1271, 1325, "Eval_pos_2"], [1442, 1479, "Eval_neg_1"], [1480, 1638, "Jus_neg_1"], [1641, 1684, "Eval_neg_2"], [1687, 1905, "Eval_neg_3"], [1906, 2048, "Jus_neg_3"]]}
{"id": 284, "review": "paper_summary\nThe paper focuses on the intent understanding of textual TODO notes. The authors propose a weakly supervised approach based on multi-task learning to pre-train better vector representations for the former task. The auxiliary tasks include: (1) autocompletion, MLM objective based on hand-crafted rules, (2) pre-action (what is needed) and goal generation (pseudo intent) and (3) action arguments prediction, using labels from FrameNet. The proposed approach outperforms strong (sentence) embedding baseline models in zero-shot setting. \nsummary_of_strengths\n1. The paper is clearly written and mostly easy to follow, the proposed approach is well motivated. \n2. The task is interesting, especially from a practical point of view. \n3. The presented results are convincing and I think the evaluated baseline models are good enough. One question left unanswered is -- if the selected similarity functions are the best choice for these models, additional ablation is welcome. \n4. Extensive ablation study of the proposed tasks. \nsummary_of_weaknesses\n1. The experiments are held on a private datasets and the exact setup is impossible to reproduce. \n2. A minor point would be that few-shot would be a more realistic setup for that task, as domain-specific TODOs are easy to acquire, however I agree that the current setup is adequate as well. \n3. More error analysis could be useful, especially on the public dataset, as their data could be included without any restrictions, e.g., error types/examples? patterns? Examples when non-contextualized embeddings outperform contextualized ones, or even LITE? \ncomments,_suggestions_and_typos\nI urge the authors to release at least some part of the dataset to the wider public, or under some  end user-agreement.\nComments: 1. I suggest the authors to focus their comparison on word2vec baselines (currently in appendix), instead of Sentence-BERT, as the latter does not show good performance on the short texts. It seems that non-contextualized embeddings are more suitable for the task. \n2. Maybe it makes more sense to try out models pre-trained on conversations, e.g., text from Twitter or natural language conversations. ", "label": [[575, 629, "Eval_pos_1"], [630, 672, "Eval_pos_2"], [676, 700, "Eval_pos_3"], [701, 744, "Jus_pos_3"], [748, 843, "Eval_pos_4"], [1064, 1159, "Eval_neg_1"], [1163, 1245, "Eval_neg_2"], [1247, 1353, "Jus_neg_2"], [1357, 1426, "Eval_neg_3"], [1428, 1614, "Jus_neg_3"]]}
{"id": 285, "review": "paper_summary\nThe paper analyzes the importance of negation in some natural language understanding tasks. It is highlighted that many of the corpora prepared for these tasks inherently contain fewer instances of negations than general English texts, moreover, a high percentage of these negations seem to be unimportant for NLU. \nsummary_of_strengths\nThe main strengths of the paper are the following: It provides a meticulous and linguistically apt analysis of the role of negation in natural language understanding tasks. \nThe results of the analysis are somewhat counterintuitive, which underlines the significance of the research questions (i.e. how negation may affect the efficacy of NLU). \nThe results may have a wide effect in the NLU field (it might influence the architecture of NLU systems). \nsummary_of_weaknesses\nThe main weaknesses of the paper are the following: More emphasis should be put on in which cases is negation important.\n****The authors have adequately answered most of the weaknesses mentioned in the previous review.***** \ncomments,_suggestions_and_typos\n****The authors have adequately answered most of the weaknesses mentioned in the previous review.***** ", "label": [[402, 524, "Eval_pos_1"], [525, 643, "Eval_pos_2"], [645, 693, "Jus_pos_2"], [697, 748, "Eval_pos_3"], [749, 800, "Jus_pos_3"], [878, 946, "Eval_neg_1"]]}
{"id": 295, "review": "paper_summary\nThis well-written paper presents \"DIBIMT\", a manually annotated and curated benchmark to measure semantic biases in word sense disambiguation in machine translation via 4 newly defined metrics. The framework has 5 translation directions (English to: Chinese, German, Italian, Russian and Spanish), and also provides statistical and linguistic analysis for 7 (non-)commerical neural MT systems. \nsummary_of_strengths\n-- thorough experimentation -- interesting benchmark that will be both tough and useful for MT system comparison in terms of their ability to perform word sense disambiguation -- especially liked the inclusion of 5 (diverse) languages and the comparison against a dedicated word sense disambiguation system \nsummary_of_weaknesses\n-- none really, maybe only the sheer amount of material that is almost too much for a conference paper format (given the long appendix, as well)  -- ...and maybe that only few annotators have been working on the task without the ability to check for agreements or majority votes \ncomments,_suggestions_and_typos\n-- some of the formal notations introduced in chapter 3 would probably be easier to understand if put in words -- but I realize that that would make the paper yet longer again ", "label": [[14, 46, "Eval_pos_1"], [433, 457, "Eval_pos_2"], [460, 482, "Eval_pos_3"], [482, 605, "Jus_pos_3"], [609, 737, "Eval_pos_4"], [912, 1039, "Eval_neg_1"], [1075, 1182, "Eval_neg_2"]]}
{"id": 311, "review": "paper_summary\nThis paper proposes an interaction-based model for role-oriented dialogue summarization. This sub-task of dialogue summarization is important for dialogues in specific domains. The idea is interesting, especially the *Attention Divergence Loss*, which guides the inter-attention by means of intra-attention. Experiments are conducted on two datasets: CSDS and the Medical dataset proposed by Song. Actually, the medical dataset is not very suitable for this task, the authors also mention this consideration. The choice of BertAbs as the backbone model also makes sense. However, I am also curious about the performance of using other pre-trained LM for Chinese, like *T5-PEGASUS-Chinese*. The experiments are extensive, both automatic and human evaluations are conducted. \nsummary_of_strengths\n1. interesting idea: Attention Divergence Loss. \n2. extensive experiments and various evaluation metrics, also perform the t-test. \n3. codes will be released. \nsummary_of_weaknesses\n1. Choose BERTAbs as the backbone model rather than fully pre-trained seq2seq LM. \n2. The experimental results on MC are not very significant, Song et al., 2020 achieve better results on agent summary in terms of ROUGE-1 and ROUGE-L. 3. For *Agent Summary Completeness Analysis*, some details are missing, how to judge agent summaries need to be integrated? \ncomments,_suggestions_and_typos\nThe line number on page 4 is missing. ", "label": [[191, 215, "Eval_pos_1"], [216, 321, "Jus_pos_1"], [413, 522, "Eval_neg_1"], [812, 828, "Eval_pos_2"], [861, 913, "Eval_pos_3"], [1077, 1132, "Eval_neg_2"], [1134, 1348, "Jus_neg_2"]]}
{"id": 316, "review": "paper_summary\nThe paper describes an approach to automatically grade essays using pre-trained language models such as BERT and Longformer. The paper also describes an approach to grade out of domain essays (i.e. essays written in response to another prompt). \nsummary_of_strengths\n1. Using multiple pre-trained language models (BERT and Longformer) as well as more than 1 dataset (ASAP Automatic Essay Grading dataset and Common Literacy Prize dataset).\n2. Analysis is quite clear and complete. \nsummary_of_weaknesses\n1. The writing needs to be improved. Structurally, there should be a \"Related Work\" section which would inform the reader that this is where prior research has been done, as well as what differentiates the current work with earlier work. A clear separation between the \"Introduction\" and \"Related Work\" sections would certainly improve the readability of the paper.\n2. The paper does not compare the results with some of the earlier research work from 2020. While the authors have explained their reasons for not doing so in the author response along the lines of \"Those systems are not state-of-the-art\", they have compared the results to a number of earlier systems with worse performances (Eg. Taghipour and Ng (2016)). \ncomments,_suggestions_and_typos\nComments: 1. Please keep a separate \"Related Work\" section. Currently \"Introduction\" section of the paper reads as 2-3 paragraphs of introduction, followed by 3 bullet points of related work and again a lot of introduction. I would suggest that you shift those 3 bullet points (\"Traditional AES\", \"Deep Neural AES\" and \"Pre-training AES\") to the Related work section.\n2. Would the use of feature engineering help in improving the performance? Uto et al. (2020)'s system reaches a QWK of 0.801 by using a set of hand-crafted features. Perhaps using Uto et al. (2020)'s same feature set could also improve the results of this work.\n3. While the out of domain experiment is pre-trained on other prompts, it is still fine-tuned during training on the target prompt essays.  Typos: 1. In Table #2, Row 10, the reference for R2BERT is Yang et al. (2020), not Yang et al. (2019).\nMissing References: 1. Panitan Muangkammuen and Fumiyo Fukumoto. \" Multi-task Learning for Automated Essay Scoring with Sentiment Analysis\". 2020. In Proceedings of the AACL-IJCNLP 2020 Student Research Workshop.\n2. Sandeep Mathias, Rudra Murthy, Diptesh Kanojia, Abhijit Mishra, Pushpak Bhattacharyya. 2020. Happy Are Those Who Grade without Seeing: A Multi-Task Learning Approach to Grade Essays Using Gaze Behaviour. In Proceedings of the 2020 AACL-IJCNLP Main Conference. ", "label": [[457, 495, "Eval_pos_1"], [521, 554, "Eval_neg_1"], [555, 883, "Jus_neg_1"], [886, 975, "Eval_neg_2"], [976, 1241, "Jus_neg_2"]]}
{"id": 321, "review": "paper_summary\nThis paper presents an English Reddit corpus which consists of Parent/Target comments to explore whether the conversational context is important to identify a message as Hate, Counter, or Neutral considering both human judgments and system predictions. Authors conduct an extensive analysis based on the annotation by humans as wells as the automatic system's decision. Moreover, an error analysis of the best systems is presented. \nsummary_of_strengths\nS1: The paper is well-written and structured. The authors define clearly the contributions, motivation, and experiments conducted.  S2: Novelty of the work lies in the creation of a Reddit corpus to explore whether the conversational context affects identifying a comment as hate speech, counter, or neutral.  S3: Extensive experiment results are conducted. Both, the main results, as well as ablation results, are strong.\nS4: The experiments in the paper confirm the hypothesis of the authors about the utility of using the conversational context to identify hate speech or counter-narratives. The author shows it affects both human judgments and system prediction. \nsummary_of_weaknesses\nW1: In the process of collecting the Redding messages, a pair (parent, target) is considered, but a better justification of this decision is needed to explain why authors have not considered more response messages.\nW2: In the process of annotation analysis, five annotators label each message. An explanation of this number is necessary.  W3: The analysis of the results is complete but didactic examples while explaining each case would help to interpret the discussion of the authors.  W4: An analysis of the difficulty of the annotators in identifying each class is not presented. Would be important to see in which cases the annotators do not agree and why.\nW5: I believe that the annotation guide may be too short for annotators who are not familiar with this type of task. Have examples been provided to the annotators? \ncomments,_suggestions_and_typos\n- The word \"context\" in the title of the paper could be more specified. I suggest the authors replace it with \"Conversational context\", otherwise could be confused by the context of the sentence.  - What is the annotator agreement between the five annotators before employing MACE's method? Would be helpful to see this analysis in order to observe the difficulty of the task.  - How the authors have resolved the possible doubts of the annotators while the annotating process in MTurk?  - Please, mention the language you focus on to retrieve the Reddit posts.  - Please, move Table 1 to page 2.  - The link of the code is not available but the authors in a footnote affirm \"Code and data available at anonymous_GitHub_link\" - A dot is missing after the \"Label distribution and linguistic insights\" paragraph title.\n- In paragraph 481, please show didactic examples where the \"Parent\" helps to identify the class of the \"Target\".  - Please, explain what do you mean by \"Intricate text\" in Table 8. ", "label": [[472, 513, "Eval_pos_1"], [514, 598, "Jus_pos_1"], [604, 776, "Eval_pos_2"], [782, 825, "Eval_pos_3"], [826, 890, "Eval_pos_4"], [895, 1062, "Eval_pos_5"], [1063, 1135, "Jus_pos_5"], [1161, 1372, "Eval_neg_1"], [1376, 1451, "Eval_neg_2"], [1452, 1495, "Jus_neg_2"], [1500, 1644, "Eval_neg_3"], [1650, 1741, "Eval_neg_4"], [1742, 1819, "Jus_neg_4"], [1823, 1936, "Eval_neg_5"]]}
{"id": 325, "review": "The paper focusses on the analysis and identification of press release documents that deviate from the original observational study that they refer to, in the sense of exaggerating results. More specifically they focus on health topics and propose a pipeline for identifying press releases that propose a causal relation while the corresponding publication proposes a correlational relation.\nThe task is interesting, and the corpus would be a useful addition to the community, allowing for the development of further systems addressing misinformation. I have some concerns on the strategy and annotation process: More specifically: -- I am wondering if there was some manual estimation while annotating sentences of whether, in the case of matching/contrasting classes between the press release and the scientific paper, were actually referring to the same thing. I understand that this was performed on the article level, but was there some sentence-level evaluation as well? \n-- It would be useful to present the process of deciding whether a scientific paper is correlational or causal, in the case where the conclusion contains multiple sentences, classified under different taxonomical classes, e.g., direct causal + correlational. Are only papers containing solely correlational and non-claim sentences considered correlational? You seem to be hinting at the latter later on in the paper, but it should be better clarified at the part where the classification strategies are described (section 5.1).\nThe paper is well written and the authors explain in detail the procedure for selecting the final documents in the corpus. Some minor re-organisation would benefit the paper since some information is mentioned in some section and revisited-expanded in some later section leaving open questions in between and obstructing the reading (see comments below).\nI am missing the details of fine-tuning the BERT models on the task: What parameters were used? Likewise, for the use of augmented data, the pre-processing details should be explained either in the main paper or the appendix. These are important for reproducibility. Furthermore, I am wondering if the authors considered other models apart from BERT that have shown to outperform it such as XLNet, ELECTRA, etc. These additions would further aid in the interpretation of results and appreciation of the task.\nFinally, for the interpretation of results, it would be good to know how fig.3a was calculated. I assume that the rate was calculated as #of exaggerated press releases over the total number of press releases referring to papers with correlational conclusions? Or was it over the total number of press releases? In either case, it would be useful to visualise the evolution of the rate of press releases referring to correlational versus direct causal conclusions, in order to be able to better interpret the results. As it is currently presented I find it hard to properly interpret the presented results and claims in the discussion.\nOther comments: Section 1- Introduction: -- \"Furthermore, 14,426 observational studies contained structured abstracts, in which the sentences in the conclusion subsection were used as main statements in research papers.\" -- > While it is made clear later on that these constitute the final set of papers used, it is not properly explained here. It is better to clarify early on. \n-- Provide either a link or a citation for EurekAlert Section 2 - Related Work: -- \"missing of information\" --> missing information -- it is better to use consistently either the numerical or the word form of numbers (e.g., you mention '16 questions', and then '10 criteria' in the next sentence. \n-- 'Causal relation can be broadly defined ... ' --> 'Causal relations ...? or 'A causal relation... -- 'Causal relation can be broadly defined as any type of cause-effect relations, such as in the NLP task' --> it would be better to provide an example here.\nSection 3 Corpus Construction: -- Perhaps precision would be a more suitable metric compared to accuracy for the LightGBM classifier, as I believe the important task is to avoid a high number of false negatives (regardless,  accuracy does seem sufficiently high). \n-- 'conclusion subsections' refers to both conclusions from the abstract and conclusions from the main body? It is a bit unclear here. I understand based on the conclusions section of your own paper that you probably take only the abstract conclusions, but it would be better to state this clearly and earlier on. \n-- In our taxonomy, \u201ccan + causal cue\u201d belongs to the category of \u201cdirect causal\u201d. -- > I am a bit uncertain of what is implied in this sentence. Is this rule in the taxonomy wrong based on the authors' error analysis, or is it the case that there are more examples that satisfy the rule rather than those that contradict it? In either case, is there a way to revise the markers in the taxonomy in ways that better capture the task at hand? \n-- The error analysis examples are very interesting! I am wondering if it would be possible to group them and show what percentage of errors do they represent in the results? \n-- Figure 2: Claim aggregation algorithm for press releases --> for clarity I would change to: \"Claim aggregation algorithm for press releases that correspond to papers identified as correlational\" ", "label": [[392, 416, "Eval_pos_1"], [421, 475, "Eval_pos_2"], [477, 551, "Jus_pos_2"], [552, 612, "Eval_neg_1"], [613, 1505, "Jus_neg_1"], [1506, 1531, "Eval_pos_3"], [1536, 1628, "Jus_pos_3"], [1629, 1679, "Eval_neg_2"], [1680, 1860, "Jus_neg_2"], [1861, 1928, "Eval_neg_3"], [1929, 2369, "Jus_neg_3"], [2370, 2464, "Eval_neg_4"], [2466, 3004, "Jus_neg_4"], [3976, 4075, "Eval_neg_5"], [4076, 4206, "Jus_neg_5"], [4209, 4341, "Eval_neg_6"], [4341, 4521, "Jus_neg_6"], [4610, 4667, "Eval_neg_7"], [4668, 4963, "Jus_neg_7"], [4967, 5016, "Eval_pos_4"]]}
{"id": 327, "review": "Learning-based metrics have been extensively studied for open-domain dialogue response generation recently. As many such metrics have achieved superior correlations with human judgement in comparison to traditional automated metrics, it is now an important issue to investigate: 1) What aspects of a response need to be considered? 2) How to assess each aspect with an evaluator? 3) How to compose the aspects to obtain an overall understanding of the response quality? This paper attempted to answer the questions with the following solutions: 1) Understandability, sensibleness, and likability. 2) BERT-VUP, BERT-NUP, and MLM-likelihood. 3) A hierarchical composition of the three scores (a_1 * score_u + a_2 * score_s + a_3 * score_s * score_l).\nSTRENGTHS: - Instead of focusing on \"what models can be used for evaluating\", the paper studies \"what aspects need to be evaluated\". The questions investigated are interesting and meaningful for the community.\n- A considerable amount of experiments have been carried out to verify the authors' hypotheses, and relatively good results have been achieved to support the hypotheses.\nWEAKNESSES: - The authors used USR (which is USL-A in this paper, [Mehri and Eskenazi, 2020]) as a main baseline, compared to which configurability is not novel as it has been proposed in USR.\n- The proposed USL-H differs from USL-A in that it composes three scores in a hierarchy. I consider such hierarchy as the main novelty. In order to verify the validity of the hierarchy (Section 6.1), the authors ought to compare three score composition methods (A. Human vanilla, B. Human USL-A, C. Human USL-L). However, only A and C have been compared (Figure 3).\n- The proposed USL-H achieves the best correlations according to Table 4, but it only achieves marginal improvement over the second best metric, BERT-NUP.  - Some terms are not explained: 1) What is the difference between PONE and EN-PONE? 2) What is the difference between \"weighted\" version and \"unweighted\" version of USL_S-A/H in Table 4? ", "label": [[761, 881, "Jus_pos_1"], [882, 958, "Eval_pos_1"], [961, 1128, "Eval_pos_2"], [1143, 1241, "Eval_neg_1"], [1243, 1321, "Jus_neg_1"], [1324, 1457, "Eval_neg_2"], [1458, 1687, "Jus_neg_2"]]}
{"id": 331, "review": "The paper describes the first phase of the Indigenous language project in Canada. The paper is very well-written and easy to follow. And the project is fascinating! I am quite impressed by the scale at which the authors have worked across so many technologies and so many languages. I wish there were more projects like this. Really well done. \nI think while the paper is very informative, for Coling audience, it might be probably more interesting as well as important to know about high level takeaways from this project. For instance, I would loved to know: a) what were some of the top challenges - scientific, linguistic, administrative, political and logistic - that you faced during this project?\nb) what were some of the strategies that you tried to overcome these challenges, and which ones worked the best?\nc) Which of these technologies have been deployed? Who are using them? What is the impact - socially and/or economically?\nd) What challenges have you faced/do you foresee in deployment or adoption of some of these technologies?\ne) What would be some of your recommendations on which technologies one should focus on while helping the indigenous communities? How should these decisions be made? What role does the speaker communities have to play in this decision making?\nand so on.\nI believe with your profound experience and expertise, you could really inform the readers about these questions. On the other hand the process of building the technologies, while useful to learn from, are mostly known to this community and probably can be described in a table where you list the language, the technologies you built, accuracy indicators and any other remark on approach used etc.\nSome specific comments: -You mention that many indigenous languages do not have a script. For those, TTS (and ASR) could be built using the following approach:  'Text to Speech in New Languages without a Standardized Orthography', Sunayana Sitaram , Gopala Anumanchipalli, Justin Chiu, Alok Parlikar and Alan W Black, SSW 8 (2013), Barcelona, Spain - You could consider PrathamBooks and their Storyweaver platform for crowdsourced creation of indigenous stories and books, whenever there is a script. https://storyweaver.org.in/prathambooks - It is unclear to me why a polysynthetic language will be difficult to teach. It is possible that the way syncretic languages are taught - that methodology may not be effective for polysynthetic languages. However, there might be other teaching methodologies that works. ", "label": [[82, 132, "Eval_pos_1"], [133, 344, "Major_claim"], [345, 388, "Eval_pos_5"], [390, 523, "Eval_neg_1"], [524, 1298, "Jus_neg_1"], [1299, 1412, "Eval_pos_6"], [1413, 1696, "Eval_neg_2"], [2239, 2316, "Eval_neg_3"]]}
{"id": 338, "review": "The paper studies review generation given a set of attribute identifiers such as user ID, product ID and rating. As this information is scarce, the authors propose to select more information related to these attributes to enrich the generation process. The evaluation results, obtained by comparison with other approaches and human judgments, are promising.\nThe paper is appropriate for the conference, well-written and easy to follow. The addressed problem is interesting, and the approach and obtained results are promising. The idea of \"enriching\" the input information by collecting other reviews that are related to the input is interesting and solves the problem of having almost no information at the beginning of the process.\nIn the SL-Reflect part, it is said that a ground-truth review Y is obtained. This Y review is very important for the rest of the process, but how is it obtained? Would it be available if this is the first review of a particular product or by a specific user?\nI am also curious about how this technology could be used in the real world... As a user who reads reviews when choosing to buy something, do I want to rely on \"false\" reviews that could not be accurate or express the real characteristics of the product? On the other hand, I think this kind of approach could be very useful to give a user the first draft of his/her review, which can later be corrected in a short time and be more beneficial for other users... ", "label": [[253, 357, "Eval_pos_1"], [358, 402, "Major_claim"], [403, 435, "Eval_pos_2"], [436, 472, "Eval_pos_3"], [478, 526, "Eval_pos_4"], [527, 733, "Eval_pos_5"], [993, 1071, "Eval_neg_1"], [1072, 1455, "Jus_neg_1"]]}
{"id": 343, "review": "This paper presents results on the UD treebanks to test delexicalized transfer parsers and an unsupervised parser which is enriched with external probabilities.\nThe paper is interesting, but I think it could be improved further.\n(5.2) \"McDonald et al. (2011) presented 61.7% of averaged accuracy over 8 languages. On the same languages, our transfer parser on UD reached 70.1%.\" \nMcdonald et al could not use the UD treebanks since they were not available, you should definitely state that this is the case here.\nIn footnote 9 you say: \"We used the Malt parser with its default feature set. \nTuning in this specific delexicalized task would probably bring a bit better results.\" You are using MaltParser with default settings, why don't you use MaltOptimizer? Optimizing one model would be very easy. \nIn the same way MSTParser could be optimized further. \nIn the same line, why don't you use more recent parsers that produce better results? These parsers have been already applied to universal dependencies with the leave one out setup (see references below). For instance, the authors say that  the unsupervised parser \"performs better for languages from less resourced language families (non-Indo-European)\", it would be interesting to see whether this holds with more recent (and cross lingual) parsers.\nProbabilities: Why do you use this probabilities? it seems like a random decision (Tables 3-4) (esp 3), at least we need more details or a set of experiments to see whether they make sense or not.\nThere are some papers that the authors should take into account.\n1. Cross-Lingual Dependency Parsing with Universal Dependencies and Predicted PoS Labels J Tiedemann 2. One model, two languages: training bilingual parsers with harmonized treebanks D Vilares, MA Alonso, C G\u00c3\u00b3mez-Rodr\u00c3\u00adguez  (it presents results with MaltParser) And for results with more recent parsers (and also delexicalized parsers): 1. Crosslingual dependency parsing based on distributed representations. \nJiang Guo, Wanxiang Che, David Yarowsky, Haifeng Wang, and Ting Liu. 2015.  In Proc. of ACL 2. Many languages, one parser W Ammar, G Mulcaire, M Ballesteros, C Dyer, NA Smith -Minor points:  I don't think we need Table 1 and Table 2, this could be solved with a footnote to the UD website. Perhaps Table 2 should be included due to the probabilities, but Table 1 definitely not. ", "label": [[161, 185, "Eval_pos_1"], [187, 228, "Eval_neg_1"]]}
{"id": 344, "review": "- Strengths: 1. The idea of assigning variable-length document segments with dependent topics is novel. This prior knowledge is worth incorporated in the LDA-based framework. \n2. Whereas we do not have full knowledge on recent LDA literature, we find the part of related work quite convincing. \n3. The method proposed for segment sampling with O(M) complexity is impressive. \nIt is crucial for efficient computation.  - Weaknesses: 1. Compared to Balikas COLING16's work, the paper has a weaker visualization (Fig 5), which makes us doubt about the actual segmenting and assigning results of document. It could be more convincing to give a longer exemplar and make color assignment consistent with topics listed in Figure 4. \n2. Since the model is more flexible than that of Balikas COLING16, it may be underfitting, could you please explain this more?\n- General Discussion: The paper is well written and structured. The intuition introduced in the Abstract and again exemplified in the Introduction is quite convincing. The experiments are of a full range, solid, and achieves better quantitative results against previous works. If the visualization part is stronger, or explained why less powerful visualization, it will be more confident. Another concern is about computation efficiency, since the seminal LDA work proposed to use Variational Inference which is faster during training compared to MCMC, we wish to see the author\u2019s future development. ", "label": [[16, 103, "Eval_pos_1"], [104, 175, "Eval_pos_2"], [179, 294, "Eval_pos_3"], [298, 375, "Eval_pos_4"], [376, 416, "Jus_pos_4"], [435, 509, "Eval_neg_1"], [517, 725, "Jus_neg_1"], [729, 792, "Jus_neg_2"], [793, 852, "Eval_neg_2"], [875, 915, "Eval_pos_5"], [917, 1020, "Eval_pos_6"], [1021, 1129, "Eval_pos_7"], [1242, 1290, "Eval_neg_3"], [1291, 1404, "Jus_neg_3"], [1405, 1454, "Eval_neg_3"]]}
{"id": 348, "review": "This paper proposes a joint neural modelling approach to PAS analysis in Japanese, based on Grid-RNNs, which it compares variously with a conventional single-sequence RNN approach.\nThis is a solidly-executed paper, targeting a well-established task from Japanese but achieving state-of-the-art results at the task, and presenting the task in a mostly accessible manner for those not versed in Japanese. Having said that, I felt you could have talked up the complexity of the task a bit, e.g. wrt your example in Figure 1, talking through the inherent ambiguity between the NOM and ACC arguments of the first predicate, as the NOM argument of the second predicate, and better describing how the task contrasts with SRL (largely through the ambiguity in zero pronouns). I would also have liked to have seen some stats re the proportion of zero pronouns which are actually intra-sententially resolvable, as this further complicates the task as defined (i.e. needing to implicitly distinguish between intra- and inter-sentential zero anaphors). One thing I wasn't sure of here: in the case of an inter-sentential zero pronoun for the argument of a given predicate, what representation do you use? Is there simply no marking of that argument at all, or is it marked as an empty argument? My reading of the paper is that it is the former, in which case there is no explicit representation of the fact that there is a zero pronoun, which seems like a slightly defective representation (which potentially impacts on the ability of the model to capture zero pronouns); some discussion of this would have been appreciated.\nThere are some constraints that don't seem to be captured in the model (which some of the ILP-based methods for SRL explicitly model, e.g.): (1) a given predicate will generally have only one argument of a given type (esp. NOM and ACC); and (2) a given argument generally only fills one argument slot for a given predicate. I would have liked to have seen some analysis of the output of the model to see how well the model was able to learn these sorts of constraints. More generally, given the mix of numbers in Table 3 between Single-Seq and Multi-Seq (where it is really only NOM where there is any improvement for Multi-Seq), I would have liked to have seen some discussion of the relative differences in the outputs of the two models: are they largely identical, or very different but about the same in aggregate, e.g.? In what contexts do you observe differences between the two models? Some analysis like this to shed light on the internals of the models would have made the difference between a solid and a strong paper, and is the main area where I believe the paper could be improved (other than including results for SRL, but that would take quite a bit more work).\nThe presentation of the paper was good, with the Figures aiding understanding of the model. There were some low-level language issues, but nothing major: l19: the error propagation -> error propagation l190: an solution -> a solution l264 (and Figure 2): a bread -> bread l351: the independence -> independence l512: the good -> good l531: from their model -> of their model l637: significent -> significance l638: both of -> both and watch casing in your references (e.g. \"japanese\", \"lstm\", \"conll\", \"ilp\") ", "label": [[181, 213, "Eval_pos_1"], [318, 401, "Eval_pos_2"], [420, 486, "Eval_neg_1"], [487, 767, "Jus_neg_1"], [1613, 1683, "Eval_neg_2"], [1753, 2081, "Jus_neg_2"], [2082, 2505, "Jus_neg_3"], [2506, 2706, "Eval_neg_3"]]}
{"id": 349, "review": "- Strengths: -- A well-motivated approach, with a clear description and solid results.\n- Weaknesses: -- Nothing substantial other than the comments below.  - General Discussion: The paper describes a new method called attention-over-attention for reading comprehension. First layers of the network compute a vector for each query word and document word, resulting in a |Q|xK matrix for the query and a |D|xK for the document. Since the answer is a document word, an attention mechanism is used for assigning weights to each word, depending on their interaction with query words. In this work, the authors deepen a traditional attention mechanism by computing a weight for each query word through a separate attention and then using that to weight the main attention over document words. Evaluation is properly conducted on benchmark datasets, and various insights are presented through an analysis of the results as well as a comparison to prior work. I think this is a solid piece of work on an important problem, and the method is well-motivated and clearly described, so that researchers can easily reproduce results and apply the same techniques to other similar tasks.\n- Other remarks: -- p4, Equation 12: I am assuming i is iterating over training set and p(w) is referring to P(w|D,Q) in the previous equation? Please clarify to avoid confusion.\n-- I am wondering whether you explored/discussed initializing word embeddings with existing vectors such as Google News or Glove? Is there a reason to believe the general-purpose word semantics would not be useful in this task?\n-- p6 L589-592: It is not clear what the authors are referring to when they say 'letting the model explicitly learn weights between individual attentions'? Is this referring to their own architecture, more specifically the GRU output indirectly affecting how much attention will be applied to each query and document word? Clarifying that would be useful. Also, I think the improvement on validation is not 4.1, rather 4.0 (72.2-68.2).\n-- p7 Table 5: why do you think the weight for local LM is relatively higher for the CN task while the benefit of adding it is less? Since you included the table, I think it'll be nice to provide some insights to the reader.\n-- I would have liked to see the software released as part of this submission.\n-- Typo p2 L162 right column: \"is not that effective than expected\" --> \"is not as effective as expected\"?\n-- Typo p7 L689 right column: \"appear much frequent\" --> \"appears more frequently\"?\n-- Typo p8 L719-721 left column: \"the model is hard to\" --> \"it is hard for the model to\"? & \" hard to made\" --> \"hard to make\"? ", "label": [[15, 42, "Eval_pos_1"], [43, 67, "Eval_pos_2"], [68, 86, "Eval_pos_3"], [952, 1173, "Major_claim"], [1597, 1736, "Eval_neg_1"], [1737, 1903, "Jus_neg_1"]]}
{"id": 350, "review": "The paper introduces a general method for improving NLP tasks using embeddings from language models. Context independent word representations have been very useful, and this paper proposes a nice extension by using context-dependent word representations obtained from the hidden states of neural language models. \nThey show significant improvements in tagging and chunking tasks from including embeddings from large language models. There is also interesting analysis which answers several natural questions.\nOverall this is a very good paper, but I have several suggestions: -Too many experiments are carried out on the test set. Please change Tables 5 and 6 to use development data -It would be really nice to see results on some more tasks - NER tagging and chunking don't have many interesting long range dependencies, and the language model might really help in those cases. I'd love to see results on SRL or CCG supertagging.\n-The paper claims that using a task specific RNN is necessary because a CRF on top of language model embeddings performs poorly. It wasn't clear to me if they were backpropagating into the language model in this experiment - but if not, it certainly seems like there is potential for that to make a task specific RNN unnecessary. ", "label": [[433, 508, "Eval_pos_1"], [509, 543, "Major_claim"], [577, 630, "Eval_neg_1"], [631, 683, "Jus_neg_1"], [685, 742, "Eval_neg_2"], [744, 879, "Eval_neg_3"]]}
{"id": 352, "review": "- Strengths: - Weaknesses: Many grammar errors, such as the abstract - General Discussion: ", "label": [[27, 68, "Eval_neg_1"]]}
{"id": 359, "review": "- Strengths: The authors propose a selective encoding model as extension to the sequence-to-sequence framework for abstractive sentence summarization. The paper is very well written and the methods are clearly described. The proposed methods are evaluated on standard benchmarks and comparison to other state-of-the-art tools are presented, including significance scores.  - Weaknesses: There are some few details on the implementation and on the systems to which the authors compared their work that need to be better explained.  - General Discussion: - Major review: - I wonder if the summaries obtained using the proposed methods are indeed abstractive. I understand that the target vocabulary is build out of the words which appear in the summaries in the training data. But given the example shown in Figure 4, I have the impression that the summaries are rather extractive. \nThe authors should choose a better example for Figure 4 and give some statistics on the number of words in the output sentences which were not present in the input sentences for all test sets.\n- page 2, lines 266-272: I understand the mathematical difference between the vector hi and s, but I still have the feeling that there is a great overlap between them. Both \"represent the meaning\". Are both indeed necessary? Did you trying using only one of them.\n- Which neural network library did the authors use for implementing the system? \nThere is no details on the implementation.\n- page 5, section 44: Which training data was used for each of the systems that the authors compare to? Diy you train any of them yourselves?\n- Minor review: - page 1, line 44: Although the difference between abstractive and extractive summarization is described in section 2, this could be moved to the introduction section. At this point, some users might no be familiar with this concept.\n- page 1, lines 93-96: please provide a reference for this passage: \"This approach achieves huge success in tasks like neural machine translation, where alignment between all parts of the input and output are required.\"\n- page 2, section 1, last paragraph: The contribution of the work is clear but I think the authors should emphasize that such a selective encoding model has never been proposed before (is this true?). Further, the related work section should be moved to before the methods section.\n- Figure 1 vs. Table 1: the authors show two examples for abstractive summarization but I think that just one of them is enough. Further, one is called a figure while the other a table.\n- Section 3.2, lines 230-234 and 234-235: please provide references for the following two passages: \"In the sequence-to-sequence machine translation (MT) model, the encoder and decoder are responsible for encoding input sentence information and decoding the sentence representation to generate an output sentence\"; \"Some previous works apply this framework to summarization generation tasks.\"\n- Figure 2: What is \"MLP\"? It seems not to be described in the paper.\n- page 3, lines 289-290: the sigmoid function and the element-wise multiplication are not defined for the formulas in section 3.1.\n- page 4, first column: many elements of the formulas are not defined: b (equation 11), W (equation 12, 15, 17) and U (equation 12, 15), V (equation 15).\n- page 4, line 326: the readout state rt is not depicted in Figure 2 (workflow).\n- Table 2: what does \"#(ref)\" mean?\n- Section 4.3, model parameters and training. Explain how you achieved the values to the many parameters: word embedding size, GRU hidden states, alpha, beta 1 and 2, epsilon, beam size.\n- Page 5, line 450: remove \"the\" word in this line? \" SGD as our optimizing algorithms\" instead of \"SGD as our the optimizing algorithms.\"\n- Page 5, beam search: please include a reference for beam search.\n- Figure 4: Is there a typo in the true sentence? \" council of europe again slams french prison conditions\" (again or against?)\n- typo \"supper script\" -> \"superscript\" (4 times) ", "label": [[151, 220, "Eval_pos_1"], [221, 371, "Eval_pos_2"], [387, 530, "Eval_neg_1"], [571, 656, "Eval_neg_2"], [657, 1073, "Jus_neg_2"], [1419, 1461, "Eval_neg_3"], [1484, 1603, "Eval_neg_4"]]}
{"id": 360, "review": "- Strengths: The idea of hard monotonic attention is new and substantially different from others.\n- Weaknesses: The experiment results on morphological inflection generation is somewhat mixed. The proposed model is effective if the amount of training data is small (such as CELEX). It is also effective if the alignment is mostly monotonic and less context sensitive (such as Russian, German and Spanish).\n- General Discussion: The authors proposed a novel neural model for morphological inflection generation which uses \"hard attention\", character alignments separately obtained by using a Bayesian method for transliteration. It is substantially different from the previous state of the art neural model for the task which uses \"soft attention\", where character alignment and conversion are solved jointly in the probabilistic model.\nThe idea is novel and sound. The paper is clearly written. The experiment is comprehensive. The only concern is that the proposed method is not necessarily the state of the art in all conditions. It is suitable for the task with mostly monotonic alignment and with less context sensitive phenomena. The paper would be more convincing if it describe the practical merits of the proposed method, such as the ease of implementation and computational cost. ", "label": [[12, 97, "Eval_pos_1"], [112, 192, "Eval_neg_1"], [193, 405, "Jus_neg_1"], [836, 864, "Eval_pos_2"], [865, 894, "Eval_pos_3"], [895, 927, "Eval_pos_4"], [929, 1031, "Eval_neg_2"], [1135, 1229, "Eval_neg_3"], [1230, 1289, "Jus_neg_3"]]}
{"id": 367, "review": "- Strengths: Authors generate a dataset of \u201crephrased\u201d captions and are planning to make this dataset publicly available.\nThe way authors approached DMC task has an advantage over VQA or caption generation in terms of metrics. It is easier and more straightforward to evaluate problem of choosing the best caption. Authors use accuracy metric. \nWhile for instance caption generation requires metrics like BLUE or Meteor which are limited in handling semantic similarity.\nAuthors propose an interesting approach to \u201crephrasing\u201d, e.g. selecting decoys. They draw decoys form image-caption dataset. E.g. decoys for a single image come from captions for other images. These decoys however are similar to each other both in terms of surface (bleu score) and semantics (PV similarity). \nAuthors use lambda factor to decide on the balance between these two components of the similarity score. I think it would be interesting to employ these for paraphrasing.\nAuthors support their motivation for the task with evaluation results. They show that a system trained with the focus on differentiating between similar captions performs better than a system that is trained to generate captions only. These are, however, showing that system that is tuned for a particular task performs better on this task.\n- Weaknesses:  It is not clear why image caption task is not suitable for comprehension task and why author\u2019s system is better for this. In order to argue that system can comprehend image and sentence semantics better one should apply learned representation, e.g. embeddings. E.g. apply representations learned by different systems on the same task for comparison.\nMy main worry about the paper is that essentially authors converge to using existing caption generation techniques, e.g. Bahdanau et al., Chen et al. They way formula (4) is presented is a bit confusing. From formula it seems that both decoy and true captions are employed for both loss terms. However, as it makes sense, authors mention that they do not use decoy for the second term. \nThat would hurt mode performance as model would learn to generate decoys as well. The way it is written in the text is ambiguous, so I would make it more clear either in the formula itself or in the text. Otherwise it makes sense for the model to learn to generate only true captions while learning to distinguish between true caption and a decoy.\n- General Discussion: Authors formulate a task of Dual Machine Comprehension. They aim to accomplish the task by challenging computer system to solve a problem of choosing between two very similar captions for a given image. Authors argue that a system that is able to solve this problem has to \u201cunderstand\u201d the image and captions beyond just keywords but also capture semantics of captions and their alignment with image semantics.\nI think paper need to make more focus on why chosen approach is better than just caption generation and why in their opinion caption generation is less challenging for learning image and text representation and their alignment.\nFor formula (4). I wonder if in the future it is possible to make model to learn \u201cnot to generate\u201d decoys by adjusting second loss term to include decoys but with a negative sign. Did authors try something similar? ", "label": [[471, 526, "Eval_pos_1"], [1307, 1429, "Eval_neg_1"], [1430, 1657, "Jus_neg_1"], [1658, 1772, "Eval_neg_2"], [1774, 1808, "Jus_neg_2"], [1808, 1861, "Eval_neg_3"], [1862, 2044, "Jus_neg_3"], [2045, 2392, "Jus_neg_3"], [2826, 3053, "Eval_neg_4"]]}
{"id": 373, "review": "26: An End-to-End Model for Question Answering over Knowledge Base with Cross-Attention Combining Global Knowledge This paper presents an approach for factoid question answering over a knowledge graph (Freebase), by using a neural model that attempts to learn a semantic correlation/correspondence between various \"aspects\" of the candidate answer (e.g., answer type, relation to question entity, answer semantic, etc.) and a subset of words of the question. A separate correspondence component is learned for each \"aspect\" of the candidate answers. The two key contributions of this work are: (1) the creation of separate components to capture different aspects of the candidate answer, rather than relying on a single semantic representation, and (2) incorporating global context (from the KB) of the candidate answers.\nThe most interesting aspect of this work, in my opinion, is the separation of candidate answer representation into distinct aspects, which gives us (the neural model developer) a little more control over guiding the NN models towards information that would be more beneficial in its decision making. It sort of harkens to the more traditional algorithms that rely on feature engineering. But in this case the \"feature engineering\" (i.e., aspects) is more subtle, and less onerous. I encourage the authors to continue refining this system along these lines.\nWhile the high-level idea is fairly clear to a reasonably informed reader, the devil in the details would make it hard for some audience to immediately grasp key insights from this work. Some parts of the paper could benefit from more explanation... Specifically: (1) Context aspect of candidate answers (e_c) is not clearly explained in the paper. Therefore, the last two sentences of Section 3.2.2 seem unclear.\n(2) Mention of OOV in the abstract and introduction need more explanation. As such, I think the current exposition in the paper assumes a deep understanding of prior work by the reader.\n(3) The experiments conducted in this paper restrict comparison to IR-based system -- and the reasoning behind this decision is reasonable. But it is not clear then why the work of Yang et al. (2014) -- which is described to be SP-based -- is part of the comparison. While, I am all for including more systems in the comparison, there seem to be some inconsistencies in what should and should not be compared. Additionally, I see not harm in also mentioning the comparable performance numbers for the best SP-based systems.\nI observe in the paper that the embeddings are learned entirely from the training data. I wonder how much impact the random initialization of these embeddings has on the end performance. It would be interesting to determine (and list) the variance if any. Additionally, if we were to start with pre-trained embeddings (e.g., from word2vec) instead of the randomly initialized ones, would that have any impact?\nAs I read the paper, one possible direction of future work that occurred to me was to possibly include structured queries (from SP-based methods) as part of the cross-attention mechanism. In other words, in addition to using the various aspects of the candidate answers as features, one could include structured queries that generate the produce that candidate answer as an additional aspect of the candidate answer. An attention mechanism could then also focus on various parts of the structured query, and its (semantic) matches to the input question as an additional signal for the NN model. Just a thought.\nSome notes regarding the positioning of the paper: I hesitate to call the model proposed here \"attention\" models, because (per my admittedly limited understanding) attention mechanisms apply to \"encoder-decoder\" situations, where semantics expressed in one structured form (e.g., image, sentence in one language, natural language question, etc.) are encoded into an abstract representation, and then generated into another structured form (e.g., caption, sentence in another language, structured query, etc.). The attention mechanism allows the \"encoder\" to jump around and attend to different parts of the input (instead of sequentially) as the output is being generated by the decoder. This paper does not appear to fit this notion, and may be confusing to a broader audience.\n------ Thank you for clarifications in the author response. ", "label": [[822, 954, "Eval_pos_1"], [955, 1302, "Jus_pos_1"], [1385, 1452, "Eval_pos_2"], [1454, 1565, "Jus_neg_1"], [1566, 1625, "Eval_neg_1"], [1629, 2502, "Jus_neg_1"]]}
{"id": 389, "review": "paper_summary\nThis paper proposes the regularity-aware and regularity-agnostic models for Chinese NER and present the combined loss to train them. The regularity-aware model first performs a local self-attention applied over span representation and combine the resulting attentive representation with original span representation. The regularity-agnostic model is based on the biaffine attention decoder to mainly focus on detecting the boundary of an entity mention. Both regularity-aware and -agnostic models are trained such their backbone hidden representations are orthogonal. In the experiment results, the regularity-aware models lead to improvements over the previous models on three Chinese NER datasets. \nsummary_of_strengths\n- The presentation is relatively clear and the paper is easy to follow.\n-The proposed regularity-aware and -agnostic models and their joint training method are interesting and novel.  -The experiment results show strong performance, outperforming recent models on three Chinese NER datasets. \nsummary_of_weaknesses\n- It seems that the regularity-aware models don\u2019t exploit any lexicon, but only relying on local self-attention over span representations formulated in Eq. (6). It is not clear how this local self-attention of Eq. (6) is related to \u201cregularity\u201d or \u201cregular types\u201d of entities. The motivation of paper is not very tightly related to the proposed regularity-aware models, i.e. how it can be seen as an alternative model of the previous lexicon-based models?  -During inference, regularity-aware models are only used as in Section 3.5. Then, the regularity-agnostic models are not used during inference? Is this model used in only the orthogonality space restriction of Eq. (16)? Detailed descriptions are provided.  -In Table 3, it is not clear what is the difference b/w reg-aware&agnostic and RICON models. The models compared in the experiments need to be clearly described.  -The proposed models could be applicable to other English NER datasets, i.e. the regularity concept can be extended to other languages. Otherwise, the authors need to present why the addressed issue of the proposed model appears in Chinese NER task, not in other English NER tasks, . \ncomments,_suggestions_and_typos\n- (please see weaknesses) ", "label": [[738, 774, "Eval_pos_1"], [779, 806, "Eval_pos_2"], [809, 919, "Eval_pos_3"], [921, 967, "Eval_pos_4"], [969, 1026, "Jus_pos_4"], [1053, 1211, "Jus_neg_1"], [1212, 1327, "Eval_neg_1"], [1328, 1419, "Eval_neg_2"], [1421, 1506, "Jus_neg_2"], [1766, 1926, "Eval_neg_3"]]}
{"id": 393, "review": "- Strengths: This paper systematically investigated how context types (linear vs dependency-based) and representations (bound word vs unbound word) affect word embedding learning. They experimented with three models (Generalized Bag-Of-Words, Generalized Skip-Gram and Glove) in multiple different tasks (word similarity, word analogy, sequence labeling and text classification). \nOverall,  1)            It is well-written and structured. \n2)            The experiments are very thoroughly evaluated. The analysis could help researchers to choose different word embeddings or might even motivate new models. \n3)            The attached software can also benefit the community.  - Weaknesses:  The novelty is limited.  - General Discussion: For the dependency-based context types, how does the dependency parsing affect the overall performance? Is it fair to compare those two different context types since the dependency-based one has to rely on the predicted dependency parsing results (in this case CoreNLP) while the linear one does not? ", "label": [[13, 179, "Eval_pos_1"], [180, 380, "Jus_pos_1"], [404, 440, "Eval_pos_2"], [455, 501, "Eval_pos_3"], [502, 609, "Eval_pos_4"], [624, 677, "Eval_pos_5"], [694, 717, "Eval_neg_1"]]}
{"id": 395, "review": "paper_summary\nThe authors introduce LexGLUE, a benchmark dataset to evaluate the performance of NLP models in legal NLU tasks. LexGLUE is based on seven English publicly available legal NLP datasets, selected using criteria largely from SuperGLUE. The datasets span text classification (6 datasets) and legal question answering (1 dataset) tasks. The authors introduced several simplifications to the datasets to make the benchmark more standardized and easily accessible. \nsummary_of_strengths\n1) The paper is very clear and well written. \n2) I believe that the legal NLP community will definitely benefit from a standardized and easily accessible benchmark to help facilitate and encourage research on legal NLP. \n3) LexGLUE could also encourage future work to push towards generalized models that could handle various legal NLP tasks with limited annotated data. \nsummary_of_weaknesses\n1) Although the authors provide a very clear description of the various tasks and datasets that are included in LexGLUE, it would be helpful for the readers to see examples of what the input(s)/output(s) are for each task as part of the Appendix. \ncomments,_suggestions_and_typos\nSee weaknesses. ", "label": [[498, 540, "Eval_pos_1"], [544, 611, "Eval_pos_2"], [612, 657, "Jus_pos_2"], [659, 713, "Eval_pos_2"], [719, 759, "Eval_pos_3"], [760, 864, "Jus_pos_3"], [892, 1136, "Eval_neg_1"]]}
{"id": 397, "review": "paper_summary\nI reviewed this paper in September. Since the authors did not update their paper, my comments and scores for this paper stay unchanged. \nsummary_of_strengths\nSee review in Sept. \nsummary_of_weaknesses\nSee review in Sept. \ncomments,_suggestions_and_typos\nSee review in Sept. ", "label": []}
{"id": 412, "review": "paper_summary\nThis paper presents an algorithm to detect if unargmaxable classes occur given a certain model. \nOn top of that, the authors find that although theoretically possible, the \"stolen probability problem (SPP)\" rarely occurs in reality.\nTo expand a bit, the authors start by revisiting the SPP, which sometimes goes by the name of softmax bottleneck in literature. \nIn its essence, the problem arises from the hidden dimension being smaller than the output vocabulary size. \nThe problem basically states that the model can not output arbitrary distribution over the vocabulary.\nMoving on, the authors propose an approximate algorithm that pre-filters candidates, and then an exact algorithm is applied to decide if a certain class is unargmaxable.\nWith the tools ready, the authors then examine various LMs and MT models for the SPP. \nUnsurprisingly, most of the models do not suffer from the problems and those who do, only fail to argmax some \"corner case\" subwords. \nsummary_of_strengths\nThe authors provided a nice geometrical intuition for the SPP. \nAlthough the problem is known before, I think the authors did a good job in presenting their take on the problem.\nThe authors propose an algorithm to find the unargmaxable classes which is a combination of an approximate filtering and exact search.\nThe authors examined a large quantity of LM and MT models and only found a limited number of models suffering from SPP. \nAdditionally, for those models, they show that the problem is not severe as the classes are \"corner case\" subwords.\nThe paper is easy-to-follow.\nThe final comment in the future work section about the connection between the difficulty of model training and the difficulty of finding the unargmaxable classes is definitely interesting. \nsummary_of_weaknesses\nWhile there exist many papers discussing the softmax bottleneck or the stolen probability problem, similar to what the authors found, I personally have not found enough evidence in my work that the problem is really severe. \nAfter all, there are intrinsic uncertainties in the empirical distributions of the training data, and it is quite natural for us to use a smaller  hidden dimension size than the vocabulary size, because after all, we call them \"word embeddings\" for a reason. \nI guess what I mean to say here is that the problem is of limited interest to me (which says nothing about a broader audience) because the results agree very well with my expectations. \nThis is definitely not against the authors because they did a good job in showing this via their algorithm and the concrete experiments. \ncomments,_suggestions_and_typos\nI feel like the authors could mention and expand on the implications when beam search is used. \nBecause in reality, especially that many MT models are considered in the paper, greedy search is seldomly used. \nIn other words, \"even if greedy search is used, SPP is not a big deal, let alone that in reality we use beam search\", something like that.\nCompared to the main text, I am personally more interested in the point brought up at L595. \nWhat implications are there for the training of our models? \nHow does the gradient search algorithm decide on where to put the word vectors and hidden state vector? \nIs there anything we, i.e. the trainers of the NNs, can do to make it easier for the NNs?\nSmall issues: -L006, as later written in the main text, \"thousands\" is not accurate here. Maybe add \"on the subword level\"?\n-L010, be predicted -L034, personally, I think \"expressiveness\" is more commonly used, this happens elsewhere in the paper as well.\n-L082, autoencoder -L104, greater than or equal to ", "label": [[1001, 1064, "Eval_pos_1"], [1065, 1178, "Eval_pos_2"], [1179, 1550, "Jus_pos_2"], [1551, 1579, "Eval_pos_3"], [1580, 1769, "Eval_pos_4"], [2277, 2357, "Eval_neg_1"], [2358, 2600, "Eval_neg_1"]]}
{"id": 420, "review": "The paper presents a new neural approach for summarization. They build on a standard encoder-decoder with attention framework but add a network that gates every encoded hidden state based on summary vectors from initial encoding stages. Overall, the method seems to outperform standard seq2seq methods by 1-2 points on three different evaluation sets.\nOverall, the technical sections of the paper are reasonably clear. Equation 16 needs more explanation, I could not understand the notation. The specific contribution,  the selective mechanism, seems novel and could potentially be used in other contexts.  The evaluation is extensive and does demonstrate consistent improvement. One would imagine that adding an additional encoder layer instead of the selective layer is the most reasonable baseline (given the GRU baseline uses only one bi-GRU, this adds expressivity), and this seems to be implemented Luong-NMT. My one concern is LSTM/GRU mismatch. Is the benefit coming from just GRU switch?  The quality of the writing, especially in the intro/abstract/related work is quite bad. This paper does not make a large departure from previous work, and therefore a related work nearby the introduction seems more appropriate. In related work, one common good approach is highlighting similarities and differences between your work and previous work, in words before they are presented in equations. Simply listing works without relating them to your work is not that useful. Placement of the related work near the intro will allow you to relieve the intro of significant background detail and instead focus on more high level. ", "label": [[352, 418, "Eval_pos_1"], [492, 605, "Eval_pos_2"], [607, 679, "Eval_pos_3"], [916, 953, "Eval_neg_1"], [953, 996, "Jus_neg_1"], [997, 1085, "Eval_neg_2"], [1086, 1147, "Eval_neg_3"], [1149, 1224, "Jus_neg_3"]]}
{"id": 424, "review": "paper_summary\nXDBERT is a cross-model model for NLU that is distilled from CLIP-T, the vision-grounded text encoder of CLIP. The resulted model outperforms its teachers on GLUE and other tasks. \nsummary_of_strengths\n- The idea behind the paper is intuitive and straightforward.\n-The results are a little surprising to me and they look good.\n-Experimental details are provided in the appendix, which is good for reproducibility.\n-The authors only use Wiki103 as the distillation corpus, which is easy to reproduce and desirable for computational budgets. \nsummary_of_weaknesses\n### Original comments from previous review -The main concern is the technical novelty of this paper. There is little novelty since the distillation technique is not new while the idea of cross-model training is not new, either.\n-I'm surprised that in Table 1 CLIP-T performs so badly even when finetuned. Can you explain why?\n-Is it possible that the improvement of the distilled cross-modal model comes from ensembling instead of visual grounding?\n-I suggest the authors make the weights publically available (can be anonymous during double-blind reviewing) so the results can be easily verified by the community.\nAlthough some details are vague and need further investigation, I think the contribution is enough to be accepted as a short paper.\n### After rebuttal Thanks for resubmitting the paper with a response. I've carefully read the response.\n-**Re. To Reviewer2.1**: First, distilling B task into A task improves A is not so surprising. This idea has been explored in Intermediate-Task Transfer Learning. I agree that distilling cross-modal model to a single-modal model is somewhat novel. However, even self-distillation can improve accuracy so I have no idea to what extent is cross-modal distillation helpful.\n-**Re. To Reviewer2.2**: Good answer.\n-**Re. To Reviewer2.3**: Sorry for the confusion. Yes, when fine-tuning there is only the BERT encoder used. However, I suspect that substituting CLIP with any pretrained LM would also work. But again, as the authors point out, \"the linguistic competence is CLIP-T very low\" so this paper has some insights and presents previously unknown results. That's why I gave this paper a score of 3.5.\n-**Re. To Reviewer2.4**: That should be easy. Just manually reassign the weights in the original `state_dict` to a `state_dict` of a Hugging Face BERT model. That would work.\nThus, I keep my recommendation of weak acceptance for this paper and I suggest other reviewers consider increasing their scores. \ncomments,_suggestions_and_typos\nN/A ", "label": [[218, 277, "Eval_pos_1"], [279, 340, "Eval_pos_2"], [342, 427, "Eval_pos_3"], [429, 554, "Eval_pos_4"], [621, 678, "Eval_neg_1"], [678, 804, "Jus_neg_1"], [1192, 1323, "Major_claim"], [2405, 2534, "Major_claim"]]}
{"id": 427, "review": "(Summary) This paper proposes a question generation (QG) method in a text-based QA system. The proposed method that adopts a reinforcement learning (RL) framework tries to improve question quality by considering three kinds of rewards: fluency, relevance, and answerability. In the experiments using the HotpotQA dataset, the proposed method partly achieved a comparative performance, in the BLEU4 automatic metrics, with the SOTA model. The paper further presented a detailed analysis that compares the efficacy of the proposed rewards and concluded that the relevance reward was most effective in improving the question quality.\n(Major comments) The incorporation of the three rewards, respectively associated with fluency, relevance, and answerability, can be appreciated as a highly reasonable solution to improve the quality of generating questions. Accordingly, the proposed RL-based framework can be thought of as a plausible one, even it may not be very novel from the perspective of deep learning. Although the overall experimental results with respect to the automatic evaluation metrics are steady but not very impressive, the more important with the present paper is the insights gained from the carefully designed experiments.  Among them, the observation that optimizing the relevance reward achieved a substantial improvement in the human evaluation ratings would be highly beneficial. As exemplified by this insight, the paper successfully delivers several informative technical discussions. In addition, the paper itself is well-organized and highly readable. In sum, the reviewer would like to recommend the paper to be accepted.\n(Questions/Minor comments) -In the proposed training regime, the self0critical sequence training (SCST) plays a substantial role, but it is not well described. A brief description of this algorithm should be provided.\n-In this regard, the impact of the parameters $\\alpha_{flu}$ in equation (3), and $\\alpha_{rel} in (6), and $\\alpha_{ans}$ in (8) should be detailed, and preferably experimentally discussed.\n-The paper concluded that the relevance reward played the most crucial role in improving the question quality, and it could be a fairly important insight. However, it is not clear that this applies to other experimental situations. More specifically, it would be more interesting and beneficial if the impact of the balancing parameters  ($\\gamma$s) in the integrated loss function is discussed.\n-The interaction between the relevancy and the answerability could be further discussed. As far as the reviewer conceives, these two aspects would correlate somehow and suspects that the present results with the answerability might be associated with this interaction.\n-The paper argued that the results with the answerability would be associated with the ability of the underlying QA model. Would it be possible to discuss how the overall results might change or not by introducing an \"ideal\" stub QA module? ", "label": [[648, 854, "Eval_pos_1"], [855, 1006, "Eval_pos_2"], [1007, 1239, "Eval_pos_3"], [1241, 1400, "Eval_pos_4"], [1401, 1507, "Eval_pos_5"], [1508, 1576, "Eval_pos_6"], [1577, 1647, "Major_claim"], [2058, 2210, "Jus_pos_7"], [2212, 2288, "Eval_neg_1"], [2289, 2452, "Jus_neg_1"]]}
{"id": 429, "review": "paper_summary\nThe paper proposes an efficient and flexible BERT-based multi-task framework. The framework first trains a single-task model for each task using partial fine-tuning, where the bottom layers of BERT are frozen. Then the task-specific layers in each single-task model are compressed using knowledge distillation. The compressed layers are finally merged into one multi-task model, while the frozen layers can be shared across tasks. Experiments show that the framework can reduce up to 2/3 of the model parameters while hardly hurting the model performance. \nsummary_of_strengths\n1. The paper is well-written, and the method is easy to follow. \n2. In addition to the reduced number of model parameters, the inference speed also increases. \nsummary_of_weaknesses\n1. Missing comparison with other parameter-efficient fine-tuning methods, e.g., adapter-based methods. \n2. The training cost is much larger than conventional fine-tuning or other parameter-efficient fine-tuning methods. \ncomments,_suggestions_and_typos\nSee above. ", "label": [[595, 621, "Eval_pos_1"], [622, 656, "Eval_pos_2"], [660, 751, "Eval_pos_3"], [777, 847, "Eval_neg_1"], [848, 876, "Jus_neg_1"], [881, 994, "Eval_neg_2"]]}
{"id": 431, "review": "paper_summary\nThis paper focuses on the problem of fine-gained sentiment analysis, i.e., aspect sentiment triplet extraction (ASTE), and propose an Enhanced Multi-Channel Graph Convolutional Network model (EMC-GCN) to fully exploit task-dependent relations and the linguistic features of word-pairs for enhancing the performance of the triplet extraction in ASTE. Specifically, a biaffine function with constraints is used to embed different relations of word pairs for multi-channel graph construction and then aggregate information over such graph, meanwhile a refined strategy is proposed for measuring the match degree of word pairs. The experimental results on the benchmark datasets demonstrate the effectiveness of the proposed model. \nsummary_of_strengths\n1. The usage of rich relations and linguistic features and the construction of multi-channel graph for aspect sentiment extraction problem are novel, which is useful for task-dependent word representation learning and may be benefit for the researchers who encounter similar problems. \n2. The experiments presented in the paper are extensively evaluated. \nsummary_of_weaknesses\n1. The proposed model seems to be similar to the multi-relational GCN model in (\u201cModeling relational data with graph convolutional networks\u201d, 2018), please give the more discussion about the difference of such two works; 2. For Eq. (6), the learning of each channel of each element in adjacency matrix A via the biaffine attention module is unclear, especially for non-numeric linguistic features like POS combination or syntactic dependency types. \n3. The proposed end-to-end framework seems to be a little ad-hoc, such as the motivation of the selection of four types of linguistic features is obscure, meanwhile the linguistic features are extracted by external toolkits, and thus I wonder the influence of toolkit errors on the performance of proposed model. Furthermore, different linguistic features with constraints (R^{pos}, R^{dep}, R^{tbd}, R^{rpd}) may have different influences on the performance of the proposed model, an ablation study is needed for analysis, such as the independent influence of different linguistic features, and the cross-influences of different linguistic features like feature pair (POS, tree-based) or other combinations. In addition, the proposed model need to aggregate the information from different channel, and thus I wonder how it handle the situation if the values of different linguistic features are not in a same scale? \n4. The refined strategy seems to be a coarse strategy, which is only useful for the relations like \u201cA\u201d, actually I wonder the analysis of the refined strategy on different types of relations of word-pairs, which is benefit for better understanding the influence of refined strategy; \ncomments,_suggestions_and_typos\nMinor errors: the definition of POS has different meanings, e.g., Part-of-Speech for linguistic features, and a type of 10 self-defined relations of word-pairs, suggest to distinguish such two definitions with different symbols. ", "label": [[767, 1049, "Eval_pos_1"], [1053, 1119, "Eval_pos_2"], [1145, 1362, "Eval_neg_1"], [1366, 1591, "Eval_neg_2"], [1595, 1656, "Eval_neg_3"], [1658, 2509, "Jus_neg_3"], [2513, 2612, "Eval_neg_4"], [2614, 2793, "Jus_neg_4"]]}
{"id": 432, "review": "- Strengths: The main strength promised by the paper is the speed advantage at the same accuracy level.\n- Weaknesses: Presentation of the approach leaves a lot to be desired. Sections 3 and 4 need to be much clearer, from concept definition to explaining the architecture and parameterization. In particular Section 4.1 and the parameter tieing used need to be crystal clear, since that is one of the main contributions of the paper.\nMore experiments supporting the vast speed improvements promised need to be presented. The results in Table 2 are good but not great. A speed-up of 4-6X is nothing all that transformative.\n- General Discussion: What exactly is \"Viterbi prediction\"? The term/concept is far from established; the reader could guess but there must be a better way to phrase it.\nReference Weiss et al., 2015 has a typo. ", "label": [[118, 174, "Eval_neg_1"], [175, 622, "Jus_neg_1"]]}
{"id": 437, "review": "paper_summary\nThe paper studies methods for incorporating knowledge in generating reflective listening responses in psychological counseling conversations. The contribution is trifold: a) the authors proposed a counseling knowledge dataset as well as the pipeline to automatically collect it. b) the authors proposed two methods for incorporating knowledge: a retrieval setup based on embedding similarity; a generation setup that completes knowledge triplets with COMET. c) an analysis of the effects of different types of knowledge on generation quality. \nsummary_of_strengths\n- The task proposed is novel. Using generative models for producing reflective listening responses in counseling conversations is a task that is being actively tackled in the psychological counseling domain, and the authors are being innovative in proposing to incorporate common sense and domain knowledge to further improve the quality of generated utterances.\n-The analysis of the importance of different types of commonsense knowledge to the task is interesting. \nsummary_of_weaknesses\n- The core contribution of the work is unclear: this work is a collection of work the authors done around reflective listening: a dataset and the pipeline to collect it; two methods for incorporating knowledge; a series of vastly different experiments. The authors are advised to limit the scope to one of the contributions and elaborate on just that.\n-Each individual contribution is limited in significance:     - The dataset itself is not carefully curated and the highlight instead is the automation of dataset creation process from Google search, which is also a complex collection of several trivial steps itself and is prone to propagated errors. \n    - The two methods proposed are mostly based on pre-existing libraries along with a series of engineering techniques (e.g., turning triplets into sentences using templates, masking out part of the input, combining K-BERT with BART, etc.). No novel architecture tailored to the studied task is being proposed. \n    - The evaluation metrics are not well-motivated thus it is not entirely clear how well do they reflect the usefulness of the proposed techniques. The human evaluation was performed by just 2 annotations with their professional background unspecified. \n    - Again, additional experiments in Section 4.5 is an overly detailed piece of information and is not super relevant to rest of the writing. \ncomments,_suggestions_and_typos\nSee weaknesses ", "label": [[581, 608, "Eval_pos_1"], [609, 941, "Jus_pos_1"], [943, 1046, "Eval_pos_2"], [1070, 1115, "Eval_neg_1"], [1117, 1420, "Jus_neg_1"], [1422, 1477, "Major_claim"], [1485, 1528, "Eval_neg_2"], [1529, 1723, "Jus_neg_2"], [1730, 1965, "Jus_neg_3"], [1966, 2036, "Eval_neg_3"], [2043, 2186, "Eval_neg_4"], [2187, 2292, "Eval_neg_5"], [2299, 2437, "Eval_neg_6"]]}
{"id": 441, "review": "Summary: This paper presents a model for embedding words, phrases and concepts into vector spaces. To do so, it uses an ontology of concepts, each of which is mapped to phrases. These phrases are found in text corpora and treated as atomic symbols. Using this, the paper uses what is essentially the skip-gram method to train embeddings for words, the now atomic phrases and also the concepts associated with them. The proposed work is evaluated on the task of concept similarity and relatedness using UMLS and Yago to act as the backing ontologies.\nStrengths: The key question addressed by the paper is that phrases that are not lexically similar can be semantically close and, furthermore, not all phrases are compositional in nature. To this end, the paper proposes a plausible model to train phrase embeddings. The trained embeddings are shown to be competitive or better at identifying similarity between concepts.\nThe software released with the paper could be useful for biomedical NLP researchers.\n- Weaknesses: The primary weakness of the paper is that the model is not too novel. It is essentially a tweak to skip-gram.  Furthermore, the full model presented by the paper doesn't seem to be the best one in the results (in Table 4). On the two Mayo datasets, the Choi baseline is substantially better. A similar trend seems to dominate Table 6 too. On the larger UMNSRS data, the proposed model is at best competitive with previous simpler models (Chiu).\n- General Discussion: The paper says that it is uses known phrases as distant supervision to train embeddings. However, it is not clear what the \"supervision\" here is. If I understand the paper correctly, every occurrence of a phrase associated with a concept provides the context to train word embeddings. But this is not supervision in the traditional sense (say for identifying the concept in the text or other such predictive tasks). So the terminology is a bit confusing.\n The notation introduced in Section 3.2 (E_W, etc) is never used in the rest of the paper.\nThe use of \\beta to control for compositionality of phrases by words is quite surprising. Essentially, this is equivalent to saying that there is a single global constant that decides \"how compositional\" any phrase should be. The surprising part here is that the actual values of \\beta chosen by cross validation from Table 3 are odd. For PM+CL and WikiNYT, it is zero, which basically argues against compositionality.  The experimental setup for table 4 needs some explanation. The paper says that the data labels similarity/relatedness of concepts (or entities). However, if the concepts-phrases mapping is really many-to-many, then how are the phrase/word vectors used to compute the similarities? It seems that we can only use the concept vectors.\nIn table 5, the approximate phr method (which approximate concepts with the average of the phrases in them) is best performing. So it is not clear why we need the concept ontology. Instead, we could have just started with a seed set of phrases to get the same results. ", "label": [[737, 814, "Eval_pos_1"], [815, 919, "Eval_pos_2"], [920, 1004, "Eval_pos_3"], [1019, 1088, "Eval_neg_1"], [1089, 1128, "Jus_neg_1"], [1130, 1241, "Eval_neg_2"], [1242, 1463, "Jus_neg_2"], [1486, 1631, "Eval_neg_3"], [1632, 1901, "Jus_neg_3"], [1902, 1940, "Eval_neg_3"], [2032, 2121, "Eval_neg_4"], [2122, 2511, "Jus_neg_4"]]}
{"id": 443, "review": "- Strengths: This paper tries to tackle a very practical problem: automated short answer scoring (SAS), in particular for Japanese which hasn't gotten as much attention as, say, English-language SAS.\n- Weaknesses: The paper simply reads like a system description, and is light on experiments or insights. The authors show a lack of familiarity with more recent related work (aimed at English SAS), both in terms of methodology and evaluation. Here are a couple: https://www.aclweb.org/anthology/W/W15/W15-06.pdf#page=97 https://www.aclweb.org/anthology/N/N15/N15-1111.pdf There was also a recent Kaggle competition that generated several methodologies: https://www.kaggle.com/c/asap-sas - General Discussion: To meet ACL standards, I would have preferred to see more experiments (feature ablation studies, algorithm comparisons) that motivated the final system design, as well as some sort of qualitative evaluation with a user study of how the mixed-initiative user interface features led to improved scores. As it is, it feels like a work in progress without any actionable new methods or insights.\nAlso, Pearson/Spearman correlation and kappa scores are considered more appropriate than accuracy for these sorts of ordinal human scores. ", "label": [[214, 262, "Eval_neg_1"], [264, 303, "Eval_neg_2"], [305, 442, "Eval_neg_3"], [443, 686, "Jus_neg_3"], [709, 1100, "Major_claim"]]}
{"id": 453, "review": "paper_summary\nThe paper creates a corpus of verbal negations and adapts QA-SRL to collect questions and answers regarding the arguments of the affirmative counterpart of a negated predicate, and manipulate them to generate the affirmative interpretation. They use the curated dataset to pose two challenges - classifying entailment and neutral relationship between the negation and its affirmative counterpart, and generating the affirmative interpretation directly. Experiments show that current models are unable to perform reasonably on the generation task, which is the more realistic setting. \nsummary_of_strengths\nThe authors adapt a question answering approach to create an annotation scheme to create a corpus of verbal negations and their affirmative interpretations. The paper describes the full annotation process clearly, and the authors are careful to manually verify significant random samples of the corpus. The annotation process is designed to guide annotators to fill in template questions before answering them factually. These answers are used to create the affirmative interpretations. Further, they only retain the records where annotators are confident about their judgment. The authors perform well-designed experiments to establish benchmarks for the tasks they propose on their corpus. These experiments show that standard models are unable to perform the generation task as well as humans do. \nsummary_of_weaknesses\n1. While it is fair to say that two annotators might have different answers to the same question and both might be correct, it would be better to verify that the answers provided are all valid. The authors manually validate a small subset of the dataset but, for a high quality dataset, it would be better to validate all of it. \n2. The paper should include automatic metrics for the generation task. While the metrics have their own problems, it would be a good way to compare systems without expensive human evaluation. \ncomments,_suggestions_and_typos\n1. It would be good if you expand on the importance of the order of wh-words used for question generation. \n2. Line 112: Consider the \u2018second\u2019 example actually refers to the first example in Figure 1. \n3. I agree with the authors about the framing of the classification task, that it isn\u2019t a realistic one. Maybe the paper would be better without it. ", "label": [[777, 832, "Eval_pos_1"], [834, 921, "Eval_pos_2"], [1198, 1312, "Eval_pos_3"], [1776, 1843, "Eval_neg_1"], [1844, 1965, "Jus_neg_1"]]}
{"id": 459, "review": "- Strengths: (1) A deep CNN framework is proposed to extract and combine cognitive features with textual features for sentiment analysis and sarcasm detection.  (2) The ideas is interesting and novelty.\n- Weaknesses: (1) Replicability would be an important concern. Researchers cannot replicate the system/method for improvement due to lack of data for feature extraction.  - General Discussion: Overall, this paper is well written and organized. The experiments are conducted carefully for comparison with previous work and the analysis is reasonable. I offer some comments as follows.\n(1)           Does this model be suitable on sarcastic/non-sarcastic utterances? \nThe authors should provide more details for further analysis.  (2)           Why the eye-movement data would be useful for sarcastic/non-sarcastic sentiment classification beyond the textual features? The authors should provide more explanations. ", "label": [[165, 202, "Eval_pos_1"], [221, 265, "Eval_neg_1"], [266, 373, "Jus_neg_1"], [396, 446, "Eval_pos_2"], [447, 520, "Eval_pos_3"], [525, 551, "Eval_pos_4"], [669, 730, "Eval_neg_2"], [745, 869, "Jus_neg_3"], [870, 916, "Eval_neg_3"]]}
{"id": 470, "review": "paper_summary\n***This is from the reviewer `2guV` of the previous version.***\nI have read the responses from the authors to these weaknesses and I think most of them are well addressed in the revised version. Thus, I have raised my overall score. \n----- This paper presents a multi-hop reasoning method to reading comprehension with structured queries in the format of triplets (subject, relation, ?) --- predicting the object of a triplet via reading multiple documents. The key idea of the proposed method is to combine neural token representations and inductive logic programming (ILP) so that the reasoning chains are more interpretable to humans. The proposed method DILR is an end-to-end model, consisting of a hierarchical attentive reader module and a multi-hop logic reasoner, where the former continually produces a query&context-aware representation of entities, and the latter use them for softly selecting atoms to form logic clauses. A highlight of such a neural-symbolic method is that the final loss can smoothly be back-propagated to the trainable modules. \nsummary_of_strengths\n- The proposed method avoids using pre-extracted NER results and limited rule templates. Instead, the module learns to select the atoms end-to-end and does improve the performance.\n- The logic rules and attentions are naturally interpretable and can be used for understanding the reasoning process of the model. \nsummary_of_weaknesses\n- The evaluation part lacks the results of recent pre-trained LMs' results such as RoBERTa and UnifiedQA, etc, which thus makes the results and conclusions less convincing. It is unclear if the performance gain from DILR is still there if with a more powerful pre-trained LM. And also, it seems that the DILR-BERT doesn't fine-tune the BERT parameters? From the description, I feel like the BERT here is only used as an encoder to get contextualized representation but is not trainable. Please correct me if I missed anything here.\n-The model is limited to the triplet-format questions, which is limited to a pre-defined list of relations and may not generalize to other MRC datasets and other relations.\n-There are a few missing baseline methods such as DrKIT (Bhuwan Dhingra et al. ICLR 2020), which is also a differentiable multi-hop reasoning framework, focusing on entity mentions. Please also look at the datasets and baseline methods this paper used for evaluation.\n-There is no qualitative analysis of the used logic clauses for case studies. It's unclear whether the logic clauses induced by the DILR are really meaningful to reason about the answers.\n---  I have read the responses from the authors to these weaknesses and I think most of them are well addressed in the revised version. Thus, I have raised my overall score. \ncomments,_suggestions_and_typos\nN/A ", "label": [[78, 247, "Major_claim"], [1433, 1505, "Eval_neg_1"], [1506, 1540, "Jus_neg_1"], [1541, 1603, "Eval_neg_1"], [1604, 1706, "Eval_neg_2"], [1964, 2017, "Jus_neg_3"], [2018, 2135, "Eval_neg_3"], [2137, 2177, "Eval_neg_4"], [2178, 2403, "Jus_neg_4"], [2405, 2481, "Eval_neg_5"], [2482, 2591, "Eval_neg_6"], [2597, 2766, "Major_claim"]]}
{"id": 472, "review": "The paper describes a method for in-domain data selection for SMT with a convolutional neural network classifier, applying the same framework as Johnson and Zhang, 2015. The method performs about 0.5 BLEU points better than language model based data selection, and, unlike the other methods, is robust even if only a very small in-domain data set is provided.  The paper claims improvements of 3.1 BLEU points. However, from the results we see that improvements of this magnitude are only achieved if there are in-domain data in the training set - training only on the in-domain data already produces +2.8 BLEU. It might be interesting to also compare this to a system which interpolates separate in- and out-domain models.  The more impressive result, in my opinion, comes from the second experiment, which demonstrates that the CNN classifier is still effective if there is very little in-domain data. However, the second experiment is only run on the zh2en task which includes actual in-domain data in the training set, possibly making selection easier. Would the result also hold for the other tasks, where there is no in-domain data in the training set? The results for the en2es and en2zh task already point in this direction, since the development sets only contain a few hundred sentence pairs. I think the claim would be better supported if results were reported for all tasks when only 100 sentence pairs are used for training.   When translating social media text one often has to face very different problems from other domains, the most striking being a high OOV rate due to non-conventional spelling (for Latin scripts, at least). The texts can also contain special character sequences such as usernames, hashtags or emoticons. \nWas there any special preprocessing or filtering step applied to the data?   Since data selection cannot address the OOV problem, it would be interesting to know in more detail what kinds of improvements are made through adaptation via data selection, maybe by providing examples.    The following remarks concern specific sections: Section 3.2: -It could be made clearer how the different vectors (word embeddings, segment vectors and one-hot vectors) are combined in the model. An illustration of the architecture would be very helpful.  -What was the \"designated loss function\"?\nSection 5.2: For completeness' sake, it could be mentioned how the system weights were tuned. ", "label": []}
{"id": 478, "review": "paper_summary\nThis paper studies whether and how contextual modeling in DocNMT is transferable via multilingual modeling. The authors explore the effect of 3 factors on the transfer with simple concatenation-based DocNMT: the number of teacher languages with document level data, the balance between document and sentence level data at training, and the data type of document level data. The experiments are conducted on Europarl-7 and IWSLT-10 show the feasibility of multilingual transfer for DocNMT, particularly on document specific metrics. \nsummary_of_strengths\nA simple approach uses different proportions of sentence-level and document-level data to train and study its impact on translation performance. In addition, a large number of experiments were done to analyze the causes and human evaluation of its generation. The motivation of this paper is worthy of recognition. \nsummary_of_weaknesses\n1. First of all, compared with other excellent papers, this paper is slightly less innovative. \n2. The baseline is is not strong enough. Expect to see experiments that compare with the baseline of the papers you cited. \n3. p indicates the proportion of documents, I would like to know how the parts of sentences and documents are extracted? Do the rules of extraction have any effect on the experiment? I hope to see a more detailed analysis. \n4. It lacks case study to show which document-level translation errors are improved by the proposed method. \ncomments,_suggestions_and_typos\n1. It is suggested to add a structure diagram to illustrate your proposed method. \n2. It is suggested to add some case studies to clarify the problems you have solved, so as to clearly show your contribution. \n3. The authors are encouraged to proofread the paper more carefully and explain their methods more clearly. ", "label": [[828, 883, "Eval_pos_1"], [909, 1001, "Eval_neg_1"], [1005, 1042, "Eval_neg_2"], [1043, 1125, "Jus_neg_2"]]}
{"id": 481, "review": "This paper presents a two-level annotation scheme for interpretation of modality in human-robot dialogues. The first level of the proposed scheme is a fine-grained annotation of modality, the second level encodes pragmatic information to interpretation of modal expressions in the context of dialogue. Finally, the authors introduce some constraints to help discriminate the interpretation of modality in dialogue and make annotation feasible. The authors employ four annotators to apply the annotation scheme based on SCOUT corpus, with the annotation results showing good agreement. This paper also presents theoretical analysis towards a formal theory of modality in human-robot dialogues.\nOverall, this paper is well organized and clearly written. Modal expressions reflect and create participant relations, which are crucial for interaction in dialogue settings. This paper aims to address a key question that how to annotate the interaction of vagueness and ambiguity in natural language. It is novel and worth exploring for further research.  Strength: 1. This paper is clearly written and it contributes a novel annotation scheme to the interpretation of modality in human-robot dialogues. \n2. Detailed related works are included. \n3. The annotation results demonstrate the effectiveness and expressiveness of the proposed scheme.\nWeakness: 1. It would be better if the authors promise to release the annotated results for public research in the field of dialogue. \n2. This paper only analyzes the annotation scheme and the annotation results theoretically. How about conducting a baseline model to validate the effectiveness of such an annotation scheme compared with other schemes in terms of the representation ability of modality? ", "label": [[693, 751, "Eval_pos_1"], [995, 1048, "Eval_pos_2"], [1063, 1092, "Eval_pos_3"], [1098, 1198, "Eval_pos_4"], [1202, 1239, "Eval_pos_5"], [1243, 1338, "Eval_pos_6"], [1477, 1565, "Eval_neg_1"], [1566, 1742, "Jus_neg_1"]]}
{"id": 484, "review": "paper_summary\nThis work studies using human feedback as contextual bandit learning to improve QA models. The user interacts with a QA model trained initially with conventional supervised learning, providing binary feedback (correct v.s incorrect) to judge the correctness of returned answers. Notably, the author does not really deploy the model. Instead, they derive pseudo-feedbacks from gold labels, which causes a significant gap between their simulation and real-world interaction.  After collecting feedback through simulation, they fine-tune the deployed QA model by maximizing the likelihood of predicted answer with positive feedback and minimizing the likelihood of the one with negative feedback. They consider two learning settings, online and offline learning.\nWithin this framework, they conduct experiments on 6 MRC datasets. The result shows that a QA model initialized with a small number of labeled training data can be improved by a large margin of accuracy after being fine-tuned with derived feedback data. When trained with full training sets in source domains, the QA model can also be adapted to target domains, where only user feedback is available.\nThe author has discussed related work on using interactive feedback for other NLP tasks. Although the author claims that their work is the first for QA, there is significant overlap between this work and Campos et al. (2020), which also uses binary user feedback to improve a Conversational QA model. But it is not cited in the paper.\nReferences: Campos, Jon Ander, et al. \"Improving Conversational Question Answering Systems after Deployment using Feedback-Weighted Learning.\" Proceedings of the 28th International Conference on Computational Linguistics. 2020. \nsummary_of_strengths\n1. This work is well motivated by using human feedback to improve QA models. \n2. Although the employed baseline methods are kind of weak, the improvement is significant. \n3. The paper is well written and easy to follow. \nsummary_of_weaknesses\n1. Despite the well-motivated problem formulation, the simulation is not realistic. The author does not really collect feedback from human users but derives them from labeled data. One can imagine users can find out that returned answers are contrastive to commonsense. For instance, one can know that \u201cTokyo\u201d is definitely a wrong answer to the question \u201cWhat is the capital of South Africa?\u201d. However, it is not very reasonable to assume that the users are knowledgeable enough to provide both positive and negative feedback. If so, why do they need to ask QA models? And what is the difference between collecting feedback and labeling data? \nIn conclusion, it would be more realistic to assume that only a small portion of the negative feedback is trustworthy, and there may be little or no reliable positive feedback. According to the experiment results, however, 20% of feedback perturbation makes the proposed method fail. \nTherefore, the experiment results cannot support the claim made by authors.\n2. There is a serious issue of missing related work. As mentioned above, Campos et al. (2020) has already investigated using user feedback to fine-tune deployed QA models. They also derive feedback from gold labels and conduct experiments with both in-domain and out-of-domain evaluation. The proposed methods are also similar: upweighting or downweighting the likelihood of predicted answer according to the feedback. \nMoreover,  Campos et al. (2020) has a more reasonable formulation, where there could be multiple feedback for a certain pair of questions and predicted answers.  3. The adopted baseline models are weak. First of all, the author does not compare to Campos et al. (2020), which also uses feedback in QA tasks. Second, they also do no comparison with other domain adaptation methods, such as those work cited in Section 8. \ncomments,_suggestions_and_typos\nLine 277: \u201cThe may be attributed\u2026\u201d -> \u201cThis may be attributed\u2026 ", "label": [[1264, 1509, "Eval_neg_1"], [1510, 1738, "Jus_neg_1"], [1763, 1837, "Eval_pos_1"], [1841, 1930, "Eval_pos_2"], [1933, 1980, "Eval_pos_3"], [2006, 2086, "Eval_neg_1"], [2087, 3008, "Jus_neg_1"], [3012, 3061, "Eval_neg_2"], [3062, 3590, "Jus_neg_2"], [3594, 3631, "Eval_neg_3"], [3632, 3849, "Jus_neg_3"]]}
{"id": 498, "review": "paper_summary\nThe authors entertain the hypothesis that syntactic information is redundantly encoded in neural language models, which has two important implications for probing and causal analysis: 1. Probes may arbitrarily choose one place a syntactic property is encoded, and the model may choose another. \n2. Changing a representation based on a probe may therefore only partially change whether the property is encoded in the model, and might not influence the model decision, even if the model is relying on syntax.\nFor contributions, the authors show first that syntactic information is redundantly encoded in language models, based on estimates of mutual information between different parts of the embeddings. They then propose a simple method to make probing more robustly rely on all the information in the representation, by adding dropout to the probe. They show that this leads to altered representations that are more influential on changing the model's behavior. \nsummary_of_strengths\n1. Well-written and clear hypothesis: redundant encoding of syntactic information poses a problem for drawing conclusions about whether models rely on the syntactic information encoded by probes.\n2. The authors show that different parts of the hidden representations in different trained networks redundantly encode information correlated with syntax.\n3. The dropout method for addressing this issue is both simple and motivated, which makes it appealing.\n4. The experimental results, in the case of the QA models, tell a different story than past work that suffered from the redundancy vulnerability. Specifically, this work finds evidence that QA models do rely on syntactic information, while past work suggested they didn't. In the case of NLI, this work agrees with past works in finding a negative result. \nsummary_of_weaknesses\n1. There is some imprecision in the discussion of the results in 4.2.2: \u201cIn contrast to the standard probes, the dropout probes, plotted in the right column, revealed much larger effects of syntactic interventions.\u201d Is this conclusion really justified? Visually, it is not clear that the red line is \u201cabove\u201d the green line any more with dropout than without. Rather, it may just have more variance (so more causal influence, but maybe not in the right direction). In any case, the clean summary you give here seems at odds with the trend in the figure, and should be made more precise.\n2. There may be some issue with the counterfactual intervention experiment with NLI-HANS, if I understand that part correctly. In 4.3, if the NLI-HANS model is already at 99% performance, why would you expect its accuracy to change significantly after counterfactual modification? You are already very close to the ceiling, and it should be hard to see any benefit of the intervention.\n3. \u201c First, we found that language models redundantly encoded syntactic information in their embeddings\u201d \u2014 Section 4.1 seems like solid evidence that networks redundantly encode information that is informative about syntax. But, to be a bit pedantic, it doesn\u2019t have to be syntactic information; if $D$ is highly correlated with something non-syntactic, then that property could be expressed redundantly, and the MI would still be high. If we believe that there are dataset artifacts in syntactic parsing, then this is a concern. \ncomments,_suggestions_and_typos\nIt seems to me that redundancy may only be a problem for the specific gradient-based representation alteration method you consider, rather than other ways of producing counterfactual interventions. For example, would [AlterRep](https://arxiv.org/abs/2105.06965) automatically handle the issue of redundancy?\n417: why is \u201cas\u201d included along with \u201cwere\u201d and \u201care\u201d?\n295: What do you mean by \u201cconservative but tight estimate of mutual information\u201d? This is pretty unclear to me 188: typo at \u201crepresented by within\u201d ", "label": [[1002, 1035, "Eval_pos_1"], [1037, 1194, "Jus_pos_1"], [1354, 1454, "Eval_pos_2"], [1837, 1905, "Eval_neg_1"], [1906, 2419, "Jus_neg_1"], [2423, 2546, "Eval_neg_2"], [2547, 2805, "Jus_neg_2"]]}
{"id": 507, "review": "paper_summary\nThis paper proposes three new tasks based on a corpus of historical texts. Moreover, it prepares and releases such a corpus of historical tests (Chronicling America), data that can be used as pre-training for language models. The data collection and preparation are described in the paper, as well as the data used for the proposed challenges/tasks. After the three tasks are introduced, the paper investigates the performance of baselines systems, mainly based on language models. Everything is to be released with the paper, providing a useful data resource for future work. \nsummary_of_strengths\n-The paper provides a useful resource in the form of a very large corpus of historical texts, carefully cleaned and prepare. \n-The paper proposed a suite of challenging machine learning-based tasks that can be useful to evaluate models in the domain of historical text and time-aware tasks. \nsummary_of_weaknesses\n-The paper lacks a clear structure and at points it is a bit hard to follow the thread (see below for more detailed comments and suggestions). This being said, the goals are clearly described. \n-The evaluation is a bit poor in which there is no error analysis or clear insights (what these results tell us, what is lacking, etc.) and the choice of baselines and experimental setting not fully described. \n-The proposed tasks lack, in my opinion, a strong motivation. There is a short paragraph describing the motivation of each task (which is good) but perhaps not entirely convincing why the tasks are necessary.\nNote: many of these weaknesses can be addressed in a subsequent submission, and I would be happy to reassess them if a resubmission is made. \ncomments,_suggestions_and_typos\nIn the following I propose a few suggestions that I would personally think that would improve the paper.\n-Reduce list of contributions in the introduction -Add a table with relevant statistics (e.g. number of documents per time period, etc.) about the collected corpus. I think this would give a better overview of the (massive) nature of the resource. \n-Move Section 3 (essentially related work) before Section 2 or before conclusion not to break the thread of the paper. \nSections 2 and 4 (and even 5) could be merged as they are all data-related - this would also give a bit of structure to the paper, as now it has a concatenation of highly-related sections. \n-Add examples from the dataset and especially from the datasets on which the three proposed tasks are based on. \n-In Section 7.4 the statistics can be presented in a table in a more summarised manner. \n-Section 8 can be renamed \u201cResults\u201d or something similar? Also, it could be integrated with the previous one. I find it a bit odd to name a section \u201cBaselines\u201d and show the results, but this may be a personal opinion. \n-I was a bit confused by Section 9. It indicates that some text has been filtered after all the results were presented. These texts were filtered for the whole corpus? If so, I believe it is better to move this section to the data preparation sections, as it is an integral part of the whole preparation. \n-I would add a bit more of a discussion on the nature of the data (fully open, consent, limitations for research/commercial purposes, etc.) and how it will be shared. ", "label": [[614, 738, "Eval_pos_1"], [740, 904, "Eval_pos_2"], [928, 1013, "Eval_neg_1"], [1015, 1067, "Jus_neg_1"], [1070, 1119, "Eval_pos_3"], [1122, 1204, "Eval_neg_2"], [1206, 1255, "Jus_neg_2"], [1261, 1331, "Eval_neg_3"], [1333, 1393, "Eval_neg_4"], [1394, 1540, "Eval_neg_5"]]}
{"id": 515, "review": "- Strengths: This paper proposes an evaluation metric for automatically evaluating the quality of dialogue responses in non-task-oriented dialogue. The metric operates on continuous vector space representations obtained by using RNNs and it comprises two components: one that compares the context and the given response and the other that compares a reference response and the given response. The comparisons are conducted by means of dot product after projecting the response into corresponding context and reference response spaces. These projection matrices are learned by minimizing the squared error between the model predictions and human annotations.\nI think this work gives a remarkable step forward towards the evaluation of non-task-oriented dialogue systems. Different from previous works in this area, where pure semantic similarity was pursued, the authors are going beyond pure semantic similarity in a very elegant manner by learning projection matrices that transform the response vector into both context and reference space representations. I am very curious on how your projection matrices M and N differ from the original identity initialization after training the models. I think the paper will be more valuable if further discussion on this is introduced, rather than focusing so much on resulting correlations.  - Weaknesses: The paper also leaves lots questions related to the implementation. For instance, it is not clear whether the human scores used to train and evaluate the system were single AMT annotations or the resulting average of few annotations. Also, it is not clear how the dataset was split into train/dev/test and whether n-fold cross validation was conducted or not. Also, it would be nice to better explain why in table 2 correlation for ADEM related scores are presented for the validation and test sets, while for the other scores they are presented for the full dataset and test set. The section on pre-training with VHRED is also very clumsy and confusing, probably it is better to give less technical details but a better high level explanation of the pre-training strategy and its advantages.\n- General Discussion: \u201cThere are many obvious cases where these metrics fail, as they are often incapable of considering the semantic similarity between responses (see Figure 1).\u201d Be careful with statements like this one. This is not a problem of semantic similarity! Opposite to it, the problem is that completely different semantic cues might constitute pragmatically valid responses. Then, semantic similarity itself is not enough to evaluate a dialogue system response. \nDialogue system response evaluation must go beyond semantics (This is actually what your M and N matrices are helping to do!!!)  \u201can accurate model that can evaluate dialogue response quality automatically \u2014 what could be considered an automatic Turing test \u2014\u201c The original intention of Turing test was to be a proxy to identify/define intelligent behaviour. It actually proposes a test on intelligence based on an \u201cintelligent\u201d machine capability to imitate human behaviour in such a way that it would be difficult for a common human to distinguish between such a machine responses and actual human responses. It is of course related to dialogue system performance, but I think it is not correct to say that automatically evaluating dialogue response quality is an automatic Turing test. \nActually, the title itself \u201cTowards an Automatic Turing Test\u201d is somehow misleading!\n\u201cthe simplifying assumption that a \u2018good\u2019 chatbot is one whose responses are scored highly on appropriateness by human evaluators.\u201d This is certainly the correct angle to introduce the problem of non-task-oriented dialogue systems, rather than \u201cTuring Test\u201d. Regarding this, there has been related work you might like to take a look at, as well as to make reference to, in the WOCHAT workshop series (see the shared task description and corresponding annotation guidelines).\nIn the discussion session: \u201cand has has been used\u201d -> \u201cand it has been used\u201d ", "label": [[658, 769, "Eval_pos_1"], [770, 1333, "Jus_pos_1"], [1349, 1416, "Eval_neg_1"], [1417, 2141, "Jus_neg_1"]]}
{"id": 517, "review": "paper_summary\nThis paper presents the idea of creating domain confused examples paired with contrastive learning for unsupervised domain adaptation. The authors propose identifying these puzzles in the representation space by utilizing adversarial examples---the method searches for an extreme direction that would shift a data point to the direction of the other domain. Then, the contrastive learning setup encodes these original and domain confused examples to be closer to one another, thus pulling examples from different domains closer to the domain discrimination boundary, encouraging models to learn domain invariant representations in the process. The authors further compare this method against other data generation schemes, such as back translation, finding that learning with domain confused examples provides far greater benefit. Further experiments show that the proposed method outperforms several strong baselines on two Amazon reviews benchmarks for sentiment analysis. I think this paper offers valuable contributions, both methodical as well as in the insights that are shared alongside quantitative results. ~I've also read the submitted previous reviews and feel confident in the author response. Overall, I think this paper could be a strong add to an ACL venue but I also have some concerns that I have laid below. I firmly believe that addressing these concerns would strengthen this paper to a great extent.~ I am confident that the authors have addressed my concerns to the extent possible and would recommend acceptance. I have reflected changes to my last review by striking the weaknesses that have been addressed satisfactorily. \nsummary_of_strengths\n- The paper is well motivated, and addresses key limitations posed by both adversarial training for domain adaptation (DANN) and -contrastive learning. Additionally, the connections to and differences from prior work in computer vision are well documented as well.\n-The experimental setup is well designed and the results suggest that the method clearly outperforms several strong baselines on various domain adaptation setups. The ablation studies presented by the authors alongside these results further help establish the efficacy of the proposed approach.\n-The paper is well written and easy to follow. The exposition is clear, which makes the experiments easy to replicate for a reader. \nsummary_of_weaknesses\n- ~I was a bit disappointed to see no error analysis presented in this paper. It would be nice to see some trends and insights into cases when this proposed approach fails to perform well.~ -~The experimental results seem to be missing significance testing. In absence of it, it's rather unclear whether some small absolute performance improvements are statistically significant. The paper would also benefit from a greater discussion of these close cases. Such a discussion is currently missing.~ -~A qualitative analysis of the generated domain confused examples would help the reader better understand what these examples could be. Such an analysis is also missing from the paper.~ \ncomments,_suggestions_and_typos\n~How does the issue of instability of DANN relate to the solution proposed by [1] to learn an asymmetrically relaxed alignment? My feeling is that their method would address some of this, plus it's a stronger baseline regardless as it also takes label shift into consideration. Perhaps, try and include it as a baseline?~ ~[1] Yifan Wu, Ezra Winston, Divyansh Kaushik, and Zachary Lipton. \" Domain adaptation with asymmetrically-relaxed distribution alignment.\" In International Conference on Machine Learning, pp. 6872-6881. PMLR, 2019.~ It might still be nice to include this in the camera ready. ", "label": [[989, 1129, "Eval_pos_1"], [1220, 1549, "Major_claim"], [1685, 1834, "Eval_pos_1"], [1835, 1947, "Eval_pos_2"], [1949, 1988, "Eval_pos_3"], [2244, 2289, "Eval_pos_4"], [2290, 2375, "Eval_pos_5"], [2401, 2475, "Eval_neg_1"], [2476, 2586, "Jus_neg_1"], [2590, 2655, "Eval_neg_2"], [2656, 2854, "Jus_neg_2"], [2855, 2894, "Eval_neg_2"], [2898, 3032, "Jus_neg_3"], [3033, 3081, "Eval_neg_3"]]}
{"id": 529, "review": "This paper investigates three simple weight-pruning techniques for NMT, and shows that pruning weights based on magnitude works best, and that retraining after pruning can recover original performance, even with fairly severe pruning.\nThe main strength of paper is that the technique is very straightforward and the results are good. It\u00e2\u0080\u0099s also clearly written and does a nice job covering previous work.\nA weakness is that the work isn\u00e2\u0080\u0099t very novel, being just an application of a known technique to a new kind of neural net and application (namely NMT), with results that aren\u00e2\u0080\u0099t very surprising.  It\u00e2\u0080\u0099s not clear to me what practical significance these results have, since to take advantage of them you would need sparse matrix representations, which are trickier to get working fast on a GPU - and after all, speed is the main problem with NMT, not space. ( There may be new work that changes this picture, since the field is evolving fast, but if so you need to describe it, and generally do a better job explaining why we should care about pruning.)\nA suggestion for dealing with the above weakness would be to use the pruning results to inform architecture changes. For instance, figure 3 suggests that you might be able to reduce the number of hidden layers to two, and also potentially reduce the dimension of source and target embeddings.\nAnother suggestion is that you try to make a link between pruning+retraining and dropout (eg \u00e2\u0080\u009cA Theoretically Grounded Application of Dropout in Recurrent Neural Networks\u00e2\u0080\u009d, Gal, arXiv 2016).\nDetailed comments: Line 111: \u00e2\u0080\u009csoftmax weights\u00e2\u0080\u009d - \u00e2\u0080\u009coutput embeddings\u00e2\u0080\u009d may be a preferable term S3.2: It\u00e2\u0080\u0099s misleading to call n the \u00e2\u0080\u009cdimension\u00e2\u0080\u009d of the network, and specify all parameter sizes as integer multiples of this number as if this were a logical constraint.\nLine 319: You should cite Bahdanau et al here for the attention idea, rather than Luong for their use of it.\nS3.3: Class-uniform and class-distribution seem very similar (and naturally get very similar results); consider dropping one or the other.\nFigure 3 suggestion that you could hybridize pruning: use class-blind for most classes, but class-uniform for the embeddings.\nFigure 4 should show perplexity too.\nWhat pruning is used in section 4.2 & figure 6?\nFigure 7: does loss pertain to training or test corpora?\nFigure 8: This seems to be missing softmax weights. I found this diagram somewhat hard to interpret; it might be better to give relevant statistics, such as the proportion of each class that is removed by class-blind pruning at various levels.\nLine 762: You might want to cite Le et al, \u00e2\u0080\u009cA Simple Way to Initialize Recurrent Networks of Rectified Linear Units\u00e2\u0080\u009d, arXiv 2015. ", "label": [[235, 333, "Eval_pos_1"], [335, 405, "Eval_pos_2"], [406, 452, "Eval_neg_1"], [454, 602, "Jus_neg_1"], [604, 674, "Eval_neg_2"], [675, 864, "Jus_neg_2"]]}
{"id": 541, "review": "paper_summary\nThis paper proposes a new pinyin input method with Chinese GPT. \nTheir method tries to output Chinese characters given perfect pinyin, which consists of both initials and finals, or abbreviated pinyin, which only uses characters' initials. \nAbbreviated pinyin input is user-friendly since the number of typing characters is fewer. Still, it is a more complex problem since the model has to search for the correct output from more candidates than perfect pinyin.\nTheir method uses Chinese GPT and adapts it by concatenating both contexts of Chinese characters and pinyin (PinyinGPT-Concat) or by using the embeddings of characters and pinyin (PinyinGPT-Embed). \nThey also construct a dataset called WD Dataset for evaluating their input methods.\nTheir experiments show that the Chinese GPT given perfect pinyin can output the correct characters even if the model is not fine-tuned for the task and surpasses previous baselines. \nHowever, their analysis shows that the vanilla GPT does not work well with the abbreviated pinyin. \nTheir proposed PinyinGPT-Concat method surpasses the vanilla GPT baseline on abbreviated pinyin settings. \nsummary_of_strengths\n1. They adapt Chinese GPT for the pinyin input method and the results show their method works well compared to the baselines.\n2. They created a new pinyin IME evaluation dataset called WD Dataset and it would be useful for further researches. \nsummary_of_weaknesses\n1. Their experiments compared methods trained with different datasets and different model parameters. It is still difficult to distinguish this model architecture is nice, or the larger dataset/parameters are good.\n2. They only compare the previous method in the prefect pinyin settings though several works on abbreviated pinyin IMEs are mentioned in the related work section. \nIt would be better to add a comparison of these.\n3. I could not fully understand why people want higher pinyin IME accuracy with full use of V100 GPU. It might be possible to run this on the standard laptops in the future, though. \ncomments,_suggestions_and_typos\nl.299, please add the explanation of the abbreviation of \"precision at top-K\"-> \"precision at top-k (P@K)\". ", "label": [[1299, 1413, "Eval_pos_1"], [1538, 1650, "Eval_neg_1"], [1867, 1965, "Eval_neg_2"], [1966, 2046, "Jus_neg_2"]]}
{"id": 543, "review": "paper_summary\nThis paper proposes a new intermediate task for leveraging tabular data for enhancing the performance of an LM on tasks of unstructured text. The enhanced model shows improvement compared to the original T5 model. \nsummary_of_strengths\n- The paper is overall well-written (though the organization seems confusing, to be detailed in \"Weaknesses\") and the figures are helpful for understanding the paper.\n-The method is intuitive and relatively simple. The semi-structured tabular data are easy to obtain (from Wikipedia or Common Crawls) so the application of this method is promising.\n-The introduced momentum sampling strategy is interesting and can be useful for multi-task training.\n-The improvement on the four datasets are significant. \nsummary_of_weaknesses\n### Original comments -The novelty is limited. This paper mainly uses a template-based generation strategy for EG. Similar techniques have been used in prior works of question generation and semantic parsing.\n-The organization of this paper is confusing. Why there is a \"related work\" subsection in Ln. 323 while there is a whole section of related work (Section 6)? If the authors feel like the related work should be discussed earlier, I suggest moving Section 6 to after the introduction.\n-More baselines should be compared. GenBERT is a very relevant baseline but I think other methods that enhance LM with external knowledge (e.g., KBERT, KnowBERT) should also be compared.\n### After revision The authors reorganize the paper and I have seen improvement in the quality of writing. Also, I've seen the authors have addressed the questions of other reviewers. I thus have updated my recommendation from 3.5 to 4. \ncomments,_suggestions_and_typos\nN/A ", "label": [[252, 285, "Eval_pos_1"], [287, 326, "Eval_neg_1"], [328, 358, "Jus_neg_1"], [364, 415, "Eval_pos_2"], [418, 464, "Eval_pos_3"], [465, 551, "Jus_pos_4"], [554, 598, "Eval_pos_4"], [600, 699, "Eval_pos_5"], [701, 755, "Eval_pos_6"], [801, 824, "Eval_neg_2"], [825, 986, "Jus_neg_2"], [988, 1032, "Eval_neg_3"], [1033, 1269, "Jus_neg_3"]]}
{"id": 544, "review": "paper_summary\nThis paper poses a problem setting named text-to-table, as a new way of information extraction. They study it as an inverse problem of table-to-text (a well-studied problem). To realize this goal,  the author formalizes this task as a seq2seq problem via equipping BART with two extensions: table constraint and table relation embeddings. Experiment on four popular table-to-text dataset reveals the effectiveness of proposed methods (a little marginal). \nsummary_of_strengths\n1. It proposes an interesting task named text-to-table to realize effective information extraction. \n2. The challenging issues discussed in Sec 5.6 and Appendix. F is very clear. \nsummary_of_weaknesses\n1. The paper need further polish to make readers easy to follow.\n2. In Table 6, the improvement of method is marginal and unstable.\n3. The motivation of this new task is not strong enough to convince the reader.  Is it a necessary intermediate task for document summarization and text mining (as stated in L261)?\n4. It directly reverse the table-to-text settings then conducts the experiments on four existing table-to-text datasets. More analysis of the involved datasets is required, such as the number of output tables, the size/schema of the output tables. \ncomments,_suggestions_and_typos\nQuestions: 1. L68-L70, Is there any further explanation of the statement \"the schemas for extraction are implicitly included in the training data\"?\n2. How to generate the table content that not shown in the text?\n3. Why not merge Table 1 and Table 2? They are both about the statistics of datasets used in experiments.\n4. What\u2019s the relation between text-to-table task and vanilla summarization task?\n5. How to determine the number of output table(s)? Appendex. C don't provide an answer about this.\n6. What\u2019s the version of BART in Table3 and Table 4?\nSuggestions: 1. The font size of Figure 2 and Figure 3 is too small.\nTypos: 1. L237: Text-to-table -> text-to-table 2. L432: \"No baseline can be applies to all four datasets\" is confusing.\n3. Table 3: lOur method -> Our method ", "label": [[494, 591, "Eval_pos_1"], [595, 670, "Eval_pos_2"], [696, 757, "Eval_neg_1"], [760, 824, "Eval_neg_2"], [828, 904, "Eval_neg_3"], [906, 1005, "Jus_neg_3"]]}