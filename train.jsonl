{"id": 0, "review": "This paper describes a state-of-the-art CCG parsing model that decomposes into tagging and dependency scores, and has an efficient A* decoding algorithm. \nInterestingly, the paper slightly outperforms Lee et al. (2016)'s more expressive global parsing model, presumably because this factorization makes learning easier. It's great that they also report results on another language, showing large improvements over existing work on Japanese CCG parsing. One surprising original result is that modeling the first word of a constituent as the head substantially outperforms linguistically motivated head rules.  Overall this is a good paper that makes a nice contribution. I only have a few suggestions: -I liked the way that the dependency and supertagging models interact, but it would be good to include baseline results for simpler variations (e.g. not conditioning the tag on the head dependency).\n-The paper achieves new state-of-the-art results on Japanese by a large margin. However, there has been a lot less work on this data - would it also be possible to train the Lee et al. parser on this data for comparison?\n-Lewis, He and Zettlemoyer (2015) explore combined dependency and supertagging models for CCG and SRL, and may be worth citing. ", "label": [[320, 381, "Eval_pos_1"], [382, 453, "Jus_pos_1"], [609, 669, "Major_claim"]]}
{"id": 1, "review": "The paper considers a synergistic combination of two non-HMM based speech recognition techniques: CTC and attention-based seq2seq networks. The combination is two-fold: 1. first, similarly to Kim et al. 2016 multitask learning is used to train a model with a joint CTC and seq2seq cost. \n2. second (novel contribution), the scores of the CTC model and seq2seq model are ensembled during decoding (results of beam search over the seq2seq model are rescored with the CTC model).\nThe main novelty of the paper is in using the CTC model not only as an auxiliary training objective (originally proposed by Kim et al. 2016), but also during decoding.\n- Strengths: The paper identifies several problems stemming from the flexibility offered by the attention mechanism and shows that by combining the seq2seq network with CTC the problems are mitigated.\n- Weaknesses: The paper is an incremental improvement over Kim et al. 2016 (since two models are trained, their outputs can just as well be ensembled). However, it is nice to see that such a simple change offers important performance improvements of ASR systems.\n- General Discussion: A lot of the paper is spent on explaining the well-known, classical ASR systems. A description of the core improvement of the paper (better decoding algorithm) starts to appear only on p. 5.  The description of CTC is nonstandard and maybe should either be presented in a more standard way, or the explanation should be expanded. Typically, the relation p(C|Z) (eq. 5) is deterministic - there is one and only one character sequence that corresponds to the blank-expanded form Z. I am also unsure about the last transformation of the eq. 5. ", "label": [[658, 845, "Eval_pos_1"], [860, 920, "Eval_neg_1"], [922, 995, "Jus_neg_1"]]}
{"id": 2, "review": "The authors propose \u2018morph-fitting\u2019, a method that retrofits any given set of trained word embeddings based on a morphologically-driven objective that (1) pulls inflectional forms of the same word together (as in \u2018slow\u2019 and \u2018slowing\u2019) and (2) pushes derivational antonyms apart (as in \u2018expensive\u2019 and \u2018inexpensive\u2019). With this, the authors aim to improve the representation of low-frequency inflections of words as well as mitigate the tendency of corpus-based word embeddings to assign similar representations to antonyms. The method is based on relatively simple manually-constructed morphological rules and is demonstrated on both English, German, Italian and Russian. The experiments include intrinsic word similarity benchmarks, showing notable performance improvements achieved by applying morph-fitting to several different corpus-based embeddings. Performance improvement yielding new state-of-the-art results is also demonstrated for German and Italian on an extrinsic task - dialog state tracking.  Strengths: - The proposed method is simple and shows nice performance improvements across a number of evaluations and in several languages. Compared to previous knowledge-based retrofitting approaches (Faruqui et al., 2015), it relies on a few manually-constructed rules, instead of a large-scale knowledge base, such as an ontology.\n- Like previous retrofitting approaches, this method is easy to apply to existing sets of embeddings and therefore it seems like the software that the authors intend to release could be useful to the community.\n- The method and experiments are clearly described. \u2028 Weaknesses: - I was hoping to see some analysis of why the morph-fitted embeddings worked better in the evaluation, and how well that corresponds with the intuitive motivation of the authors.  - The authors introduce a synthetic word similarity evaluation dataset, Morph-SimLex. They create it by applying their presumably semantic-meaning-preserving morphological rules to SimLex999 to generate many more pairs with morphological variability. They do not manually annotate these new pairs, but rather use the original similarity judgements from SimLex999. \nThe obvious caveat with this dataset is that the similarity scores are presumed and therefore less reliable. Furthermore, the fact that this dataset was generated by the very same rules that are used in this work to morph-fit word embeddings, means that the results reported on this dataset in this work should be taken with a grain of salt. The authors should clearly state this in their paper.\n- (Soricut and Och, 2015) is mentioned as a future source for morphological knowledge, but in fact it is also an alternative approach to the one proposed in this paper for generating morphologically-aware word representations. The authors should present it as such and differentiate their work.\n- The evaluation does not include strong morphologically-informed embedding baselines.  General Discussion: With the few exceptions noted, I like this work and I think it represents a nice contribution to the community. The authors presented a simple approach and showed that it can yield nice improvements using various common embeddings on several evaluations and four different languages. I\u2019d be happy to see it in the conference.\nMinor comments: - Line 200: I found this phrasing unclear: \u201cWe then query \u2026 of linguistic constraints\u201d.\n- Section 2.1: I suggest to elaborate a little more on what the delta is between the model used in this paper and the one it is based on in Wieting 2015. It seemed to me that this was mostly the addition of the REPEL part.\n- Line 217: \u201cThe method\u2019s cost function consists of three terms\u201d - I suggest to spell this out in an equation.\n- Line 223:  x and t in this equation (and following ones) are the vector representations of the words. I suggest to denote that somehow. Also, are the vectors L2-normalized before this process? Also, when computing \u2018nearest neighbor\u2019 examples do you use cosine or dot-product? Please share these details.\n- Line 297-299: I suggest to move this text to Section 3, and make the note that you did not fine-tune the params in the main text and not in a footnote.\n- Line 327: (create, creates) seems like a wrong example for that rule. \u2028 - I have read the author response ", "label": [[1021, 1051, "Eval_pos_1"], [1057, 1147, "Eval_pos_2"], [1345, 1443, "Jus_pos_3"], [1448, 1552, "Eval_pos_3"], [1556, 1607, "Eval_pos_4"], [2288, 2407, "Jus_neg_1"], [2420, 2507, "Eval_neg_1"], [2996, 3075, "Major_claim"], [3249, 3290, "Major_claim"]]}
{"id": 3, "review": "This paper outlines a method to learn sense embeddings from unannotated corpora using a modular sense selection and representation process. The learning is achieved by a message passing scheme between the two modules that is cast as a reinforcement learning problem by the authors.\n- Strengths: The paper is generally well written, presents most of its ideas clearly and makes apt comparisons to related work where required. The experiments are well structured and the results are overall good, though not outstanding. However, there are several problems with the paper that prevent me from endorsing it completely.\n- Weaknesses: My main concern with the paper is the magnification of its central claims, beyond their actual worth.\n1) The authors use the term \"deep\" in their title and then several times in the paper. But they use a skip-gram architecture (which is not deep). This is misrepresentation.\n2) Also reinforcement learning is one of the central claims of this paper. \nHowever, to the best of my understanding, the motivation and implementation lacks clarity. Section 3.2 tries to cast the task as a reinforcement learning problem but goes on to say that there are 2 major drawbacks, due to which a Q-learning algorithm is used. This algorithm does not relate to the originally claimed policy.\nFurthermore, it remains unclear how novel their modular approach is. Their work seems to be very similar to EM learning approaches, where an optimal sense is selected in the E step and an objective is optimized in the M step to yield better sense representations. The authors do not properly distinguish their approach, nor motivative why RL should be preferred over EM in the first place.\n3) The authors make use of the term pure-sense representations multiple times, and claim this as a central contribution of their paper. I am not sure what this means, or why it is beneficial.\n4) They claim linear-time sense selection in their model. Again, it is not clear to me how this is the case. A highlighting of this fact in the relevant part of the paper would be helpful.  5) Finally, the authors claim state-of-the-art results. However, this is only on a single MaxSimC metric. Other work has achieved overall better results using the AvgSimC metric. So, while state-of-the-art isn't everything about a paper, the claim that this paper achieves it - in the abstract and intro - is at least a little misleading. ", "label": [[295, 330, "Eval_pos_1"], [332, 424, "Jus_pos_1"], [425, 460, "Eval_pos_2"], [465, 518, "Eval_pos_3"], [519, 615, "Major_claim"], [630, 731, "Eval_neg_1"], [732, 2417, "Jus_neg_1"]]}
{"id": 4, "review": "- Strengths: This is  a well written paper. \nThe paper is very clear for the most part. \nThe experimental comparisons are very well done. \nThe experiments are well designed and executed. \nThe idea of using KD for zero-resource NMT is impressive.\n- Weaknesses: There were many sentences in the abstract and in other places in the paper where the authors stuff too much information into a single sentence. This could be avoided. One can always use an extra sentence to be more clear. \nThere could have been a section where the actual method used could be explained in a more detailed. This explanation is glossed over in the paper. It's non-trivial to guess the idea from reading the sections alone. \nDuring test time, you need the source-pivot corpus as well. This is a major disadvantage of this approach. This is played down - in fact it's not mentioned at all. I could strongly encourage the authors to mention this and comment on it.  - General Discussion: This paper uses knowledge distillation to improve zero-resource translation. \nThe techniques used in this paper are very similar to the one proposed in Yoon Kim et. al. The innovative part is that they use it for doing zero-resource translation. They compare against other prominent works in the field. Their approach also eliminates the need to do double decoding.\nDetailed comments: -Line 21-27 - the authors could have avoided this complicated structure for two simple sentences. \nLine 41 - Johnson et. al has SOTA on English-French and German-English. \nLine 77-79 there is no evidence provided as to why combination of multiple languages increases complexity. Please retract this statement or provide more evidence. Evidence in literature seems to suggest the opposite.\nLine 416-420 - The two lines here are repeated again. They were first mentioned in the previous paragraph. \nLine 577 - Figure 2 not 3! ", "label": [[13, 44, "Eval_pos_1"], [45, 88, "Eval_pos_2"], [89, 138, "Eval_pos_3"], [139, 187, "Eval_pos_4"], [188, 245, "Eval_pos_5"], [260, 404, "Eval_neg_1"], [699, 862, "Eval_neg_2"], [863, 936, "Jus_neg_2"]]}
{"id": 5, "review": "- Strengths:  * Elaborate evaluation data creation and evaluation scheme. \n * Range of compared techniques: baseline/simple/complex - Weaknesses:  * No in-depth analysis beyond overall evaluation results.\n- General Discussion: This paper compares several techniques for robust HPSG parsing.\nSince the main contribution of the paper is not a novel parsing technique but the empirical evaluation, I would like to see a more in-depth analysis of the results summarized in Table 1 and 2. \nIt would be nice to show some representative example sentences and sketches of its analyses, on which the compared methods behaved differently.\nPlease add EDM precision and recall figures to Table 2. \nThe EDM F1 score is a result of a mixed effects of (overall and partial) coverage, parse ranking, efficiency of search, etc. \nThe overall coverage figures in Table 1 are helpful but addition of EDM recall to Table 2 would make the situations clearer.\nMinor comment: -Is 'pacnv+ut' in Table 1 and 2 the same as 'pacnv' described in 3.4.3? ", "label": [[16, 74, "Eval_pos_1"], [149, 204, "Eval_neg_1"]]}
{"id": 7, "review": "This paper presents a corpus of annotated essay revisions.  It includes two examples of application for the corpus: 1) Student Revision Behavior Analysis and 2) Automatic Revision Identification The latter is essentially a text classification task using an SVM classifier and a variety of features. The authors state that the corpus will be freely available for research purposes.\nThe paper is well-written and clear. A detailed annotation scheme was used by two annotators to annotate the corpus which added value to it. I believe the resource might be interesting to researcher working on writing process research and related topics. I also liked that you provided two very clear usage scenarios for the corpus.  I have two major criticisms. The first could be easily corrected in case the paper is accepted, but the second requires more work.\n1) There are no statistics about the corpus in this paper. This is absolutely paramount. When you describe a corpus, there are some information that should be there.  I am talking about number of documents (I assume the corpus has 180 documents (60 essays x 3 drafts), is that correct?), number of tokens (around 400 words each essay?), number of sentences, etc.  I assume we are talking about 60 unique essays x 400 words, so about 24,000 words in total. Is that correct? If we take the 3 drafts we end up with about 72,000 words but probably with substantial overlap between drafts.\nA table with this information should be included in the paper.\n2) If the aforementioned figures are correct, we are talking about a very small corpus. I understand the difficulty of producing hand-annotated data, and I think this is one of the strengths of your work, but I am not sure about how helpful this resource is for the NLP community as a whole. Perhaps such a resource would be better presented in a specialised workshop such as BEA or a specialised conference on language resources like LREC instead of a general NLP conference like ACL.\nYou mentioned in the last paragraph that you would like to augment the corpus with more annotation. Are you also willing to include more essays?\nComments/Minor: - As you have essays by native and non-native speakers, one further potential application of this corpus is native language identification (NLI).\n- p. 7: \"where the unigram feature was used as the baseline\" - \"word unigram\". \nBe more specific.\n- p. 7: \"and the SVM classifier was used as the classifier.\" - redundant. ", "label": [[381, 417, "Eval_pos_1"], [522, 635, "Eval_pos_2"], [636, 714, "Eval_pos_3"], [715, 845, "Major_claim"], [849, 935, "Eval_neg_1"], [935, 1493, "Jus_neg_1"], [1498, 1581, "Jus_neg_2"], [1582, 1786, "Eval_pos_2"], [1786, 1979, "Major_claim"]]}
{"id": 8, "review": "This paper presents several weakly supervised methods for developing NERs. The methods rely on some form of projection from English into another language. The overall approach is not new and the individual methods proposed are improvements of existing methods. For an ACL paper I would have expected more novel approaches.\nOne of the contributions of the paper is the data selection scheme. The formula used to calculate the quality score is quite straightforward and this is not a bad thing. However, it is unclear how the thresholds were calculated for Table 2. The paper says only that different thresholds were tried. Was this done on a development set? There is no mention of this in the paper. The evaluation results show clearly that data selection is very important, but one may not know how to tune the parameters for a new data set or a new language pair.  Another contribution of the paper is the combination of the outputs of the two systems developed in the paper. I tried hard to understand how it works, but the description provided is not clear.  The paper presents a number of variants for each of the methods proposed. Does it make sense to combine more than two weakly supervised systems? Did the authors try anything in this direction.\nIt would be good to know a bit more about the types of texts that are in the \"in-house\" dataset. ", "label": [[155, 322, "Eval_neg_1"], [323, 563, "Eval_neg_2"], [564, 699, "Jus_neg_2"], [867, 1061, "Eval_neg_3"]]}
{"id": 9, "review": "The paper proposes a model for the Stanford Natural Language Inference (SNLI) dataset, that builds on top of sentence encoding models and the decomposable word level alignment model by Parikh et al. (2016). The proposed improvements include performing decomposable attention on the output of a BiLSTM and feeding the attention output to another BiLSTM, and augmenting this network with a parallel tree variant.\n- Strengths: This approach outperforms several strong models previously proposed for the task. The authors have tried a large number of experiments, and clearly report the ones that did not work, and the hyperparameter settings of the ones that did. This paper serves as a useful empirical study for a popular problem.\n- Weaknesses: Unfortunately, there are not many new ideas in this work that seem useful beyond the scope the particular dataset used. While the authors claim that the proposed network architecture is simpler than many previous models, it is worth noting that the model complexity (in terms of the number of parameters) is fairly high. Due to this reason, it would help to see if the empirical gains extend to other datasets as well. In terms of ablation studies, it would help to see 1) how well the tree-variant of the model does on its own and 2) the effect of removing inference composition from the model.\nOther minor issues: 1) The method used to enhance local inference (equations 14 and 15) seem very similar to the heuristic matching function used by Mou et al., 2015 (Natural Language Inference by Tree-Based Convolution and Heuristic Matching). You may want to cite them.\n2) The first sentence in section 3.2 is an unsupported claim. This either needs a citation, or needs to be stated as a hypothesis.\nWhile the work is not very novel, the the empirical study is rigorous for the most part, and could be useful for researchers working on similar problems. \nGiven these strengths, I am changing my recommendation score to 3. I have read the authors' responses. ", "label": [[424, 506, "Eval_pos_1"], [506, 660, "Jus_pos_2"], [661, 729, "Eval_pos_2"], [744, 864, "Eval_neg_1"], [864, 1339, "Jus_neg_1"], [1615, 1673, "Eval_neg_2"], [1674, 1742, "Jus_neg_2"], [1743, 1964, "Major_claim"]]}
{"id": 10, "review": "- Strengths: This paper contributes to the field of knowledge base-based question answering (KB-QA), which is to tackle the problem of retrieving results from a structured KB based on a natural language question. KB-QA is an important and challenging task.\nThe authors clearly identify the contributions and the novelty of their work, provide a good overview of the previous work and performance comparison of their approach to the related methods.\nPrevious approaches to NN-based KB-QA represent questions and answers as fixed length vectors, merely as a bag of words, which limits the expressiveness of the models. And previous work also don\u2019t leverage unsupervised training over KG, which potentially can help a trained model to generalize. \nThis paper makes two major innovative points on the Question Answering problem.\n1) The backbone of the architecture of the proposed approach is a cross-attention based neural network, where attention is used for capture different parts of questions and answer aspects. The cross-attention model contains two parts, benefiting each other. The A-Q attention part tries to dynamically capture different aspects of the question, thus leading to different embedding representations of the question. And the Q-A attention part also offer different attention weight of the question towards the answer aspects when computing their Q-A similarity score. \n2) Answer embeddings are not only learnt on the QA task but also modeled using TransE which allows to integrate more prior knowledge on the KB side. \nExperimental results are obtained on Web questions and the proposed approach exhibits better behavior than state-of-the-art end-to-end methods. The two contributions were made particularly clear by ablation experiment. Both the cross-attention mechanism and global information improve QA performance by large margins.\nThe paper contains a lot of contents. The proposed framework is quite impressive and novel compared with the previous works.\n- Weaknesses: The paper is well-structured, the language is clear and correct. Some minor typos are provided below. \n1. Page 5, column 1, line 421:                                       re-read                    \uf0e0  reread 2. Page 5, column 2, line 454: pairs be  \uf0e0  pairs to be - General Discussion: In Equation 2: the four aspects of candidate answer aspects share the same W and b. How about using separate W and b for each aspect? \nI would suggest considering giving a name to your approach instead of \"our approach\", something like ANN or CA-LSTM\u2026(yet something different from Table 2).   In general, I think it is a good idea to capture the different aspects for question answer similarity, and cross-attention based NN model is a novel solution for the above task. The experimental results also demonstrate the effectiveness of the authors\u2019 approach. Although the overall performance is weaker than SP-based methods or some other integrated systems, I think this paper is a good attempt in end-to-end KB-QA area and should be encouraged. ", "label": [[257, 333, "Eval_pos_1"], [335, 448, "Eval_pos_2"], [745, 824, "Eval_pos_3"], [825, 1858, "Jus_pos_3"], [1859, 1897, "Eval_pos_4"], [1897, 1983, "Eval_pos_5"], [1998, 2062, "Eval_pos_6"], [2941, 3029, "Major_claim"]]}
{"id": 11, "review": "paper_summary\nSince only minor revisions have been made to the paper, my views of the paper have not changed. For details, please see my previous review comments.\nThe author\u2019s response has answered my previous questions very well and added relevant analysis to the revised draft. In my opinion, the analysis of the negative phenomenon on NLU corpora in this paper is comprehensive. But as its contribution is incremental, it is unlikely to be improved through minor modifications. In summary, I think it is a borderline paper of ACL, or as a Findings paper. \nsummary_of_strengths\nHow to deal with negation semantic is one of the most fundamental and important issues in NLU, which is especially often ignored by existing models. This paper verifies the significance of the problem on multiple datasets, and in particular, proposes to divide the negations into important and unimportant types and analyzes them (Table 2). The work of the paper is comprehensive and solid. \nsummary_of_weaknesses\nHowever, I think the innovation of this paper is general. The influence of negation expressions on NLP/NLU tasks has been widely proposed in many specialized studies, as well as in the case/error analysis of many NLP/NLU tasks. In my opinion, this paper is the only integration of these points of view and does not provide deeper insights to inspire audiences in related fields. \ncomments,_suggestions_and_typos\nNA ", "label": [[280, 381, "Eval_pos_1"], [389, 420, "Eval_neg_1"], [422, 479, "Eval_neg_2"], [481, 557, "Major_claim"], [729, 920, "Jus_pos_2"], [921, 971, "Eval_pos_2"], [994, 1051, "Eval_neg_2"], [1052, 1221, "Jus_neg_2"], [1237, 1295, "Eval_neg_3"], [1300, 1371, "Eval_neg_3"]]}
{"id": 12, "review": "paper_summary\nThe paper defines a CBMI metric over the NMT source and a target word (given the target history) and then uses it to re-weight the NMT training loss. The definition is simplified to the quotient of NMT probability and the LM probability. Experiments shows that the training strategy improves the translation quality, over two training datasets, outperforming previous works. The paper further shows the method also improves the human evaluation. \nsummary_of_strengths\n- The proposed method appears to be simple, but works; -Paper appears to be well written; -Experiments comparison and analysis, human evaluation; Overall, paper did a good job in presenting and examining the effectiveness of a simple idea. \nsummary_of_weaknesses\nI think the paper (and related works) presented the works in a way that they presented a hypothesis (eg, importance of token reweighing), then conduct experiments and analysis showing the effectiveness of the method, then saying re-weighing the token importance works. After finishing reading, I felt the need to go back go re-examine the hypothesis to understand more and realized that I still don't understand the problem in a machine learning sense. The authors are encouraged to (at least) post some \"aha\" examples showing re-weighting this way indeed is the one that matters. Also, discussing and revealing the reason why NMT still needs this re-weighting even though the NMT model can in principle implicitly capture them would be really helpful. \ncomments,_suggestions_and_typos\nPlease see the weakness section. ", "label": [[482, 536, "Eval_pos_1"], [537, 571, "Eval_pos_2"], [628, 721, "Eval_pos_3"], [1014, 1197, "Eval_neg_1"], [1198, 1498, "Jus_neg_1"]]}
{"id": 13, "review": "paper_summary\nThis paper describes the development of a data set that can be used to develop a system that can generate automated feedback to learners' responses to short-answer questions.  The data set includes questions, their answers, and their feedback in the domain of computer networking, mostly in English but with a sizable German subset as well.  The paper describes the construction of the data set, including agreement and encountered challenges, as well as experimental results that can serve as a baseline for future work. \nsummary_of_strengths\nAlthough the domain is niche, since the authors do an extremely thorough job of thoughtfully constructing their data set with expert annotators and guidance, agreement-measurement, and validity evidence, this paper should serve as a model to the community with respect to how to compile similar data sets.\nWhile the authors mention that the data set is small -- 4,519 response/feedback pairs covering 22 English and 8 German questions -- it's actually quite large for something that is completely human-compiled and human-reviewed.\nThis paper is very clear, easy to follow, and well-organized. \nsummary_of_weaknesses\nUnfortunately, the final data set contains imbalanced classes, something the authors aim to address in future versions of the data set.  I wouldn't use this as a reason to reject this paper, however.\nSome in our community may find this work, and its domain, rather niche; this paper would be a great fit for the BEA workshop. \ncomments,_suggestions_and_typos\nCan the authors mention the dates during which the data was collected?  Since this was such a big manual effort, I wouldn't be surprised if the bulk of the work was done in 2021 on data collected in 2020, for instance. This is also important since the domain is computer networking which changes fairly rapidly.\nOn line 005, insert \"many\" between \"by\" and \"Automatic\".\nOn line 040, change \"interesting\" to \"useful\".\nOn line 054, \"in the last decades\" should read \"over the past decades\".\nOn line 154, \"detrimental for\" should be \"detrimental to\".\nThe last sentence of section 2.2, beginning with \"Lastly, structured collections...\" seems out-of-place here.  Should this be a separate paragraph?  Or can you do more to tie it in with the preceding sentences?\nOn line 395, \"refined for multiple years\" should be \"refined over multiple years\".\nIn this field, it's typical to refer to learners' responses to questions as \"responses\" rather than \"submissions\".  Just a minor thing you may want to consider :) ", "label": [[558, 761, "Jus_pos_1"], [762, 862, "Eval_pos_1"], [1090, 1152, "Eval_pos_2"], [1175, 1310, "Eval_neg_1"], [1375, 1446, "Eval_neg_2"], [1447, 1501, "Major_claim"]]}
{"id": 14, "review": "paper_summary\nThis paper presents a cross-lingual information retrieval approach using knowledge distillation. The underlying model is ColBERT with XLM-R as the pretained language model. The approach makes use of a teacher model based on query translation and monolingual IR in English. The student model is trained with two objectives. One is an IR objective to match the teacher model's query-passage relevance predictions. The second objective is to learn a representation of the non-english text that most closely matches the teacher's representation at the token level. This relies on a cross lingual token alignment based on greedily aligning tokens with the highest cosine similarity. The authors do abalations of their two objectives and find they are both useful and also compare against fine-tuning ColBERT directly on cross lingual data. On the XOR-TyDi leaderboard, one of this paper's models is the current best. \nsummary_of_strengths\n- Novel approach that does cross lingual IR where the resulting model does not use MT  -New cross lingual token alignment based on multilingual pretrained langauge model -Good abalations and comparisons with fine-tuning on cross lingual data -Strong performance on zero-shot settings as well -The paper has best performance on XOR-TyDi \nsummary_of_weaknesses\nNo major weaknesses \ncomments,_suggestions_and_typos\nline 62-64 asks whether a high performance CLIR model can be trained that can be operate without having to rely on MT. But the training process still relies on MT, so this approach does still rely on MT, right? I guess the point is that it only relies on MT at training time and not at evaluation / inference. It might be possible to try to make this clearer. ", "label": [[950, 1033, "Eval_pos_1"], [1036, 1117, "Eval_pos_2"], [1119, 1189, "Eval_pos_3"], [1191, 1239, "Eval_pos_4"], [1307, 1326, "Major_claim"]]}
{"id": 15, "review": "paper_summary\nThe paper investigates methods to automatically generate morally framed arguments (relying on a specific stance, on the given topic focusing on the given morals), and analyses the effect of these arguments on different audiences (namely, as liberals and conservatives). \nsummary_of_strengths\n- The topic of the paper is potentially interesting to the ACL audience in general, and extremely interesting in particular to the Argument Mining (and debating technology) research community. Investigating methods to inject morals into argument generation systems to make arguments more effective and convincing is a very valuable step in the field (opening at the same time ethical issues).\n-The paper is clear, well written and nicely structured -The experimental setting is well described and the applied methods are technically sound. It relies on the solid framework of the IBM Debater technology. \nsummary_of_weaknesses\n- very limited size of the user study  (6 people in total, 3 liberals and 3 conservatives). Moreover, a \"stereotypical\" hypothesis of their political vision is somehow assumed) -the Cohen\u2019s \u03ba agreement was 0.32 on the moral assignment -> while the authors claim that this value is in line with other subjective argument-related annotations, I still think it is pretty low and I wonder about the reliability of such annotation. \ncomments,_suggestions_and_typos\n[line 734] Ioana Hulpu? - > check reference ", "label": [[308, 498, "Eval_pos_1"], [499, 698, "Eval_pos_2"], [700, 754, "Eval_pos_3"], [756, 845, "Eval_pos_4"], [846, 910, "Jus_pos_4"], [935, 970, "Eval_neg_1"], [972, 1023, "Jus_neg_1"], [1025, 1109, "Eval_neg_2"], [1274, 1358, "Eval_neg_3"]]}
{"id": 17, "review": "paper_summary\nThis paper is about improving the prosody of neural text-to-speech (NTTS) systems using the surrounding context of a given input text. The study introduced an extension to a well known NTTS system i.e., FastSpeech-2. The extension is a phoneme level conditional VAE. As cited in the current paper both FastSpeech-2 and conditional VAE are already proposed in the literature. The main novelty of this paper is representation of surrounding utterances using a pre-trained  BERT model and generation of prosodically varied samples with the help of learned contextual information. Authors followed standard TTS evaluation protocols to evaluate their proposed architecture, and evaluation results are in favor of the proposed architecture. \nsummary_of_strengths\n- This paper introduced a new component to FastSpeech-2, a well known non-autoregressive NTTS architecture, called as cross utterance conditional VAE (CUC-VAE).  -The CUC-VAE contains two main components 1) cross utterance (CU) embedding and 2) CU enhanced conditional VAE. \nsummary_of_weaknesses\n- As a reviewer, I found the paper slightly difficult to read -- some long sentences can be rewritten to improve the clarity of the paper reading.  -The subjective results are derived on a small set of utterances (11 audios) using a small number of listeners (23 subjects), this may not be substantial enough for statistical significance of the results published in the paper.\n-It is not clear why CUC-VAE TTS system with L=1 performed worse than baseline system -- an appropriate reason or further analysis may be required to validate this.  -In general, there are quite a few things missing -- details provided in comments section. \ncomments,_suggestions_and_typos\n**Typos:** -Background section: \"...high fidelity thank to\u2026\" -> \"...high fidelity thanks to\u2026\" -Background section: \" \u2026 Fang et al., 2019).Many\u2026\" -> \" \u2026 Fang et al., 2019). Many\u2026\" -Figure-1: \"...which integrated to into\u2026\" -> \"...which integrated into\u2026\" **Comments:** -Author did not mention how the initial durations of phonemes are obtained.\n-Are durations of phonemes predicted in frames or seconds?\n-Figure-1 did not mention how the proposed CUC-VAE TTS system works in the inference time. Moreover, it is hard to understand the color schema followed in the Figure-1, there is no legend.\n-There is no mentioning of train, valid and test set splits in the dataset section.\n-In Table-2 the baseline system received a better MOS score than the baseline + fine-grained VAE and baseline + CVAE, why is it? Whereas in Table-4 the baseline system show high MCD and FFE error than the baseline + fine-grained VAE and baseline + CVAE systems, why is it?\n-How do you represent the reference mel-spectrogram at phoneme level?\n-Did you use pre-trained HiFi-GAN to synthesize speech from the predicted mel-spectrograms? ", "label": [[1070, 1129, "Eval_neg_1"], [1130, 1214, "Jus_neg_1"], [1216, 1340, "Jus_neg_2"], [1342, 1444, "Eval_neg_2"], [1446, 1530, "Eval_neg_3"], [1531, 1609, "Jus_neg_3"], [1612, 1660, "Eval_neg_4"], [1661, 1701, "Jus_neg_4"]]}
{"id": 18, "review": "paper_summary\nThis paper proposes a novel refinement method to synchronously refine the previously generated words and generate the next word for language generation models. The authors accomplish this goal with an interesting implementation without introducing additional parameters. Specifically, the authors reuses the context vectors at previous decoding steps (i.e., c_1, c_2, ..., c_{i-2}) to calculate the refined probabilities in a similar way to the standard generation probabilities (the only difference is that using c_{0<n<i-1} instead of c_{i-1}). A refinement operation will be conducted at a previous position, where the refinement probability is greater than the generation probability.\nTo reduce the computational cost and potential risk of \"over-refinement\", the authors design a local constraint that narrow the refinement span to the N nearest tokens. In model training, the authors randomly select future target words not greater than N to cover a variety of different future contexts as bleu parts. \nsummary_of_strengths\n1. A novel approach to accomplish the modeling of future context. \n2. Comprehensive experiments to validate the effectiveness of the proposed approach across different tasks (e.g., standard and simultaneous machine translation, storytelling, and text summarization). \n3. Detailed analyses to show how each component (e.g., the hyper parameter N, local constraints and refinement mask) works. \nsummary_of_weaknesses\nThe main concern is the measure of the inference speed. The authors claimed that \"the search complexity of decoding with refinement as consistent as that of the original decoding with beam search\" (line 202), and empirically validated that in Table 1 (i.e., #Speed2.).\nEven with local constraint, the model would conduct 5 (N=5) more softmax operations over the whole vocabulary (which is most time-consuming part in inference) to calculate the distribution of refinement probabilities for each target position. Why does such operations only marginally decrease the inference speed (e.g., form 3.7k to 3.5k tokens/sec for Transformer-base model)?\nHow do we measure the inference speed? Do you follow Kasai et al., (2021) to measure inference speed when translating in mini-batches as large as the hardware allows. I guess you report the batch decoding speed since the number is relatively high. Please clarify the details and try to explain why the refinement model hardly affect the inference speed.\nThe score will be increased if the authors can address the concern.\n[1] Jungo Kasai, Nikolaos Pappas, Hao Peng, James Cross, and Noah Smith. Deep Encoder, Shallow Decoder: Reevaluating Non-autoregressive Machine Translation. ICLR 2021. \ncomments,_suggestions_and_typos\n1. Line118: SelfAtt_c => Cross Attention, the attention network over the encoder representations is generally called as cross attention.\n2. Ablation study in Section 4.1.3 should be conducted on validation sets instead of test sets (similar to Section 4.1.2). In addition, does the refinement mask in Table 2 denote that randomly selecting future target words no greater than N in model training (i.e., Line 254)?\n3. Is PPL a commonly-used metric for storytelling? ", "label": [[1046, 1108, "Eval_pos_1"], [1113, 1216, "Eval_pos_2"], [1218, 1308, "Jus_pos_2"], [1314, 1358, "Eval_pos_3"], [1359, 1427, "Jus_pos_3"], [1428, 1433, "Eval_pos_3"], [1458, 1513, "Eval_neg_1"], [1514, 2458, "Jus_neg_1"]]}
{"id": 19, "review": "paper_summary\nThe paper proposes 6 test corpora for vision and language captioning systems that target specific competency. For each competency, examples are generated semi-automatically from existing language\u00a0+ vision tasks, such QA in V7W, and are created in a FOIL style, where one example correctly describes the image, while another example\u00a0makes a minimal change to caption and does not describe the image. Systems are then challenged\u00a0to prefer captions that correctly identify\u00a0the image. The competencies tested include existence, plurality/counting, spatial reasoning (via prepositions), situational knowledge (via imSitu\u00a0data), and coreference. The paper evaluates several\u00a0recent pre-training based models, finding that many fail at their challenges, and that the multi-task model 12-in-1, works best. \nsummary_of_strengths\nProposes a fairly diverse set of challenges that could be a useful diagnostic going forward.\nThe paper evaluates currently relevant model on the diagnostic, establishing clear baselines for their dataset moving forward.  Because the paper encompasses essentially 5 independent datasets, it a very substantial body of work. It seems larger than a standard paper. \nsummary_of_weaknesses\n(being a previous reviewer R BWRg, I will respond to previously identified weakness) I still find the argument of what is and is not included in the diagnostic unclear. In many ways, this seems like a case of a subset of competencies that we have enough visual annotations to semi-automatically create data for. In my opinion, the paper should steer away from making arguments that these examples are deeply linguistic, beyond, involving nouns, counting, verbs, and coreference. As such, I find the title and some of the introduction over-claiming, but, this is really a matter of opinion, resting on what exactly 'linguistic' means.\nThe main body of the paper still lacks examples but I appreciate their inclusion in the appendix. It's very hard to imagine the foils from the descriptions alone. This may be asking a lot, but the paper would be significantly improved if the last page were almost entirely made of examples from the appendix. This is a CVPR style of presentation, and would require significant text trimming.  The examples were good overall, but the co-ref part of the benchmark stands out. It is essentially a QA task, which isn't really compatible with just caption based training that most of the evaluated most are setup to do (with the exception of 12-1). This isn't an issue, because its not really the benchmark's problem, but I am not sure the format of the foil is that sensible. I suspect this will be the least used of the new foils, but I don't have a concrete proposal how it could be improved to really be a captioning task. \ncomments,_suggestions_and_typos\n- ", "label": [[833, 925, "Eval_pos_1"], [1054, 1118, "Jus_pos_2"], [1120, 1195, "Eval_pos_2"], [1303, 1385, "Eval_neg_1"], [1386, 1851, "Jus_neg_1"], [1852, 1949, "Eval_neg_2"], [1950, 2243, "Jus_neg_2"], [2245, 2325, "Eval_neg_3"], [2326, 2774, "Jus_neg_3"]]}
{"id": 20, "review": "paper_summary\nMotivated by empirical findings that training models with Prompt Tuning can achieve the same performance as fully fine-tuning a model, but the training takes much longer to reach the same performance, they explore ways to exploit knowledge from already trained prompts.\nThey explore using already trained prompts to transfer knowledge between tasks (using the same frozen model) and also the transfer of prompt between _different_ frozen models. For between task transfer, they either directly re-use a prompt from a source task on the target task or they use the prompt learned from the source task as the initialization point for the target task.\nFor between model transfer, they uses these same methods but include a learnable `projector` (a small, 2 layer neural-network) that maps the prompts from one frozen model to another be using the projected prompt in one of the methods mentioned above. They have two methods for learning this `projector`. In the first method, which they call _Distance Minimization_, they minimize the $L_2$ distance between a projected source prompt (trained on the source frozen model) and a target prompt (a prompt trained on the same task using the target model). In the second method (_Task Tuning_) they learn the `projector` via backpropagation. In this case they take a prompt trained on a source task $P_s$, project it ($Proj(P_s)$) and then use that when prompt tuning the target model. Gradient updates are only applied to the projector.\nThey also look at several methods of prompt similarity and use them to predict prompt transferability. They main methods are Cosine and Euclidean distances between prompt tokens and their novel model activation similarity where prompts are fed into frozen models and the activations of the feed-forward layers are recorded. The call this method _ON_. ### Results Their first results look at the performance of directly re-using a prompt trained on a source task for a downstream task. They find that this can produce strong performance (measured in relative performance, the direct source to target prompt transfer performance divided by the performance researched from directly training on the target task) within clusters of similar tasks.\nTheir second results look at the performance of using a prompt learned on a source task to initialize the prompt for a target task and then doing Prompt Tuning. They find that this method can give consistent gains in terms of task performance as well as speed of convergence.\nTheir third set results examine transfer across models. They find that direct re-use of a prompt projected by the `projector` learned via the _Distance Minimization_ method results in poor performance, especially within the Sentiment tasks. They find that direct reuse of a prompt projected by a `projector` learned with their _Task Tuning_ method does better especially when the tasks are within the same cluster. They also look at how using a _Task Tuning_ prompt to initialize training of a new prompt performs and finds that it can lead to some improvements in task performance and small improvements in convergence speed.\nTheir final set of results examine use prompt similarity methods to predict prompt transferablity (in the context of direct prompt reuse). They find that all methods are able to distinguish between multiple prompts (created by training with different random seeds) trained for the same task from prompts trained for other tasks. They also find that _ON_ produces a ranking of similar prompts that best correlate with direct reuse performance (using Spearman's rank correlation scores). They also find that the correlation decreases as the size of the frozen model grows. \nsummary_of_strengths\nThe strengths of the paper include:  * Experiments on many different and diverse datasets, 17 with a good mixture of sentiment, NLI, EJ, Paraphrase detection, and Question answers. \n  * Experiments across many model sizes and architectures, including encoder-only models like RoBERTa instead of just the encoder-decoder and decoder-only models we see else where. \n  * The inclusion of small motivating experiments like the convergence speed are a great way to establish the importance of the work and the impact it would have. \n   * The use of the same methods (direct reuse of prompts and using prompts as initialization) in different settings (cross task transfer with the same model and cross model transfer with the same task) and similar results in each demonstrate the robustness of the method. \n   * Not only does their novel prompt similarity method (_ON_ based on model activations when processing the prompt) work great at predicting direct use similarity, it also captures the non-linear way the model interacts with the prompt in a way that simple methods like token similarity can. \nsummary_of_weaknesses\nThe majority of the weaknesses in the paper seem to stem from confusion and inconsistencies between some of the prose and the results.\n1. Figure 2, as it is, isn't totally convincing there is a gap in convergence times. The x-axis of the graph is time, when it would have been more convincing using steps. Without an efficient, factored attention for prompting implementation a la [He et al. (2022)](https://arxiv.org/abs/2110.04366) prompt tuning can cause slow downs from the increased sequence length. With time on the x-axis it is unclear if prompt tuning requires more steps or if each step just takes more time. Similarly, this work uses $0.001$ for the learning rate. This is a lot  smaller than the suggested learning rate of $0.3$ in [Lester et al (2021)](https://aclanthology.org/2021.emnlp-main.243/), it would have been better to see if a larger learning rate would have closed this gap. Finally, this gap with finetuning is used as a motivating examples but the faster convergence times of things like their initialization strategy is never compared to finetuning. \n2. Confusion around output space and label extraction. In the prose (and Appendix A.3) it is stated that labels are based on the predictions at `[MASK]` for RoBERTa Models and the T5 Decoder for generation. Scores in the paper, for example the random vector baseline for T5 in Table 2 suggest that the output space is restricted to only valid labels as a random vector of T5 generally produces nothing. Using this rank classification approach should be stated plainly as direct prompt reuse is unlikely to work for actual T5 generation. \n3. The `laptop` and `restaurant` datasets don't seem to match their descriptions in the appendix. It is stated that they have 3 labels but their random vector performance is about 20% suggesting they actually have 5 labels? \n4. Some relative performance numbers in Figure 3 are really surprising, things like $1$ for `MRPC` to `resturant` transfer seem far too low, `laptop` source to `laptop` target on T5 doesn't get 100, Are there errors in the figure or is where something going wrong with the datasets or implementation? \n5. Prompt similarities are evaluated based on correlation with zero-shot performance for direct prompt transfer. Given that very few direct prompt transfers yield gain in performance, what is actually important  when it comes to prompt transferability is how well the prompt works as an initialization and does that boost performance. Prompt similarity tracking zero-shot performance will be a good metric if that is in turn correlated with transfer performance.  The numbers from Table 1 generally support that this as a good proxy method as 76% of datasets show small improvements when using the best zero-shot performing prompt as initialization when using T5 (although only 54% of datasets show improvement for RoBERTa). However Table 2 suggests that this zero-shot performance isn't well correlated with transfer performance. In only 38% of datasets does the best zero-shot prompt match the best prompt to use for transfer (And of these 5 successes 3 of them are based on using MNLI, a dataset well known for giving strong transfer results [(Phang et al., 2017)](https://arxiv.org/abs/1811.01088)). Given that zero-shot performance doesn't seem to be correlated with transfer performance (and that zero-shot transfer is relatively easy to compute) it seems like _ON_'s strong correlation would not be very useful in practice. \n6. While recent enough that it is totally fair to call [Vu et al., (2021)](https://arxiv.org/abs/2110.07904) concurrent work, given the similarity of several approaches there should be a deeper discussion comparing the two works. Both the prompt transfer via initialization and the prompt similarity as a proxy for transferability are present in that work. Given the numerous differences (Vu et al transfer mostly focuses on large mixtures transferring to tasks and performance while this work focuses on task to task transfer with an eye towards speed. _ ON_ as an improvement over the Cosine similarities which are also present in Vu et al) it seems this section should be expanded considering how much overlap there is. \n7. The majority of Model  transfer results seem difficult to leverage. Compared to cross-task transfer, the gains are minimal and the convergence speed ups are small. Coupled with the extra time it takes to train the projector for _Task Tuning_ (which back propagation with the target model) it seems hard to imagine situations where this method is worth doing (that knowledge is useful). Similarly, the claim on line 109 that model transfer can significantly accelerate prompt tuning seems lie an over-claim. \n8. Line 118 claims `embedding distances of prompts do not well indicate prompt transferability` but Table 4 shows that C$_{\\text{average}}$ is not far behind _ON_. This claim seems over-reaching and should instead be something like \"our novel method of measuring prompt similarity via model activations is better correlated with transfer performance than embedding distance based measures\" \ncomments,_suggestions_and_typos\n1. Line 038: They state that GPT-3 showed extremely large LM can give remarkable improvements. I think it would be correct to have one of their later citations on continually developed LM as the one that showed that. GPT-3 mostly showed promise for Few-Shot evaluation, not that it get really good performance on downstream tasks. \n2. Line 148: I think it would make sense to make a distinction between hard prompt work updates the frozen model (Schick and Sch\u00fctez, etc) from ones that don't. \n3. Line 153: I think it makes sense to include [_Learning How to Ask: Querying LMs with Mixtures of Soft Prompts_ (Qin and Eisner, 2021)](https://aclanthology.org/2021.naacl-main.410.pdf) in the citation list for work on soft prompts. \n4. Figure 3: The coloring of the PI group makes the text very hard to read in Black and White. \n5. Table 1: Including the fact that the prompt used for initialization is the one that performed best in direct transfer in the caption as well as the prose would make the table more self contained. \n6. Table 2: Mentioning that the prompt used as cross model initialization is from _Task Tuning_ in the caption would make the table more self contained. \n7. Line 512: It is mentioned that _ON_ has a drop when applied to T$5_{\\text{XXL}}$ and it is suggested this has to do with redundancy as the models grow. I think this section could be improved by highlighting that the Cosine based metrics have a similar drop (suggesting this is a fact of the model rather than the fault of the _ON_ method). Similarly, Figure 4 shows the dropping correlation as the model grows. Pointing out the that the _ON_ correlation for RoBERTA$_{\\text{large}}$ would fit the tend of correlation vs model size (being between T5 Base and T5 Large) also strengths the argument but showing it isn't an artifact of _ON_ working poorly on encoder-decoder models. I think this section should also be reordered to show that this drop is correlated with model size. Then the section can be ended with hypothesizing and limited exploration of model redundancy. \n8. Figure 6. It would have been interesting to see how the unified label space worked for T5 rather than RoBERTAa as the generative nature of T5's decoding is probably more vulnerable to issue stemming from different labels. \n9. _ ON_ could be pushed farther. An advantage of prompt tuning is that the prompt is transformed by the models attention based on the value of the prompt. Without having an input to the model, the prompts activations are most likely dissimilar to the kind of activations one would expect when actually using the prompt. \n10. Line 074: This sentence is confusing. Perhaps something like \"Thus\" over \"Hence only\"? \n11. Line 165: Remove \"remedy,\" ", "label": [[3769, 3821, "Eval_pos_1"], [3823, 3911, "Jus_pos_1"], [3918, 3972, "Eval_pos_2"], [3972, 4095, "Jus_pos_2"], [4850, 4984, "Eval_neg_1"], [4985, 9951, "Jus_neg_1"]]}
{"id": 21, "review": "paper_summary\nThis paper focuses on using bandit learning to learn from user feedback for Extractive QA (EQA), the binary supervisory signals from user feedback serve as rewards pushing QA systems to evolve. The learning algorithm aims to maximise the rewards of all QA examples, which consists of online learning and offline learning, the online learning receives user feedback and updates model parameters after seeing one QA example, whereas offline learning updates model parameters after seeing all QA examples.  The experimental results on QA datasets from MRQA support the effectiveness of the proposed bandit learning approach, proving that the proposed approach can consistently improve model\u2019s performance on SQuAD, HotpotQA and NQ in in-domain experiments under online learning especially when there are extremely little QA examples available for SQuAD. Besides, a set of experiments are conducted to investigate the difference between online learning and offline learning, and the importance of model initialisation in the proposed bandit learning approach. \nsummary_of_strengths\n1. The proposed bandit learning approach that learns from user feedback for EQA is novel, which simulates real deployment environment and provides insights for further exploration in bridging the gap between QA model training and deployment. \n2. Empirical results show the effectiveness of the proposed approach, especially the in-domain experimental results for online learning. \n3. Conducting extensive experiments studying the effect of domain transfer and model initialisation. \nsummary_of_weaknesses\n1. The binary reward from user feedback is weak due to the large search space for EQA, resulting in the incapability of providing precise supervisory signals. Need to design a more sophisticated reward. \n2. The proposed approach heavily relies on how accurate the initial model is, which means it is highly sensitive to model initialisation, limiting its usefullness. \n3. In in-domain experiments of online and offline learning, bandit learning approach hurts model\u2019s performance under some scenarios especially for TriviaQA and SearchQA. \n4. Some other papers of learning from feedback for QA should be compared, such as Learning by Asking Questions, Misra et al. CVPR 2017. \ncomments,_suggestions_and_typos\nQuestions:      1. Why only use single-pass in online learning? ", "label": [[1095, 1334, "Eval_pos_1"], [1338, 1471, "Eval_pos_2"], [1476, 1574, "Eval_pos_3"], [1600, 1755, "Jus_neg_1"], [1756, 1799, "Eval_neg_1"], [1804, 1938, "Jus_neg_2"], [1939, 1963, "Eval_neg_2"]]}
{"id": 22, "review": "paper_summary\nThis paper works on the problem of personalization in knowledge grounded conversation (KGC). To develop a benchmark, the authors collected a new KGC dataset based on Reddit containing personalized information (e.g. user profile and dialogue history). The authors propose a probabilistic model for utterance generation conditioned on both personalized profile (personal memory) and personal knowledge. Dual learning is employed to better learn the unconstrained relation between personal memory $Z^m$ and knowledge $Z^k$ , and variational method is proposed to approximately marginalize out $Z^m$ and $Z^k$ during inference. The results with automatic evaluation show promising improvement and human evaluation also validates this. Finally, various ablation studies are conducted to reveal the contribution of each model component. \nsummary_of_strengths\n- The problem of personalization in KGC is a relatively overlooked yet important problem. The authors developed a promising method and benchmark for this new challenge.\n- The idea of incorporating dual learning to link personalized sources (e.g. personal memory and knowledge) is very interesting and convincing. I\u2019d like to see follow-up works comparing the ideas against this paper\u2019s.\n- The improvement in automatic evaluation is significant (though not fully reliable, as the author\u2019s acknowledge in line 522). Human evaluation also corroborates the proposed model\u2019s superiority, though the improvement becomes less significant. \nsummary_of_weaknesses\n- The paper is generally well-written and easy to follow, but the definition of personal memory was quite ambiguous and not fully defined. For instance, does this concept include false beliefs (incorrect knowledge), subjective opinions (unsupported knowledge) or inferential knowledge? What would be the unit of personal memory in the context of visually grounded dialogues (line 134)? How can we extend the idea to inter-personal knowledge, i.e. common ground?\n- I understand the space is limited, but I think more information/explanation on the collected dataset should be added (e.g. data collecting procedure and reviewing process). \ncomments,_suggestions_and_typos\n- In lines 198-220, explanation of $\\phi$, $\\psi$ and $\\pi$ is not clear. Can they be better explained or incorporated in Figure 2?\n- In Figure 2, should the distilled distribution of $Z^p$ not be conditioned on $Z^k$? In the text, $q_\\phi (Z^p | C, R)$ is not conditioned on $Z^k$ (lines 199, 207) - Typo: \u201cthe the\u201d in line 278 - For Table 3, did you also evaluate against human answers (e.g. original response)? If available, it may be better to be incorporated.\n- What exactly is personal memory? How is this defined, esp. in other domains? I\u2019d like to see more discussion on this in the updated paper. ", "label": [[957, 1034, "Eval_pos_1"], [1038, 1179, "Eval_pos_2"], [1256, 1498, "Eval_pos_3"], [1584, 1659, "Eval_neg_1"], [1661, 1983, "Jus_neg_1"], [2025, 2102, "Eval_neg_2"], [2103, 2157, "Jus_neg_2"]]}
{"id": 24, "review": "paper_summary\nThis paper proposes a solution for \"Contrastive Conflicts\". What exactly are \u201cContrastive Conflicts\u201d? They occur when multiple questions are derived from a passage, each with different semantics. The questions are going to be close to the passage in representation space and by transitivity they are going to be close among themselves even though they are semantically different (Transitivity Conflict). In addition to this, if multiple questions derived from the same passage are in the same training batch, then the questions will see that passage as both positive and negative (In-Batch Conflict).\nThe solution proposed by the paper is to use smaller granularity units, i.e. contextualized sentences. Per sentence representations are computed by using per sentence special indicator tokens, then a similar approach to DPR is used to finetune sentence representations. Because different questions have answers in different sentences the contrastive conflict is generally resolved.\nImprovements are reported on NQ, TriviaQA and SQuAD, especially on SQuAD where conflicts are reported to be severe (i.e. often multiple different questions are extracted from the same passage). Extensive experiments show that the method does well even in transfer learning. \nsummary_of_strengths\nStrengths: -The paper obtains small but convincing improvements on NQ and TriviaQA, and large but a bit puzzling results on SQuAD (considering that one of the baselines does not match the DPR paper and that SQuAD can benefit dramatically from combining DPR with BM25, but it is not done in this paper).\n-The paper presents many interesting ablations and transfer learning experiments that help further convince the reader of the efficacy of the method. \nsummary_of_weaknesses\nWeaknesses: -Retrieving (avg # sentences) * 100 sentences (see section 3.3) instead of just 100 sentences seems to be a bit of a cheat. For a strict comparison to DPR, Top-20 and Top-100 performance should be reported with exactly those numbers of retrieved elements and without post-processing on larger sets of retrieved passages. One could argue that allowing for more expensive passage retrieval is what is giving the improvements in this paper, other than for SQuAD where the lower granularity does seem to be helping, except it doesn\u2019t help as much as BM25.\n-The idea of having more granular representations for passage retrieval is far from new. The authors do cite DensePhrases (Lee et al. 2021), but don\u2019t mention that it\u2019s already at lower granularity than passage level. They could also cite for example ColBERT (Khattab et al. 2021).\n-The big improvement reported in Table 2 for SQuAD \u201cSingle\u201d is a bit confusing since it relies on a Top-20 number that is much lower that what is reported on the DPR paper (although this seems to be a common problem). On the positive side, the number reported for SQuAD \u201cMulti\u201d matches the DPR paper. \ncomments,_suggestions_and_typos\nSuggestions: -Line 91: the authors claim that contrastive conflicts are *the* cause for bad performance on SQuAD, but the statement seems unjustified at that point. It might make sense to refer to later results in the paper. ", "label": [[1305, 1422, "Eval_pos_1"], [1597, 1746, "Eval_pos_2"], [1781, 1904, "Eval_neg_1"], [1905, 2332, "Jus_neg_1"], [2334, 2421, "Eval_neg_2"], [2422, 2614, "Jus_neg_2"], [2617, 2693, "Eval_neg_3"], [2694, 2832, "Jus_neg_3"], [2963, 3113, "Eval_neg_4"], [3114, 3174, "Jus_neg_4"]]}
{"id": 26, "review": "paper_summary\nThe paper presents a novel approach to understanding math problems from their textual formulations. The approach builds on those from related work, choosing syntactic representations. The key novelties are (1) an internal graph representation of the operators and (2) a novel pretraining setting. The model achieves vast improvements over prior art. \nsummary_of_strengths\nThe new model addresses several key problems of previous work and appears to contribute a very logically motivated extension, modeling the structure of the required mathematical operations. The model description is clear and the experimental setup and results are reasonably clear and allow for an easy comparison with related work. There is also an ablation study to analyze the contribution of the individual components of the model.  The paper is easy to read. \nsummary_of_weaknesses\nThe model section seems to lack comparison with prior work. It is not entirely clear what is novel here and what is taken from prior work. It is also not entirely clear to me if pretraining is performed with data from all tasks and whether the same setup had been used previously. If this is different from prior work, that would be unfair and a major flaw. \ncomments,_suggestions_and_typos\nI'd like to see my doubt about the pretraining cleared up. ", "label": [[311, 364, "Eval_pos_8"], [386, 575, "Eval_pos_7"], [576, 606, "Eval_pos_1"], [611, 719, "Eval_pos_2"], [823, 848, "Eval_pos_3"], [873, 932, "Eval_neg_1"], [933, 1231, "Jus_neg_1"]]}
{"id": 29, "review": "paper_summary\nSee the prior review for a summary. Based upon the author response I do raise my score slightly from 2.5 to 3.0 to reflect that the definitions referenced in the author response might be sufficient for a target audience that is intimately familiar with WSD. On the other hand, it remains open as to what the impact of the proposed approach would be on any of the noted downstream applications, or beyond English. While WSD can be considered part of the traditional NLP preprocessing pipeline, it's impact on modern end-to-end solution is likely small. Nevertheless there might be high-impact cases such as token-based retrieval (which is used widely), and investigating the impact of the proposed approach on such applications might provide a convincing data point that can provide evidence for the impact of the proposed work. \nsummary_of_strengths\nSee the prior review. \nsummary_of_weaknesses\nSee the prior review. \ncomments,_suggestions_and_typos\nSee the prior review. ", "label": []}
{"id": 31, "review": "paper_summary\nThe paper describes a new approach towards MeSH label prediction, utilizing the title abstract journal relative information. The proposed model combines BiLSTMs, Dilated CNNs and GCNNs to extract features from abstracts, titles and the mesh term hierarchy respectively. Limiting the search MeSH space with information extraction from metadata (such as other articles published in that journal) allows for a boost in performance by building dynamic attention masks. The final model shows good performance compared to related approaches, one of which uses the full article. \nsummary_of_strengths\n- Utilized information past the document itself to limit the MeSH search space -Introduces novel end-to-end architecture that can be used in other tasks involving scholarly articles -Achieves good performance compared to related approaches. \nsummary_of_weaknesses\n- Threshold is said to have a very big impact but is not discussed in detail with different ablations. How does threshold affect computational complexity (outside of performance)?  -Some of the design choices are not explained well (e.g. why IDF-weighting) -Training time (epochs) and computational complexity of the kNN and GCNN component is not discussed. \ncomments,_suggestions_and_typos\n- Equations 10 & 11 should be H_{abstract} instead of D_{abstract}? If not, when is H_{abstract} used?  -There is a significant drop in performance for MeSH terms when metadata are not available, leading to a worse performance than other methods (Ablations-d). In case of new journals or preprints, is this the expected performance?  -With the tuned threshold, how many MeSH terms are not selected during the dynamic masking on average in the different data splits? What is the hierarchical level of these terms?  -A few minor typos, proof reading should fix them. Nothing major. ", "label": [[688, 789, "Eval_pos_1"], [791, 847, "Eval_pos_2"], [875, 974, "Eval_neg_1"], [974, 1052, "Jus_neg_1"], [1054, 1103, "Eval_neg_2"], [1104, 1230, "Jus_neg_2"]]}
{"id": 34, "review": "paper_summary\nThe performance of structured prediction models can be greatly improved by scaling to larger state spaces, yet the inference complexity of these models scales poorly w.r.t. the size of the state space. The goal of this work is to reduce the inference complexity of structured models by factorizing the clique potentials using low-rank tensor decompositions and performing message passing in an induced rank space instead of the original state space.\nThis work makes three contributions: 1. Using the language of factor graph grammars, this work unifies previous low-rank tensor decomposition works such as Yang et al 2021b  and Chiu et al 2021. This work shows that those works are essentially performing message passing on a factor graph with two types of nodes: the original state nodes and auxiliary rank nodes induced by the low-rank tensor decomposition. \n2. On a sub-family of factor graph grammars which subsume most commonly-used structured prediction models such as HMMs, HSMMs, and PCFGs, this work proposes to marginalize the state nodes first and only perform inference in the induced rank nodes, which reduces the complexity by replacing a factor of the state size by a factor of the rank size which is usually smaller. \n3. Empirically this work scales HMMs and PCFGs to very large state spaces and achieves strong performance. \nsummary_of_strengths\n1. This work is insightful in pointing out that by performing message passing only in the rank space after marginalizing the original state nodes (which is a one-time cost), a factor of the number of states in the total complexity can be replaced by a factor of the rank size. This idea is generally applicable to a large family of factor graph grammars that have one external node per hypergraph fragment, and it might enable scaling many structured prediction models. \n2. This work gets strong empirical performance by scaling to very large state spaces when compared to previous structured prediction works. In particular, this work trains the largest-ever PCFG in the task of unsupervised parsing on PTB (to my knowledge) and establishes a new state-of-the-art performance in this particular task. \n3. This work confirms findings of previous works such as Chiu and Rush 2020 that scaling structured prediction models can improve performance. For example, Figure 6 (b) suggests that scaling PCFGs to beyond 10k pre-terminals might further improve modeling performance. \nsummary_of_weaknesses\nBy showing that there is an equivalent graph in the rank space on which message passing is equivalent to message passing in the original joint state and rank space, this work exposes the fact that these large structured prediction models with fully decomposable clique potentials (Chiu et al 2021 being an exception) are equivalent to a smaller structured prediction model (albeit with over-parameterized clique potentials).  For example, looking at Figure 5 (c), the original HMM is equivalent to a smaller MRF with state size being the rank size (which is the reason why inference complexity does not depend on the original number of states at all after calculating the equivalent transition and emission matrices). One naturally wonders why not simply train a smaller HMM, and where does the performance gain of this paper come from in Table 3.\nAs another example, looking at Figure 4 (a), the original PCFG is equivalent to a smaller PCFG (with fully decomposable potentials) with state size being the rank size. This smaller PCFG is over-parameterized though, e.g., its potential $H\\in \\mathcal{R}^{r \\times r}$ is parameterized as $V U^T$ where $U,V\\in \\mathcal{R}^{r \\times m}$ and $r < m$, instead of directly being parameterized as a learned matrix of $\\mathcal{R}^{r \\times r}$. That being said, I don't consider this a problem introduced by this paper since this should be a problem of many previous works as well, and it seems an intriguing question why large state spaces help despite the existence of these equivalent small models. Is it similar to why overparameterizing in neural models help? Is there an equivalent form of the lottery ticket hypothesis here? \ncomments,_suggestions_and_typos\nIn regard to weakness #1, I think this work would be strengthened by adding the following baselines: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared. \n2. For each HMM with rank r, add a baseline smaller HMM with state size being r. ", "label": [[1380, 1403, "Eval_pos_1"], [1404, 1653, "Jus_pos_1"], [1851, 1987, "Eval_pos_2"], [1988, 2179, "Jus_pos_2"], [4207, 4280, "Eval_neg_1"], [4282, 4707, "Jus_neg_1"]]}
{"id": 37, "review": "paper_summary\nIn this work, the authors proposed a unified model of task-oriented dialogue understanding and response generation. The two major enhancements are adopting task-oriented dialogue pre-training on a data collection, and introducing the prompt-based learning for the multi-task capability via one model. From the experimental results, the pre-training strategy proved useful to improve the performance on the benchmark MultiWOZ. \nsummary_of_strengths\nWhile the idea of task-specific pre-training is not new, it is still interesting, and the proposed method proved effective in leveraging the language backbone T5, and can be potentially applied to other models and tasks. \nsummary_of_weaknesses\n1. There are some other contemporary state-of-the-art models, the authors can consider citing and including them for an extensive comparison. \n2. It will be good to see some analysis and insights on different combinations of pre-training datasets introduced in Table 1. \ncomments,_suggestions_and_typos\nHere are some questions: 1. Since some of the sub-tasks, like dialogue state tracking, require a fixed format of the output, if the model generation is incomplete or in an incorrect format, how can we tackle this issue? \n2. The dialogue multi-task pre-training introduced in this work is quite different from the original language modeling (LM) pre-training scheme of backbones like T5. Thus I was curious about why not pre-train the language backbone on the dialogue samples first with the LM scheme, then conduct the multi-task pre-training? Will this bring some further improvement? \n3. It will be good to see some results and analysis on the lengthy dialogue samples. For instance, will the performance drop on the lengthy dialogues? ", "label": [[462, 542, "Eval_pos_1"], [548, 681, "Eval_pos_2"], [709, 848, "Eval_neg_1"], [851, 976, "Eval_neg_2"]]}
{"id": 38, "review": "paper_summary\nThe paper studies the benefits of introducing a Bayesian perspective to abstractive summarization. The authors run the MC dropout method on two pre-trained summarization models, sampling different summarization texts according to specific dropout filters. They use BLEUVarN as a metric of uncertainty for a possible summarization, showing the variations across the summary samples. The authors conduct experiments on three datasets on the correlation between the uncertainty and summarization performances, and show that the performance of the summarization can slightly improve by selecting the \"median\" summary across the pool of sampled ones. \nsummary_of_strengths\n- To the extent of my knowledge, it is the first work that study model uncertainty (in the particular form of the variability of generated summaries) in abstractive summarization.  - The paper provides an analyses on three collections, showing the (cor)relations between the metric of summarization uncertainty (or in fact summarization variability) and ROUGH. They observe that in general the higher the uncertainty score of a summary, the lower its ROUGH score.\n- The work shows that the performance of summarization can be slightly improved by selecting the summary that lays in the \"centroid\" of the pool of generated summaries. \nsummary_of_weaknesses\nMy main concerns are the lack of novelty and proper comparison with a previous study.\n- As correctly mentioned in the paper, the work of Xu et al. is not based on MC dropout. However, that work still provide a metric of uncertainty over a generated summary. In fact, the metric of Xu et al. (namely the entropy of the generation distributions) comes with no or little extra computational costs, while the MC dropout of 10 or 20 introduces considerably large feedforward overheads. I believe the method of Xu et al. can be compared against in the experiments of 5.1. This can let the reader know whether the extra cost of MC dropout method comes with considerable benefits.\n- There is no specific novelty in the method. The observation regarding the correlation between uncertainty and performance is in fact an expected one, and has already observed in several previous studies (also in the context of language generation), like: Not All Relevance Scores are Equal: Efficient Uncertainty and Calibration Modeling for Deep Retrieval Models Daniel Cohen, Bhaskar Mitra, Oleg Lesota, Navid Rekabsaz, Carsten Eickhoff In proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2021)- - The reported improvement is marginal, while achieved with the large overhead of MC sampling. My guess is that the improvement is only due to the effect of ensembling, inherent in MC droupout. \ncomments,_suggestions_and_typos\nAs mentioned above: - I believe the method of Xu et al. can be compared against in the experiments of 5.1. This can let the reader know whether the extra cost of MC dropout method comes with considerable benefits.\n- More evidences regarding the performance improvement, showing that it is not only due to the effect of ensembling.\n- Studying more efficient and recent Bayesian approaches, such as: Agustinus Kristiadi, Matthias Hein, and Philipp Hennig. 2020. Being Bayesian, Even Just a Bit, Fixes Overconfidence in ReLU Networks. In Proceedings of the 37th International Conference on Machine Learning (Proceedings of Machine Learning Research, Vol. 119), Hal Daum\u00e9 III and Aarti Singh (Eds.). PMLR ", "label": [[684, 861, "Eval_pos_1"], [1338, 1423, "Eval_neg_1"], [1424, 2010, "Jus_neg_1"], [2011, 2057, "Eval_neg_2"], [2057, 2579, "Jus_neg_2"], [2581, 2675, "Eval_neg_3"]]}
{"id": 39, "review": "paper_summary\nThis work has created a benchmark dataset for multi-task learning for biomedical datasets. Based on this new benchmark dataset, this work has proposed instruction learning based multi-task learning, which has shown to outperform single-task learning as well as vallina multi-task learning. \nsummary_of_strengths\n1. This work has newly aggregated more than 20 biomedical datasets in 9 categories into a new multi-task paradigm and formalize them into a text to text format so that we can build one unified model for all different tasks. \n2. This work has proposed using manually created instructions for multi-task learning so that the model can be instructed to perform each task without confusion. And this method has been shown to outperform a lot the vanilla multi-task learning and also outperform single-task learning in some cases. \nsummary_of_weaknesses\n1. In the proposed method, the BI would be concatenated with instances as the input to the BART model, and in the BI, examples are provided. Actually these examples are extracted from those instances, then why should we still have examples in BI? How about just having those instructions in the BI? \n2. One important baseline is missing: in those methods proposed for DecaNLP and UnifiedQA, etc., other types of tokens or phrases are used to indicate which task/dataset each input instance belongs to, which is very important to let the model know what the input instance it is. However, in the baseline of vanilla multi-task learning (V-BB), no such kinds of special tokens are used at all, which forms a very unfair baseline to be compared with. The model are fed by so many instances from various kinds of tasks without any differentiation, which for sure would lead to deteriorate performance. For this reason, the effectiveness or the necessity of BI is questionable. \n3. More deep analysis over the impacts of different kinds of designs of the BI is needed, since such designs can vary a lot among different designers or writers. If so, the performance would be very unstable due to the variance of BI, which makes this type of method not applicable to real-world problems. \n4. Only Rouge-L is used for evaluation, which makes the evaluation not that reliable. Especially for some classification tasks, Rouge-L is not sensitive enough. \ncomments,_suggestions_and_typos\n1. In lines 382-384, it is mentioned that \"We have discarded long samples (>1024 token length) from validation and testing data as well.\". I think it is not appropriate to throw any examples from the test set. ", "label": [[1178, 1212, "Eval_neg_1"], [1213, 1622, "Eval_neg_1"], [1623, 1772, "Jus_neg_2"], [1773, 1848, "Eval_neg_2"], [1852, 1938, "Eval_neg_3"], [1939, 2010, "Jus_neg_3"], [2159, 2242, "Eval_neg_4"], [2242, 2317, "Jus_neg_4"]]}
{"id": 41, "review": "paper_summary\nThis paper propose prefix-based models for controllable text generation. Similar to [1], prefixes are token embeddings of language models (e.g., GPT-2) used for learning attribute-specific information and steering the generation of the fixed language models. The authors further add a contrastive loss to enhance the models' controllability. In addition, an unsupervised learning method is introduced to handle scenarios where labels are not available. The authors evaluated the proposed models on multiple controllable text generation tasks, such as controlling sentiment and topics. The experimental results show that comparing to baselines like PPLM and GeDi, the proposed model can achieve a good balance between fluency and controllability.\n[1] Prefix-tuning: Optimizing continuous prompts for generation. ACL 2021 \nsummary_of_strengths\n- The proposed lightweight model achieved strong performance in multiple controllable text generation tasks.\n-The idea of controlling language models in unsupervised way is interesting and new. \nsummary_of_weaknesses\n- Missing human evaluation for the proposed unsupervised learning method. The major technical contribution (novelty) of the paper is controlling language models in unsupervised manner. Unfortunately, human evaluation is absent (in table 4) to demonstrate its effectiveness.\n-For the multi-aspect controlling experiments, CTRL[1] and PPLM[2] should be good baselines.\n[1] CTRL: A conditional transformer language model for controllable generation.  [2] Plug and play language models: A simple approach to controlled text generation. ICLR 2020 \ncomments,_suggestions_and_typos\nPlease consider adding new human evaluation results and baselines as mentioned in weaknesses. ", "label": [[858, 964, "Eval_pos_1"], [965, 1050, "Eval_pos_2"], [1075, 1146, "Eval_neg_1"], [1147, 1258, "Jus_neg_1"], [1258, 1346, "Eval_neg_1"]]}
{"id": 42, "review": "paper_summary\nThe paper provides a benchmark dataset that can be used for training & evaluation of automated fact checking systems. The major contribution of this paper is that they provide a large collection of 33,697 claims with associated review articles and premise articles. In the experiment, this work presents a two-stage detection framework, including evidence sentence extraction and claim veracity inference.  LSTM-based baselines and RoBERTa-based baselines are included and compared. \nsummary_of_strengths\n1. The idea of using premises articles for claim inference in automated fact checking is interesting. \n2. The paper is overall well-structured and the methods are explained clearly. \nsummary_of_weaknesses\n1. The methods are not novel as they are largely borrowing from existing work. \n2. It would be nice to have more detailed descriptions of the data collection process, e.g., label mapping, and data statistics, (how many articles per claims? how many sentences per articles? sentence length?) If not enough space on the main text, these information could be added on appendix. \n3. It would be better if the authors evaluate more state-of-the-art methods on this benchmark dataset. \n4. In section 3.3, the authors claim that the disadvantage of using web search is indirect data leak. Can we eliminate the data leak through filtering with publishing time. \ncomments,_suggestions_and_typos\n1. The prequential evaluation is well-written.  It would be interesting to see more such analysis and discussion of the datasets. \n2. Did you try the combination of TF-IDF and dense retrieval for evidence sentence extraction? \n3. As your dataset is imbalanced, it would be better to see some analysis of the outputs. ", "label": [[522, 621, "Eval_pos_1"], [625, 701, "Eval_pos_2"], [727, 803, "Eval_neg_1"], [807, 891, "Eval_neg_2"], [891, 1099, "Jus_neg_2"], [1413, 1457, "Eval_pos_3"]]}
{"id": 43, "review": "paper_summary\n*Note: I reviewed this paper in an earlier ARR cycle. There are no changes in the updated version that warrant a change in my score or the review. I\u2019ve updated a summary of weaknesses to reflect the updates, and have listed a few suggestions on grammar.*\nThis work presents a method (X-GEAR) for zero-shot, cross-lingual event argument extraction. X-GEAR takes as input i) a passage, ii) a trigger word (a predicate, e.g., \"killed\"), and iii) a template indicating the desired roles (e.g., <Victim>NONE</Victim>). The output is the template filled with event arguments extracted from the passage (e.g., NONE might be replaced with civilian). X-GEAR is built using the standard Seq2seq framework with a copy mechanism, where the input is composed of the triplet (passage, template, trigger word) flattened as a sequence, and the output is the template filled with desired roles.\nThe method relies on recent advances in large, multilingual pre-trained language models (PTLM) such as MT5, which have been shown to perform robust cross-lingual reasoning. The key insight of the method is to use language-agnostic special tokens (e.g., <Victim>) for the template. Fine-tuning on the source language helps learn meaningful representations for templates, which allows their approach to work across target languages supported by the PTLM. \nsummary_of_strengths\n- The paper presents a simple but intuitive method for solving an important problem. The simplicity of the proposed method is a significant strength of this work. As the authors note, existing systems that perform structured extraction often rely on a pipeline of sub-modules. X-GEAR replaces that with a simple Seq2seq framework which is considerably easier to use and maintain.\n- The proposed method is clearly defined, the experiments are thorough and show considerable gains over the baselines.\n- The analysis provides several insights into the strengths and weaknesses of the proposed approach. \nsummary_of_weaknesses\nThe authors have addressed some of the weaknesses highlighted in the previous review. However, it would be great if the weakness of the proposed approach is also highlighted in the future version. Specifically, the method is not *truly* zero-shot as it can only work in cases where a PLTM for the target languages is available. I believe that this is an important point and should be highlighted in conclusion or related work. \ncomments,_suggestions_and_typos\n- L100 \u201cZero-shot cross-lingual learning *is an\u201d -L104: Various structured prediction tasks have *been studied, -The footnote markers should be placed after the punctuation mark (e.g., L557). ", "label": [[1369, 1530, "Eval_pos_1"], [1530, 1746, "Jus_pos_1"], [1748, 1787, "Eval_pos_2"], [1789, 1865, "Eval_pos_3"], [1868, 1967, "Eval_pos_4"], [2085, 2186, "Eval_neg_1"], [2187, 2317, "Jus_neg_1"]]}
{"id": 45, "review": "In the present paper, the authors describe the results of a quantitative analysis of various genres in terms of coreference. They analyse a number of coreference-related features and compare the genres from the point of view of their distribution. The aim is to find the differences between spoken and written genres. The work is interesting and useful, as the number of studies of coreference in spoken texts is limited. The paper address a number of important issues in both coreference analysis (e.g. how should the distance be measured) and the analysis of spoken language. As the authors use a number of existing resources, they also assure the comparability of the categories used. Here, it would be interesting to know, if there were any problems, e.g. if there were still some incompatible categories that the authors came across.\nSpecific comments I like the discussion about the variation in distance measured by different means at the beginning of section 2. Specifically, in a cross-lingual task, token-based measure is a problem. However, there could be differences across studies using various metrics. If measured in sentences or clauses, the distance may vary depending on a genre, if there is a variation in sentence length in terms of words (in spoken texts, there could be shorter sentences, etc.). The question is, if the distance should be measured in characters, but I believe that the decision depends on the conceptual background on what one wants to find out.\nAnother point in Section 2 in the discussion of the diverging results could be variation within spoken and written texts various authors use in their analysis. There could be further dimensions that have an impact on the choice of referring expressions in a language, e.g. narrativeness, if there are dialogues or monologues, etc.\nConcerning related work, Kunz et al. (2017) point out that coreference devices (especially personal pronouns) and some features of coreference chains (chain length and number of chains) contribute to the distinction between written and spoken registers.\nThere are several works concerned with lexicogrammar suggesting that distinction between written vs. spoken, and also between formal vs. colloquial are weak in English  (Mair 2006: 183).\nTable 1: the statistics on different parts of OntoNotes and the total number in OntoNotes are given in one table in the same column formatting, which is slightly misleading. \n  4.1: large NP spans vs. shot NP spans \u2013 sometimes only heads of  nouns or full NPs are considered.\nReferences to examples: 1\u2192 (1), etc.\nPersonal pronouns: 1st and 2nd person pronouns are not considered in the analysis of coreference in some frameworks. The authors should verify which cases they include into their analysis.\nThe finding about NPs being more dominant is not surprising (and was also expected by the authors) and has also something to do with the fact that spoken texts reveal a reduced information density if compared to the written ones.\nThe discussion about the results on spoken vs. written is good and important. Even within written text, there could be a continuum, e.g. political speeches, which are written to be spoken or fictional texts that contain dialogues (as the authors point out themselves), could be closer to further spoken texts. At the same time, academic speeches or TED talks that contain less interaction with the audience (depending on a speaker\u2019s style) could be closer to written texts, also in terms of referring expressions \u2013 we would expect them to contain more NPs, and probably complex Nps describing some notions.\nOverall, it is interesting to know if there are more dimensions than just the difference between spoken and written in the data, e.g.  narrativeness (narrative vs. non-narrative) or dialogicity (dialogic vs. monologic), etc. In fact, genre classification can and should be sometimes more fine-grained than just drawing a rigid line between texts that are considered to be spoken and those that are considered to be written.\nTextual problems: Page 2, Section 2: interfering mentions. - These \u2192 There are some typographical problems in the text.\nPage 3, Section 3: I am not sure if the abbreviation Sct. is allowed by the Coling style. \nIn the reference list, the authors should check spelling of the some entries, e.g. english\u2192 English in Berfin et al. (2019), Zeldes (2018). There is an empty space in Godfrey et al. (1992).\nCited references: Kunz, Kerstin and Degaetano-Ortlieb, Stefania and Lapshinova-Koltunski, Ekaterina and Menzel, Katrin and Steiner, Erich (2017).  GECCo -- an empirically-based comparison of English-German cohesion. In De Sutter, Gert and Lefer, Marie-Aude and Delaere, Isabelle (eds.), Empirical Translation Studies: New Methodological and Theoretical Traditions. Mouton de Gruyter, pages 265\u2013312.\n@BOOK{Mair2006,   title = {Twentieth-Century English: History, Variation and Standardization},   publisher = {Cambridge University Press},   year = {2006},   author = {Mair, Christian},   address = {Cambridge} } ", "label": [[318, 352, "Eval_pos_1"], [354, 421, "Jus_pos_1"], [857, 969, "Eval_pos_2"], [970, 1043, "Jus_pos_2"], [2257, 2400, "Jus_neg_1"], [2401, 2431, "Eval_neg_1"], [2989, 3067, "Eval_pos_3"], [3067, 3595, "Jus_pos_3"]]}
{"id": 46, "review": "Summary     - The paper studies the problem of under-translation common in auto-regressive neural machine translation. \n    - Two main pieces are introduced in this research work, random noise to the length constraint, output length prediction using BERT. \n    - The English-Japanese ASPEC dataset is used to evaluate the contribution of the two proposed improvements. \n    - A stronger or similar performance is shown for all the 4 length language groups using the proposed approach. Especially in the shorted range, the authors show more than 3 points improvement over the vanilla transformer. \n     - An interesting insight I got for the long sentences, vanilla transformer tend to produce shorter length sentences. The proposed approach generated translation close to the gold reference length, atleast for the dataset in use.\nStrenghts     - Ablation is performed for both the new component, random noise and BERT based output length prediction. \n    - Strong BLEU score for short sentence range and relatively close to gold reference length compared to the vanilla transformer.  Concerns     - The work of Lakew et al, uses English-Italian and English-German datasets for evaluation. These datasets should be used to have a consistent evaluation with the past work. \n    - Following from the last one, any specific reason why the English-Japanese dataset is a better choice for your proposed methods? Perhaps you can **motivate on linguistic grounds** why the language Japanese is a better testing ground for your method. \n    - Including an extra BERT-based-output-length prediction can incur additional computational overhead. The overhead of this computation should be stated in the work. \n    - In the introduction, you mention __However, the input sentence length is not a good estimator of the output length.__. I'm not sure why is this the case. ", "label": []}
{"id": 47, "review": "Overviews: This paper focuses on Abusive Language Detection (ALD) and proposes a generic ALD model MACAS with multi-aspect embeddings for generalised characteristics of several types of ALD tasks across some domains.\nStrengths: The motivation of this paper is clear, i.e., to solve the problem that \"What would be the best generic ALD ...\", as described at the beginning of paragraph 2, section 1. The generic abusive language typology is categorised as two aspects, i.e., target aspect and content aspect, and multi-aspect embedding layer considers embeddings of both target and content, followed by cross-attention gate flow to refine four types of embeddings. The proposed model outperforms baselines on all the seven datasets. Detailed ablation studies have been given in section 5 as well.\nWeaknesses: For the structure of the paper, section 4 can be integrated into section 5 as a sub-section.\nthe description of baselines in section 4 is too detailed, which should be refined and shortened appropriately.\nThe paragraph 3, section 5.1 can be titled as a sub-section called \"case study\" since this paragraph analyzes some prediction examples in Table 3. ", "label": [[227, 266, "Eval_pos_1"], [267, 397, "Jus_pos_1"], [900, 958, "Eval_neg_1"], [959, 1011, "Jus_neg_1"]]}
{"id": 48, "review": "This work built a fake news prediction model using both news and user representation from user-generated texts. Experimental results showed that the user text information contributed to predicting the fake. Moreover, the paper showed linguistic analysis to show typical expressions by users in real and fake news. Cosine similarities between users are calculated using proposed user vectors to confirm the echo chamber effect. \n  Introducing vectors of news spreading users sounds an interesting idea. The paper's investigation, the user vector made from linguistic features contribute, is interesting and important. The results of active topics by users for both real and fake is also impressive. \n  There are some ways to build user vectors not only from their timeline and profiles but also from their tweets itself (e.g., Persona chat model). Does the proposed method have a clear advantage to such models? ", "label": [[430, 501, "Eval_pos_1"], [502, 616, "Eval_pos_2"], [617, 698, "Eval_pos_3"]]}
{"id": 49, "review": "In this paper, the authors argue that using the topmost encoder output alone is problematic or suboptimal to neural machine translation. They propose multi-view learning, where the topmost encoding layer is regarded as the primary view and one intermediate encoding layer is used as an auxiliary view. Both views, as encoder outputs, are transferred to corresponding decoder steams, with shared model parameters except for the encoder-decoder attention. Prediction consistency loss is used to constrains these two streams. The authors claim that this method can improve the robustness of the encoding representations. Experiments on five translation tasks show better performance compared to vanilla baselines, and generalization to other neural architectures.\nOn one hand, the experiments conducted in this paper are rich, including five translation tasks, two NMT architectures, shallow and deep models, and many ablations and analysis.  On the other hand, I have several concerns regarding motivation, claims and experiments: -The authors pointed out two problems for using the topmost encoder output alone: 1) overfitting; 2) \u201cIt cannot make full use of representations extracted from lower encoder layers,\u201d. I\u2019m not convinced by the second one especially. For example, in PreNorm-based Transformer, the final encoder output is actually a direct addition of all previous encoding layers. Although there is a layer normalization, I believe this output carries critical information from lower layers.\n-The authors claim that \u201ccircumventing the necessity to change the model structure.\u201d, but the proposed method requires to change the decoder, and manipulate the parameter sharing pattern. In my opinion, the method still requires structure modification.\n-The major ablations and analysis are performed on IWSLT De-En task, which is actually a low-resource task, where regularization is the main bottleneck. From Table 1, it seems like the proposed approach yields much smaller gains on large-scale WMT En-De task compared to low-resource tasks. Thus, it\u2019s still questionable whether the conclusion from experiments on low-resource task can generalize to high-resource tasks.\n-Which WMT En-De test set did you use? WMT14 or WMT16? It seems like the authors used WMT16 for test, but the baseline (33.06 tokenized BLEU) is below standard (~34 BLEU).\n-Besides, some experiment has mixed results and is hard to draw convincing conclusions. For example, in Table 5, MV-3-6 (shared) achieves the best performance on De->En while MV-3-6 is the best on Ro->En. It seems like different tasks have different preferences (share or separate). In the paper, the author only highlights the superiority of separate settings on Ro->En task.\nOverall, I'm not convinced by the motivation and the analysis on low-resource tasks (In particular, this paper doesn't target at low-resource translation. Note that the authors claim that \"our method has a good generalization for the scale of data size.\"). I think the score of this paper is around 3.5 with several unclear questions to be solved. Since we don't have this option, I prefer to give the score of 3. ", "label": [[773, 823, "Eval_pos_1"], [823, 938, "Jus_pos_1"], [959, 1027, "Major_claim"], [1029, 1260, "Eval_neg_1"], [1261, 1502, "Jus_neg_1"], [1504, 1690, "Jus_neg_2"], [1691, 1755, "Eval_neg_2"], [1757, 2047, "Jus_neg_3"], [2047, 2176, "Eval_neg_3"], [2350, 2436, "Eval_neg_4"], [2437, 2725, "Jus_neg_4"], [2726, 2809, "Eval_neg_5"], [2810, 2982, "Jus_neg_5"], [2983, 3140, "Major_claim"]]}
{"id": 50, "review": "This paper is about characters in narrative texts, and it claims to contribute a) an operational definition of characters that is \u201enarratologically grounded\u201c, b) an annotated corpus (which will be released) and c) classification experiments on the automatic distinction between characters and non-characters.\nThis paper is well written and good to read. The topic is interesting and clearly relevant. I have some concerns, however: 1. The definition of a \u201acharacter\u2018 is based on the concept \u201aplot\u2018. While this is naturally following from the narratological literature, it begs the question what a plot is. And of course, this also presumes that there is \u201athe plot\u2018 \u2014 what if there are more than one, or if it is highly subjective? Another term that is used for defining a \u201acharacter\u2019 is animacy. In factual texts, there is a pretty clear distinction between animate and inanimate beings, but in fictional texts, this boundary might become blurry quickly, because it is entirely conceivable that objects have properties that are usually reserved for animate beings. Thus, this term would need to be defined more concretely. The definition thus rests on other, not defined terms. \n2. The annotation experiments yields high agreement, so maybe this is not so relevant in practice. But the agreement has been measured on only one of the three sub corpora, and presumably on the easiest one: Fairy tales, which have a pretty clear plot. It would be much more convincing if the annotation comparison would have been done on a portion from each corpus, and I do not see a reason why this was not done. \n3. The annotation procedure description contains the sentence \u201eFirst, we read the story and \ufb01nd the events important to the plot.\u201c I am not sure what this means exactly \u2014 was there an agreement across the annotators what the events important to the plot are, before the annotation? This of course would make the annotation task much easier. \n4. One of the corpora the authors use consists of broadcast news transcripts from OntoNotes. I would need a lot more arguing about this in the paper, in order to believe the authors that news broadcast is a narrative. While it clearly has narrative elements, they have very different goals and textual properties. Firstly, the \u201aplot\u2018 (understood as a sequence of events in the real world) is only partially represented in a news text, while you have a full plot in many narrative texts. \n5. From the third corpus, the authors annotated only one chapter from each novel. This also seems questionable to me, in particular because length of a coreference chain later is such an important feature. In a full novel, the picture might be very different than in a single chapter. Concretely: The evaluation of an event being relevant to a plot could be very different if the full plot is known. \n6. What I feel is missing from the paper is a quantitative data analysis independent of the classification experiments. What is the distribution of character- and non-character-chains? How long are they in comparison? This would make it much easier to interpret and evaluate the results properly. \n7. The length of a coreference chain has been used as \u201ean integer feature\u201c (4.2.1). Should this not be normalized in some way, given the very different text lengths? \n8. Why is there no baseline for the OntoNotes and CEN corpora?\nTo sum up: While I think this is an interesting task, and the paper is very well written, it makes several assumptions that do not hold general and has a somewhat weak theoretical basis. The classification experiments are pretty straightforward (as the title suggests), and \u2014 given the assumptions and restrictions introduced earlier \u2014 deliver not very surprising results. ", "label": [[309, 353, "Eval_pos_1"], [354, 400, "Eval_pos_2"], [401, 430, "Major_claim"], [2508, 2542, "Eval_neg_2"], [2544, 2826, "Jus_neg_2"], [2830, 2946, "Eval_neg_1"], [2947, 3124, "Jus_neg_1"], [3372, 3407, "Eval_pos_3"], [3413, 3443, "Eval_pos_4"], [3445, 3540, "Eval_neg_3"], [3542, 3728, "Eval_neg_4"]]}
{"id": 52, "review": "paper_summary\nThe paper presents QuALITY, a new benchmark for question answering over long passages. All the questions are multiple-choice, composed by professional writers and validated by MTurk annotators to be answerable and unambiguous. A subset of especially challenging questions is also selected in a task where annotators answer the question under strict time constraints. The paper presents a detailed analysis of the dataset and a thorough evaluation of long-context and extractive QA models on the presented data, demonstrating that all the models are far behind human performance. \nsummary_of_strengths\n- Long-passage QA datasets are harder to collect and relatively scarce, so the new dataset would be a valuable addition to the field.\n-The data collection and annotation process is very well thought out and includes multiple validation steps. The data is further validated in qualitative and quantitative analysis.\n-The experimental part is thorough: both long-context models and extractive models are evaluated, and there are additional experiments with supplementary training data and no-context baselines. The choice of the QA baselines seems reasonable to me (although my expertise in QA is limited).  -The paper is clearly written and easy to follow, and both the data collection and the experimental evaluation are documented in detail. \nsummary_of_weaknesses\nMy only (very minor) concern: the qualitative analysis is somewhat hard to understand without reading the Appendix (see comment below). That can easily be addressed given an extra page. \ncomments,_suggestions_and_typos\n- Without looking at the Appendix, I found it difficult to interpret the different reasoning strategies mentioned in Section 3.6 and Table 5. This section might read more smoothly if you include an example question or a very short explanation for a few most popular types, such as \"Description\" or \"Symbolism\". It was also not clear to me how the questions were annotated for reasoning strategy without reading the passages: was it just by looking at the question, or with the Ctrl+F type keyword search in the passage?\n-This is perhaps too much to ask, but I am very curious about the 4% where the annotator-voted gold label does not match the writer\u2019s label. If the authors have done any analysis on why the annotators might disagree with the writer, I would love to see it!\n-L275: this inclusion criteria -> these inclusion criteria -L441: perhaps you meant Table 6, not Table 9? Not having to go to the Appendix for the results would make things easier for the reader. ", "label": [[617, 685, "Jus_pos_1"], [690, 747, "Eval_pos_1"], [750, 857, "Eval_pos_2"], [931, 964, "Eval_pos_3"], [966, 1123, "Jus_pos_3"], [1124, 1178, "Eval_pos_4"], [1222, 1269, "Eval_pos_5"], [1275, 1356, "Eval_pos_6"], [1381, 1516, "Eval_neg_1"], [1600, 2119, "Jus_neg_1"]]}
{"id": 53, "review": "This paper presents a comparison of several vector combination techniques on the task of relation classification.\n- Strengths: The paper is clearly written and easy to understand.\n- Weaknesses: My main complaint about the paper is the significance of its contributions. I believe it might be suitable as a short paper, but certainly not a full-length paper.\nUnfortunately, there is little original thought and no significantly strong experimental results to back it up. The only contribution of this paper is an 'in-out' similarity metric, which is itself adapted from previous work. The results seem to be sensitive to the choice of clusters and only majorly outperforms a very naive baseline when the number of clusters is set to the exact value in the data beforehand.\nI think that relation classification or clustering from semantic vector space models is a very interesting and challenging problem. This work might be useful as an experimental nugget for future reference on vector combination and comparison techniques, as a short paper. Unfortunately, it does not have the substance to merit a full-length paper. ", "label": [[127, 179, "Eval_pos_1"], [194, 357, "Major_claim"], [358, 469, "Eval_neg_1"], [470, 771, "Jus_neg_1"], [904, 1044, "Eval_pos_1"], [1044, 1120, "Major_claim"]]}
{"id": 54, "review": "paper_summary\nThe paper has not changed materially from the previous version. Please refer to my previous detailed summary.  The new version addresses a few weaknesses I had pointed out previously, such as to include important results that were initially deferred to the appendix and to drop a misleading comparison. It also adds more comparisons to BitFit in table 2. I do appreciate that these changes improve the clarity of the paper, however, the present version still lacks an in-depth comparison to other related work on parameter efficient models as criticized in my previous review. Likewise, experimentation on only GLUE provides an inherently limited picture on the performance of the proposed approach and can draw an overly positive conclusion (refer to Figure 2 in [1] from the previous review). ** I am increasing my score due to improved clarity to 3, but underscore that a more in-depth comparison on other datasets and with other parameter-efficient approaches is still missing.** Currently, the paper could be interesting to a narrow audience that is knowledgeable in the area, i.e., being able to assess the proposed solutions amid the limited experimental setup.\n[1] He et al. (ICLR 2022) \"Towards a Unified View of Parameter-Efficient Transfer Learning.\" https://arxiv.org/pdf/2110.04366.pdf \nsummary_of_strengths\nThe paper has not changed materially. Please refer to previous summary. \nsummary_of_weaknesses\nA few weaknesses have been addressed, especially as to the lack of information and to remove misleading information. Some major points of criticism, however still stand: More comparisons would be necessary to get a better sense of whether AdapterBias performs universally well. This concerns both datasets and models/methods.\n1) Experimentation on only the GLUE datasets is limited in that it often draws an overly positive picture. Please refer to [1] from the summary above and other references from my prior review. This raises the question in which setups the proposed approach would be usable. \n2) Various baselines are missing. A comparison to other adapter architectures would be reasonable and a few other approaches such as LoRA [2], prefix tuning [3], parallel adapter [4], and Compacter [5].\n[1] He et al. (ICLR 2022) \"Towards a Unified View of Parameter-Efficient Transfer Learning.\" https://arxiv.org/pdf/2110.04366.pdf [2] Hu et al. (ArXiv 2021). \" LoRA: Low-rank adaptation of large language models.\" https://arxiv.org/abs/2106.09685 [3] Li et al. (ACL 2021). \" Prefix-tuning: Optimizing continuous prompts for generation.\" https://arxiv.org/abs/2101.00190 [4] Zhu et al. (ArXiv 2021). \" Serial or Parallel? Plug-able Adapter for multilingual machine translation.\" https://arxiv.org/abs/2104.08154v1 [5] Mahabadi et al. (NeurIPS 2021). \" Compacter: Efficient Low-Rank Hypercomplex Adapter Layers.\" https://arxiv.org/pdf/2106.04647.pdf \ncomments,_suggestions_and_typos\nno further comments ", "label": [[369, 436, "Eval_pos_1"], [447, 590, "Eval_neg_1"], [591, 808, "Eval_neg_2"], [812, 865, "Major_claim"], [867, 995, "Eval_neg_3"]]}
{"id": 55, "review": "paper_summary\n**Note**: *This is only a slight revision of my previous review for a previous version of this paper. I did not re-check all the details of the paper carefully, I mostly focused on checking the parts where I had reservations towards the previous version; I simply hope that the parts which I already found good in the previous version stayed good or were improved in this version. But I found already the previous version of the paper to be very good.*\nThe paper describes a model called AlephBERT, which is a BERT language model for Hebrew that surpasses previous such models thanks to being trained on larger data and with better handling of the morphological richness of Hebrew. The paper also compiles together an evaluation toolkit for evaluating Hebrew language models, based on pre-existing tasks and datasets. The model and all code is planned to be released with the camera-ready version of the paper.\nThe paper is definitely mostly a resource paper: most of the stuff is laborious but mostly straightforward, gathering data from available sources, training a model using existing approaches, compiling a benchmarking toolkit from existing tasks and datasets, and evaluating the trained model with this toolkit. The only part which is more research-heavy is handling the rich morphology of Hebrew, where the authors experiment with introducing a morphological segmentation component into the neural setup (a task which is highly non-trivial for Hebrew).\nThe authors evaluate all of their contributions and prove that each of them brings improvements over the previous state of the art. \nsummary_of_strengths\nThe resources created by the authors seem to be extremely useful for nearly anyone dealing with Hebrew in NLP, as large pretrained language models are the core of most current approaches.\nThe approach used for handling complex Hebrew morphology is novel and potentially inspirative for other morphologically complex languages.\nWhile I have a feeling that ACL does not prefer publishing pure resource papers, I believe that in case where the created resource is very useful, these papers should have their place at ACL. Besides, there is also a research component to the paper (although the research component itself would not suffice for a long paper).\nThe paper is very well written and very nice to read and easy to understand. \nsummary_of_weaknesses\nI found several minor problems and uncertainties in the previous version of the paper, but the authors managed to address practically all of these in their revised version. My only remaining reservation thus is towards the claimed but not demonstrated language-agnosticity of the presented approach, which I find to be too strong a claim (or maybe I have a different understanding of what \"language agnostic\" means). \ncomments,_suggestions_and_typos\nIn their response to the previous reviews, the authors list the following improvement: \"We describe the Twitter data acquisition and cleanup process.\", but I have not found this improvement in the current version (but I admit I might have simply overlooked it; all I am saying is I did not find it at the places where I would expect it). ", "label": [[1631, 1818, "Eval_pos_1"], [1819, 1957, "Eval_pos_2"], [2038, 2150, "Major_claim"], [2159, 2282, "Eval_pos_3"], [2284, 2361, "Eval_pos_4"], [2557, 2682, "Eval_neg_1"], [2684, 2801, "Jus_neg_1"]]}
{"id": 56, "review": "paper_summary\nThe paper introduces a pre-trained vision language model (FewVLM) for prompt-based few-shot vision language tasks such as image captioning and vision question answering. The model is pre-trained with a combined objective of masked language modeling and prefix language modeling. Compared to giant pre-trained vision language models, FewVLM is relatively smaller, but it achieves significantly better zero-shot and few-shot performances, as reported. The authors also conducted a fine-grained analysis understanding the effect of different prompts, data sizes, and pre-training objectives. Their findings include that 1) zero-shot tasks are more sensitive to prompt crafting than few-shot tasks. 2) low-quality prompt also learn fast when increasing data size 3) the masked language modeling objective helps vqa more while the prefix language modeling objective boosts captioning performance. \nsummary_of_strengths\n- The idea is straightforward and the results presented are solid and strong. It shows that with the proper objective for pre-training, the pre-trained models could be more performant on zero-shot and few-shot tasks even when the model size is much smaller than those giant pre-trained vision language models -The analysis is comprehensive and interesting, and some of the conclusions align well with the findings in NLP tasks. For example, prompt crafting is essential for zero-shot prediction, which inspires better prompt searching. \nsummary_of_weaknesses\n- The baselines are not very well explained in the paper, making it hard to understand the difference between the proposed model and the baselines. It would be much better if the authors could add some brief introductions for each baseline model.  -The paper also lacks analysis or an intuitive explanation as to why the proposed model outperforms large pre-trained models like Frozen. The numbers look strong, but the analysis focus on how different factors affect FewVLM instead of why FewVLM outperforms baselines. \ncomments,_suggestions_and_typos\n- I also wonder why some numbers are missing from table 2-5? Is it because these numbers are not reported in the original papers? ", "label": [[930, 1005, "Eval_pos_1"], [1006, 1236, "Jus_pos_1"], [1238, 1355, "Eval_pos_2"], [1356, 1464, "Jus_pos_2"], [1489, 1634, "Eval_neg_1"], [1635, 1734, "Jus_neg_1"], [1736, 1872, "Eval_neg_2"], [1873, 2005, "Jus_neg_2"]]}
{"id": 57, "review": "paper_summary\nThe paper presents a method for representing the relevance of a linguistic dataset to the corresponding language and its speakers. As a proxy for speakers of a certain language the authors use geographical entities, particularly countries. The representation they aim to build relies on entity linking, so the authors explore this problem on several multilingual datasets, and draw conclusions regarding the cross-lingual consistency of NER and EL systems. \nsummary_of_strengths\nThe paper addresses an important problem, that gives a new way of assessing the representativeness of a dataset for a specific language. Since such text collections are at the basis of every other language task, and provide language models on which much of the higher level processing is based, it is important to have collections that are representative for the language (and speakers) that are targeted. \nsummary_of_weaknesses\nWhile the main idea of the paper is valuable and interesting, and thoroughly explored, it is based on some assumptions whose soundness is debatable. Details are in the comments section.\n-there is a disconnect between the visualizations and the rest of the processing.\n-the preprocessing of the datasets (many for low-resource languages) needs resources that are themselves scarce, incomplete, or borrowed from other languages (that may use other scripts, and hence there is a transliteration problem on top of others). This makes the kind of processing presented here a bit unrealistic, in the sense that it could not be deployed on any collected dataset, and give an objective view of the representativeness of that dataset for the corresponding language (this is linked to the first point, and explanations are below) -some information in the data is discarded (topical adjectives, historical entities), and it is not clear what impact using it would have on the final geographical mapping. \ncomments,_suggestions_and_typos\nWith regards to the disconnect between the visualizations and the rest of the processing: the visualizations are based on geographical statistics for entities in a text, but these entities are already marked. It would have been useful to see how an end-to-end process performs: apply NER on the NER and QA datasets, and build the same visualizations as in section 3. How do the visualization using imperfect NER/EL resources and processing tools compare to the visualizations obtained on the annotated data? Are they very far apart, or the underlying \"character\" of the dataset is still retrievable even in such imperfect conditions? This links to the second potential weakness, regarding the applicability of this method to newly collected datasets (which is the aim, right?).  The geographical mapping presented is left to the subjective inspection of a human judge. Which is not necessarily bad in itself, but as the more detailed maps in the appendix show, the characteristics of some datasets are very very similar (e.g. for European countries for example, or other geographically close countries). It may be useful to have a more rigorous evaluation of the geographical mapping, by showing that from the geographical distribution of entities, one can predict the country corresponding to the dataset's language. This could be done in an unsupervised manner, or using a linear regression model, or something similarly simple -- maybe by deducting an \"average\" entity geographical distribution model, such that local characteristics become more prominent, or by computing (in an unsupervised manner) some weights that would downplay the contribution of entities from countries that are always represented (like a \"country tfidf\" maybe?).  Some geographical indicators are disregarded, and that may have an impact on the visualizations. Annotating topical adjectives that indicate countries seems doable, based on the anchor texts of links pointing to countries, which are easy to obtain (for some languages). The same for some of the historical entities that no longer exist, but some of which have corresponding GPS coordinates that could be used. The point is that both the resources and the process used to build the geographical maps of the datasets are incomplete. Some are by necessity (because the available resources are incomplete), some by choice (the adjectives and historical figures). We need to know the impact of such processing constraints.\nIt is interesting to analyze the correlation between socio-economic factors, but how does that impact the construction or characteristics of the datasets? Some of these factors -- e.g. the GDP, -- could be (in this experiment) a proxy for the level of web presence of the population, and the level of information digitization of that particular population. Maybe some parameters that measure these issues more explicitly -- which seem more closely relevant to the process of textual collection building -- would provide better insights into data characteristics.\nUsing a country as a proxy for language is useful, but it may skew the data representation, as the authors themselves recognize. What happens with languages that occupy the same country-level geographical space, but are distinct, as happens with multi-lingual countries? The same with languages that cross many borders. A bit more insight into how these are reflected in dataset characteristics and how they impact the usefulness of the dataset would be very useful.\nWhy does the cross-language consistency matter here? Each dataset (for the geographical mapping) is analyzed separately, so while cross-lingual consistency is indeed a problem, it is not clear how it is related to the problem of dataset mapping. Is the cross-lingual consistency a signal of something other than the general performance of NER/EL systems?  Some little typos: were => where (almost everywhere \"were\" appears) then => than (line 319) ", "label": [[493, 629, "Eval_pos_1"], [630, 899, "Jus_pos_1"], [1009, 1107, "Eval_neg_1"], [1109, 1189, "Eval_neg_2"], [1441, 1508, "Eval_neg_3"], [1509, 1741, "Jus_neg_3"], [1743, 1915, "Eval_neg_4"], [1948, 2581, "Jus_neg_2"]]}
{"id": 58, "review": "- Strengths: The approach described in the manuscript outperformed the previous approaches and achieved the state-of-the-art result.\nRegarding data, the method used the combination of market and text data.\nThe approach used word embeddings to define the weight of each lexicon term by extending it to the similar terms in the document.\n- Weaknesses: Deep-learning based methods were known to be able to achieve relatively good performances without much feature engineering in sentimental analysis. More literature search is needed to compare with the related works would be better.\nThe approach generally improved performance by feature-based methods without much novelty in model or proposal of new features.\n- General Discussion: The manuscript described an approach in sentimental analysis. The method used a relatively new method of using word embeddings to define the weight of each lexicon term. However, the novelty is not significant enough. ", "label": [[13, 132, "Eval_pos_1"], [498, 581, "Eval_neg_1"], [582, 709, "Eval_neg_2"], [911, 948, "Eval_neg_3"]]}
{"id": 59, "review": "paper_summary\nThis paper investigates the effectiveness of entity representations in multilingual language models. The proposed mLUKE model exhibits strong empirical results with the word inputs (mLUKE-W), it also also shows even better performance with the entity representations (mLUKE-E) in cross-lingual transfer tasks. The authors' analysis reveals that entity representations provide more language-agnostic features to solve the downstream tasks. Extensive experimental results suggest a promising direction to pursue further on how to leverage entity representations in multilingual tasks. \nsummary_of_strengths\n1. The authors explore the effectiveness of leveraging entity representations for downstream cross-lingual tasks. They train a multilingual language model with 24 languages with entity representations and show mLUKE model consistently outperforms word-based pretrained models in various cross-lingual transfer tasks. \n2. The authors show that a cloze-prompt-style fact completion task can effectively be solved with the query and answer space in the entity vocabulary. \n3. The results show that entity-based prompt elicits correct factual knowledge more likely than using only word representations. \nsummary_of_weaknesses\nMost of languages in LAMA are rich-resourced languages indeed, the authors may need to test mLUKE on some low-resourced languages. \ncomments,_suggestions_and_typos\nThis paper has done a solid work on Multilingual Pretrained Language Models. \nThis paper is well written and easy to read. ", "label": [[1241, 1371, "Eval_neg_1"], [1405, 1482, "Eval_pos_1"], [1483, 1528, "Eval_pos_2"]]}
{"id": 60, "review": "This paper describes (1) new corpus resources for the under-resourced Kinyarwanda and Kirundi languages, (2) preliminary experiments on genre classification using these corpora. The resources are described thoroughly, and a useful survey of related work on these languages is presented. A variety of models are used in the experiments, and strong baseline results on this task are achieved, including experiments on transfer learning from the better-resourced Kinyarwanda to Kirundi; an approach likely to play an important role in scaling NLP to the Bantu language family, which has a small number of reasonably-resourced languages, e.g. Swahili, Lingala, Chichewa. Overall the paper should be of interest to COLING attendees.\nGeneral comments: Abstract: \"datasets... for multi-class classification\".  It would be good to note here and in the introductions that this is specifically a genre or subject classification task.\nIntroduction: \"has made access to information more easily\" => \"has made access to information easier\" Introduction, p.2 \"In this family, they are...\" => \"In this family, there are...\" Introduction: \"fourteen classes... twelve classes\". Again, as in the abstract, should make clear what these classes are!\nLast line of p. 2 \"who have not been\" => \"which have not been\" Related work.  You might also note Jackson Muhirwe's PhD work at Makerere; some of which was published here: Muhirwe J. (2010) Morphological Analysis of Tone Marked Kinyarwanda Text. In: Yli-Jyr\u00e4 A., Kornai A., Sakarovitch J., Watson B. (eds) Finite-State Methods and Natural Language Processing. FSMNLP 2009. Lecture Notes in Computer Science, vol 6062. Springer, Berlin, Heidelberg. https://doi.org/10.1007/978-3-642-14684-8_6 3.3 Dataset cleaning.  I know it's just a change in perspective, but I'd prefer viewing the cleaning and stopword removal as standard pre-processing steps; suggesting distributing these as tools vs. distributing the corpora with these steps applied.  Classifiers should work on un-preprocessed text in any case.   3.4 I don't understand how the cleaning steps you described could reduce the vocabulary from 370K to 300K.  Please clarify.\n4.1 In training the word embeddings, you say \"removing stopwords\".  Does that mean removed from the corpus before training?  I'm not sure I see the value in doing so, and wonder if it negatively impacts the quality of the embeddings.\n4.1 Given the morphological complexity of these languages, I wonder whether results might be improved by working at the subword level (syllables, or morphemes... cf. Muhirwe's work above). This could conceivably help is the cross-lingual training as well. You do have Char-CNN experiments but there may not be enough data to get competitive results at the character level.\n4.3.2 \"different epochs and number of features... different train sets\"; this is fine, but you should refer to the table where these choices are actually laid out 4.4.1 Had the Char-CNN converged at 20 epochs? ", "label": [[178, 217, "Eval_pos_1"], [222, 285, "Eval_pos_2"], [287, 334, "Eval_pos_3"], [340, 390, "Eval_pos_4"], [391, 665, "Jus_pos_4"], [667, 727, "Major_claim"]]}
{"id": 61, "review": "- Strengths: This paper proposes the use of HowNet to enrich embedings. The idea is interesting and gives good results.\n- Weaknesses: The paper is interesting, but I am not sure the contibution is important enough for a long paper. Also, the comparision with other works may not be fair: authors should compare to other systems that use manually developed resources.\nThe paper is understandable, but it would help some improvement on the English.\n- General Discussion: ", "label": [[72, 119, "Eval_pos_1"], [164, 230, "Major_claim"], [238, 286, "Eval_neg_1"], [288, 366, "Jus_neg_1"], [400, 446, "Eval_neg_2"]]}
{"id": 62, "review": "[update after reading author response: the alignment of the hidden units does not match with my intuition and experience, but I'm willing to believe I'm wrong in this case.  Discussing the alignment in the paper is important (and maybe just sanity-checking that the alignment goes away if you initialize with a different seed).  If what you're saying about how the new model is very different but only a little better performing -- a 10% error reduction -- then I wonder about an ensemble of the new model and the old one.  Seems like ensembling would provide a nice boost if the failures across models are distinct, right?  Anyhow this is a solid paper and I appreciate the author response, I raise my review score to a 4.]\n- Strengths:   1)  Evidence of the attention-MTL connection is interesting   2)  Methods are appropriate, models perform well relative to state-of-the-art - Weaknesses:   1)  Critical detail is not provided in the paper   2)  Models are not particularly novel - General Discussion: This paper presents a new method for historical text normalization.  The model performs well, but the primary contribution of the paper ends up being a hypothesis that attention mechanisms in the task can be learned via multi-task learning, where the auxiliary task is a pronunciation task.  This connection between attention and MTL is interesting.\nThere are two major areas for improvement in this paper.  The first is that we are given almost no explanation as to why the pronunciation task would somehow require an attention mechanism similar to that used for the normalization task. \n Why the two tasks (normalization and pronunciation) are related is mentioned in the paper: spelling variation often stems from variation in pronunciation. \nBut why would doing MTL on both tasks result in an implicit attention mechanism (and in fact, one that is then only hampered by the inclusion of an explicit attention mechanism?).                    This remains a mystery.  The paper can leave some questions unanswered, but at least a suggestion of an answer to this one would strengthen the paper.\nThe other concern is clarity.  While the writing in this paper is clear, a number of details are omitted.                    The most important one is the description of the attention mechanism itself.  Given the central role that method plays, it should be described in detail in the paper rather than referring to previous work.  I did not understand the paragraph about this in Sec 3.4.\nOther questions included why you can compare the output vectors of two models (Figure 4), while the output dimensions are the same I don't understand why the hidden layer dimensions of two models would ever be comparable.  Usually how the hidden states are \"organized\" is completely different for every model, at the very least it is permuted.                    So I really did not understand Figure 4.\nThe Kappa statistic for attention vs. MTL needs to be compared to the same statistic for each of those models vs. the base model.\nAt the end of Sec 5, is that row < 0.21 an upper bound across all data sets?\nLastly, the paper's analysis (Sec 5) seems to imply that the attention and MTL approaches make large changes to the model (comparing e.g. Fig 5) but the experimental improvements in accuracy for either model are quite small (2%), which seems like a bit of a contradiction. ", "label": [[39, 120, "Eval_neg_1"], [174, 328, "Jus_neg_1"], [625, 724, "Major_claim"], [744, 799, "Eval_pos_1"], [806, 880, "Eval_pos_2"], [900, 944, "Eval_neg_2"], [951, 984, "Eval_neg_3"], [1299, 1355, "Eval_pos_3"], [1357, 1414, "Eval_neg_4"], [1415, 2492, "Jus_neg_4"], [2493, 2836, "Jus_neg_5"], [2856, 2896, "Eval_neg_5"]]}
{"id": 63, "review": "paper_summary\nThis paper proposes a unified representation model Prix-LM for multilingual knowledge base (KB) construction and completion. Specifically, they leverage monolingual triples and cross-lingual links from existing multilingual KBs DBpedia, and formulate them as the autoregressive language modeling training objective via starting from XLM-R\u2019s pretrained model. They conduct experiments on four tasks including Link Prediction (LP), Knowledge probing from LMs (LM-KP), Cross-lingual entity linking (XEL), and Bilingual lexicon induction (BLI). The results demonstrate the effectiveness of the proposed approach. \nsummary_of_strengths\n1. They propose a novel approach Prix-LM that can be insightful to the community about how to integrate structural knowledge from multilingual KBs into the pretrained language model. \n2. They conduct comprehensive experiments on four different tasks and 17 diverse languages with significant performance gains which demonstrate the effectiveness of their approach. \nsummary_of_weaknesses\nThough this paper has conducted comprehensive experiments on knowledge related tasks, it would be even stronger if they demonstrate there also exists improvement on the multilingual knowledge-intensive benchmark, like KILT. \ncomments,_suggestions_and_typos\nN/A ", "label": [[648, 828, "Eval_pos_1"], [832, 1010, "Eval_pos_2"]]}
{"id": 64, "review": "paper_summary\n*(minor edits from previous review XYZ)* Text style transfer is the task of rewriting a sentence into a target style while approximately preserving its content. Modern style transfer research operates in an \"unsupervised\" setting, where no parallel training data (pairs of sentences differing in style) is available, but assume access to a large unpaired corpus in each style.\nThis paper argues that a large unpaired corpus to train style transfer systems might be hard to obtain in practice, especially in certain domains. To tackle this issue, the authors present a new meta-learning approach (DAML) which trains a style transfer system that can quickly adapt to unseen domains during inference (with a few unpaired examples). The authors build their style transfer system using a discriminative learning objective (via a style classifier) while fine-tuning T5, which they call ST5. The authors approach DAML-ST5 outperforms several baselines on sentiment transfer and Shakespeare author imitiation, and ablation studies confirm the design decisions. \nsummary_of_strengths\n*(identical to my previous review GAJd, see \"Weaknesses\" for my response to the revised manuscript)* 1. This paper tackles a practically relevant problem. While current style transfer research does not leverage supervised data, it requires a large amount of unpaired data which may not be practical to obtain in low-resource languages or domains. Hence, building style transfer systems which can quickly adapt in low-resource settings is important, since it eliminates the expensive requirement of hand-curating unpaired datasets for each low-resource domain / language.\n2. The paper presents an interesting method based on model-agnostic meta learning [1] (with modifications to make it suitable for domain adaptation) to learn a good initialization which works well across domains. During inference, the model can quickly adapt to a new domain, with decent performance with just 1% of the target domain data. Experimental results confirm the proposed approach outperforms several strong baselines. The paper also has ablation studies to justify the various design decisions used in the approach.\n[1] - https://arxiv.org/abs/1703.03400 \nsummary_of_weaknesses\nThe authors presented an excellent response and addressed all the concerns in my previous review GAJd in their revised manuscript. In particular, the authors added experiments on the new Shakespeare dataset, used extra automatic metrics to evaluate their approach and found consistent trends, clarified some questions I had about the modeling, added comparisons to recent few-shot style transfer approaches.\nI have increased my score to 4. It would be nice to move some of the new results into the main body of the paper with the extra 9th page, especially the experiments on the Shakespeare dataset. \ncomments,_suggestions_and_typos\nSeveral references are missing their venues / journals / arXiv identifiers, you can get the correct bib entries for papers from https://aclanthology.org, Google Scholar or arXiv. ", "label": [[1192, 1243, "Eval_pos_1"], [1244, 1659, "Jus_pos_1"], [1663, 1872, "Eval_pos_2"], [1873, 2186, "Jus_pos_2"]]}
{"id": 65, "review": "paper_summary\nThis paper proposes a simple but powerful approach that uses a single Transformer architecture to tackle KG link prediction and question answering treated as sequence-to-sequence tasks. This approach can reduce the model size up to 90% compared to conventional Knowledge graph embedding (KGE) models, and the performance of this approach is best among small-sized baseline models. \nsummary_of_strengths\n1. \tThis paper uses the Transformer structure for KG link prediction and question answering tasks, and this simple approach seems powerful. \n2. \tThis paper conducts a large number of experiments on multiple datasets and analyzes the experimental results. \nsummary_of_weaknesses\nMinor: The paper only contains a high-level description of the proposed approach that benefits the performance of KGQA. It would be better if the authors provide some explicit cases or discussions to explain how pre-training on KG link prediction can improve performance on KGQA compared with the previous representative works later. \ncomments,_suggestions_and_typos\nSpecific comments for improving the work: 1. The authors may provide some explicit cases or discussions to explain how pre-training on KG link prediction can improve performance on KGQA. \n2. This paper shows KG link prediction performance from the proposed model trained on Wikidata5M in section 4.4. it would be better to show the KG link prediction performance from KGT5 after finetuning for QA, and showing performance on KG link prediction and KGQA with multi-task setting is also a good choice. ", "label": [[520, 555, "Eval_pos_1"], [562, 672, "Eval_pos_2"], [702, 814, "Eval_neg_1"], [815, 1029, "Jus_neg_1"]]}
{"id": 66, "review": "paper_summary\nThis paper introduces a new method for MeSH indexing, combining multiple methods including, but not limited to, dilated CNNs, masked attention, and graph CNNs. Overall, the proposed approach makes substantial improvements over prior state-of-the-art methods. For example, Micro F1 improves over BERTMeSH from 0.685 to 0.745. Similar improvements are also found for the example-based measures (e.g., example-based F1). Furthermore, a comprehensive ablation study was performed, showing that the label feature model has the largest impact on model performance, yet, other parts of the method still impact performance substantially. \nsummary_of_strengths\nOverall, the paper is well-written and easy to read. Furthermore, the improvement over prior work is substantial. It is neither easy nor trivial to make such considerable performance improvements for MeSH indexing, especially for Micro F1. For instance, BERTMeSH [1] only improves DeepMeSH [2] by only 2% in Micro F1 [1] after five years of work. Hence, seeing a Micro F1 near 0.75 is a huge breakthrough.\nReferences: [1] Peng, Shengwen, et al. \"DeepMeSH: deep semantic representation for improving large-scale MeSH indexing.\" Bioinformatics 32.12 (2016): i70-i79.\n[2] You, Ronghui, et al. \"BERTMeSH: deep contextual representation learning for large-scale high-performance MeSH indexing with full text.\" Bioinformatics 37.5 (2021): 684-692. \nsummary_of_weaknesses\nOverall, there are three major weaknesses in this paper.\nFirst, the paper uses a custom training and validation dataset pulled from PubMed, making comparisons difficult. Using the data from the yearly BioASQ shared tasks would be better to use their data so new methods are more easily comparable. I understand this is common in similar studies (e.g., by BERTMeSH [3]), but a standardized dataset seems possible and useful.\nSecond, while the hyperparameters are discussed, it is not clear whether hyperparameters were optimized for the baseline models. What were the chosen parameters? Was the validation dataset used to optimize them similarly to the proposed method? If so, why is the standard deviation not reported for the baseline models (e.g., in Table 1)? Given the substantial performance differences between the proposed model and prior work, this additional information must be reported to ensure fair comparisons.\nThird, while this may be the first paper to use GCNNs for MeSH indexing, it is widely used for similar biomedical text classification tasks (e.g., ICD Coding). For instance, [1] directly combines BiLSTMs with GCNNs and label features in a very similar manner to the method proposed in this paper, albeit with exceptions such as [1] does not use dilated CNNs. Furthermore, that work has been expanded on to better understand the impact of the GCNNs and whether they are needed [2]. Hence, the paper would substantially help if the related work sections were expanded to include citations with similar methodologies. In my opinion, the \"Dynamic Knowledge-enhanced Mask Attention Module\" is one of the most innovative parts of the paper and should be highlighted more in the introduction.\nReferences: [1] Chalkidis, Ilias, et al. \"Large-Scale Multi-Label Text Classification on EU Legislation.\" Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. 2019.\n[2] Chalkidis, Ilias, et al. \"An Empirical Study on Large-Scale Multi-Label Text Classification Including Few and Zero-Shot Labels.\" Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2020.\n[3] You, Ronghui, et al. \"BERTMeSH: deep contextual representation learning for large-scale high-performance MeSH indexing with full text.\" Bioinformatics 37.5 (2021): 684-692. \ncomments,_suggestions_and_typos\nPage 3, Line 240-252: There are a few variations of LSTMs [1]. Is the one used in this paper the same as the 1997 paper? \n  Page 2, Line 098-100: The phrase \"latent semantics\" is unclear. It may help the paper if that phrase is expanded, e.g., does this mean the contextual information from combining multiple layers of neural networks?\nPage 4, Line 287: I believe \"edges are implement MeSH hierarchies\" should be \"edges represent relationships in the MeSH hierarchy\" Page 6, Line 416-417: I believe the phrase \", and we converted all words are lowercased\" Should be \", and we converted all words to lowercase\" References: [1] Graves,A. et al. (2012) Supervised Sequence Labelling with Recurrent Neural Networks. Vol. 385. Springer, Berlin. ", "label": [[666, 718, "Eval_pos_1"], [719, 779, "Eval_pos_2"], [780, 905, "Eval_pos_3"], [906, 1071, "Jus_pos_3"]]}
{"id": 67, "review": "paper_summary\nThis paper describes a contrastive learning approach to automatically solving math word problems (MWP) and investigates multilingual approaches to the problem. Additionally, it provides further evidence that the top layers of BERT will learn task-specific patterns, as shown in prior works.  This paper treats MWP solving as a text-to-equation-tree translation problem using an encoder-decoder architecture. To motivate the use of contrastive learning, the paper opens with an analysis of the effect of training epoch and encoder layer on the clustering of MWPs by prototype equation. t-SNE plots show expected clustering effects as layer/epoch increases. Analysis of raw high dimensional representations show that problems with similar lexical semantics or topic are given different representations when the prototype equation differs, especially in layer 12, while problems with the same prototype equation are embedded closer together. Moreover, it is shown that MPWs that are represented closer to the center of the cluster of problems with the same prototype equation are more likely to be correctly solved.  The contrastive learning approach proposed here involves finding difficult negative examples, which is done by choosing structurally similar equation trees with different operations in the intermediate nodes. Additional positive examples come from either trees or subtrees which consist of the same structure and operations as the target equation. For the multilingual approach, mBERT is substituted as the encoder. Results show that the contrastive learning method improves MWP solving in both the monolingual and multilingual settings compared to recent baselines. Ablations show the value of choosing difficult negative examples and other design decisions. Analysis shows that the contrastive learning objective results in well defined clusters. Accuracy is especially improved for examples farther from the cluster center. \nsummary_of_strengths\nThe contrastive learning for MWP solving seems to improve performance \nsummary_of_weaknesses\nTechnique is limited to problems that can be modeled by equation trees.  A lot of paper real estate is given to an analysis that basically shows:\u2028 -undertrained models don\u2019t work -only using part of the encoding function (the bottom N layers) doesn\u2019t work I don\u2019t think this analysis will be of much use to the ACL community.  It seems like the cosine similarity of lower layers in figure 3 are relatively high, while the t-SNE visualizations in Figure 2 are more mixed. Do you think t-SNE is accurately representing the latent space? \ncomments,_suggestions_and_typos\nThe paper would benefit from connections to prior work on BERTology. An intro to this line of research can be found at https://huggingface.co/docs/transformers/bertology ", "label": [[1977, 2047, "Eval_pos_1"], [2070, 2141, "Eval_neg_1"], [2143, 2325, "Jus_neg_2"], [2326, 2395, "Eval_neg_2"]]}
{"id": 68, "review": "paper_summary\nExisting self-explaining models mostly generate the short rationales with the assumption that short rationales are more intuitive to humans, while this work discusses the question that whether the shortest rationale is the most understandable for humans. In this work, the authors design a self-explaining model, LIMITEDINK, that can take controls on rationale length by incorporating contextual information and supporting flexibly extracting rationales at any target length. By generating rationales at different length levels, the authors study how much rationale would be sufficient for humans to confidently make predictions. Experiments on various tasks demonstrate that the proposed method outperforms most prior works, and meanwhile show that the shortest rationales are not the best for human understanding. \nsummary_of_strengths\n1. The method proposed in this work is effective and can outperform several strong baselines on the performance of both label predictions and rationale predictions.\n2. The problem, the effect of the rationales at different length levels, discussed in this work is meaningful and the conclusions may serve as good guidance for further research in this field. \nsummary_of_weaknesses\n1. Although this work points out that shortest rationales are largely not the best for human understanding, the appropriate lengths are still subject to the datasets or even the instances. The length of meaningful rationales may largely depend on the density of the information related to the task. As pointed in Section 5, a more rigorous evaluation is needed to better understand what is a good rationale explanation.\n2. This work does not report how \"short\" the rationales generated by prior works are. As shown in Section 1, recent works agree that good rationales should be \"shortest yet sufficient\", while this work seems to simply focus more on \"shortest\". This brings out the concern that whether the main question discussed in this work can really stand for the trend of current works on this task. \n         (a). I think one potential solution to handle this concern is that - by extending or shortening the golden rationales and see whether such perturbations outperform or underperform the original one. \ncomments,_suggestions_and_typos\n1. I would like to see some examples of the generated rationales at different length levels from the proposed methods, as well as the rationales generated by the baselines. Such examples can help the readers to better understand the influence of rationale lengths. ", "label": [[855, 1016, "Eval_pos_1"], [1020, 1210, "Eval_pos_2"], [1236, 1531, "Jus_neg_1"], [1532, 1652, "Eval_neg_1"], [1897, 2040, "Eval_neg_2"], [2056, 2249, "Jus_neg_2"]]}
{"id": 69, "review": "paper_summary\nThis paper compares two figures, Firth and Harris, who are often cited as foundational in modern computational linguistics but who are rarely actually read, perhaps even not by the people who cite them. It does a deep dive into their work and takes an opinionated stance that Harris was \u201cintroverted\u201d, focused on language as a system in isolation and Firth was extroverted, focusing on language as it exists in the world. \nsummary_of_strengths\nThis is an interesting paper, of a type rarely seen at ACL venues: intellectual history with an opinionated thesis. I genuinely enjoyed reading it and learned from it. I imagine the same would be true for many folks in the ACL community.\nThere is some real scholarship here, as it investigates the works of these mid-20th c scholars, derives an opinionated synthesis, and applies it to modern NLP.\nGiven that NLP as a field is extremely forward-looking, often considering something even a year or two old to be ancient history, this is a valuable perspective. \nsummary_of_weaknesses\nThe paper points out that Firth\u2019s work is somewhat scattered and hard to get a clear grip on. Yet it ends up coming down at times in a way that feels to me a bit too much \u201cHarris bad, Firth good\u201d. The claim is that Firth\u2019s views are well aligned with a strand of thought, currently popular in NLP (and well articulated in Bender & Koller\u2019s position piece and Bisk et al.) that \u201cyou can\u2019t learn language on the radio\u201d and that language meaning needs to be in embedded context in a way that is heavily socially mediated. The argument is that, by contrast, Harris misses the boat on this.\nI wasn\u2019t quite convinced on this point. It makes for an interesting contrast for the two thinkers, but it also seems to me to be a bit unfair to Harris since it\u2019s hard to counterfactually reason about how Harris would have reacted to the current state of NLP. And I could imagine a variety of other arguments about the relevance of his work in NLP today. Firth\u2019s positions are, according to the paper, admittedly sometimes murky and not always spelled out, which means it is easy to attribute a wider variety of perspectives to him. So I think there should be some caution in that framing.\nIt also seems possible that a \u201cradically distributional\u201d, like the kind attributed to Harris, could in fact capture a wide range of rich social contexts and individual variation. For instance, GPT-3 which is trained as if it\u2019s trained on a single monolithic dialect, can be a quite effective code-switcher when prompted with different registers.\nI\u2019ll mention one other thing, which isn\u2019t really a weakness but is more of a meta-concern: One potential pitfall of submitting and publishing this kind of work in an ACL venue is that the reviewers (like me) and audience are not necessarily going to be experts in this methodology and so care should be taken to make sure it is well reviewed by people who have the relevant expertise. An example of the way in which ACL is not necessarily set up for this kind of work is that I have to select whether the work is reproducible: I picked \"1 = They would not be able to reproduce the results here no matter how hard they tried.\" since it's hard to imagine some other set of authors deciding to read Harris and Firth and writing the same paper :-).  But the broader point is that some of this veers methodologically into intellectual history, which I\u2019m certainly not an expert in, and the ACL reviewing process is not necessarily set up to review a paper with this method. That's not a reason not to publish it! In fact, it's all the more reason to give it serious consdieration. But I think there should be some thought given to make sure the work is well evaluated. \ncomments,_suggestions_and_typos\n-The paper says that computational linguists routinely cite Harris and Firth. This is true of textbooks and big review papers. But my impression is that many in the ACL community do not engage with them at all. ", "label": [[458, 486, "Eval_pos_1"], [488, 573, "Jus_pos_1"], [574, 695, "Major_claim"], [696, 731, "Eval_pos_2"], [733, 855, "Jus_pos_2"], [856, 986, "Jus_pos_3"], [986, 1018, "Eval_pos_3"], [1135, 1238, "Eval_neg_1"], [1238, 2562, "Jus_neg_1"]]}
{"id": 70, "review": "paper_summary\nNote - I reviewed this paper in the past and had a positive criticism about it. The authors also addressed my previous comments and I keep my positive review from before.\nThis paper discusses methods for improving multi-domain training for dialog response generation. The authors experiment with several approaches to improve multi-domain models, namely (1) \"Interleaved Learning\", when data from multiple domains/corpora is concatenated and used for training, (2) \"Labeled Learning\" where each example is encoded using an additional corpus-specific embedding/label that guides the model, (3) \"Multi-Task Labeled Learning\" where the model has an additional classification head that determines the domain/corpora label based on the given context, and (4) \"Weighted Learning\" where the authors propose a weighted loss function that give more weight on words that are especially salient in a given domain.\nThe authors run experiments that evaluate the different approaches using 4 dialog datasets (PersonaChat, OpenSubtitles, Ubuntu and Twitter) where they show the effect of each approach on the resulting model as measured using BLEU, perplexity and F1. While the experiments show that there is no single best approach on all metrics, the proposed approaches improve the results over simple corpora concatenation or single-corpora training. A human evaluation showed that the proposed \"Weighted Learning\" approach was favorable in comparison to the other methods. \nsummary_of_strengths\nThe main strengths of the paper are as follows: The highlighted task of multi-domain dialog generation is important, practical and relatively understudied. \nTo the best of my knowledge, the proposed \"Weighted Learning\" approach is novel The experiments are thorough and convincing, especially as they include a human evaluation \nsummary_of_weaknesses\nThe main weakness of the paper is that some of the proposed approaches lack novelty - \"interleaved learning\", \"labeled learning\", \"multi-task labeled learning\" were studied extensively in the MT community. Having said that, I am not aware of works applying those approaches to open-domain dialog generation. \ncomments,_suggestions_and_typos\nline 230 - \"learning material\" --> \"training data\" ", "label": [[1547, 1655, "Eval_pos_1"], [1656, 1827, "Eval_pos_2"], [1850, 1933, "Jus_neg_1"], [1936, 2056, "Eval_neg_1"]]}
{"id": 71, "review": "paper_summary\nThis paper proposes a new task formulation to solve complex tasks. In this new formulation, there are multiple agents, each of which is capable of solving some specific types of tasks. For example, there can be a QA agent that answers natural language (NL) questions and an instruction following agent that could execute actions to accomplish an NL intent. Given a complex task described in NL, the model is asked to communicate with each agent for their task-specific knowledge and use the returned answers to proceed with the task.\nIn this work, they instantiate the complex task as multi-hop QA and the agents as a TextQA agent that is able to reason over large text corpus, TableQA agents that could answer questions given structured data like tables, and MathQA agent that could perform numerical reasoning. Each agent also has their own auxiliary data like (NL, answer) supervision and their independent KBs. They design a model that is able to decompose a multi-hop question to simple questions that could be answered by one agent. They compare this model with other black-box models that do not perform communication with agents and show significant improvements on a synthetic dataset they create. \nsummary_of_strengths\n- The proposed new task formulation is novel and interesting. Intuitively, it is a promising way to resolve the complex tasks people encounter daily. The paper also provides a detailed and clear definition of this new task. \nsummary_of_weaknesses\n- The instantiation of the task could not fully justify the benefit of the new task formulation. In this new proposed setting, an ideal criterion for designing individual agents is that each has mutually exclusive functionalities, and it is challenging to develop a unified model. For example, the google search agent and the Alexa shopping agent described in the introduction make such a case. However, this work design a synthetic dataset, and the agents are separated by the different forms of knowledge (text vs table) and the different proportions of knowledge in the KB. This separation is OK as long as it could reveal the true distribution in reality -- there is some knowledge that is more accessible through text than structured data and vice versa. However, the data construction process did not consider this and did a random split. A more realistic setting will bring up some interesting questions like \"how does the decomposer know which agent is more suitable to answer the current question?\", \" how can we curate such annotations?\" etc, which are not explicitly touched by the current work. To me, my main takeaway is that question decomposition is helpful, which has been studied in previous works like BREAK (Wolfson el at + 2020). Related to this concern, I also have a question regarding training the question decomposition component. According to F3, the NL questions to the text agent and the table agent look pretty similar (e.g. [table] What movies has #1 written? vs. [text] #1 produces which materials?), what are the supervision signals that hint the model to predict one agent over another?\n- Some descriptions of the experiment setting are somewhat vague, and therefore it is not super clear whether the comparisons are fair. My main question is how factual knowledge is provided to each model? \n     * In *Models with Access to Agent Knowledge*, how do you construct the context? Do you randomly sample some context from the *possible world* of the question? \n     * Do you somehow distinguish the source (e.g., knowledge of TextQA, knowledge of TableQA)? \n     * After decomposing the question through `NextGen`, how do you provide the context when querying an individual agent? Do you provide the ground truth context without distractors? Or do you train some retriever (like in *Models with Fact Supervision*) to retrieve the context? \ncomments,_suggestions_and_typos\n- Some case study and more systematic error analysis can probably help the readers to understand in which cases the proposed method works and how. ", "label": [[1245, 1304, "Eval_pos_1"], [1305, 1392, "Eval_pos_2"], [1393, 1467, "Eval_pos_3"], [1492, 1586, "Eval_neg_1"], [1587, 3108, "Jus_neg_1"], [3111, 3244, "Eval_neg_2"], [3245, 3858, "Jus_neg_2"]]}
{"id": 72, "review": "paper_summary\nThe authors proposed a Locally Aggregated Feature Attribution method and claimed that this is a novel gradient-based feature attribution method for NLP models. \nsummary_of_strengths\nThe authors proposed a Locally Aggregated Feature Attribution method \nsummary_of_weaknesses\nResults are varying so much on two different datasets \ncomments,_suggestions_and_typos\nDid you use an attention mechanism? If yes, what are the significant changes you observed between the two approaches? If not, could you please check a performance comparison with any attention mechanism? \n Why are the results varying so much on two different datasets? Is your model biased on a particular data? Did you check the disparity and fairness of data? ", "label": [[288, 341, "Eval_neg_1"]]}
{"id": 73, "review": "This paper presents a corpus study of coreferences comparing different genres (news, blogs, conversations) and media (written, transcribed speech, microblogging) based on the Ontonotes and Switchboard corpora and a dataset from Twitter sub-threads. The analysed factors include the use of pronouns and noun phrases, the characteristics of such NP mentions (syntactic complexity) and various distances measured between mentions of the same entity.  This is an interesting study, and could be potentially useful for models trying to do domain adaptation, as coreferecen systems for written text perform poorly on conversations and microblogging. \nOverall it seems the contributions are only moderately significant however, for the following reasons: (1) the paper builds on two papers:  (Aktas et al., 2018), where the twitter data was collected and described, and (Aktas et al., 2019) which described coreferences in  Ontonotes sub-corpora/genres in what I assume is a similar manner (the paper is not freely available, only the abstract). It is not clear how the present paper adds to these papers, and should be made more explicit. \n(2) the interest for coreference model is rather vaguely described, and it would have been interesting to have a more detailed descriptions of how the knowledge derived from the study could be used in such models. The paper mentions experiments using models trained on written texts applied to other genres/media, how hard would it have been to experiment training on other data, or to combine them ? \nThis seems too preliminary to assess the real interest for automated models.  More minor points: -the introduction is rather strangely constructed, and almost reads as a post-introduction section/a related work section already. The context should be made clearer and a few examples wouldn't hurt.\n-i'm not sure I understand the term \"coreference strategies\", which seem to imply an intentionality in the way coreferences are produced in different contexts. A lot of what is shown in the paper could be attributed to more general aspects of the genres/media (longer sentences for purely written text, more context available, etc) and some of the properties of coreferences could just be by-product of that. The use of specific personal pronouns (1st/2nd/3rd) is another example.  -there is zero descriptions of the statistical tests used, and of the assumptions made, if a parametric model was used. This should be addressed. Also some conclusions are based on multiple-testing, which should include some kind of correction (it might have been done, but again, there is zero details about this).  -some technical details are presented a little vaguely, which could be understood given size constraints, but sometimes it is a bit too much: for instance, instead of explaining what hierarchical clustering method was applied, the paper only mentions using some R implementation with default settings, which is rather uninformative.  -about the clustering, why not cluster on all the dimensions at the same time ? ( with some normalization of features of course) Details: -Tables/figures have rather cursory captions. For instance table 1 coud recall the meanings of abbreviations for all sub-corpora, especially from Ontonotes. It is also not a good idea to have ontonotes as a whole *and* all the subcorpora without making it clear.  -section 3.1, the paper mentions the use of a sentence splitter from (Proisl and Uhrig, 2016) which is a German sentence splitter ?\n-table 2: why not give relative (to corpus size) frequency instead of absolute frequency ? this would make it easier to interpret. ", "label": [[448, 552, "Eval_pos_1"], [553, 644, "Jus_pos_1"], [645, 719, "Major_claim"], [752, 1038, "Jus_neg_1"], [1039, 1133, "Eval_neg_1"], [1138, 1202, "Eval_neg_2"], [1206, 1347, "Jus_neg_2"], [1536, 1613, "Eval_neg_2"], [1634, 1763, "Eval_neg_3"], [1764, 1832, "Jus_neg_3"], [1834, 1992, "Eval_neg_4"], [1993, 2313, "Jus_neg_4"], [2316, 2434, "Eval_neg_5"], [2460, 2558, "Eval_neg_6"], [2633, 2772, "Eval_neg_7"], [2774, 2964, "Jus_neg_7"]]}
{"id": 74, "review": "paper_summary\nThis paper is about determining the syntactic ability of two Dutch variants of transformer based language model BERT: BERTje and RobBERT. The authors use a Multiple Context Free Grammar (MCFG) formalism to model two patterns of Dutch syntax: control verb nesting and verb raising. These rule-based grammatical models are used to generate a test set which is limited by a bound on recursion depth and populated from a lexicon. For evaluation, each verb occurrence garners a prediction of which referential noun phrase is selected by it, and the resulting accuracy is reported. The authors show results that demonstrate drastically worse performance as recursive depth and number of noun phrases increase, and conclude that the models have not properly learned the underlying syntax of the linguistic phenomena they describe; ie discontinuous constituents/cross-serial dependencies. \nsummary_of_strengths\nAs someone unfamiliar with Dutch and with this area of research, I felt this paper did an excellent job of motivating their reasoning for their research and of describing the ways that Dutch syntax is different from English. Figures and examples were clear and well-done. The article was clearly and concisely written, and appears to be a valuable contribution that adds counter-evidence to claims about how much syntax BERT-based models actually \u201cknow\u201d. Authors are careful not to exaggerate the consequences of their findings and make suggestions for how this work could be expanded with other languages or other tasks. \nsummary_of_weaknesses\nI was unable to get the provided code to work. I tried both on my Macbook and on a Linux-based computing cluster. To be fair, I did not try for very long (< 15 minutes), and I also did not have access to a GPU so I tried to run it on a CPU. It\u2019s possible that was the problem, but it wasn\u2019t stated that that was a requirement. It seems that if I understood the instructions in the readme properly, there were a few __init__ files missing. However, even after changing those, I ran into a number of other errors. The readme was also a bit sparse, ie \u201cPlay around with the results as you see fit\u201d. I commend the authors for including the code and data with the submission, but I would have liked to see a script included already (i.e. not just a snippet in the readme) along with a brief description of any dependencies required beyond the requirements.txt and what one might expect when running the script.\nAnother weakness I felt, was a lack of description of previous/related work. They mention in the very beginning that \"Assessing the ability of large-scale language models to automatically acquire aspects of linguistic theory has become a prominent theme in the literature ever since the inception of BERT\", but didn't reference other work to provide similar counter evidence to the consensus they referenced in Rogers et al. (2020). As someone not familiar with this area of research, maybe there is not so much to cite here, but if that is the case, I feel it should be mentioned why there is no related work section. \ncomments,_suggestions_and_typos\nCitation should be formatted like so: \"The consensus points to BERT-like models having some capacity for syntactic understanding (Rogers et al., 2020).\" ", "label": [[982, 1141, "Eval_pos_1"], [1142, 1188, "Eval_pos_2"], [1189, 1235, "Eval_pos_3"], [1240, 1277, "Eval_pos_4"], [1279, 1371, "Jus_pos_4"], [2468, 2544, "Eval_neg_1"], [2545, 3087, "Jus_neg_1"]]}
{"id": 75, "review": "paper_summary\nThis work introduces a new dataset, the Hindi Legal Documents Corpus (HLDC), a corpus with 900 thousand legal documents in Hindi. This corpus is collected from public data, and the authors intend to release (in addition to the corpus) the scripts necessary for its creation and processing, along with models and code for the experiments in the paper. The authors examine the task of predicting the verdict of bail applications (a binary task, which is to predict whether or not the application was denied or granted). A variety of models are explored for this task; while accuracy is better than the majority baseline, there is still much room for progress. The headroom in performance even for this simple task highlights the challenges in using natural language processing and machine learning systems for legal use cases. Overall, I believe the data and experiments introduced by this work would be interesting to many, and I recommend it's acceptance. \nsummary_of_strengths\n1. This work introduces a new, large-scale dataset containing legal documents in a low-resource language. This can be a valuable resource for many, and could help advance research in natural language processing for legal use cases.\n2. Authors thoroughly describe the process of data collection and cleaning, and intend to open-source code for reproducing these steps.\n3. Through experiments, authors demonstrate the challenges of current techniques in a simple (yet telling) task of predicting the outcome of bail applications. The authors report multiple baselines and will publicly release their code and models.\n4. The authors take many steps to anonymize the dataset, removing names, gender information, titles, locations, times, etc.   5. This paper is clear and well written. \nsummary_of_weaknesses\nSome minor considerations: 1. It would be informative to users if authors reported sensitivity of their experiments to hyper-parameters, along with standard deviations on their numbers.\n2. The presented error analyses are anecdotal, and might not be reflective of the overall behavior of the system. It would strengthen this paper if authors further explored systematic biases in their datasets and models (e.g. how does accuracy/F1 vary by district?) \ncomments,_suggestions_and_typos\nFootnote marks should come after punctuation. ", "label": [[995, 1098, "Eval_pos_1"], [1098, 1223, "Eval_pos_2"], [1227, 1298, "Eval_pos_3"], [1363, 1520, "Eval_pos_4"], [1610, 1663, "Eval_pos_5"], [1664, 1730, "Jus_pos_5"], [1736, 1774, "Eval_pos_6"], [1986, 2096, "Eval_neg_1"], [2097, 2249, "Jus_neg_1"]]}
{"id": 76, "review": "paper_summary\nThis paper proposed a confidence estimation method for neural machine translation (NMT) by jointly training the NMT model with a confidence network which learns to output a confidence score per example. The confidence score (a scalar between 0 and 1) is used to provide \u201chints\u201d for the NMT model, that is interpolating the original prediction probabilities with the ground truth probability distribution. Higher confidence indicates less hints provided. The two models are trained jointly, where NMT learns the task and the confidence network learns to produce the correct confidence. Besides, the confidence is also utilized to smooth labels for preventing miscalibration. Experiments on several quality estimation tasks demonstrate the effectiveness of the proposed method in improving model performance and detecting noisy samples and out-of-domain data. \nsummary_of_strengths\n1. \tThis paper focused on an important problem in estimating confidence for poorly calibrated NMT models. Different from previous work based on Monte Carlo dropout, the proposed method, learning confidence estimation during training, is more efficient and may be benefit for future research. \n2. \tThe paper is well-written and easy to follow. The experiments are sufficient and promising. \nsummary_of_weaknesses\n1. \tSince an additional confidence network has been involved in producing confidence score, how to ensure the confidence network would not be over-confident or under-confident? Would this be an endless loop if another network is needed to assess the uncertainty of the confidence network? \n2. \tThe improvement compared to other unsupervised methods is not impressive, while there is still a big gap with the strong QE model BERT-BiRNN. \ncomments,_suggestions_and_typos\nN/A ", "label": [[898, 1000, "Eval_pos_1"], [1000, 1186, "Eval_pos_2"], [1191, 1236, "Eval_pos_3"], [1237, 1283, "Eval_pos_4"], [1600, 1742, "Eval_neg_1"]]}
{"id": 77, "review": "The paper explores the use of probabilistic models (gaussian processes) to regress on the target variable of post-editing time/rates for quality estimation of MT output. \nThe paper is well structured with a clear introduction that highlights the problem of QE point estimates in real-world applications. I especially liked the description of the different asymmetric risk scenarios and how they entail different estimators. \nFor readers familiar with GPs the paper spends quite some space to reflect them, but I think it is worth the effort to introduce these concepts to the reader. \nThe GP approach and the choices for kernels and using warping are explained very clearly and are easy to follow. In general the research questions that are to be answered by this paper are interesting and well phrased.\nHowever, I do have some questions/suggestions about the Results and Discussion sections for Intrinsic Uncertainty Evaluation: -Why were post-editing rates chosen over prediction (H)TER? TER is a common value to predict in QE research and it would have been nice to justify the choice made in the paper.\n-Section 3.2: I don't understand the first paragraph at all: What exactly is the trend you see for fr-en & en-de that you do not see for en-es? NLL and NLPD 'drastically' decrease with warped GPs for all three datasets.\n-The paper indeed states that it does not want to advance state-of-the-art (given that they use only the standard 17 baseline features), but it would have been nice to show another point estimate model from existing work in the result tables, to get a sense of the overall quality of the models.\n-Related to this, it is hard to interpret NLL and NLPD values, so one is always tempted to look at MAE in the tables to get a sense of 'how different the predictions are'. Since the whole point of the paper is to say that this is not the right thing to do, it would be great provide some notion of what is a drastic reduce in NLL/NLPD worth: A qualitative analysis with actual examples.\nSection 4 is very nicely written and explains results very intuitively!\nOverall, I like the paper since it points out the problematic use of point estimates in QE. A difficult task in general where additional information such as confidence arguably are very important. The submission does not advance state-of-the-art and does not provide a lot of novelty in terms of modeling (since GPs have been used before), but its research questions and goals are clearly stated and nicely executed.\nMinor problems: -Section 4: \"over and underestimates\" -> \"over- and underestimates\" -Figure 1 caption: Lines are actually blue and green, not blue and red as stated in the caption.\n-If a certain toolkit was used for GP modeling, it would be great to refer to this in the final paper. ", "label": [[171, 225, "Eval_pos_1"], [226, 303, "Jus_pos_1"], [304, 423, "Eval_pos_2"], [585, 697, "Eval_pos_3"], [698, 803, "Eval_pos_4"], [1108, 1166, "Eval_neg_1"], [1168, 1326, "Jus_neg_1"], [1641, 1792, "Eval_pos_5"], [1795, 2009, "Jus_pos_5"], [2010, 2081, "Eval_pos_6"], [2082, 2173, "Major_claim"], [2279, 2386, "Eval_neg_2"], [2387, 2420, "Jus_neg_2"], [2426, 2498, "Eval_pos_7"]]}
{"id": 78, "review": "This is a highly satisfying paper. It is a report of various NLP efforts for several Indigenous languages of Canada It goes deeply enough into the technical details of the projects to show that the efforts are viable and successful, without getting bogged down in numbers or linguistic details that are unimportant to people external to the projects. Where the paper does get technical is in a discussion of the differing difficulties of speech recognition for different languages, providing a useful case study to demonstrate that one-size technology approaches are not necessarily universal stand-alone solutions.\nThe paper understates two points that could be further investigated.  1 \"Rule based approaches may seem outdated in contrast to statistical or neural methods. However, with most Indigenous languages, existing corpora are not large enough to produce accurate statistical models.\" Why apologize for using a better approach? Rules may be \"outdated\" because they are inefficient for certain languages with reams of available data and scads of phenomena that don't fit. For polysynthetic languages, though, one could posit that a fairly small set of rules might be highly predictive - humans invoke algorithms to construct patterned speech that would otherwise be incomprehensible for the listener to deconstruct, and those same algorithms can be encoded for use by machines. At the least, it would be worth proposing that the languages in this study can offer a test of rule-based vs. inference-based processes, and propose performing such comparisons when the data for the study languages is sufficiently mature.\n2. This paper shows remarkable achievement for minority languages as a result of a $6 million grant. This is a crucial scientific finding: money works! Important research can make great strides regarding languages that are usually neglected, if and only if funding is available for people to take the time to do the work. The billions that have been pumped into languages like English have in fact resulted in technologies that can be applied at much lower cost to languages like Kanyen\u2019k\u00e9ha, but there are still costs. The paper could make more of an advocacy point for what relatively modest funding could do for languages in places where leaders have not yet had the same impetuses as witnessed in Canada, including India and Africa where \"minority\" language is often a misnomer.\nThe paper nicely shows what can be done for languages well outside of the research mainstream, particularly in collaboration between the researchers and the communities. Without a doubt, this paper should be part of the program. ", "label": [[0, 34, "Major_claim"], [116, 350, "Eval_pos_1"], [1629, 1726, "Eval_pos_2"], [1727, 1777, "Eval_pos_3"], [1778, 2408, "Jus_pos_3"], [2409, 2578, "Eval_pos_4"], [2579, 2638, "Major_claim"]]}
{"id": 79, "review": "The aim of this paper is to show that distributional information stored in word vector models contain information about POS labels. They use a version of the BNC annotated with UD POS and in which words have been replaced by lemmas. They train word embeddings on this corpus, then use the resulting vectors to train a logistic classifier to predict the word POS. Evaluations are performed on the same corpus (using cross-validation) as well as on other corpora. Results are clearly presented and discussed and analyzed at length.\nThe paper is clear and well-written. The main issue with this paper is that it does not contain anything new in terms of NLP or ML. It describe a set of straightforward experiments without any new NLP or ML ideas or methods. Results are interesting indeed, in so far that they provide an empirical grounding to the notion of POS. In that regard, it is certainly worth being published in a (quantitative/emprirical) linguistic venue.\nOn another note, the literature on POS tagging and POS induction using word embeddings should be cited more extensively (cf. for instance Lin, Ammar, Duer and Levin 2015; Ling et al. 2015 [EMNLP]; Plank, S\u00c3\u00b8gaard and Goldberg 2016...). ", "label": [[530, 566, "Eval_pos_1"], [567, 661, "Eval_neg_1"], [662, 754, "Jus_neg_1"], [755, 785, "Eval_pos_2"], [787, 859, "Jus_pos_2"], [860, 962, "Major_claim"], [980, 1082, "Eval_neg_2"], [1083, 1197, "Jus_neg_2"]]}
{"id": 80, "review": "- Strengths: This paper reports on an interesting project to enable people to design their own language for interacting with a computer program, in place of using a programming language. The specific construction that the authors focus on is the ability for people to make definitions. Very nicely, they can make recursive definitions to arrive at a very general way of giving a command. The example showing how the user could generate definitions to create a palm tree was motivating. The approach using learning of grammars to capture new cases seems like a good one.  - Weaknesses: This seems to be an extension of the ACL 2016 paper on a similar topic. It would be helpful to be more explicit about what is new in this paper over the old one.  There was not much comparison with previous work: no related work section.  The features for learning are interesting but it's not always clear how they would come into play. For example, it would be good to see an example of how the social features influenced the outcome. I did not otherwise see how people work together to create a language.  - General Discussion: ", "label": [[286, 387, "Eval_pos_1"], [388, 485, "Eval_pos_2"], [486, 569, "Eval_pos_3"], [585, 656, "Eval_neg_1"], [657, 746, "Jus_neg_1"], [748, 796, "Eval_neg_2"], [798, 821, "Jus_neg_2"], [824, 922, "Eval_neg_3"], [923, 1092, "Jus_neg_3"]]}
{"id": 81, "review": "paper_summary\nThis work proposes to explicitly model sentence-level representations of both the source and target side of unsupervised machine translation. The authors utilize normalizing flows to model the sentence representations in a flexible space as transformed from a (shared between languages) simple base distribution. At translation time the invertibility of normalizing flows can be used to map between sentence representations in different languages. In experiments the authors test the methods' viability on many language pairs and show competitive performance across the board. \nsummary_of_strengths\n- The proposed method seems sound and novel.\n- The authors run extensive experiments on unsupervised machine translation and show moderate improvements across the board. Applying the method on top of XLM seems to result in good improvements over existing techniques, except for MASS.\n- The paper is mostly well-written except for one crucial point mentioned below in the weaknesses. \nsummary_of_weaknesses\n- The unsupervised translation tasks are all quite superficial, taking existing datasets of similar languages (e.g. En-De Multi30k, En-Fr WMT) and editing them to an unsupervised MT corpus.\n- Improvements on Multi30k are quite small (< 1 BLEU) and reported over single runs and measuring BLEU scores alone. It would be good to report averages over multiple runs and report some more modern metrics as well like COMET or BLEURT.\n- It is initially quite unclear from the writing where the sentence-level representations come from. As they are explicitly modeled, they need supervision from somewhere. The constant comparison to latent variable models and calling these sentence representations latent codes does not add to the clarity of the paper. I hope this will be improved in a revision of the paper. \ncomments,_suggestions_and_typos\nSome typos: -001: \"The latent variables\" -> \"Latent variables\" -154: \"efficiently to compute\" -> \"efficient to compute\" -299: \"We denote the encoder and decoder for encoding and generating source-language sentences as the source encoder and decoder\" - unclear -403: \"langauge\" -> \"language\" ", "label": [[615, 657, "Eval_pos_1"], [660, 782, "Eval_pos_2"], [783, 896, "Eval_pos_3"], [899, 996, "Eval_pos_4"], [1021, 1081, "Eval_neg_1"], [1083, 1208, "Jus_neg_1"], [1211, 1251, "Eval_neg_2"], [1252, 1262, "Jus_neg_2"], [1449, 1547, "Eval_neg_3"], [1548, 1823, "Jus_neg_3"]]}
{"id": 82, "review": "paper_summary\nThe authors present an approach for knowledge enhanced counseling reflection generation. It uses dialogue context as well as commonsense and domain knowledge for generating responses in counseling conversations. Two methods for knowledge integration are proposed: a retrieval-based method and a generative method. Experimental results show that both methods for knowledge incorporation improve the system's performance.\nCONTRIBUTIONS: (1) The authors propose a pipeline that collects domain knowledge (medical) through web mining and apply it to build up a counseling knowledge base. \n(2) The authors use the domain knowledge they collected along with commonsense knowledge bases for the task of reflection generation. \n(3) The authors analyze different types of commonsense and domain knowledge, as well as their effect on the generation task. \nsummary_of_strengths\n- Overall, the paper is clear in its objectives and methodology followed. The work is well structured, easy to read and follow.\n-The authors show empirical success of their approach.\n-The overall story is convincing. The proposed approach is tested with reasonable models and appropriate experiments. The experimental results are promising, demonstrating the effectiveness of the proposed method. Thus, the paper makes valuable contributions to the field.\n-The approach is well motivated and addresses a problem that is relevant to the community. \nsummary_of_weaknesses\n- Lack of illustrative examples regarding the model outputs.\n-Some details regarding the knowledge collection process have been omitted (see \"Questions\" below). \ncomments,_suggestions_and_typos\nQUESTIONS: -Fig. 2: Why did you discard the \"anatomy\" category?\n-l. 221: How many query templates did you specify in total?\n-l. 227: What's the size of the set of knowledge candidates?\n-l. 550: Did you calculate the agreement between the annotators? Were the annotators authors of the paper?\nMINOR: -Try to better align the figures with the text.\n-fix punctuation: l. 336, l. 433, l. 445, l. 534 -Table 2: The highlighting of the numbers does not correspond to the caption (\"highest scores are in bold, second highest scores in italic\") ", "label": [[883, 954, "Eval_pos_1"], [955, 1008, "Eval_pos_2"], [1010, 1063, "Eval_pos_3"], [1065, 1097, "Eval_pos_4"], [1098, 1181, "Eval_pos_5"], [1182, 1277, "Eval_pos_6"], [1284, 1336, "Eval_pos_7"], [1338, 1428, "Eval_pos_8"], [1453, 1511, "Eval_neg_1"], [1513, 1612, "Eval_neg_2"], [1645, 1936, "Jus_neg_2"]]}
{"id": 84, "review": "paper_summary\nThis paper presents an interesting finding, i.e., fine-tuning only the bias terms of pre-trained language models is competitive with fine-tuning the entire model. The authors compared the proposed method Bias-terms Fine-tuning (BitFit) with other parameter-efficient fine-tuning methods (e.g., Adapters, Diff-Pruning). The experimental results on GLUE benchmark show that BitFit can achieve strong performance with less trainable parameters. \nsummary_of_strengths\n- The paper is well written and easy to understand.\n-The proposed method (BitFit) is neat and novel.\n-The authors show strong empirical results on GLUE benchmark. \nsummary_of_weaknesses\nI do not have any concerns about this paper. \ncomments,_suggestions_and_typos\nIt would be helpful to compare BitFit with Adapter and Diff-Pruning base on other language models (e.g.,RoBERTa, T5). But current version is good enough for a short paper. ", "label": [[480, 529, "Eval_pos_1"], [531, 578, "Eval_pos_2"], [580, 641, "Eval_pos_3"], [664, 709, "Major_claim"], [864, 914, "Major_claim"]]}
{"id": 85, "review": "paper_summary\nThis paper proposes a novel method to explore the search space of neural text generation models. The proposed method includes two key components of a modified best-first search and a path recombination mechanism. The authors conduct experiments on text summarization and machine translation tasks. The experiment results show that the proposed method generates massive-scale candidate sentences and obtain comparable or even better metric scores. \nsummary_of_strengths\n- The description of the proposed approach is clear and easy to follow.\n- The paper presents a well-rounded set of experiments on text summarization and machine translation.\n- The authors provide a lot of details in the appendix, which helps the reproducibility. \nsummary_of_weaknesses\n- Although BFS is briefly introduced in Section 3, it's still uneasy to understand for people who have not studied the problem. More explanation is preferable. \ncomments,_suggestions_and_typos\n- Algorithm 1, line 11: the function s(\u00b7) should accept a single argument according to line 198.\n- Figure 6: the font size is a little bit small. ", "label": [[485, 554, "Eval_pos_1"], [557, 656, "Eval_pos_2"], [659, 746, "Eval_pos_3"], [771, 896, "Jus_neg_1"], [897, 929, "Eval_neg_1"]]}
{"id": 86, "review": "The paper proposes a convolutional neural network approach to model the coherence of texts. The model is based on the well-known entity grid representation for coherence, but puts a CNN on top of it.  The approach is well motivated and described, I especially appreciate the clear discussion of the intuitions behind certain design decisions (e.g. why CNN and the section titled 'Why it works').\nThere is an extensive evaluation on several tasks, which shows that the proposed approach beats previous methods. It is however strange that one previous result could not be reproduced: the results on Li/Hovy (2014) suggest an implementation or modelling error that should be addressed.\nStill, the model is a relatively simple 'neuralization' of the entity grid model. I didn't understand why 100-dimensional vectors are necessary to represent a four-dimensional grid entry (or a few more in the case of the extended grid). How does this help? I can see that optimizing directly for coherence ranking would help learn a better model, but the difference of transition chains for up to k=3 sentences vs. k=6 might not make such a big difference, especially since many WSJ articles may be very short.\nThe writing seemed a bit lengthy, the paper repeats certain parts in several places, for example the introduction to entity grids. In particular, section 2 also presents related work, thus the first 2/3 of section 6 are a repetition and should be deleted (or worked into section 2 where necessary). The rest of section 6 should probably be added in section 2 under a subsection (then rename section 2 as related work).\nOverall this seems like a solid implementation of applying a neural network model to entity-grid-based coherence. But considering the proposed consolidation of the previous work, I would expect a bit more from a full paper, such as innovations in the representations (other features?) or tasks.\nminor points: - this paper may benefit from proof-reading by a native speaker: there are articles missing in many places, e.g. '_the_ WSJ corpus' (2x), '_the_ Brown ... toolkit' (2x), etc.\n- p.1 bottom left column: 'Figure 2' -> 'Figure 1' - p.1 Firstly/Secondly -> First, Second - p.1 'limits the model to' -> 'prevents the model from considering ...' ?\n- Consider removing the 'standard' final paragraph in section 1, since it is not necessary to follow such a short paper. ", "label": [[201, 245, "Eval_pos_1"], [247, 394, "Jus_pos_1"], [396, 509, "Eval_pos_2"], [765, 918, "Eval_neg_1"], [920, 1193, "Jus_neg_1"], [1194, 1277, "Eval_neg_2"], [1279, 1612, "Jus_neg_2"], [1613, 1727, "Eval_pos_3"], [1792, 1835, "Major_claim"], [1924, 1985, "Eval_neg_3"], [1987, 2096, "Jus_neg_3"]]}
{"id": 88, "review": "paper_summary\nThe authors present a method of calibrating learned multiclass classification models during training, that is, improving model calibration (in other words, pushing accuracy-versus-model-confidence graphs towards the identity line---well-calibrated models have confidence perfectly reflecting prediction accuracy).\nThe proposed method is done during training, rather than being a post-hoc model which recalibrates a learned model to maximize performance on a held-out calibration set. This has the benefit of allowing the model to use the whole train set (i.e. not requiring a held-out calibration set), in addition to ideally leveraging the entire train set to do calibration. In this sense it follows a small number of published methods.\nOverall the core writeup of the method (sec 3) was difficult to follow. The core design decisions of the method were difficult to find the motivation of. I believe the decisions pivot around an increased sample-efficiency claim (line 316), but this claim was not stated precisely (the free variable $\\epsilon$ bounds \"calibration error,\" which I do not believe is defined), and the claim does not have a proof, so the relationship between the system setup decisions and sample efficiency claims is not at all clear.\nGenerally, the calibrated systems' plots do not seem obviously notably superior to the baseline calibrations (Fig 2, Fig 1(top)), though the methods do indeed achieve superior ECE (expected calibration error) performance across some of the tasks (xSLUE, a suite of multiclass classification NLP tasks, Table 1) and, somewhat surprisingly, superior top-level accuracy/F1 (table 1).\nGiven the importance of proper calibration to so many applications of NLP, I suspect these methods would be of interest to practitioners in the field even if they do not provide an across-the-board calibration improvement, but it is not clear from the writeup when they work and when they do not.  That is, this writeup does not really address the question \"when should a practitioner use this very complex method of calibration, rather than the much conceptually simpler post-hoc Platt scaling, given that the curves in Fig 2 and Fig 1 seem to hint that the calibration benefits of this method seem to be very noisy.\" Is there a property of the dataset or predictor where we can expect this method to really help?\nGenerally, I think with notable revisions this paper would be of interest to practitioners, but it is somewhat difficult to follow the method exactly and know how we should expect it to affect performance on a given novel task. \nsummary_of_strengths\n- The authors present a novel calibration technique in multiclass classification setups that leverages the entire train set (rather than a held-out calibration set). This method learns both a Platt-scaling transform (an affine transform on logits, basically), in addition to iteratively adaptively binning the train set based on this learned transform, I believe so that we can calculate the ECE without a held-out set.\n-The proposed methods perform superior to baselines with respect to calibration error on more tasks than they perform worse.\n-The proposed modifications improve against the baseline MLE systems in terms of accuracy/F1 across many tasks; that is, the calibration term appears to act as an effective model regularizer in some setups. \nsummary_of_weaknesses\n- The technical description of the method (sec 3) was quite difficult to follow. The central methodological decisions (adaptive binning, discretization thereafter by doing what appears to be essentially isotonic regression) were difficult to find the motivation of. I believe these decisions center around being able to make a sample efficiency claim (line 317), but I'm not sure why this method gives that sample efficiency property, and no proof of the claim is presented. More concrete questions/concerns are given in \"comments\" below, written as I read the presentation.\n-There are a few properties of the evaluation/results which are a bit confusing and call conclusions into question. For example:   - PosPS (post-hoc Platt Scaling) is lower than MLE in accuracy on a few tasks---If I understand correctly, this should not be true in principle, since the max predictions are supposed to be invariant. Is this just model retrain noise from starting off at different initial model params? Or do I misunderstand? \n  - The reliability diagrams (Fig 1, top), that is the accuracy vs confidence plots, certainly don't make the proposed methods look better (a perfect system will have the identify function here), and in fact exhibit some pretty notable pathologies (high-accuracy spikes in low confidence regimes, e.g.). Is this a property of dataset pathologies or does it reflect variance/unpredictability in the proposed methods? \n  - In the ECE-versus-number-of-bins plots in Figure 2, the two calibrated systems (PosCal and the proposed) all have ECEs (calibration performance) very close to the MLE for most values. This hints at the fact that the methods may be very sensitive to bin size and often provide no actual expected calibration improvement, is that right? How was this bin-size hyperparam selected? Was its selected value chosen from the held-out dev set without looking at test at all?\n-Difficult to see how this can extend to the structured prediction encoder/decoder setups used quite often across NLP. Does this only work for relatively few-class multiclass classification? This is not a fatal flaw. \ncomments,_suggestions_and_typos\n- \"These observations prove the efficacy of our method in maintaining a perfect balance between model performance and model uncertainity-a testimony of an ideal calibrator\" this is a pretty over-the-top claim! \" Perfect balance?\" This doesn seem like isn't an \"ideal calibrator,\" it has nontrivial ECE still, right? I'd scale this paragraph's claims back to things that are empirically supportable -tab 2 is somewhat confusing. Can you turn P1 and P2 into one confusion matrix per row and present it that way? These different columns are all just very different sort of things, strange that they're presented next to each other -What is Fig 2's dataset? the average across all the tasks? Just one task?\n-The definition of perfect calibration in the info is perhaps a bit confusing as-is (namely, $P$ has to be a joint over the covariates $x$ and the predictions $f$ right so you need some sort of metric over both, is that right? Or perhaps you just require $f$ to be Borel-measurable and deterministic or something. ( Unrelatedly, I also realize that if $f$ is itself nondeterministic, then you probably need an expectation operator right?). Anyways, it might be helpful to add a short sentence to the text here explaining this eq in words, no need to be too precise.\n-Is the output of step (3) differentiable? Do you need subgradients or something to get the loss? I guess this is why it's helpful to know exactly what $\\beta$ is.\n-A nit but i might use something other than \"distance\" to describe $d$ in line 231, since it's not in general a valid distance metric, maybe \"calibration mismatch\" or something.\n-Also maybe change $q$ to $\\hat q$ in the eq on line 232? Since the thing you're minimizing isn't the true value $q$, which we don't have access to even in principle.\n-I'm a bit perplexed by the discrepancy between the matrix $Q$, which is essentially a function of the predictor network, and the $q$ in 232, which is not. Does $Q$ as given suffice to allow us to calculate arbitrary distance functionals between $p$ and $q$, as described in line 231?\n-You haven't defined calibration error but instantiate it via $\\epsilon$ in line 254. It's abs(p - q) to use the terminology of line (232), is that right? ECE is usually given with the expectation randomness integrating over the simplex $D_K$ right, do we have to do that here? Not sure if this is essential to get all the details precise here but probably it would be good to define what \"calibration error\" is at the least.\n-Don't think you define what the Platt-scaling set $G$ is ranged over by the argmin in step 1.\n-What is the set $\\beta$ in line 311?\n-It's not clear to me why step (3) of the algorithm should be necessary at all. It seems like this wouldn't change the calibration but allows us to estimate the ECE? Is that right? Or does this adaptive binning + discretization (essentially isotonic regression, right?) actually affect calibration in expectation?\n-Line (7) of alg 1's pseucode is referenced in the text but the fig doesn't have line numbers, is it possible to add them \"the achieved reduction in ECE as compared to all baselines is significant\" what does this mean? Paired t-test? p < 0.05? Either describe the test or remove this significance claim -do you have a proof of the sample-efficiency claim in line 317? would be good to put this into an appendix -Is it possible to compare to MCDropout? It would be really nice to see that comparison, but this isn't crucial. ", "label": [[753, 824, "Eval_neg_1"], [825, 1268, "Jus_neg_1"], [1280, 1377, "Eval_neg_3"], [1378, 1397, "Jus_neg_3"], [1650, 1724, "Jus_pos_1"], [1735, 1871, "Eval_pos_1"], [1877, 1946, "Eval_neg_4"], [1948, 2364, "Jus_neg_4"], [2376, 2455, "Major_claim"], [2461, 2593, "Eval_neg_7"], [2617, 2702, "Eval_pos_2"], [2703, 2779, "Jus_pos_2"], [3161, 3270, "Eval_pos_3"], [3272, 3367, "Jus_pos_3"], [3392, 3470, "Eval_neg_8"], [3471, 3964, "Jus_neg_8"], [3966, 4081, "Eval_neg_2"], [4081, 4406, "Jus_neg_2"], [6248, 6330, "Eval_neg_10"], [6331, 6976, "Jus_neg_10"], [8167, 8245, "Eval_neg_9"], [8246, 8479, "Jus_neg_9"]]}
{"id": 90, "review": "- Strengths: This paper tries to use the information from arguments, which is usually ignored yet actually quite important, to improve the performance of event detection. The framework is clear and simple. With the help of the supervised attention mechanism, an important method that has been used in many tasks such as machine translation, the performance of their system outperforms the baseline significantly.\n- Weaknesses:  The attention vector is simply the summation of two attention vectors of each part. Maybe the attention vector could be calculated in a more appropriate approach. For the supervised attention mechanism, two strategies are proposed. \nBoth of them are quite straightforward. Some more complicated strategies can work better and can be tried.\n- General Discussion:  Although there are some places that can be improved, this paper proposed a quite effective framework, and the performance is good. The experiment is solid. It can be considered to be accepted. ", "label": [[171, 205, "Eval_pos_1"], [206, 412, "Eval_pos_2"], [512, 590, "Eval_neg_1"], [591, 767, "Jus_neg_1"], [844, 892, "Eval_pos_3"], [897, 921, "Eval_pos_4"], [922, 946, "Eval_pos_5"], [947, 984, "Major_claim"]]}
{"id": 91, "review": "paper_summary\nThis paper empirically studied CLIP models as few-shot learners for two vision-language understanding tasks: VQA and Visual entailment. In the VQA task, the paper proposed a two-step method to mitigate the gap between natural language description and question answering. In addition, the paper used only a very small set of parameters in CLIP models during fine-tuning, including bias and normalization terms. \nsummary_of_strengths\nIt studied how to transfer CLIP zero-shot capabilities into VLU tasks and confirms CLIP models can be good few-shot learners.\nThe paper proposed a two-step prompt generation method to apply CLIP on VQA.\nThe paper identified only a small number of parameters are enough to fine-tune the CLIP few-shot learner. \nsummary_of_weaknesses\nThe way of using T5 for template generation is unclear, and lack of evaluation of the template generation quality.\nIn model fine-tuning experiments, it is unclear about the learning rates and epochs for different parameter settings, which could largely affect the results. \ncomments,_suggestions_and_typos\nSee the weakness. ", "label": [[778, 833, "Eval_neg_1"], [838, 892, "Eval_neg_2"], [893, 1051, "Eval_neg_3"]]}
{"id": 92, "review": "Summary: This paper proposes a novel solution for abstractive dialogue summarization, which is a challenging task because it requires modeling discourse structure and long dependencies between utterances in the dialogue. The proposed approach consists of several parts: (1) Representing the input dialogue as a graph, using topic words and utterance embeddings to represent nodes, and topic word overlap to create edges. ( Topic words also include the names of participants in the dialogue.) \n(2) Graph encoder with masked self-attention to encode the graph, while focusing on the most important utterances (3) Standard sequence-to-sequence model to encode the entire dialogue context as a sequence. \n(4) A topic-word-aware decoding method that uses the topic words in two ways:  with a coverage mechanism to ensure coverage/prevent repetition, and with a pointer mechanism to allow expressing topic words.\nOverall, I found the proposed method in this paper very convincing. The various parts of the solution (in particular the graph representation) are well designed, and the incorporation of topic words is intuitively a good approach. At the same time though, I think the paper misses out on some crucial analysis and evaluation, so as it stands right now, the results are insufficient to convincingly declare that the proposed solution works really really well (although the results presented in the paper are definitely a very positive sign that this approach does work really well). Despite this weakness, I think the proposed solution is novel enough and (especially if addressed) the results positive enough that the community would gain value from this paper.\nStrengths: -Proposed method is intuitive and well motivated, and explained clearly for the most part. There are several moving parts, but they are all tied together well.\n-Experimental results are very promising, and the provided examples illuminate the advantages of the proposed solution well.\n-In addition to automatic evaluation, the authors also took the effort to perform human evaluation and other attention-based analyses of the model.\n-Paper is well-structured and easy to follow.\nWeaknesses: 1) There's very little dataset analysis in this paper, which makes it hard to know exactly how challenging the problems posed by these datasets really are. The original SAMSum dataset paper doesn't have any analysis either, which makes it all the more important to have some kind of analysis here in this paper. The other dataset used here (the Automobile Master Corpus) has neither an analysis nor a citation, and there are also no examples whatsoever of this dataset. Some questions that would be important to answer, at the very least, are:    a) What are the length distributions of the summaries and the dialogues respectively, and what's the relationship between the length of a dialogue and the length of its summary? \n  b) How many topic words does each dialogue have? How many of those actually occur in the final summary? \n  c) What are some interesting discourse phenomena that need to be correctly modeled in order to generate an accurate summary? \n2) There's no analysis of examples that this model performs poorly on, or other gaps that need to be addressed. \n3) While it's great that human evaluation was performed, it's lacking in a couple of different ways:   a) The human evaluation metrics should be described in further detail. What exactly do \"relevance\" and \"readability\" encompass? What were the guidelines given to the evaluators to rate these? \n  b) Were the examples double/triple reviewed in any way? \n  c) Related to (a) - another important aspect of a good summary is its completeness. Did either of the human evaluation metrics encompass completeness? \n  d) Human evaluation wasn't conducted on the gold summaries, which is unfortunate because it would provide a sense of an appropriate ceiling for the human evaluation numbers. \n  4) Some of the claims in the paper don't really seem to follow from the results of the experiments. For example, the paper makes the following claim from the human evaluation results:      \"As we can see, Pointer Generator suffers from repetition and generates many trivial facts. For Fast Abs RL Enhanced model, it successfully concentrates on the salient information, however, the dialogue structure is not well constructed. By introducing the topic word information and coverage mechanism, our TGDGA model avoids repetitive problems and better extracts the core information in the dialogue.\" \n   However, I don't know if any of these claims can be directly inferred from the results of the human evaluation (unless there are aspects of the human evaluation that were not described).\n5) For a model as complex as the one proposed here, I would have liked to have seen some kind of ablation analysis that shows the importance of each of the moving parts. While the proposed approach makes sense intuitively, there's not enough convincing experimental evidence to show that each of its parts is crucial (even though the results section seems to claim this).\nOther comments/suggestions: 1) in Section 4.2 - why are stop words filtered from the vocab? Does that mean that they can only be predicted through the pointer mechanism? That seems like too strict a restriction. \n2) How is temporal information represented in the graph representation? Or does the model rely entirely on the seq2seq to learn temporal information, while the graph just captures structural relationships? \n3) In section 5.1 -  what is the Separator? It was not introduced before this section, but maybe it should be introduced in Sec 4.3. \n4) Another claim (in Sec 5.1) that doesn't seem to be supported by the results: \"Besides, the TGDGA model outperformsthe Transformer model based on fully connected relationships, which demonstrates that our dialogue graph structures effectively prune unnecessary connections between utterances\" ", "label": [[907, 974, "Eval_pos_1"], [975, 1137, "Jus_pos_1"], [1163, 1231, "Eval_neg_1"], [1260, 1487, "Jus_neg_1"], [1489, 1668, "Major_claim"], [1681, 1770, "Eval_pos_2"], [1771, 1839, "Jus_pos_2"], [1841, 1881, "Eval_pos_3"], [1886, 1964, "Eval_pos_4"], [2114, 2158, "Eval_pos_5"], [2174, 2326, "Eval_neg_2"], [2327, 3131, "Jus_neg_2"], [3135, 3244, "Eval_neg_3"], [3248, 3345, "Eval_neg_4"], [3349, 3753, "Jus_neg_4"], [3936, 4032, "Eval_neg_5"], [4033, 4718, "Jus_neg_5"], [4889, 5090, "Eval_neg_5"]]}
{"id": 93, "review": "The paper is clearly written, and the claims are well-supported.  The Related Work in particular is very thorough, and clearly establishes where the proposed work fits in the field.\nI had two main questions about the method: (1) phrases are mentioned in section 3.1, but only word representations are discussed.  How are phrase representations derived? \n(2) There is no explicit connection between M^+ and M^- in the model, but they are indirectly connected through the tanh scoring function.  How do the learned matrices compare to one another (e.g., is M^- like -1*M^+?)?  Furthermore, what would be the benefits/drawbacks of linking the two together directly, by enforcing some measure of dissimilarity?\nAdditionally, statistical significance of the observed improvements would be valuable.\nTypographical comments: -Line 220: \"word/phase pair\" should be \"word/phrase pair\" -Line 245: I propose an alternate wording: instead of \"entities are translated to,\" say \"entities are mapped to\".  At first, I read that as a translation operation in the vector space, which I think isn't exactly what's being described.\n-Line 587: \"slightly improvement in F-measure\" should be \"slight improvement in F-measure\" -Line 636: extraneous commas in citation -Line 646: \"The most case\" should be \"The most likely case\" (I'm guessing) -Line 727: extraneous period and comma in citation ", "label": [[0, 64, "Eval_pos_1"], [66, 181, "Eval_pos_2"]]}
{"id": 96, "review": "paper_summary\nThe paper investigates the problem of identifying unanswerable questions in multiple choice MRC. It proposes two ways of tackling this problem: Firstly, by explicitly augmenting training data with unanswerable examples and secondly, by thresholding on (estimated) prediction uncertainty. The paper goes on to show that this can help to identify and abstain from (falsely) predicting hard examples and to identify unanswerable questions on a constructed dataset. \nsummary_of_strengths\nThe stregth of the paper is that it touches upon a topic that appears under-explored in the literature. The paper is written well and the results are presented clearly. Evaluation metrics are well motivated and discussed in detail, which is important as they deviate from the usual F1/Accuracy measures used for evaluating MC-MRC. \nsummary_of_weaknesses\nI have identified some weaknesses in the paper:  - How general are the obtained results? From what I can tell, most of the analysis is performed on the results of one optimised model (ensemble), on one dataset (ReClor). I believe the methodology is general enough to be applied to other MC-MRC datasets. Why was ReClor and only ReClor chosen? It would be interesting to see, whether the reported results pertain across different datasets. For example CosmosQA (https://arxiv.org/pdf/1909.00277.pdf) comes with built-in unanswerable questions. This work should be mentioned. \n - While most of the results are clear, I had difficulties interpreting the figures. What makes it difficult is that they all have different axes and threshold over different values. I believe compacting section 2 that discusses the high-level overview of the MC-MRC task to create more room for more thorough explanations of the results would be beneficial. This could be done by providing more informative captions of the figures or linking back to the equations and introduced names (e.g. beta). \ncomments,_suggestions_and_typos\nWhat follows are some minor remarks, questions and comments: - The hyperparameter section is rather terse. It is not clear which hyper-parameters are selected from. Appropriate information should be added, at least in the appendix.\n-What is \"Fraction unanswerable\" in Figure 5 and how is it obtained?\n-I don't believe it is fair to make the comparison in Table 4. While the manuscript acknowledges that, it does not mention the precise nature of the unfairness. The point is that for MAP, the %UNANS is based on model predictions, it's not a parameter that can be freely chosen, unlike the Implicit method. Here, the dev-mixed set was used both for reporting the final accuracy comparison and to select the best %UNANS threshhold for implicit. It would be more fair to either estimate %UNANS from the training data (i.e. 25%) or to split the DEV-mixed in half, use one half to estimate the best %UNANS threshhold and the other half to compare Implicit with the chosen threshhold to MAP. This wouldn't change much of the argument as from what I can tell from Figure 5, Implicit is still better than MAP with %UNANS of 25. ", "label": [[498, 601, "Eval_pos_1"], [602, 666, "Eval_pos_2"], [667, 829, "Eval_pos_3"], [852, 898, "Major_claim"], [903, 940, "Eval_neg_1"], [941, 1426, "Jus_neg_1"], [1430, 1511, "Eval_neg_2"], [1512, 1926, "Jus_pos_1"], [2022, 2065, "Eval_neg_3"], [2066, 2190, "Jus_neg_3"], [2261, 2322, "Eval_neg_4"], [2323, 3080, "Jus_neg_4"]]}
{"id": 97, "review": "paper_summary\nThe paper presents AcTune, an active learning framework that combines self training on high-confident samples and data annotation on low-confident samples. The paper also proposes two new methods: (1) region-based sampling and (2) momentum-based memory bank, to improve the sampling strategy in active learning and to reduce label noise in self-training. The paper provides extensive experiments to show the advantage of the proposed method (both performance and label efficiency) and and offered ablation studies to analyse inner-workings of this method. The paper is overall a solid contribution for combining active learning and self-training in NLP and I recommend acceptance. \nsummary_of_strengths\nThe paper proposes a novel and effective framework to combine active learning + self training in NLP. The additional two strategies designed in the paper (region-based sampling and momentum-based memory bank) are well-motivated. The experiments are thorough with convincing ablation studies. \nsummary_of_weaknesses\nNo major concerns. \ncomments,_suggestions_and_typos\nn/a ", "label": [[570, 695, "Major_claim"], [717, 818, "Eval_pos_1"], [819, 945, "Eval_pos_2"], [946, 1009, "Eval_pos_3"], [1032, 1051, "Major_claim"]]}
{"id": 100, "review": "- strengths This is a novel approach to modeling the compositional structure of complex categories that maintains a set theoretic interpretation of common nouns and modifiers, while also permitting a distributional interpretation of head modification. The approach is well motivated and clearly defined and the experiments show that show that this decomposed representation can improve upon the Hearst-pattern derived IsA relations upon which it is trained in terms of coverage.\n- weaknesses The experiments are encouraging. However, it would be nice to see ROC curves for the new approach alone, not in an ensemble with Hearst patterns. Table 5 tells us that Mods_I increases coverage at the cost of precision and Figure 2 tells us that Mods_I matches Hearst pattern precision for the high precision region of the data. However, neither of these tell us whether the model can distinguish between the high and low precision regions, and the ROC curves (which would tell us this) are only available for ensembled models.\nI believe that Eqn. 7 has an unnecessary $w$ since it is already the case that $w=D(\\rangle e, p, o \\langle)$. - discussion Overall, this is a nice idea that is well described and evaluated. I think this paper would be a good addition to ACL. ", "label": [[12, 98, "Eval_pos_1"], [99, 251, "Jus_pos_1"], [252, 302, "Eval_pos_2"], [1144, 1263, "Major_claim"]]}
{"id": 101, "review": "This paper develops an LSTM-based model for classifying connective uses for whether they indicate that a causal relation was intended. The guiding idea is that the expression of causal relations is extremely diverse and thus not amenable to syntactic treatment, and that the more abstract representations delivered by neural models are therefore more suitable as the basis for making these decisions.\nThe experiments are on the AltLex corpus developed by Hidley and McKeown. The results offer modest but consistent support for the general idea, and they provide some initial insights into how best to translate this idea into a model. The paper distribution includes the TensorFlow-based models used for the experiments.\nSome critical comments and questions: - The introduction is unusual in that it is more like a literature review than a full overview of what the paper contains. This leads to some redundancy with the related work section that follows it. I guess I am open to a non-standard sort of intro, but this one really doesn't work: despite reviewing a lot of ideas, it doesn't take a stand on what causation is or how it is expressed, but rather only makes a negative point (it's not reducible to syntax). We aren't really told what the positive contribution will be except for the very general final paragraph of the section.\n- Extending the above, I found it disappointing that the paper isn't really clear about the theory of causation being assumed. The authors seem to default to a counterfactual view that is broadly like that of David Lewis, where causation is a modal sufficiency claim with some other counterfactual conditions added to it. See line 238 and following; that arrow needs to be a very special kind of implication for this to work at all, and there are well-known problems with Lewis's theory (see http://bcopley.com/wp-content/uploads/CopleyWolff2014.pdf). There are comments elsewhere in the paper that the authors don't endorse the counterfactual view, but then what is the theory being assumed? It can't just be the temporal constraint mentioned on page 3!\n- I don't understand the comments regarding the example on line 256. The authors seem to be saying that they regard the sentence as false. If it's true, then there should be some causal link between the argument and the breakage. \nThere are remaining issues about how to divide events into sub-events, and these impact causal theories, but those are not being discussed here, leaving me confused.\n- The caption for Figure 1 is misleading, since the diagram is supposed to depict only the \"Pair_LSTM\" variant of the model. My bigger complaint is that this diagram is needlessly imprecise. I suppose it's okay to leave parts of the standard model definition out of the prose, but then these diagrams should have a clear and consistent semantics. What are all the empty circles between input and the \"LSTM\" boxes? The prose seems to say that the model has a look-up layer, a Glove layer, and then ... what? How many layers of representation are there? The diagram is precise about the pooling tanh layers pre-softmax, but not about this. I'm also not clear on what the \"LSTM\" boxes represent. It seems like it's just the leftmost/final representation that is directly connected to the layers above. I suggest depicting that connection clearly.\n- I don't understand the sentence beginning on line 480. The models under discussion do not intrinsically require any padding. I'm guessing this is a requirement of TensorFlow and/or efficient training. That's fine. If that's correct, please say that. I don't understand the final clause, though. How is this issue even related to the question of what is \"the most convenient way to encode the causal meaning\"? I don't see how convenience is an issue or how this relates directly to causal meaning.\n- The authors find that having two independent LSTMs (\"Stated_LSTM\") is somewhat better than one where the first feeds into the second. This issue is reminiscent of discussions in the literature on natural language entailment, where the question is whether to represent premise and hypothesis independently or have the first feed into the second. I regard this as an open question for entailment, and I bet it needs further investigation for causal relations too. \nSo I can't really endorse the sentence beginning on line 587: \"This behaviour means that our assumption about the relation between the meanings of the two input events does not hold, so it is better to encode each argument independently and then to measure the relation between the arguments by using dense layers.\" This is very surprising since we are talking about subparts of a sentence that might share a lot of information.\n- It's hard to make sense of the hyperparameters that led to the best performance across tasks. Compare line 578 with line 636, for example. Should we interpret this or just attribute it to the unpredictability of how these models interact with data?\n- Section 4.3 concludes by saying, of the connective 'which then', that the system can \"correctly disambiguate its causal meaning\", whereas that of Hidey and McKeown does not. That might be correct, but one example doesn't suffice to show it. To substantiate this point, I suggest making up a wide range of examples that manifest the ambiguity and seeing how often the system delivers the right verdict. This will help address the question of whether it got lucky with the example from table 8. ", "label": [[475, 543, "Eval_pos_1"], [761, 880, "Eval_neg_1"], [882, 1338, "Jus_neg_1"], [1362, 1465, "Eval_neg_2"], [1466, 2093, "Jus_neg_2"], [2096, 2162, "Eval_neg_3"], [2163, 2490, "Jus_neg_3"], [2616, 2681, "Eval_neg_4"], [2682, 3334, "Jus_neg_4"], [3337, 3391, "Eval_neg_5"], [3392, 3833, "Jus_neg_5"], [3834, 4298, "Jus_neg_6"], [4299, 4615, "Eval_neg_6"], [4730, 4823, "Eval_neg_7"], [4824, 4978, "Jus_neg_7"], [5155, 5221, "Eval_neg_8"], [5222, 5474, "Jus_neg_8"]]}
{"id": 102, "review": "paper_summary\nThis paper is about the design of an automatic phoneme transcription system for transcription assistance. \nIn particular, it targets endangered languages with only one speaker data, and the goal is to reduce the cost of transcription such languages. \nWith all due respect to previous research, the authors note that each of them uses a different system, and that each of them has been tested on a different language. \nThe authors also propose a model that is retrained from pre-trained models in multiple languages, and hypothesize that this will be effective for speech recognition with small amounts of data, such as for endangered languages. \nThe authors designed a unified experimental setup, the STP test bed, and used it to compare 4 different models under 11 different languages. \nThe experiments showed that the system with the multilingual pre-trained model performed better in many languages. \nThe authors also estimated that there is a boundary where the recognition rate drops significantly around 90 minutes of training data. \nsummary_of_strengths\nThe strength of this paper is that it uses a unified experimental setup to conduct experiments on endangered language automatic transcriptions, which have traditionally been conducted with different models and different languages. \nThe discussion of the experiment, for example, whether it is suitable for fieldwork or not, is given from a humanistic perspective as well, so that it can be considered as a contribution not only to technology but also to the humanities. \nIn addition, the authors have published a container for reproducing some of the experiments, which is expected to have a significant impact not only on the paper itself but also on future research in this field. \nsummary_of_weaknesses\nThe motivation, purpose, and selection of models for the study are appropriate. \nHowever, I have some concerns about the experiment.\n1. Due to the characteristics of endangered language evaluation, I feel that the test data is very limited. \nThe authors split the data 9:1 between training and testing, but for example, with 90 minutes of data, there are only 10 minutes of test data. \nOf course I understand that this is unavoidable due to data limitations, but I think some approach or support for this is needed. \nFor example, cross-validation can be considered. ( Note that I am not referring to the cross-validation set commonly employed in neural net training.) \nOther possibilities include showing that the distribution of phonemes in the test data is not significantly different from the overall distribution, or calculating perplexity. \nAveraging the languages with the same weights is also anxious in terms of the reliability of the test set mentioned above. \nFor experiments with such a small amount of data, I think a confidence interval should be shown.\n2. As the authors describe at the end of their discussion, the number of speakers is very small. \nTherefore, I feel that it is not possible to distinguish whether the experimental results are speaker-dependent or language-dependent. \nOf course I understand the difficulty of the experiments with endangered languages, but this is not a reason to relax the experimental conditions for generalization. \nFor example, I think the authors could conduct a quasi-limited experiment using a European language for which a large amount of data is available. \nIf it is inappropriate in those languages, the authors should explain why.\nI am very sympathetic to the philosophy of this paper and understand its importance. \nHowever, I believe that the experimental setup should be very carefully designed, as this study could be a baseline for future work in this field. \ncomments,_suggestions_and_typos\nEven if I take into account the convenience of fieldwork, there seems to be little need to compare training times. \nAnd if it only takes 24 hours, it seems acceptable. \nIf you want to describe the training time, I encourage you to discuss it more. ", "label": [[1075, 1306, "Eval_pos_2"], [1307, 1445, "Jus_pos_1"], [1455, 1543, "Eval_pos_1"], [1546, 1758, "Eval_pos_3"], [1781, 1861, "Eval_pos_4"], [1862, 1913, "Major_claim"], [1917, 2022, "Eval_neg_1"], [2023, 2847, "Jus_neg_1"], [2851, 2945, "Eval_neg_2"], [2946, 3471, "Jus_neg_2"], [3472, 3557, "Eval_pos_5"], [3738, 3852, "Eval_neg_3"]]}
{"id": 103, "review": "paper_summary\nThe paper presents a framework for abstractive summarization of long documents that repeatedly segments text, summarizes each segment and then feeds the concatenated summaries as an input to the next iteration. When the input is below a predefined number of tokens, the final summary is generated. This design has the advantages of being able to use powerful pretrained models (e.g. BART) that are designed for shorter text, and avoiding expensive attention computation over long spans. \nsummary_of_strengths\n- The approach is simple, powerful and flexible - a good idea for applying powerful pretrained models to longer text without truncation.\n-A very comprehensive evaluation is provided with a good selection of datasets and both automatic and human evaluation which show the model\u2019s clear advantage over baselines.\n-The paper is well-written and easy to understand. \nsummary_of_weaknesses\n- **Inefficient design of target segmentation**: If I understand correctly, all input segments are assigned a target segment. This probably causes unnecessary summarization of irrelevant input segments - if the text is massive and the gold summary is very short, why not ignore most of the input segments and only select sufficient input segments to cover the targets? Also, this allows targets to be duplicated over different input segments.  -**Missing analysis of model behavior**: It would be useful and interesting to know what exactly is happening at each stage, e.g. how much text is produced at each stage, and how noisy or detailed intermediate summaries look like.\n-**Missing analysis of empirical running time** \ncomments,_suggestions_and_typos\n**Suggestions** -Line 249: \u201cHuge time costs\u201d sounds very informal, maybe change to \u201cconsiderable running time\u201d or similar -Please indicate if f-score, precision or recall is used when ROUGE is mentioned -Follow-up on weakness 1): ignoring input segments could be implemented by assigning \u201cempty\u201d targets, e.g. using a single special token that when predicted would trigger a segment to be discarded in the next step.\n**Questions** -Table 9 in Appendix: what does \u201ckeyword emerged in gold summary\u201d mean?\n-\u201cSeparate model for each stage\u201d is mentioned, but I am still not sure whether this means 2 (fine and coarse) or N models?\n-Does the number of input segments at each coarse stage always stay the same as it was in stage 1, before reaching the fine-grained stage? ", "label": [[525, 659, "Eval_pos_1"], [661, 833, "Eval_pos_2"], [835, 885, "Eval_pos_3"], [910, 955, "Eval_neg_1"], [957, 1582, "Jus_neg_1"], [1584, 1631, "Eval_neg_2"], [1691, 1729, "Eval_neg_3"], [1731, 1785, "Jus_neg_3"]]}
{"id": 105, "review": "This work proposes a self-learning bootstrapping approach to learning bilingual word embeddings, which achieves competitive results in tasks of bilingual lexicon induction and cross-lingual word similarity although it requires a minimal amount of bilingual supervision: the method leads to competitive performance even when the seed dictionary is extremely small (25 dictionary items!) or is constructed without any language pair specific information (e.g., relying on numerals shared between languages).  The paper is very well-written, admirably even so. I find this work 'eclectic' in a sense that its original contribution is not a breakthrough finding (it is more a 'short paper idea' in my opinion), but it connects the dots from prior work drawing inspiration and modelling components from a variety of previous papers on the subject, including the pre-embedding work on self-learning/bootstrapping (which is not fully recognized in the current version of the paper). I liked the paper in general, but there are few other research questions that could/should have been pursued in this work. These, along with only a partial recognition of related work and a lack of comparisons with several other relevant baselines, are my main concern regarding this paper, and they should be fixed in the updated version(s).\n*Self-learning/bootstrapping of bilingual vector spaces: While this work is one of the first to tackle this very limited setup for learning cross-lingual embeddings (although not the first one, see Miceli Barone and more works below), this is the first truly bootstrapping/self-learning approach to learning cross-lingual embeddings. However, this idea of bootstrapping bilingual vector spaces is not new at all (it is just reapplied to learning embeddings), and there is a body of work which used exactly the same idea with traditional 'count-based' bilingual vector spaces. I suggest the authors to check the work of Peirsman and Pado (NAACL 2010) or Vulic and Moens (EMNLP 2013), and recognize the fact that their proposed bootstrapping approach is not so novel in this domain. There is also related work of Ellen Riloff's group on bootstrapping semantic lexicons in monolingual settings.\n*Relation to Artetxe et al.: I might be missing something here, but it seems that the proposed bootstrapping algorithm is in fact only an iterative approach which repeatedly utilises the previously proposed model/formulation of Artetxe et al. The only difference is the reparametrization (line 296-305). It is not clear to me whether the bootstrapping approach draws its performance from this reparametrization (and whether it would work with the previous parametrization), or the performance is a product of both the algorithm and this new parametrization. Perhaps a more explicit statement in the text is needed to fully understand what is going on here.\n*Comparison with prior work: Several very relevant papers have not been mentioned nor discussed in the current version of the paper. For instance, the recent work of Duong et al. (EMNLP 2016) on 'learning crosslingual word embeddings without bilingual corpora' seems very related to this work (as the basic word overlap between the two titles reveals!), and should be at least discussed if not compared to. Another work which also relies on mappings with seed lexicons and also partially analyzes the setting with only a few hundred seed lexicon pairs is the work of Vulic and Korhonen (ACL 2016) 'on the role of seed lexicons in learning bilingual word embeddings': these two papers might also help the authors to provide more details for the future work section (e.g., the selection of reliable translation pairs might boost the performance further during the iterative process). Another very relevant work has appeared only recently: Smith et al. (ICLR 2017) discuss 'offline bilingual word vectors, orthogonal transformations and the inverted softmax'. This paper also discusses learning bilingual embeddings in very limited settings (e.g., by relying only on shared words and cognates between two languages in a pair). As a side note, it would be interesting to report results obtained using only shared words between the languages (such words definitely exist for all three language pairs used in the experiments). This would also enable a direct comparison with the work of Smith et al. (ICLR 2017) which rely on this setup.\n*Seed dictionary size and bilingual lexicon induction: It seems that the proposed algorithm (as discussed in Section 5) is almost invariant to the starting seed lexicon, yielding very similar final BLI scores regardless of the starting point. While a very intriguing finding per se, this also seems to suggest an utter limitation of the current 'offline' approaches: they seem to have hit the ceiling with the setup discussed in the paper; Vulic and Korhonen (ACL 2016) showed that we cannot really improve the results by simply collecting more seed lexicon pairs, and this work suggests that any number of starting pairs (from 25 to 5k) is good enough to reach this near-optimal performance, which is also very similar to the numbers reported by Dinu et al. (arXiv 2015) or Lazaridou et al. (ACL 2015). I would like to see more discussion on how to break this ceiling and further improve BLI results with such 'offline' methods. Smith et al. (ICLR 2017) seem to report higher numbers on the same dataset, so again it would be very interesting to link this work to the work of Smith et al. In other words, the authors state that in future work they plan to fine-tune the method so that it can learn without any bilingual evidence. This is an admirable 'philosophically-driven' feat, but from a more pragmatic point of view, it seems more pragmatic to detect how we can go over the plateau/ceiling which seems to be hit with these linear mapping approaches regardless of the number of used seed lexicon pairs (Figure 2).\n*Convergence criterion/training efficiency: The convergence criterion, although crucial for the entire algorithm, both in terms of efficiency and efficacy, is mentioned only as a side note, and it is not entirely clear how the whole procedure terminates. I suspect that the authors use the vanishing variation in crosslingual word similarity performance as the criterion to stop the procedure, but that makes the method applicable only to languages which have a cross-lingual word similarity dataset. I might be missing here given the current description in the paper, but I do not fully understand how the procedure stops for Finnish, given that there is no crosslingual word similarity dataset for English-Finnish.\n*Minor: -There is a Finnish 'Web as a Corpus' (WaC) corpus (lines 414-416): https://www.clarin.si/repository/xmlui/handle/11356/1074 -Since the authors claim that the method could work with a seed dictionary containing only shared numerals, it would be very interesting to include an additional language pair which does not share the alphabet (e.g., English-Russian, English-Bulgarian or even something more distant such as Arabic and/or Hindi).\n*After the response: I would like to thank the authors for investing their time into their response which helped me clarify some doubts and points raised in my initial review. I hope that they would indeed clarify these points in the final version, if given the opportunity. ", "label": [[506, 556, "Eval_pos_1"], [557, 584, "Eval_pos_2"], [585, 973, "Jus_pos_2"], [975, 1003, "Major_claim"], [1005, 1096, "Eval_neg_1"], [1116, 1158, "Eval_neg_2"], [1163, 1223, "Eval_neg_3"], [1660, 1729, "Eval_neg_1"], [1730, 2209, "Jus_neg_1"], [2896, 2999, "Eval_neg_3"], [3000, 4398, "Jus_neg_3"], [5963, 6173, "Eval_neg_4"], [6174, 6635, "Jus_neg_4"]]}
{"id": 106, "review": "paper_summary\nThe authors introduce a new annotated event detection dataset that is focused on annotation suicidal events. \nThe annotation focuses on the following event types: suicide-related actions, thoughts/ideation, risk factors related to life, relationship, health, and other, in addition to protective factors related to positive activities such as taking medication. \nThe author apply state-of-the-art event detection models on their dataset, where the performance is poorer in comparison to more established event detection domains. As a result, the authors are calling the community to build models that can improve the performance. \nsummary_of_strengths\n- The developed dataset is interesting and the task is very important to NLPers who work on mental health. I see its impact going beyond the event detection task to other tasks in the same domain that are related to understanding what kind of triggers affect suicidality. This, hopefully, would yield better performance of the suicide risk assessment and other mental-health related classifiers.  -The fact that the annotators are mental health domain experts makes of a more reliable data.  -I find the discussion of the challenges of annotations very valuable and it resonates with many previous research where cases of someone talking about a friend trying to committing suicide should be differentiated than the person who is posting attempting suicide.  -The baseline models that the authors use are strong and recent.  -Thorough reproducibility check list (e.g model parameters, annotation examples) \nsummary_of_weaknesses\n- Although it is not uncommon to see the \"OTHER\" label in annotation schema as a candidate label when the annotators cannot find a better mapping with the rest of the labels, I think OTHER label makes the annotations weaker. The annotators would default to it in many cases and as a result the number of annotations for that label become much higher in comparison to the other labels (in your case 15343 for OTHER vs. 21635 for the rest of the labels). This would generate problems such as data imbalance, potential noisy labels where the model might over predict the OTHER label.  -The call to the community by the authors for better performance models is a valid one, but we cannot eliminate putting some burden of the poor performance on the dataset itself. This might be due to embedded annotations' issues that can be simply a result of the domain itself. Did the authors do a thorough analysis to confirm that hypothesis?\n-I think it is very important to see the performance of the models on each label separately. For instance, a valuable analysis would be to compare the performance per label with the annotation challenges. \ncomments,_suggestions_and_typos\n- Please check the weaknesses section for suggestions on how to improve the work. For instance, I would emphasize again that the readers need to see the performance on each event type to better understand the challenges and how to improve the models. \nFor RF-HEALTH event type, you mention that it is about mentions that directly affect the subject's health. This can be ambiguous, for instance you might have a transitive relationship between life events and health, thus the event types are not exclusive (as you mention this was part of your annotation design). Could you please elaborate more on that? \nIs your dataset intersected with CLPsych 2019 dataset (Zirikly et al. 2019)? It would be very interesting and valuable if we have the ED annotations on that dataset to further push the performance of the risk assessment models with the use of the triggers.   Typos: Preposition is missing after models on line 276 ", "label": [[668, 704, "Eval_pos_1"], [709, 771, "Eval_pos_2"], [773, 937, "Eval_pos_3"], [938, 1061, "Jus_pos_3"], [1064, 1156, "Eval_pos_4"], [1159, 1227, "Eval_pos_5"], [1232, 1423, "Jus_pos_5"], [1426, 1489, "Eval_pos_6"], [1492, 1572, "Eval_pos_7"], [1597, 1819, "Eval_neg_1"], [1820, 2175, "Jus_neg_1"], [2178, 2355, "Eval_neg_2"], [2356, 2522, "Jus_neg_2"], [3013, 3142, "Eval_neg_3"], [3143, 3367, "Jus_neg_3"]]}
{"id": 109, "review": "This paper introduces Neural Symbolic Machines (NSMs) --- a deep neural model equipped with discrete memory to facilitate symbolic execution. An NSM includes three components: (1) a manager that provides weak supervision for learning, (2) a differentiable programmer based on neural sequence to sequence model, which encodes input instructions and predicts simplified Lisp programs using partial execution results stored in external discrete memories. ( 3) a symbolic computer that executes programs and provide code assistance to the programmer to prune search space. The authors conduct experiments on a semantic parsing task (WebQuestionsSP), and show that (1) NSM is able to model language compositionality by saving and reusing intermediate execution results, (2) Augmented REINFORCE is superior than vanilla REINFROCE for sequence prediction problems, and (3) NSM trained end-to-end with weak supervision is able to outperform existing sate-of-the-art method (STAGG).\n- Strengths - The idea of using discrete, symbolic memories for neural execution models is novel.                    Although in implementation it may simply reduce to copying previously executed variable tokens from an extra buffer, this approach is still impressive since it works well for a large-scale semantic parsing task.\n- The proposed revised REINFORCE training schema using imperfect hypotheses derived from maximum likelihood training is interesting and effective, and could inspire future exploration in mixing ML/RL training for neural sequence-to-sequence models.\n- The scale of experiments is larger than any previous works in modeling neural execution and program induction. The results are impressive.\n- The paper is generally clear and well-written, although there are some points which might require further clarification (e.g., how do the keys ($v_i$'s in Fig. 2) of variable tokens involved in computing action probabilities? \nConflicting notations: $v$ is used to refer to variables in Tab. 1 and memory keys in Fig 1.).\nOverall, I like this paper and would like to see it in the conference.\n- Weaknesses - [Choice of Dataset] The authors use WebQuestionsSP as the testbed. Why not using the most popular WebQuestions (Berant et al., 2013) benchmark set? Since NSM only requires weak supervision, using WebQuestions would be more intuitive and straightforward, plus it could facilitate direct comparison with main-stream QA research.\n- [Analysis of Compositionality] One of the contribution of this work is the usage of symbolic intermediate execution results to facilitate modeling language compositionality. One interesting question is how well questions with various compositional depth are handled. Simple one-hop questions are the easiest to solve, while complex multi-hop ones that require filtering and superlative operations (argmax/min) would be highly non-trivial. The authors should present detailed analysis regarding the performance on question sets with different compositional depth.\n- [Missing References] I find some relevant papers in this field missing. For example, the authors should cite previous RL-based methods for knowledge-based semantic parsing (e.g., Berant and Liang., 2015), the sequence level REINFORCE training method of (Ranzato et al., 2016) which is closely related to augmented REINFORCE, and the neural enquirer work (Yin et al., 2016) which uses continuous differentiable memories for modeling neural execution.\n- Misc.\n- Why is the REINFORCE algorithm randomly initialized (Algo. 1) instead of using parameters pre-trained with iterative ML?\n- What is KG server in Figure 5? ", "label": [[988, 1072, "Eval_pos_1"], [1091, 1241, "Eval_pos_2"], [1242, 1301, "Jus_pos_2"], [1305, 1551, "Eval_pos_3"], [1554, 1664, "Eval_pos_4"], [1665, 1692, "Eval_pos_5"], [1695, 1741, "Eval_pos_6"], [1751, 1814, "Eval_neg_1"], [1815, 2015, "Jus_neg_1"], [2017, 2087, "Major_claim"], [3018, 3068, "Eval_neg_2"], [3069, 3446, "Jus_neg_2"]]}
{"id": 110, "review": "paper_summary\nThis paper presents a new dataset - DISAPERE, which includes discourse related annotations over scientific peer reviews and rebuttals. Each review is paired with the first rebuttal text. The authors develop four levels for review annotation: (1) review-action, (2) aspect, (3) polarity, and (4) fine-review action; and two levels for rebuttals: (1) argumentative and (2) non-argumentative ones. This dataset could help better characterize the intentions and interactions between reviewers and authors, which in turn can assist decision making for area chair.\nThe authors also tested two machine learning tasks: (1) sentence classification for the proposed schema, and (2) sentence ranking to determine the context mapping from rebuttal to review. Preliminary results show that pre-trained transformer models achieve moderate performance, therefore leaving room for future work. \nsummary_of_strengths\n1. This work releases a new dataset of 506 review-rebuttal pairs with sentence level annotation for discourse related aspects. \n1. The proposed taxonomy is comprehensive and captures various aspects of peer review text. \n1. The authors framed practical machine learning tasks over the dataset, and benchmarked performance of baseline transformer models. \n1. In the updated draft, the authors have accounted for the majority of the comments in the previous review. Overall the paper is more clear, and certain minor mistakes have been rectified. \nsummary_of_weaknesses\nN/A \ncomments,_suggestions_and_typos\n- The hyperlink for footnote 3 and 4 do not seem to work.  -Line 172: an argument level -> on argument level ", "label": [[1045, 1134, "Eval_pos_1"], [1378, 1459, "Eval_pos_2"]]}
{"id": 111, "review": "The authors proposed an unsupervised algorithm for Universal Dependencies that does not require training. The tagging is based on PageRank for the words and a small amount of hard-coded rules. \nThe article is well written, very detailed and the intuition behind all prior information being added to the model is explained clearly. \nI think that the contribution is substantial to the field of unsupervised parsing, and the possibilities for future work presented by the authors give rise to additional research. ", "label": [[194, 331, "Eval_pos_1"], [332, 414, "Eval_pos_2"], [419, 512, "Eval_pos_3"]]}
{"id": 113, "review": "paper_summary\nThis paper presentes a multi task learning approach for automatic grading of English essays, by considering a holistic score as well as scores on individual essay traits. The authors proposed an LSTM based model and compared single task and multi task settings to show that Multi-task learning based system gives better performance, and is also much faster than the single task setup. They also present a comparison with a BERT model, and report a series of ablation tests to understand the relationship between traits and holistic scores. \nsummary_of_strengths\nIn Automatic Essay Scoring research, it is more common to develop models for a holistic scoring, although in general, there is an agreement that a score may have many dimensions (e.g., content, spelling/grammar, organization etc). This paper is among the few papers in the direction of modeling multiple dimensions of essay scoring.\nThey performed ablation tests in the multi task learning setup, to understand what traits are useful for each set of essays - I think this is an interesting experiment I did not see before in this task's context.\nThey use a popular dataset which is publicly available and uploaded their code along with the paper. \nsummary_of_weaknesses\n-  The paper does not seem to have any comparison with previous work on this topic at all. They directly do different experiments using their own architecture. Simple baselines (e.g., document length) that are commonly used for this problem can be used as a comparison point, in a STL setup, where the predicted variable can be different (holistic score, individual trait scores), keeping the text representation constant.\n- The paper misses a discussion on the limitations of the current approach. For example, the authors commented in the response pdf to one of the reviewers that performance difference across traits is due to topical variation. The modeling process does not have any specific component to account for such topical variations resulting in different scores. It could be a potential limitation. I am not saying a paper is bad because it has a limitation. I think acknowledging limitations gives a more holistic perspective for the reader about the approach. \ncomments,_suggestions_and_typos\nI reviewed the previous version of this paper, and most of the minor comments I mentioned have been addressed in this version.  One other comment:  -How are the trait  scores obtained for the prompts that did not have them in the original dataset? The authors claim they took from another source, but understanding how they are created is relevant for this paper.  - I think having a few points of comparison to your approach will give a better perspective for us as readers. Comparison between LSTM and BERT is good, but not enough, as  this topic still has a dominant approach of combining some linguistic features with neural models, for modeling overall score. For individual traits too, predicting the trait score instead of individual score, keeping rest of the set up same, may give you a quick comparison point, and make the paper more complete. ", "label": [[909, 1032, "Jus_pos_1"], [1035, 1121, "Eval_pos_1"], [1249, 1336, "Eval_neg_1"], [1337, 1668, "Jus_neg_1"], [1671, 1744, "Eval_neg_2"], [1745, 2222, "Jus_neg_2"], [2622, 2730, "Eval_neg_3"], [2731, 3109, "Jus_neg_3"]]}
{"id": 115, "review": "paper_summary\nThis paper investigates the degree of knowledge that pre-trained LM, with only access to a vocabulary of subword tokens, have about the character composition of these tokens, and if enriching these models with orthographic information about the tokens can improve them. It proposes to use a probe they name SpellingBee: a generative character-based LM that takes as input an uncontextualized word embedding from a model, and tries to predict the correct character sequence. It is trained on part of the model's vocabulary, and tested on the other: if it manages to succesfully generalize, the embedding must contain orthographic information. The probe is tested on 4 models: Roberta-base, and 3 others showing change in a particular aspect: Roberta-large for size, AraBert for language, and GPT2-Medium for an autoregressive model. The probe's capacity to predict the character sequence is evaluated by counting the exact matches, and with a finer-grained metric measuring overlap. Compared to a control experiment where the probe is not trained and only randomly initialized, the probe is able to better rebuild character sequences when fed with embeddings from the LMs (up to 30-40% from 0 for exact matches); however, it's performance is weakened when the training part of the vocabulary is filtered (removing token too similar to those in the testing part, or with the same lemma). However, using a probe trained on the full vocabulary as a way to initialize a LM does not seem to be useful, as the LM reaches the same training loss as a control one rather quickly. \nsummary_of_strengths\n- This paper poses a research question of great interest, and answers it while carefully considering many possible factors (models, filtering the training vocabulary).\n-It investigates a potential application of this answer.\n-The paper is very clearly written, and easy to follow. \nsummary_of_weaknesses\n- The results given by the probe are a little difficult to interpret; as, while there is a control experiment where the probe has no information, it would be very useful to have an idea of what the probe can do when fed with embeddings that we know contain orthographic information. \nIf testing the probe on static embeddings (word2vec, glove), fasttext embeddings could work; in this setting, I believe uncontextualized embeddings from CharacterBERT (El Boukkouri et al, 2020) could work. \ncomments,_suggestions_and_typos\n- The samples of errors shown in Table 3 seem to often have the first, or few first characters right. Did you at some point try to filter by prefix rather than lemmas ? ", "label": [[1608, 1662, "Eval_pos_1"], [1668, 1728, "Eval_pos_2"], [1729, 1772, "Jus_pos_2"], [1832, 1887, "Eval_pos_3"], [1912, 1979, "Eval_neg_1"], [1980, 2400, "Jus_neg_1"]]}
{"id": 118, "review": "paper_summary\nThis paper proposes to learn discriminative representations for open relation extraction. In specific, the authors first introduce three data augmentation strategies to generate positive, hard negative, and semi-hard negative samples. Then, the proposed model not only uses instance ranking to optimize each instance's relation representations but also learns relation representations by grouping them together. Experimental results demonstrate the effectiveness of the proposed method. So, I recommend accepting the paper as a short paper. \nsummary_of_strengths\n1. The presentation is clear.\n2. The experiments are convincing as compared with six SOTA baselines and three variants of the proposed model.\n3. The method is simple yet effective. \nsummary_of_weaknesses\nDoes the proposed method heavily depends on the data augmentation quality? It is better to give further discussion. \ncomments,_suggestions_and_typos\nNA ", "label": [[426, 500, "Eval_pos_4"], [501, 554, "Major_claim"], [580, 606, "Eval_pos_1"], [610, 640, "Eval_pos_2"], [641, 718, "Jus_pos_2"], [722, 757, "Eval_pos_3"], [856, 896, "Eval_neg_1"]]}
{"id": 120, "review": "This paper introduces a joint decoder model that generates both transcript and translation, conditioned on some speech utterance as input. The decoder model, on an intuitive level, decodes transcript and translation tokens jointly with separately parameterized decoders which are conditioned not only on the source speech, but also on the (partial) hidden representations of each other. While there exists a related prior paper, this paper introduces a tighter coupling, alongside a more comprehensive evaluation with stronger baselines and comparison across a large number of setting. The conclusions are convincing. The description of both high-level intuitions and low-level details is excellent and makes this paper very interesting to readers. A couple of years into research on end-to-end models on speech translation, with much focus on direct models that do not create transcripts at all, turning toward end-to-end models that do create both transcripts and translations has been a recent trend, making this paper timely and relevant.\nWhile the paper is strong as-is, there are some weaknesses which if addressed would lead to an even stronger camera-ready version: -The paper does not discuss linguistic aspects in detail. In particular, I'd appreciate some more discussion on why decisions on transcripts should lead to improved translations. This paper gives empirical evidence, and justifies the approach from an engineer's perspective only. To make this suggestion more precise: In the intro, please consider elaborating further on this sentence: \"We believe that these are two edge design choices and that a tighter coupling of ASR and MT is desirable for future end-to-end ST applications.\"\n-The focus seems to be on improving translations only (and probably lead to the choice of alpha=0.3 for the training objective). This is a reasonable choice, but should be stated more explicitly, given that one could also target improvements in both BLEU score *and* WER. In fact, the proposed model seems to experience a trade-off between translation accuracy and transcription accuracy, which in itself is a very interesting observation. It might be worth citing a highly relevant, concurrent work that goes the other direction, assuming that both translation and transcript are of equal importance: https://arxiv.org/pdf/2007.12741.pdf  . Another related work to cite might be https://ieeexplore.ieee.org/document/5947637 who have reported on BLEU/WER tradeoff quite a few years ago.\n-On \"chained decoders\": are these conceptually related / identical to http://arxiv.org/abs/1802.06655 ?\n-Evaluation: please do not say \"significant\" unless you have actually formally verified statistical significance, in which case it would be necessary to report the details of the stat. significance check. Also, the standard nowadays is to use sacreBLEU to compute comparable BLEU scores (please see https://www.aclweb.org/anthology/W18-6319/ on why it is impossible to compare BLEU scores when the tokenization details are not known or not consistent).\n-typo: \"weekly tight\" -> \"weakly tied\" ", "label": [[387, 585, "Eval_pos_1"], [586, 617, "Eval_pos_2"], [618, 748, "Eval_pos_3"], [749, 1003, "Jus_pos_4"], [1004, 1041, "Eval_pos_4"], [1043, 1173, "Major_claim"], [1175, 1231, "Eval_neg_1"], [1232, 1352, "Jus_neg_1"], [1353, 1453, "Eval_neg_2"], [1454, 1705, "Jus_neg_2"]]}
{"id": 121, "review": "Related to a recent notion of Visual Dialog tasks, the paper introduces and evaluates a sophisticated neural architecture to answer queries related to images through a dialog (a sequence of several queries and answers). \nThere are several components in the system but the paper focuses on two of them, namely VTA to map visual features to textual features found in the dialog history and current query; and VGAT to build graphs from these visual-textual pairs An evaluation is done on the VisDial dataset for 5 metrics and with comparison with several SOTA systems covering different approaches. The proposed system performs best for all metrics. Ablation seems to confirm the interest of both components (VTA and VGAT), even if the results are maybe not so significative. A few exemples are provided for illustration. \nThe task and the proposed architecture are interesting. However, it is difficult to follow the details of the model, also because of (maybe) some errors. There are also a lot a parameters and some hypothesis that may be discussed. For instance, it is unclear why a graph representation is really needed instead of some ranking of the visual-textual pairs and how exactly the graph is exploited, a priori (from Fig. 4) the top-5 strongest connections in the graph More specific remarks: - in 3.1, is the number k of visual features a fixed parameter (and then which value is used for the experiments) or a parameter depending on the image being processed ?\n-In eq. (1), (2), and maybe (3) and (4), I wonder if i should range from 1 to h rather than from 1 to k ?\n-in 3.3, I don't understand what you mean by \"homogenous information\" -in 3.3, what do you mean by two textual operations (with different colors) -the construction of the sequence of graphs in 3.3 is really unclear; in eq (10), do you mean G(i>0) rather than G(i) ? If I understand correctly, G(i=0) and G(i>0) are a kind of serialization of the graphs but it is unclear if e(i>0)=e(i) ? In eq (6), are you using new multiheads or are they related to those defined in 3.2 ? Why k iteration steps ? Is it to identify at each step (i) the most interesting neighbor nodes for node (i) ?\n-in 3.1: how if build the list of 100 answers for each query ? Are some false answers randomly added to the right answer, or a specific set of 100 answers is provided for each query ? It is a single set of 100 answers for each dialog ? ", "label": [[820, 875, "Eval_pos_1"], [876, 973, "Eval_neg_1"], [975, 1050, "Eval_neg_2"], [1051, 1282, "Jus_neg_2"]]}
{"id": 122, "review": "paper_summary\nThe paper proposes a new approach named BEEP to combine information from clinical notes and relevant medical literature to enhance the prediction of patient outcomes (prolonged mechanical ventilation, in-hospital mortality and length of stay). The medical literature is retrieved from PubMed and then ranked per relevance. The embeddings of clinical notes and top relevant literature are then combined to predict the patient outcomes on MIMIC-III datasets. Experiments show improved accuracy on 2 out of 3 tasks. \nsummary_of_strengths\n- Overall well-written narratives with clear descriptions of methodology and experiments, though a lot of information is in the appendix which makes it a bit difficult to switch between main content and appendix.\n- The idea of retrieving medical literature to enhance and provide evidence to patient outcome prediction is attractive. The proposed methods of retrieval and reranking are reasonable.\n- Experiments are well established and clear. The proposed method BEEP shows advantages on PMV and MOR tasks. \nsummary_of_weaknesses\n- The proposed method heavily relies on BERT-based encoders and BERT has a word limit of 512 tokens. But most discharge summaries in MIMIC-III have much more than 512 tokens. This may mean a lot of information in discharge summaries is truncated and the model may not be able to build a comprehensive representation of patient condition.\n- The reliability and interoperability of the proposed method are in doubt based on Figure 3 which shows a high percentage of unhelpful literature is retrieved especially for the LOS task. How will such unhelpful literature impact patient outcomes? How can this be improved?\n- The performance on LOS is not convincing and the paper does not provide much insight on why.\n- The experiments do not seem to consider structured features at all (e.g. 17 clinical features from [1] based on MIMIC-III) which however are critical for patient outcome prediction from both clinical and ML perspectives [2]. The experiments may need a baseline that leverages structured features to show the advantage of using clinical notes and interpret BEEP's performance.\n[1] https://www.nature.com/articles/s41597-019-0103-9 [2] https://arxiv.org/abs/2107.11665 \ncomments,_suggestions_and_typos\n- In the abstract and experiment section, expressions like \"5 points\" are confusing. \" 5% increase\" or \"0.05 increase\" would be clearer.\n- In the abstract, what is \"increasing F1 by up to t points and precision @Top-K by a large margin of over 25%\" based on? The paper may make the statement clearer by mentioning the specific setup that achieves the largest improvement margin.\n- Based on Appendix B, the bi-encoder is trained on TREC 2016 with 30 EHRs. The paper may discuss how representative these 30 EHRs are to MIMIC-III EHRs. Also as 30 is rather small, the paper may discuss whether it is enough empirically.\n- Line 559, the paper may discuss why the model does not perform well on LOS, why a high percentage of unhelpful literature are retrieved (even for correct predictions) and how such a high percentage of unhelpful literature impact the reliability of the model.\n- The paper may discuss why use MeSH rather than other ontologies like UMLS, SNOMED, HPO etc.\n- Are those 8 categories in Appendix I mutually exclusive? ", "label": [[551, 761, "Eval_pos_1"], [764, 882, "Eval_pos_2"], [883, 946, "Eval_pos_3"], [949, 992, "Eval_pos_4"], [993, 1056, "Eval_pos_5"], [1420, 1492, "Eval_neg_1"], [1493, 1605, "Jus_neg_1"], [1695, 1787, "Eval_neg_2"], [1790, 1856, "Eval_neg_3"], [1856, 1912, "Jus_neg_3"], [1913, 2014, "Eval_neg_3"], [2292, 2374, "Eval_neg_4"], [2375, 2426, "Jus_neg_4"]]}
{"id": 123, "review": "paper_summary\nThe paper deals with multi-task learning. The authors find that having separate networks to learn separate tasks would lead to good performance, but requires a large memory. Using MT-DNN would save memory, but the results are not satisfying. The authors thus propose an approach that saves memory and at the same time achieves good results on GLUE.\nFigure 1 gives a good summary. First, train task-specific models (but for each model, only finetune the top n layers). Second, do knowledge distillation, so that we can compress the n layers into a smaller number of layers. Third, merge all the models together as shown in Figure 1(c). The GLUE performance is comparable to full fine-tuning (i.e., tuning k models separately where k is the number of tasks), and the proposed approach saves 2/3 of memory. \nsummary_of_strengths\n- Writing is clear.\n-Many baseline experiments are performed, including DistillBERT, BERT-of-Theseus, MT-DNN, and many others.  -The observation about MT-DNN\u2019s degradation on QQP (line 237) is interesting. This observation reminds me of the intermediate training empirical paper, where in Table 1 they find that QQP is a very different task compared to others: https://arxiv.org/pdf/2005.00628.pdf \nsummary_of_weaknesses\nOther models -Another simple baseline is to have two separate models: one for tasks that lead to \u201ctask interference\u201d like QQP, another for other tasks. I wonder if this baseline will perform better than the authors\u2019 approaches (both in terms of memory and performance). There could be other ways of clustering the tasks.  -Adapter is a widely used framework that performs well for MTL. Although not directly connected to the authors' approach, I think that the authors should at least discuss it a bit more in the paper. Given the adaptor framework, would the authors\u2019 approaches perform better? Are the authors\u2019 approaches complementary?  Experimental details -Given that this is an empirical paper, I believe that much more detailed descriptions on hyperparameters (for example, on each task) are necessary.  More tasks -Relatively minor: Efficient BERT related papers recently often report SQuAD results as well, given that it\u2019s a different format (span selection instead of multiple choice) and the skillset may be different from GLUE. Do authors think that their approach would adapt to SQuAD?  Motivation -Relatively minor: If there is a much larger number of tasks, the authors\u2019 approach may not be as efficient as shown in table 2 (i.e., two thirds of memory), given that the authors\u2019 approach is still O(k) where k is the number of task. \ncomments,_suggestions_and_typos\nAbstract: \u201coverhead\u201d -> it\u2019ll be great to elaborate what overhead you\u2019re referring to (especially because this is the abstract).\nMinor: commas should follow \u201ci.e.\u201d Line 136: \u201cWe find that as long as the student model is properly initialized, the vanilla KD can be as performant as those more sophisticated methods\u201d <- it'll be great if the authors can elaborate. ", "label": [[842, 859, "Eval_pos_1"], [861, 901, "Eval_pos_2"], [902, 964, "Jus_pos_2"], [969, 1045, "Eval_pos_3"], [1046, 1238, "Jus_pos_3"], [1646, 1781, "Eval_neg_1"], [1782, 1899, "Jus_neg_1"], [1923, 2027, "Eval_neg_2"], [2028, 2055, "Jus_neg_2"], [2056, 2070, "Eval_neg_2"]]}
{"id": 124, "review": "paper_summary\nThe authors aim to improve interpretability for structure and style control in knowledge-grounded conversational models. They propose to use two sequential latent variables for structure and style respectively.  1) m - binary indicator for segment boundaries within a sentence 2) z - style controller attribute to switch between content, knowledge, and style decoders. They use a variational framework for training using an evidence lower bound of the likelihood. Overall their encoder-decoder model outperforms the baselines in automatic and human metrics on two knowledge grounded dialog datasets WizardsOfWikipedia and CMU_DoG. Their models is also more robust and generalizable as it consistently outperforms the baselines with 10% or lower amount of in-domain data. While adding adapters to their decoders they can adjust the style (sentiment) of their responses while maintaining decent performance in automatic evaluation. Their metrics pklg and lklg suggest that their models can easily adapt the latent variables' distribution for different datasets. \nsummary_of_strengths\n- Interpretable model which predicts segment boundaries and is able to switch style decoders based on the context. Potentially useful for many applications.\n-Robust and generalizable model which can easily adapt to new styles with limited training data. \nsummary_of_weaknesses\n- The writing structure and flow can be improved. Many of the crucial details regarding automatic labeling, baselines, human evaluation results etc. are moved to appendix which disrupts the reading flow. \ncomments,_suggestions_and_typos\n- Their model shows improvement in low resource setting. But, it will be interesting to see what are the overall gains compared to the baselines with 100% of the in-domain data.\n-missing section in line 492 ", "label": [[645, 695, "Eval_pos_1"], [696, 783, "Jus_pos_1"], [1098, 1209, "Eval_pos_1"], [1211, 1252, "Eval_pos_2"], [1254, 1348, "Eval_pos_2"], [1375, 1422, "Eval_neg_1"], [1423, 1576, "Jus_neg_1"]]}
{"id": 126, "review": "paper_summary\nPrior work has used interpretation to improve inference, while ignoring \u201cusing inference logic to enhance interpretation.\u201d This work deals with \u201cmutual promotion\u201d where they \u201cpromote\u201d in both directions. Specifically, the \u201cmutual promotion\u201d is done using stepwise integration mechanism (SIM; Section 2.2). Additionally, adversarial fidelity regularization is used to further \u201cimprove the fidelity of inference and interpretation\u201d (Section 2.3).   Essentially, during training, the model generates explanation first, and at the last time-step, generates the prediction. Therefore, the explanation and prediction are using largely the same parameters, and gradient updates would influence both the explanation and prediction.\nThe authors experiment on NLI and conversational QA tasks. The explanation is scored against the gold-standard human-provided evaluation results in e-SNLI and CoS-E datasets. \nsummary_of_strengths\nInterpretation is an important problem.  The figures are well-designed and help readers understand the algorithms.\nThe method performs well on out-of-domain datasets (MNLI and SICK-E).\nThe mutual information discussion is well-motivated. \nsummary_of_weaknesses\nI\u2019m not convinced that AFiRe (the adversarial regularization) brings significant improvement, especially because -BLEU improvements are small (e.g., 27.93->28.64; would humans be able to identify the differences?)\n-Hyperparameter details are missing.\n-Human evaluation protocols, payment, etc. are all missing. Who are the raters? How are they \"educated\" and how do the authors ensure the raters provide good-faith annotations? What is the agreeement?\nOther baselines are not compared against. For example, what if we just treat the explanation as a latent variable as in Zhou et al. (2021)? https://arxiv.org/pdf/2011.05268.pdf A few other points that are not fatal: -Gold-standard human explanation datasets are necessary, given the objective in line 307.  -Does it mean that inference gets slowed down drastically, and there\u2019s no way to only do inference (i.e., predict the label)? I don\u2019t think this is fatal though.  What\u2019s the coefficient of the p(L, E | X) term in line 307? Why is it 1?  Hyperparamter details are missing, so it\u2019s not clear whether baselines are well-tuned, and whether ablation studies provide confident results.  The writing is not careful, and often impedes understanding.\n-Line 229: What\u2019s t?\n-Line 230: What\u2019s n?\n-Line 273: having X in the equation without defining it is a bit weird; should there be an expectation over X?\n-Sometimes, the X is not bolded and not italicized (line 262). Sometimes, the X is not bolded but italicized (line 273). Sometimes, the X is bolded but not italicized (line 156).  -Line 296: L and E should be defined in the immediate vicinity. Again, sometimes L, E are italicized (line 296) and sometimes not (line 302).\n-Line 187: It\u2019s best to treat Emb as a function. Having l\u2019 and e\u2019 as superscripts is confusing.\n-In Table 4, why sometimes there are punctuations and sometimes there are no punctuations? \ncomments,_suggestions_and_typos\n- Perplexity does not necessarily measure fluency. For example, an overly small perplexity may correspond to repeating common n-grams. But it\u2019s okay to use it as a coarse approximation of fluency.\n-Line 191: \\cdot should be used instead of regular dot Section 2.1: It would be best to define the dimensionalities of everything.\n-Line 182: A bit confusing what the superscript p means.\n-Line 229: What\u2019s t?\n-Line 230: What\u2019s n?  -Line 255: Comma should not start the line. ", "label": [[935, 974, "Eval_pos_1"], [976, 1049, "Eval_pos_2"], [1050, 1100, "Eval_pos_3"], [1101, 1118, "Jus_pos_3"], [1120, 1172, "Eval_pos_4"], [1196, 1289, "Eval_neg_1"], [1290, 1409, "Jus_neg_1"], [1411, 1446, "Eval_neg_2"], [1448, 1506, "Eval_neg_3"], [1507, 1647, "Jus_neg_3"], [1648, 1689, "Eval_neg_4"], [1690, 1824, "Jus_neg_4"], [2192, 2226, "Jus_neg_5"], [2227, 2334, "Eval_neg_5"], [2336, 2396, "Eval_neg_6"]]}
{"id": 128, "review": "The paper describes a model for morphological segmentation. \nThe model is a neural network that takes as input a representation of the word to segment and a representation of the context. \nThe model is trained and tested on mongolian data and reaches good performances (98% f measure). \nThe work is sound and well conducted but lacks a well fromulated scientific question. \nThis question could be \"is context important for morphological analysis\" and the answer will be yes, with a quanfication of the role of context. \nBut such a question is not really novel.\nIn theory, the model could be applied on other languages, it would have been interesting to see the performances on different languages offering different morphological systems.\nthe paper is generally well written, there are some typos and some formulations are not very natural  some of the typos and/or questionable formulations: p1 close related -> closely related p1 several segmentation results -> different segmentation results ? \np2 artificial linguistics ?? what is this ?? \np2 LTMS -> LSTM p3 For Self-attention -> Self-attention p5 following Vaswani -> follows Vaswani p6 we used has annotated -> we used has been annotated p7 but there is the principle a confound -> not clear what that means p9 our-word -> ?? ", "label": [[189, 268, "Eval_pos_1"], [269, 284, "Jus_pos_1"], [287, 372, "Eval_neg_1"], [374, 738, "Jus_neg_1"], [739, 775, "Eval_pos_2"]]}
{"id": 129, "review": "paper_summary\nThis paper addresses the question of whether the spelling of words has been retained / encoded by large language models. First, it introduces a probe to discover the spelling of a word based on the embeddings in the model input. This probe, spelling bee, is essentially a character level language model which is conditioned on the word embedding. They find that a significant amount of information about spelling is retained by the embedding, and that explicitly providing information about spelling during training by using spelling bee does not provide additional advantage to the model. The authors conclude that this indicates language models \u201ccan quickly acquire all the character level information they need without directly observing the composition of each token\u201d. \nsummary_of_strengths\nI liked the fact that the training and test data splits considered the possibility that words in the test set might benefit too much from training  spelling bee on a train set with similarly spelled words.\nI think that the question of whether the model has implicitly learned character composition is valuable, and there should be some significant interest in this paper as a result. \nsummary_of_weaknesses\nI want to see a baseline that tests spelling bee on representations specifically optimized for morphology or spelling, so that we can see what the performance of this probe would be on complete information about spelling. As is, it's not clear what a true upper-bound performance would be for such a probe.\n I felt that the conclusions drawn from the attempt to train with additional spelling information were not well justified. After all, if training with the additional information actually damaged performance, the authors would not have concluded that the model doesn\u2019t use the information or that the information was somehow harmful or misleading. Instead, they would rightly assume that the particular procedure they were using to add that information was not providing it in a structure that the model could easily use. However, because it doesn\u2019t change  performance at all, the authors conclude that the model is able to acquire everything it needs without direct observation of the spelling. It\u2019s not clear to me that these results contradict the idea that some information about character composition might be able to help a model.\nThere needs to be more detail on the implementation of spelling bee. \ncomments,_suggestions_and_typos\nI felt that this paper could do with more citations to the existing literature on morphological/character composition in neural models (e.g., https://aclanthology.org/P17-1184/) ", "label": [[1015, 1192, "Eval_pos_1"], [1216, 1437, "Jus_neg_1"], [1438, 1522, "Eval_neg_1"], [1524, 1645, "Eval_neg_2"], [1646, 2359, "Jus_neg_2"], [2360, 2428, "Eval_neg_3"], [2462, 2596, "Eval_neg_4"], [2597, 2639, "Jus_neg_4"]]}
{"id": 130, "review": "- Strengths: Introduces  a new document clustering approach and compares it to several established methods, showing that it improves results in most cases. \nThe analysis is very detailed and thorough--quite dense in many places and requires careful reading.\nThe presentation is organized and clear, and I am impressed by the range of comparisons and influential factors that were considered. Argument is convincing and the work should influence future approaches.\n- Weaknesses:  The paper does not provide any information on the availability of the software described.\n- General Discussion: Needs some (minor) editing for English and typos--here are just a few: Line 124: regardless the size > regardless of the size Line 126: resources. Because > resources, because Line 205: consist- ing mk > consisting of mk Line 360: versionand > version and ", "label": [[13, 155, "Eval_pos_1"], [157, 257, "Eval_pos_2"], [258, 298, "Eval_pos_3"], [303, 391, "Eval_pos_4"], [392, 414, "Eval_pos_5"], [419, 463, "Eval_pos_6"]]}
{"id": 131, "review": "paper_summary\nThe paper investigates what linguistic features are beneficial for cross-lingual transfer for language models. Based on the assumption that the LM encoder is able to learn language-agnostic knowledge during a transfer, the authors construct a synthesized language with different linguistic features to probe for. Notably, the authors are interested in two features. The first is word distribution, for which a uniform distribution, a NL-like distribution based on Zipf's law, and another with word co-occurrences are considered. The second is dependency structure, for which a head-to-tail token grouping method is used.  During evaluation, the paper pre-trains LMs with causal language modeling objective on the synthesized languages and evaluate its perplexity on natural language data, and reaches a series of interesting observations about LSTM and Transformer on these languages with different linguistic features. For LMs with masked language modeling objective, downstream tasks such as POS tagging and dependency parsing are also considered.  The main takeaway is that LMs benefit the most from a nested structure from the synthesized language, with an especially strong downstream performance. \nsummary_of_strengths\n- The paper tackles the fascinating question of how LMs are able to transfer cross-lingually. Specifically, readers like myself who are aware that LMs _can_ transfer from L1 to L2 without the same alphabets, are interested in knowing _how_.  -The proposed approach of using a synthesized language as L1 is creative and reasonable. While the idea stems from (Papadimitriou and and Jurafsky, 2020), I still applaud the novelty, especially with a suite of improvements and additions to the method.  -The experiments seem sound.  -The observations are informed and faithful to the results, and are clearly presented.  -The paper is very well written. For a reader who is no expert on such line of probing work, it is very easy to follow. \nsummary_of_weaknesses\n- In Section 3.2.3, I struggle to understand the intuition and motivation behind the head-to-tail method. How does the fact that two tokens (i.e. \"<xyz\" and \"xyz>\") always appear in the same sentence help create a dependency relation? While the method seems to stem from Papadimitriou and Jurafsky (2020), it's desirable to explain a bit more for self-containment.  -In Section 3.2.2, I would appreciate more intuition behind the \"Random walk\" language. Mathematically (for someone who is not an expert on linear algebra), why would the dot product between a \"topic vector\" and a \"word vector\" give rise to \"non-trivial cooccurrence\"?  -I'd like to know why the authors perform different experiments on language models with CLM (causal) and MLM (masked) objectives. Specifically, why not also calculate perplexity of an MLM encoder on an NL L2? Why not also run the CLM encoder on some downstream tasks?  -For the CLM experiments, is it widely believed that perplexity is a faithful metric to evaluate \"performance\" of the encoder in question?\n-In Section 5.2, it's confusing to choose POS tagging as a downstream task, and then neglect it in the analysis because \"PoS tagging seems to require little structural knowledge on language\". I would appreciate a bit more discussion here. Does the linguistic features studied help with POS, or not? What is the purpose of reporting the POS experiment?\n-Circling back to the motivation of this work, which is the fact that \"an encoder only pretrained on L1 can be transferred to L2 without any parameter updates\". I wonder in what downstream tasks the said encoder transferred to? Are there any higher-level tasks such as NLI, QA, SRL, etc.? Would you consider other downstream tasks than dependency parsing? \ncomments,_suggestions_and_typos\nNone at this time. The paper is well-presented. ", "label": [[1241, 1332, "Eval_pos_1"], [1333, 1569, "Jus_pos_1"], [1570, 1664, "Eval_pos_2"], [1665, 1733, "Jus_pos_2"], [1736, 1763, "Eval_pos_3"], [1766, 1852, "Eval_pos_4"], [1854, 1885, "Eval_pos_5"], [1886, 1972, "Jus_pos_5"], [1998, 2101, "Eval_neg_1"], [2102, 2360, "Jus_neg_1"], [2363, 2449, "Eval_neg_2"], [2450, 2630, "Jus_neg_2"], [3041, 3231, "Eval_neg_3"], [3232, 3391, "Jus_neg_3"], [3800, 3828, "Eval_pos_6"]]}
{"id": 132, "review": "This is a nice paper on morphological segmentation utilizing word  embeddings. The paper presents a system which uses word embeddings to  both measure local semantic similarity of word pairs with a potential  morphological relation, and global information about the semantic validity of potential morphological segment types. The paper is well written and  represents a nice extension to earlier approaches on semantically driven  morphological segmentation.\nThe authors present experiments on Morpho Challenge data for three  languages: English, Turkish and Finnish. These languages exhibit varying  degrees of morphological complexity. All systems are trained on Wikipedia  text.  The authors show that the proposed MORSE system delivers clear  improvements w.r.t. F1-score for English and Turkish compared to the well  known Morfessor system which was used as baseline. The system fails to  reach the performance of Morfessor for Finnish. As the authors note, this  is probably a result of the richness of Finnish morphology which leads to  data sparsity and, therefore, reduced quality of word embeddings. To  improve the performance for Finnish and other languages with a similar  degree of morphological complexity, the authors could consider word  embeddings which take into account sub-word information. For example, @article{DBLP:journals/corr/CaoR16,   author    = {Kris Cao and                Marek Rei},   title     = {A Joint Model for Word Embedding and Word Morphology},   journal   = {CoRR},   volume    = {abs/1606.02601},   year                  = {2016},   url                 = {http://arxiv.org/abs/1606.02601},   timestamp = {Fri, 01 Jul 2016 17:39:49 +0200},   biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/CaoR16},   bibsource = {dblp computer science bibliography, http://dblp.org} } @article{DBLP:journals/corr/BojanowskiGJM16,   author    = {Piotr Bojanowski and                Edouard Grave and                Armand Joulin and                Tomas Mikolov},   title     = {Enriching Word Vectors with Subword Information},   journal   = {CoRR},   volume    = {abs/1607.04606},   year                  = {2016},   url                 = {http://arxiv.org/abs/1607.04606},   timestamp = {Tue, 02 Aug 2016 12:59:27 +0200},   biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/BojanowskiGJM16},   bibsource = {dblp computer science bibliography, http://dblp.org} } The authors critique the existing Morpho Challenge data sets. \nFor example, there are many instances of incorrectly segmented words in  the material. Moreover, the authors note that, while some segmentations  in the the data set may be historically valid (for example the  segmentation of business into busi-ness), these segmentations are no  longer semantically motivated. The authors provide a new data set  consisting of 2000 semantically motivated segmentation of English word  forms from the English Wikipedia. They show that MORSE deliver highly  substantial improvements compared to Morfessor on this data set.\nIn conclusion, I think this is a well written paper which presents  competitive results on the interesting task of semantically driven  morphological segmentation. The authors accompany the submission with  code and a new data set which definitely add to the value of the  submission. ", "label": [[326, 351, "Eval_pos_1"], [357, 457, "Eval_pos_2"], [3045, 3193, "Major_claim"], [3194, 3314, "Eval_pos_3"]]}
{"id": 133, "review": "TMP Strength: The paper propose DRL-Sense model that shows a marginal improvement on SCWS dataset and a significant improvement on ESL-50 and RD-300 datasets.\nWeakness: The technical aspects of the paper raise several concerns: Could the authors clarify two drawbacks in 3.2? The first drawback states that optimizing equation (2) leads to the underestimation of the probability of sense. As I understand, eq(2) is the expected reward of sense selection, z_{ik} and z_{jl} are independent actions and there are only two actions to optimize. \nThis should be relatively easy. In NLP setting, optimizing the expected rewards over a sequence of actions for episodic-task has been proven doable (Sequence Level Training with Recurrent Neural Networks, Ranzato 2015) even in a more challenging setting of machine translation where the number of actions ~30,000 and the average sequence length ~30 words. The DRL-Sense model has maximum 3 actions and it does not have sequential nature of RL. This makes it hard to accept the claim about the first drawback.\nThe second drawback, accompanied with the detail math in Appendix A, states that the update formula is to minimize the likelihood due to the log-likelihood is negative. Note that most out-of-box optimizers (Adam, SGD, Adadelta, \u2026) minimize a function f, however, a common practice when we want to maximize f we just minimize -f. Since the reward defined in the paper is negative, any standard optimizer can be use on the expected of the negative reward, which is always greater than 0. This is often done in many modeling tasks such as language model, we minimize negative log-likelihood instead of maximizing the likelihood. The authors also claim that when \u201cthe log-likelihood reaches 0, it also indicates that the likelihood reaches infinity and computational flow on U and V\u201d (line 1046-1049). Why likelihood\u2192infinity? Should it be likelihood\u21921?\nCould the authors also explain how DRL -Sense is based on Q-learning? The horizon in the model is length of 1. There is no transition between state-actions and there is not Markov-property as I see it (k, and l are draw independently). I am having trouble to see the relation between Q-learning and DRL-Sense.  In (Mnih et al., 2013), the reward is given from the environment whereas in the paper, the rewards is computed by the model. What\u2019s the reward in DRL-Sense? Is it 0, for all the (state, action) pairs or the cross-entropy in eq(4)?   Cross entropy is defined as H(p, q) = -\\sum_{x} q(x)\\log q(x), which variable do the authors sum over in (4)? I see that q(C_t, z_{i, k}) is a scalar (computed in eq(3)), while Co(z_{ik}, z_{jl}) is a distribution over total number of senses eq(1). These two categorial variables do not have the same dimension, how is cross-entropy H in eq(4) is computed then?\nCould the authors justify the dropout exploration? Why not epsilon-greedy exploration? Dropout is often used for model regularization, preventing overfitting. How do the authors know the gain in using dropout is because of exploration but regularization?\nThe authors states that Q-value is a probabilistic estimation (line 419), can you elaborate what is the set of variables the distribution is defined? When you sum over that set of variable, do you get 1? I interpret that Q is a distribution over senses per word, however  definition of q in eq(3) does not contain a normalizing constant, so I do not see q is a valid distribution. This also related to the value 0.5 in section 3.4 as a threshold for exploration. \nWhy 0.5 is chosen here where q is just an arbitrary number between (0, 1) and the constrain \\sum_z q(z) = 1 does not held? Does the authors allow the creation of a new sense in the very beginning or after a few training epochs? I would image that at the beginning of training, the model is unstable and creating new senses might introduce noises to the model.  Could the authors comment on that?\nGeneral discussion What\u2019s the justification for omitting negative samples in line 517? Negative sampling has been use successfully in word2vec due to the nature of the task: learning representation. Negative sampling, however does not work well when the main interest is modeling a distribution p() over senses/words. Noise contrastive estimation is often preferred when it comes to modeling a distribution. The DRL-Sense, uses collocation likelihood to compute the reward, I wonder how the approximation presented in the paper affects the learning of the embeddings.\nWould the authors consider task-specific evaluation for sense embeddings as suggested in recent research [1,2] [1] Evaluation methods for unsupervised word embeddings. Tobias Schnabel, Igor Labutov, David Mimno and Thorsten Joachims.\n[2] Problems With Evaluation of Word Embeddings Using Word Similarity Tasks . \nManaal Faruqui, Yulia Tsvetkov, Pushpendre Rastogi, Chris Dyer --- I have read the response. ", "label": [[14, 158, "Eval_pos_1"], [169, 226, "Eval_neg_1"], [228, 3921, "Jus_neg_1"]]}
{"id": 134, "review": "paper_summary\nThis is a revision of a previously submitted article. The article discusses scoring essays holistically and scoring their single traits. \nThe article clearly contains revisions, based on my suggestions and I can also see some of the revisions, requested by the other reviewers. \nsummary_of_strengths\n- Scoring the essay both as a whole and according to its different aspects (depending on the essay type) is a very useful contribution both for the research community and for the end-users.\n-The method and the results are interesting.\n-The authors are sharing their code and data.\n-The article is written in a very clear way.\n-The authors have addressed my suggestions and also the transformer baseline comparison pointed out by the Area Chair and one of the other reviewers \nsummary_of_weaknesses\nI cannot see any \ncomments,_suggestions_and_typos\nNone ", "label": [[316, 503, "Eval_pos_1"], [505, 548, "Eval_pos_2"], [549, 594, "Eval_pos_3"], [596, 639, "Eval_pos_4"]]}
{"id": 135, "review": "- Strengths: Improves over the state-of-the-art. Method might be applicable for other domains.\n- Weaknesses: Not much novelty in method.  Not quite clear if data set is general enough for other domains.\n- General Discussion: This paper describes a rule-based method for generating additional weakly labeled data for event extraction.  The method has three main stages.  First, it uses Freebase to find important slot fillers for matching sentences in Wikipedia (using all slot fillers is too stringent resulting in too few matches).  Next, it uses FrameNet to to improve reliability of labeling trigger verbs and to find nominal triggers.  Lastly, it uses a multi-instance learning to deal with the noisily generated training data.\nWhat I like about this paper is that it improves over the state-of-the-art on a non-trival benchmark.  The rules involved don't seem too obfuscated, so I think it might be useful for the practitioner who is interested to improve IE systems for other domains.  On the other hand, some some manual effort is still needed, for example for mapping Freebase event types to ACE event types (as written in Section 5.3 line 578).  This also makes it difficult for future work to calibrate apple-to-apple against this paper.              Apart from this, the method also doesn't seem too novel.\nOther comments: - I'm also concern with the generalizability of this method to other   domains.  Section 2 line 262 says that 21 event types are selected   from Freebase.  How are they selected?  What is the coverage on the 33 event types in the ACE data.\n- The paper is generally well-written although I have some   suggestions for improvement.              Section 3.1 line 316 uses \"arguments liked time, location...\".  If you mean roles or arguments, or maybe you want to use actual realizations of time and location as examples.  There are minor typos, for e.g. line 357 is missing a \"that\", but this is not a major concern I have for this paper. ", "label": [[14, 48, "Eval_pos_1"], [49, 94, "Eval_pos_2"], [109, 136, "Eval_neg_1"], [138, 202, "Eval_neg_2"], [732, 833, "Eval_pos_3"], [835, 880, "Jus_pos_4"], [881, 990, "Eval_pos_4"], [992, 1153, "Jus_neg_3"], [1155, 1248, "Eval_neg_3"], [1261, 1317, "Eval_neg_4"], [1336, 1413, "Eval_neg_5"], [1415, 1573, "Jus_neg_5"]]}
{"id": 139, "review": "paper_summary\nThis paper reviews recent state of NLP in South Asia with special focus on historical comparative linguistics. The authors argue that the most fundamental problem of South Asian NLP is data scatteredness instead of data scarcity. The data is available to extract even for endangered languages, while the challenges are how to wrangle this data and to digitise existing non machine readable texts. The authors then present three ongoing language resource projects they are working on, i.e. dependency treebanks, etymological database, and historical linguistic analysis. In the future, the authors plan to work on text digitisation and OCR. \nsummary_of_strengths\nSeveral research works presented in this paper adds to the horizon of readers' knowledge about NLP research in South Asia. \nsummary_of_weaknesses\n- This paper does not provide a clear and explicit definition of historical-comparative linguistics. Not all *ACL audiences familiarize with this area, except the member of special working group in limited workshop scope.\n- It is hard to follow what are the opinion piece the authors want to share with the readers through this paper. The paper is more likely reporting what the authors are currently do and what they plan to do, but still lacks the reflection points. \ncomments,_suggestions_and_typos\nIt is better if the authors can relate the discussion points with the more general context. Does the linguistics issues discussing in this paper applied only for the languages in South Asia? ", "label": [[676, 798, "Eval_pos_1"], [824, 922, "Eval_neg_1"], [923, 1043, "Jus_neg_1"], [1046, 1156, "Eval_neg_2"], [1157, 1290, "Jus_neg_2"]]}
{"id": 140, "review": "paper_summary\nThis paper proposes a dynamic inference approach to speedup inference with large language models. \nIn particular, an energy based router is adopted to determine whether the swift model or the super model should be used to run inference on the given input example.  Experiments on classification as well as translation tasks show that the proposed approach achieves substantial speedup while retain the accuracy of the super model. \nsummary_of_strengths\nThe paper provides solutions for both classification and sequence-to-sequence models. \nMore importantly, the proposed approach is simple and can be easily applied to existing production enviroments. \nThe experiments are solid, the proposed approach is tested against both BERT and T5 models on several benchmarks. \nExtensive ablation studies show that energy based approach is better than alternatives such as softmax-based approach and entropy-based approach. \nIn addition, it is great to see that combining the E-Lang with Distillation further improves model accuracy. \nReleasing the code helps others to reproduce the results. \nsummary_of_weaknesses\nThis is no necessarily a weakness:  The approach proposed here reminds me a lot about coarse-to-fine inference [1, 2], where the coarse/weak model is used to drastically pruning the search space in order to speedup inference.  Although these approaches were developed for structured prediction tasks and were designed for non-neural models, it would still be nice to have discussion about the connection between E-lang and this line of work.\n[1] Improved inference for unlexicalized parsing [2] Structured Prediction Cascades \ncomments,_suggestions_and_typos\nIt would be interesting to show/compare some examples that are routed to the swift model by energy/softmax/entropy based approaches. ", "label": [[554, 665, "Eval_pos_1"], [667, 693, "Eval_pos_2"], [694, 780, "Jus_pos_2"], [782, 868, "Eval_pos_3"], [869, 927, "Jus_pos_3"]]}
{"id": 141, "review": "paper_summary\nThe authors propose a new method for the KG-to-text generation task. Similar to earlier work, their method S-OSC leverages pre-trained LMs (PLMs) and models the generation task as a sequence-to-sequence task by linearizing the KG triples. However, unlike previous PLM based methods, S-OSC pays specific attention to two things: (1) order of triples in the linearized sequence should match the ordering in GT sentence (2) part-of-speech tags can help the model know when to copy and when to generate new words. By having additional models/training procedures for these sub-goals, S-OSC is able to outperform the state-of-the-art on 2 public KG-to-text datasets (WebNLG and DART). \nsummary_of_strengths\n1. Through well-motivated and relatively simple modifications to the vanilla PLM-based KG-to-text models, the authors are able to achieve SOTA results on 2 large public datasets (WebNLG and DART). \n2. The authors perform extensive ablation studies and show how each of their modifications impacts model performance. Along with the use of multiple automated metrics (BLEU, ROUGE, METEOR), human evaluation is also carried out. \nsummary_of_weaknesses\n1. The results on WebNLG seem only marginally better compared to the second best model by Li et al [1] (BLEU-4 61.88 -> 61.90, Chrf++ 79.1 -> 79.7, CIDEr score is lower). For DART, there seems to be a significant jump compared to the baselines; however, the model by Li et al [1] is absent from this comparison (Table 2). \n2. It is unclear whether some of the baseline results reported are author reproductions or original reported numbers, since the numbers do not always seem to match what is reported in the original papers. Since the main point of the work is improvement in empirical performance, this needs to be made more clear.\n[1] Few-shot Knowledge Graph-to-Text Generation with Pretrained Language Models, Findings of ACL 2021 \ncomments,_suggestions_and_typos\n1. Why is the font in tables so small? Also, spacing between sections seems to have been significantly reduced compared to the ACL template. \n2. The original numbers in [2] for BART-* and T5-* do not exactly match the ones reported in table 1. Why is that?\n[2] Investigating Pretrained Language Models for Graph-to-Text Generation (https://github.com/UKPLab/plms-graph2text) ", "label": [[718, 911, "Eval_pos_1"], [916, 1030, "Eval_pos_2"], [1167, 1267, "Eval_neg_1"], [1268, 1334, "Jus_neg_1"], [1490, 1604, "Eval_neg_2"], [1605, 1799, "Jus_neg_2"]]}
{"id": 143, "review": "paper_summary\nIn this work, the authors introduced a multi-lingual benchmark for the development and testing of bias-mitigation models or algorithms for legal text, namely FairLex. They have dealt with four datasets covering four jurisdictions, five languages, and various sensitive attributes.  They have provided competitive baselines implemented with state-of-the-art transformer-based models. As per the experimental results, we cannot identify a single algorithm that performs better across datasets. \nsummary_of_strengths\nThis work deals with an interesting problem of fairness in legal text processing. The proposed benchmark would be very useful in this direction of research.  Choosing the datasets covering four different jurisdictions and five different languages could motivate to build generalized models as well. Overall, the paper is nicely written, easy to follow. \nsummary_of_weaknesses\nNothing as such!! \ncomments,_suggestions_and_typos\nNothing as such!! ", "label": [[528, 609, "Eval_pos_1"], [610, 684, "Eval_pos_2"], [827, 880, "Eval_pos_3"]]}
{"id": 144, "review": "You present a new annotation schema and a new corpus to detect emotional and cognitive empathy. \nPro: 1) Useful resource, 2) interesting analysis Con: 1) Partly difficult to read, 2) could be better structured, 3) first baseline results on data would be a nice to have General comments: -You often create quite long sentences which are difficult to read.\n-You frequently put things in brackets, which breaks the reading flow. Please try to make proper sentences out of it. Particularly I see this when you refer to related work => (e.g. ABC et al., (2000) and XYC et al., (2008)), but also in other cases. \n-> for instance do you really need to use all these brackets here: \"To capture the differences in the empathy level of the peer reviews (i.e., the way the writer is conveying her feedback (Hattie and Timperley, 2007)), we followed the approach of Davis (1983) and Spreng et al. (2009) of cognitive and emotional empathy (taking perspective and empathetic concern)\"?\nSome more comments I wrote down reading the paper: Note: I wrote down most of my comments while reading the paper. Therefore sometimes if I mention that I miss something or something is not clear, and later you address this \"concern\". However as I was stating this already earlier, you might want to think about addressing this concern, by including another sentence earlier or restructuring.\nAbstract: --------- -second sentence too long and needs some reformulation: \"... based on an annotation guideline...based on the three annotated...\" - \"... indicate that the proposed annotation scheme successfully guides annotators to substantial to moderate agreement\" => Is it the schema or the guidelines that guide annotators? I would guess the guidelines.\nIntroduction: ------------- -Make sentence \"As Barack Obama, former president of the..\" a bit simpler/shorter/slit in two - I count 3x \"However\" at the beginning of a sentence in the introduction. Try to use some more variations.\n- \"... recent years (Liu, 2015). Recently, studies\" => try to re-formulate Corpus construction: --------------------- - \"consists of a sufficient corpus size to be able to train models in a real-world scenario\"  => Not sure, whether it is a sufficient corpus size to train models. Would be nice to see some baseline here!\n- Why did you choose 92 and not 100 peer reviews?\n- About the development of the corpus: -> So you selected 92 reviews, analysed them and developed the guidelines. You also had eight workshops with three people in order to improve the guidelines. However, many problems actually occur when you finally start the annotations. Do you actually say (this part is not clear), that the three people who met were the three annotators and they already started a first round of annotations on the 92 documents? Probably (hopefully) this becomes clearer later, but it might be helpful if you make this a bit clearer also here.\n- I am not sure whether Figure 1 is really needed. I mean it somehow helps to follow the process, but the figure does not show anything new. Everything you show there, you also mention in the text.\n- What I miss so far is the information whether each review has been annotated by multiple annotators or only one.\n3.1: ----- - Maybe you can provide some more information here. How long is a review, how is it structured? etc.\n- Are you allowed to share the data? It seems that students wrote this text for your lecture, but did they give the permission to publish it? I mean you didn't pay them to create the text for you, so this probably makes a difference. However, I don't know anything about the legal side, but it might be better if you clarify this, in case you haven't done this yet.\n3.2: ---- - \"Our basic annotation scheme is illustrated in Figure 2.\" \n=> It would be good, if you also write something about this. It is not enough to just refer to the image, better write at least also 1-2 sentences about it. \n=> Actually you kind of start explaining the different components. Each component within a different subsubsection. However, you could link to those subsubsections by using some text.  3.2.1: ------ -You have got two sentences starting with \"Accordingly\" very close together.\n- After reading 3.2.1, it is not 100% what you do regarding \"review component\". I suggest, to provide a review as an example, including annotations. You could provide this example somewhere around 3.1 and 3.2. And in 3.2.1 you could then refer to this annotation to better support your explanation.\n- \"A typical peer review therefore consists of three parts: 1) elaboration of strengths, 2) elaboration of weaknesses and 3) suggestions for improvements (to answer...\" -> So you spot those areas in the review and label them? But what if it is not so easy? I mean 1-3 can be mixed up, would you then label 1-3 multiple times?\n3.2.2: - Do you provide the empathy level per review component? What if (as I mentioned above) the same review component, such as \"weakness\" would occur multiple times, do you then give one overall empathy level per \"weakness\" or for each single one? Also empathy level on sentence level might be interesting as well.\n4.2: -Interesting analysis!  References: -> some references have errors, please check. here some examples: - Emotion Representation Mapping for Automatic Lexicon Construction (Mostly) Performs on Human Level -> venue missing -\"University of New . . .\"\n-\"Introduction to Reasoning.\" - > where was this published? ", "label": [[105, 121, "Eval_pos_1"], [125, 145, "Eval_pos_2"], [154, 179, "Eval_neg_1"], [183, 210, "Eval_neg_2"], [214, 268, "Eval_neg_3"], [288, 354, "Eval_neg_4"], [355, 972, "Jus_neg_4"], [2898, 2946, "Eval_neg_5"], [2947, 3093, "Jus_neg_5"], [5141, 5162, "Eval_pos_3"], [5164, 5221, "Eval_neg_6"], [5222, 5447, "Jus_neg_6"]]}
{"id": 145, "review": "paper_summary\nThis work deals with a purported flaw of modern neural OpenIE systems: their tendency to extract overly specific arguments which are not useful for upstream tasks. The authors present a neural OpenIE system which produces \u201ccompact\u201d extractions. The concept of \u201ccompactness\u201d isn\u2019t explicitly defined, but examples are provided and various proxies are supplied (e.g. triplets should be as short as possible without affecting the semantics, triplets shouldn\u2019t contain arguments which are in themselves triplets).    The suggested solution is innovative in two ways: 1. It is based on a new training and evaluation dataset. This dataset is derived from a standard one using a heuristic algorithm which identifies and extracts compact triplets while filtering out complex ones.  2. A novel OpenIE algorithm is employed: it is based on a slot filling architecture recently suggested for relation extraction, but in this work it\u2019s employed as part of a pipeline system which the authors show further increases performance. \nsummary_of_strengths\n1. The paper deals with an important practical problem of how to optimize the usefulness of Open IE triplets for upstream tasks.  2. The paper is clear and does a great job of reviewing the literature in this area and citing relevant sources.  3. The authors suggest a pipeline approach: a table filling stage for the identifying arguments and predicates, and a second stage model which links the two. I think combining these ideas is indeed novel, and the authors also show that the suggested solution performs better than an end-to-end solution based on table filling alone.  4. Both automatic and manual evaluation methods are used to show that the suggested solution performs better than existing approaches on IE datasets modified to contain compact extractions. \nsummary_of_weaknesses\n1. I think the paper goes a bit too quickly from the problem description of systems producing extractions which are too specific, to their specific flavor of \u201ccompact\u201d extractions designed to solve this problem. The paper does cite different sources which explain the problem in more depth, but I don\u2019t think there\u2019s a consensus that the type of compact extractions the authors promote is the ideal solution for the problem. \nE.g. based on the criteria for informative extractions described in the paper \u201cAnnotating and predicting non-restrictive noun phrase modifications\u201d (Stanovsky et. al 2016), extracting \u201cHercule Poirot\u201d \u201cis\u201d \u201ca Belgian detective\u201d from the sentence  \u201cHercule Poirot is a Belgian detective, created by Agatha Christie\u201d might not be specific enough (as the modification here seems restrictive). More discussion of this criteria and how it relates to compact extractions will be helpful.\n2. Continuing the above point, the evaluation in this paper is intrinsic and uses datasets modified to include extractions of the type the authors are promoting, so comparison to existing systems is not 1:1. Some kind of extrinsic evaluation (e.g. on a KBC or KB alignment task) could be helpful in showing that the suggested openIE method does in fact work better for upstream tasks.\n3. The authors give no indication of whether the modified datasets, code, system, or the data used in manual evaluation will be made available. Assuming these are not public, it is harder still to estimate whether the suggested approach is really a good fit for upstream tasks (this is also why I give this paper low reproducibility scores. If the authors were to share code/data, I would have assigned a higher score). \ncomments,_suggestions_and_typos\nDespite the stated weaknesses, I'm narrowly in favor of accepting this paper. It deals with an important and not frequently discussed practical problem (making OpenIE triplets more useful for upstream tasks), the approach is interesting and the evaluation shows that the system produces triplets of the type the authors advocate more accurately than other systems (I think more can be done to convince that this style of queries is indeed optimal for upstream tasks, but still, this work seems like a step in the right direction).\nI would highly encourage the authors to make the code, datasets and sample extractions public, as it can really increase the impact of this work, allow others to consume and evaluate the outputs in real-world tasks, etc. ", "label": [[526, 576, "Eval_pos_1"], [577, 1030, "Jus_pos_1"], [1055, 1180, "Eval_pos_2"], [1185, 1294, "Eval_pos_3"], [1454, 1500, "Eval_pos_4"], [1846, 2054, "Eval_neg_1"], [2055, 2750, "Jus_neg_1"], [2782, 2958, "Eval_neg_2"], [2959, 3135, "Jus_neg_2"], [3139, 3279, "Jus_neg_3"], [3280, 3412, "Eval_neg_3"], [3589, 3666, "Major_claim"], [3667, 3796, "Eval_pos_5"], [3798, 3952, "Eval_pos_6"], [4067, 4117, "Major_claim"]]}
{"id": 146, "review": "This paper proposes integrating word sense inventories into existing approaches for the lexical substitution task by using these inventories to filter candidates. To do so, the authors first propose a metric to measure the mutual substitutability of sense inventories with human judgments for the lexsub task, and empirically measure the substitutability of inventories from various sources such as WordNet and PPDB. Next, they propose clustering different paraphrases of a word from PPDB using a multi-view clustering approach, to automatically generate a sense inventory instead of using the aforementioned inventories. Finally, they use these clusters with a naive (majority in top 5) WSD technique to filter existing ranked list of substitution candidates.\n- Strengths: - The key idea of marrying vector space model based approaches and sense inventories for the lexsub task is useful since these two techniques seem to have complementary information, especially since the vector space models are typically unaware of sense and polysemy.\n- The oracle evaluation is interesting as it gives a clear indication of how much gain can one expect in the best case, and while there is still a large gap between the oracle and actual scores, we can still argue for the usefulness of the proposed approach due to the large difference between the unfiltered GAP and the oracle GAP.\n- Weaknesses: - I don't understand effectiveness of the multi-view clustering approach. \nAlmost all across the board, the paraphrase similarity view does significantly better than other views and their combination. What, then, do we learn about the usefulness of the other views? There is one empirical example of how the different views help in clustering paraphrases of the word 'slip', but there is no further analysis about how the different clustering techniques differ, except on the task directly. Without a more detailed analysis of differences and similarities between these views, it is hard to draw solid conclusions about the different views.                                   - The paper is not fully clear on a first read. Specifically, it is not immediately clear how the sections connect to each other, reading more like disjoint pieces of work. For instance, I did not understand the connections between section 2.1 and section 4.3, so adding forward/backward pointer references to sections should be useful in clearing up things. Relatedly, the multi-view clustering section (3.1) needs editing, since the subsections seem to be out of order, and citations seem to be missing (lines 392 and 393).\n- The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n- General Discussion: The paper is mostly straightforward in terms of techniques used and experiments. Even then, the authors show clear gains on the lexsub task by their two-pronged approach, with potentially more to be gained by using stronger WSD algorithms.\nSome additional questions for the authors : - Lines 221-222 : Why do you add hypernyms/hyponyms?\n-Lines 367-368 : Why does X^{P} need to be symmetric?\n-Lines 387-389 : The weighting scheme seems kind of arbitrary. Was this indeed arbitrary or is this a principled choice?\n-Is the high performance of SubstClus^{P} ascribable to the fact that the number of clusters was tuned based on this view? Would tuning the number of clusters based on other matrices affect the results and the conclusions?\n-What other related tasks could this approach possibly generalize to? Or is it only specific to lexsub? ", "label": [[776, 888, "Eval_pos_1"], [889, 1041, "Jus_pos_1"], [1044, 1080, "Eval_pos_2"], [1081, 1374, "Jus_pos_2"], [1391, 1462, "Eval_neg_1"], [1464, 2030, "Jus_neg_1"], [2066, 2111, "Eval_neg_2"], [2112, 2589, "Jus_neg_2"], [2592, 2649, "Eval_neg_3"], [2650, 3025, "Jus_neg_3"]]}
{"id": 147, "review": "This article presents several parsing models that are able to do domain adaptation from large scale source-domain labeled data to small scale target-domain labeled data via adversarial learning. They also use large scale unlabeled data to improve word representations. Experiments show that these approaches lead to consistent improvements, reaching SOTA results comparable to those for more complex ensemble models.\nThe article is well written and in my view the contribution is important and well supported. All the proposed ideas are compared and validated with experiments.\nIn terms of results, the biggest contribution seems to be given by the usage of BERT and fine-tuned BERT, maybe the least original aspect of the work, but the adversarial models consistently show improvements in every comparison w.r.t. non-adversarial ones.\nThe final results and comparison with previous work is also meaningful. In particular, it is interesting to observe that the best results are obtained for the PC domain, the one with the largest difference w.r.t. the source domain, and that the adversarial training plays a key role here.\nMinor comments: 1: \u201ckim et al.\u201d -> \u201cKim\u201d 1: feature argumentation -> feature augmentation 1: last paragraph, 1st sentence, too long. Or not? \n1 \u201ca large-scale\u201d -> \u201clarge-scale\u201d 1: \u201cour codes\u201d -> \u201cour code\u201d 1: Put link in footnote. \nUse \u201cFig.\u201d to reference figures in text. \n2: \u201cAdditionally. we \u2026\u201d -> \u201cAdditionally, we \u2026\u201d 2: BiLSTM \u201cforward and backward two directions\u201d 2: (3) use two formulas 2: loss is standard, no need to include it. \n3.1: \u201cthe adversarial network on BiAffine parser\u201d -> \u201can adversarial network on BiAffine parser\u201d 3.3: \u201cfine-tuning domain embedding\u201d -> \u201cfine-tuned domain embedding\u201d 3.3: \u201cdomain-special features\u201d -> \u201cdomain-specific features\u201d 3.3: \u201canother BiLSTM\u201d -> \u201cthe other BiLSTM\u201d 3.4: \u201cstart point\u201d -> \u201cstarting point\u201d 3.4: \u201ccomputation resource\u201d -> \u201ccomputation[al] resources\u201d 3.4: \u201c... once, Thus \u2026\u201d -> \u201c... once. Thus \u2026\u201d 3.4: explicitly say that no \u201cnext sentence\u201d loss is used. \n4: \u201cproduct blog\u201d -> \u201cproduct blog (PB)\u201d 4: \u201cdependence parsing\u201d -> \u201cdependency parsing\u201d 4.2: \u201cdirectly applies\u201d -> \u201cdirectly applying\u201d 4.4: \u201ccompared the results\u201d ->  \u201ccomparing the results\u201d ", "label": [[417, 444, "Eval_pos_1"], [449, 509, "Eval_pos_2"], [578, 728, "Eval_neg_1"], [729, 835, "Eval_pos_3"], [836, 907, "Eval_pos_4"], [908, 1124, "Jus_pos_4"]]}
{"id": 149, "review": "paper_summary\nThis paper proposes, as a form of data augmentation, to replace the one-hot sequences that are typically consumed by text classification models with an interpoloation between this one-hot distribution and a distribution over word-types obtained by running the sequence through BERT. The authors show that this form of augmentation helps on its own as well as when combined with other standard data augmentation techniques. \nsummary_of_strengths\n- The paper obtains good results with a straightforward approach.\n-The paper is written fairly clearly. \nsummary_of_weaknesses\n- The paper needs some light editing (especially Section 4.1) but I don't see any significant weaknesses. \ncomments,_suggestions_and_typos\n- One thing I wasn't sure I understood is whether the \"smoothed\" inputs are used on their own to train the model, or if they're used in addition to the standard inputs. Lines 236-239 make me think they're used in addition. This is fine, but should be emphasized more explicitly.\n-Since, as the authors note, they are are merely approximating the token-distribution given by BERT (by not using any MASK tokens), it might be interesting to see whether this approximation is in fact hurting the performance or not. That is, if we obtain token-level distributions by masking each token in the input in turn, and then use the resulting smoothed representations, is this better or worse for augmentation than the approximation the authors propose? ", "label": [[461, 524, "Eval_pos_1"], [526, 562, "Eval_pos_2"], [588, 622, "Eval_neg_1"], [623, 647, "Jus_neg_1"], [727, 893, "Eval_neg_2"], [894, 1003, "Jus_neg_2"]]}
{"id": 151, "review": "- Strengths:  - the model if theoretically solid and motivated by formal semantics.  - Weaknesses:  - The paper is about is-a relation extraction but the majority of literature about taxonomization is not referenced in the paper, inter alia: Flati Tiziano, Vannella Daniele, Pasini Tommaso, Navigli Roberto. \n2016. MultiWiBi: The multilingual Wikipedia bitaxonomy project.\nSoren Auer, Christian Bizer, Georgi Kobilarov, Jens \u00a8 Lehmann, Richard Cyganiak, and Zachary Ive. \n2007. DBpedia: A nucleus for a web of open data.\nGerard de Melo and Gerhard Weikum. 2010. MENTA: Inducing Multilingual Taxonomies from Wikipedia.\nZornitsa Kozareva and Eduard H. Hovy. 2010. A Semi-Supervised Method to Learn and Construct Taxonomies Using the Web.  Vivi Nastase, Michael Strube, Benjamin Boerschinger, Caecilia Zirn, and Anas Elghafari. 2010. WikiNet: A Very Large Scale Multi-Lingual Concept Network.\nSimone Paolo Ponzetto and Michael Strube. 2007. \nDeriving a large scale taxonomy from Wikipedia.\nSimone Paolo Ponzetto and Michael Strube. 2011. \nTaxonomy induction based on a collaboratively built knowledge repository.  Fabian M. Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2008. YAGO: A large ontology from Wikipedia and WordNet.  Paola Velardi, Stefano Faralli, and Roberto Navigli. \n2013. OntoLearn Reloaded: A graph-based algorithm for taxonomy induction.   - Experiments are poor, they only compare against \"Hearst patterns\" without taking into account the works previously cited.\n- General Discussion:  The paper is easy to follow and the supplementary material is also well written and useful, however the paper lack of references of is a relation extraction and taxonomization literature. The same apply for the experiments. \nIn fact no meaningful comparison is performed and the authors not even take into account the existence of other systems (more recent than hearst patterns).\nI read authors answers but still i'm not convinced that they couldn't perform more evaluations. I understand that they have a solid theoretical motivation but still, i think that comparison are very important to asses if the theoretical intuitions of the authors are confirmed also in practice. While it's true that all the works i suggested as comparison build taxonomies, is also true that a comparison is possible considering the edges of a taxonomy.\nAnyway, considering the detailed author answer and the discussion with the other reviewer i can rise my score to 3 even if i still think that this paper is poor of experiments and does not frame correctly in the is-a relation extraction / taxonomy building literature. ", "label": [[16, 83, "Eval_pos_1"], [102, 229, "Eval_neg_1"], [230, 1353, "Jus_neg_1"], [1358, 1379, "Eval_neg_2"], [1380, 1479, "Jus_neg_2"], [1503, 1530, "Eval_pos_2"], [1535, 1594, "Eval_pos_3"], [1595, 1690, "Eval_neg_3"], [1691, 1726, "Eval_neg_4"], [1728, 1883, "Jus_neg_4"], [1884, 1979, "Eval_neg_5"], [1980, 2337, "Jus_neg_5"], [2338, 2606, "Major_claim"]]}
{"id": 152, "review": "paper_summary\nThis paper proposed to format the event argument extraction task as a generation task and designed language-agnostic templates to fine-tune multilingual seq2seq language models for zero-shot cross lingual transfer. The experimental results show its superiority over traditional classification models. Overall the idea is clear and simple with strong motivations. The analysis introduced its advantages as well as its current limitations. This work is a well-done paper. \nsummary_of_strengths\n1. The problem formulation for zero-shot cross-lingual event extraction is new and clear. \n2. The results show the strong potential of generative models for information extraction. \n3. The analysis is detailed and informative for future work. \nsummary_of_weaknesses\n1.The novelty is a bit limited for the small focused task. \ncomments,_suggestions_and_typos\nIn the information extraction area, recently a lot of work has been explored to replace the traditional classification methods with the generative models. It is not novel enough to directly adapt to another task. For the major contribution of this work,  the special design of inputs and outputs could well capture the characteristics of the task. Fine-tuning multilingual LMs enables good generalization for zero-shot cross-lingual transfer. If the language-agnostic templates could be applied to other structure prediction tasks with good improvements, the contribution could be larger. Currently this paper is okay but not novel enough. It would be better to discuss the potential of language-agnostic templates (html tag). ", "label": [[315, 376, "Eval_pos_1"], [452, 483, "Major_claim"], [509, 595, "Eval_pos_2"], [600, 686, "Eval_pos_3"], [691, 748, "Eval_pos_4"], [774, 830, "Eval_neg_1"], [1077, 1211, "Eval_pos_5"], [1453, 1503, "Eval_neg_2"]]}
{"id": 153, "review": "paper_summary\nThe paper uncovers critical issues around interpretability and explainability of the reasoning and performance of modern deep learning models. Using several theoretical and experimental analysis, it specifically reveals the weaknesses of existing attribution methods designed to qualify and support research propositions by assessing model predictions. Because of their deceptive nature i.e. easily disregarded or even missed, the paper presents these weaknesses as logic traps. For example, if input features responsible for certain predictions are perturbed, scores provided by attribution methods should reflect the significant change or difference in the model\u2019s predictions given the perturbations. The paper highlights the potential damage of neglecting these traps such as inaccurate evaluation, unfair comparison, unmerited pressure to re-use unreliable evaluation techniques etc. In its analysis, the paper shows factual information of how attribution methods can be misleading when they approve of a model\u2019s prediction countering the actual norm or expectation.  Two helpful examples of logic traps include, (1) the assumption that a humans' decision making process is equivalent to that of a neural network. In a question answering task, BERT base maintained its test set performance despite replacing development set samples with empty strings. Ideally, the performance would drop because of the perturbations, but it did not, therefore clearly misleading human decision making. Moreover, the paper empirically proves this error by showing a drop in the model's confidence in its prediction on unchanged samples. ( 2) A second example of a logic trap is using arribution methods as ground truth to evaluate target attribution method. In an evaluation based on meaningful perturbation, such as Area over the perturbation curve (AOPC) which is precisely the average difference between probabilities predicted with original input feature and those predicted with perturbed input features. While the norm expects this AOPC to be significant or at least representative of the degree of perturbation, the paper illustrates that the perturbation (modification) strategy will dictate the eventual AOPC value, i.e. it varies with respect to the modification strategy. Scores of different attribution methods when inputs are perturbed by token deletion are inconsistent with their scores when inputs are perturbed by token replacement. The paper concludes with suggestions of limiting the impact of logic traps such as the ones discussed. Enhancing the target model as well as Excluding predictions with lower confidence. \nsummary_of_strengths\nPrecise and concise abstract.\nThe paper is organised and well written.\nThe subject addressed is crucial for the deep learning community. Redirecting attention to the appropriately selecting attribution methods (during research task evaluation stages) can subtly reduce the immense effort and focus researchers have in outperforming previous works which in any case can potentially be premised on unreliable evaluation methods.\nA sufficient number of examples to illustrate the logic traps is used, which is very helpful especially because they are deceptively obvious. \nsummary_of_weaknesses\nLines 179-182 Surprisingly, the trained MRC model maintained the original prediction on 64.0% of the test set samples (68.4% on correctly answered samples and 55.4% on wrongly answered samples).\nIt\u2019s clear that the MRC model surprisingly maintains prediction accuracy when evaluated on 64% of the test samples, however, what follows in the brackets is unclear i.e. \"68.4% on correctly answered samples and 55.4% on wrongly answered samples\" is an unclear statement.  Lines 183-185 Moreover, we analyze the model confidence change in these unchanged samples, where the probability on the predicted label is used as the confidence score.\nWhat unchanged samples are you referring to? Did you replace all the development samples with empty strings or was it just a portion you replaced with empty strings hence retaining a few that you refer to as unchanged? If not, are the unchanged samples in the test set or?\nEvaluation 3 and Logic trap 3.\nYou use a hard to follow example to illustrate the logic trap you define as  \u201cthe change in attribution scores is brought about by the model reasoning process rather than the attribution method unreliability\u201d. Because you indicate that deep models are vulnerable to adversarial samples, which indeed is right and therefore you would expect attribution scores to be faithful to the shift caused by the attack.\nThe argument feels more like, the change in attribution scores is with respect to the change in samples which eventually will meet a different model reasoning process?\nFor the results you discuss and summarize, Is the claim that the original evaluation method correctly obtains a low similarity in the F1 scores of the adversarial sample subset and the original sample subset, whereas the attribution method says otherwise?\nA rewrite of this section particularly the experiment and its results to add clarity or rather use of a different example would improve the work. \ncomments,_suggestions_and_typos\nLines 088-092 Last, the over-belief in existing evaluation metrics encourages efforts to propose more accurate attribution methods, notwithstanding the evaluation system is unreliable.\nThe statement above looks more like you intended to say discourages rather than encourages. Please have a look.\nLines 276 and 328 AOPC rather than APOC Lines 528 With no overlap between the two subsets, there is no way we can hypothesis the adversarial samples share similar model reasoning to the original samples.\nhypothesise rather than hypothesis in the above sentence.\nYou can probably do away with some repetitions of long sentences such as what is in the introduction as well as in the conclusion. \u201c Though strictly accurate evaluation metrics for attribution methods might be a \u201cunicorn\u201d which will likely never be found, we should not just ignore logic traps in existing evaluation methods and draw conclusions recklessly.\u201d ", "label": [[2660, 2689, "Eval_pos_1"], [2690, 2730, "Eval_pos_2"], [2731, 2796, "Eval_pos_3"], [2797, 3086, "Jus_pos_3"], [3087, 3179, "Eval_pos_4"], [3180, 3227, "Jus_pos_4"], [4161, 5024, "Jus_neg_1"], [5025, 5169, "Eval_neg_1"], [5763, 5827, "Eval_neg_2"], [5828, 6121, "Jus_neg_2"]]}
{"id": 154, "review": "- Strengths: This paper tackles an interesting problem and provides a (to my knowledge) novel and reasonable way of learning and combining cognitive features with textual features for sentiment analysis and irony detection. The paper is  clearly written and organized, and the authors provided a lot of useful detail and informative example and plots. Most of the results are convincing, and the authors did a good job comparing their approach and results with previous work.\n- Weaknesses: 1. Just from the reading abstract, I expected that the authors' approach would significantly outperform previous methods, and that using both the eye-gaze and textual features consistently yields the best results. Upon reading the actual results section, however, it seems like the findings were more mixed. I think it would be helpful to update the abstract and introduction to reflect this. \n2. When evaluating the model on dataset 1 for sentiment analysis, were the sarcastic utterances included? Did the model do better on classifying the non-sarcastic utterances than the sarcastic ones? \n3. I understand why the eye-movement data would be useful for sarcasm detection, but it wasn't as obvious to me why it would be helpful for (non-sarcastic) sentiment classification beyond the textual features.  - General Discussion: This paper contains a lot of interesting content, and the approach seems solid and novel to me. The results were a little weaker than I had anticipated from the abstract, but I believe would still be interesting to the larger community and merits publication. ", "label": [[13, 54, "Eval_pos_1"], [59, 223, "Eval_pos_2"], [224, 268, "Eval_pos_3"], [273, 351, "Eval_pos_4"], [352, 387, "Eval_pos_5"], [392, 475, "Eval_pos_6"], [1317, 1576, "Major_claim"]]}
{"id": 155, "review": "The paper describes a method for improving two-step translation using deep learning. Results are presented for Chinese->Spanish translation, but the approach seems to be largely language-independent.\nThe setting is fairly typical for two-step MT. The first step translates into a morphologically underspecified version of the target language. The second step then uses machine learning to fill in the missing morphological categories and produces the final system output by inflecting the underspecified forms (using a morphological generator). The main novelty of this work is the choice of deep NNs as classifiers in the second step. The authors also propose a rescoring step which uses a LM to select the best variant.\nOverall, this is solid work with good empirical results: the classifier models reach a high accuracy (clearly outperforming baselines such as SVMs) and the improvement is apparent even in the final translation quality.\nMy main problem with the paper is the lack of a comparison with some straightforward deep-learning baselines. Specifically, you have a structured prediction problem and you address it with independent local decisions followed by a rescoring step. ( Unless I misunderstood the approach.) But this is a sequence labeling task which RNNs are well suited for. How would e.g. a bidirectional LSTM network do when trained and used in the standard sequence labeling setting? After reading the author response, I still think that baselines (including the standard LSTM) are run in the same framework, i.e. independently for each local label. If that's not the case, it should have been clarified better in the response. This is a problem because you're not using the RNNs in the standard way and yet you don't justify why your way is better or compare the two approaches.\nThe final re-scoring step is not entirely clear to me. Do you rescore n-best sentences? What features do you use? Or are you searching a weighted graph for the single optimal path? This needs to be explained more clearly in the paper. \n(My current impression is that you produce a graph, then look for K best paths in it, generate the inflected sentences from these K paths and *then* use a LM -- and nothing else -- to select the best variant. But I'm not sure from reading the paper.) This was not addressed in the response.\nYou report that larger word embeddings lead to a longer training time. Do they also influence the final results?\nCan you attempt to explain why adding information from the source sentence hurts? This seems a bit counter-intuitive -- does e.g. the number information not get entirely lost sometimes because of this? I would appreciate a more thorough discussion on this in the final version, perhaps with a couple of convincing examples.\nThe paper contains a number of typos and the general level of English may not be sufficient for presentation at ACL.\nMinor corrections: context of the application of MT -> context of application for MT In this cases, MT is faced in two-steps -> In this case, MT is divided into two steps markov -> Markov CFR -> CRF task was based on a direct translation -> task was based on direct translation task provided corpus -> task provided corpora the phrase-based system has dramatically -> the phrase-based approach... investigated different set of features -> ...sets of features words as source of information -> words as the source... correspondant -> corresponding Classes for gender classifier -> Classes for the... for number classifier -> for the... This layer's input consists in -> ...consists of to extract most relevant -> ...the most... Sigmoid does not output results in [-1, 1] but rather (0, 1). A tanh layer would produce (-1, 1).\ninformation of a word consists in itself -> ...of itself this $A$ set -> the set $A$ empty sentences and longer than 50 words -> empty sentences and sentences longer than... classifier is trained on -> classifier is trained in aproximately -> approximately coverage raises the 99% -> coverage exceeds 99% (unless I misunderstand) in descendant order -> in descending order cuadratic -> quadratic (in multiple places) but best results -> but the best results Rescoring step improves -> The rescoring step... are not be comparable -> are not comparable ", "label": [[545, 635, "Eval_pos_1"], [731, 777, "Eval_pos_2"], [779, 940, "Jus_pos_2"], [941, 1050, "Eval_neg_1"], [1051, 1804, "Jus_neg_1"], [1805, 1859, "Eval_neg_2"], [1860, 2444, "Jus_neg_2"], [2769, 2885, "Eval_neg_3"]]}
{"id": 156, "review": "Strengths: The paper presents a new method that exploits word senses to improve the task of lexical substitutability.  Results show improvements over prior methods.\nWeaknesses: As a reader of a ACL paper, I usually ask myself what important insight can I take away from the paper, and from a big picture point of view, what does the paper add to the fields of natural language processing and computational linguistics.  How does the task of lexical substitutability in general and this paper in particular help either in improving an NLP system or provide insight about language?  I can't find a good answer answer to either question after reading this paper.\nAs a practitioner who wants to improve natural language understanding system, I am more focused on the first question -- does the lexical substitutability task and the improved results compared to prior work presented here help any end application?  Given the current state of high performing systems, any discrete clustering of words (or longer utterances) often break down when compared to continuous representations words (see all the papers that utilitize discrete lexical semantics to achieve a task versus words' distributed representations used as an input to the same task; e.g. machine translation, question answering, sentiment analysis, text classification and so forth).  How do the authors motivate work on lexical substitutability given that discrete lexical semantic representations often don't work well?  The introduction cites a few papers from several years back that are mostly set up in small data scenarios, and given that this word is based on English, I don't see why one would use this method for any task.  I would be eager to see the authors' responses to this general question of mine.\nAs a minor point, to further motivate this, consider the substitutes presented in Table 1. \n1. Tasha snatched it from him to rip away the paper. \n2. Tasha snatched it from him to rip away the sheet.\nTo me, these two sentences have varying meanings -- what if he was holding on to a paper bag?  In that scenario, can the word \"paper\" be substituted by \"sheet\"?  At least, in my understanding, it cannot.  Hence, there is so much subjectivity in this task that lexical substitutes can completely alter the semantics of the original sentence.\nMinor point(s):  - Citations in Section 3.1.4 are missing.\nAddition: I have read the author response and I am sticking to my earlier evaluation of the paper. ", "label": [[420, 659, "Major_claim"], [910, 1635, "Jus_neg_1"], [1636, 1691, "Eval_neg_1"], [1774, 2176, "Jus_neg_2"], [2178, 2313, "Eval_neg_2"], [2333, 2372, "Eval_neg_3"]]}
{"id": 157, "review": "paper_summary\nA method to train dense retrieval without the need of any supervised data using a combination of ICT and SimCSE. Further, they propose Iterative Contrastive Learning (ICoL) as a new way for caching negative embeddings. \nsummary_of_strengths\n- The paper is well written and easy to understand.\n-The paper conducts many experiments and presents many interesting insights -The results are convincing on the BEIR benchmark -ICoL appears to be an interesting approach showing improvements over MoCo -Overall a strong paper but with some weaknesses in evaluating the individual components / individual changes. \nsummary_of_weaknesses\n**What is contributing to the performance improvement?**\n-Because the paper presents so many novelties, it is a bit hard to grasp what led to the improvement on BEIR, i.e. what are the main factors that contribute to the improvement?\n-It appears (Table 3) the strongest improvement come from Lexicon-Enhanced Dense Retrieval, i.e. combining the dense sim score with a BM25 score. This hybrid approach has been shown effective in several previous works, e.g. https://arxiv.org/abs/2005.00181 or https://arxiv.org/pdf/2004.13969.pdf (and many more) -It would be interesting to get the results for other dense + BM25 combinations. The dense retriever appears to be not the strongest (cf. Table 3, col. w/o LEDR), i.e. it is weaker than TAS-B (0.396 vs 0.415). So what would be the results of TAS-B+LEDR?\n**Pre-training approach** -A large contribution of the author is the proposal of a new pre-training approach that combines ICT + SimCSE with a large negative cache (ICoL) -Table 3 shows an improvement for the sole dense retrieval model if pre-training is used (e.g. w/o LEDR vs. w/O LEDR & PT) -However, we know that BERT is undertrained (see RoBERTa paper) and performing more pre-training steps yields an improvemed BERT model -A comparison against other pre-training approaches would have been interesting. What happens when I train e.g. with MLM for the same amount of compute on C4 and then do fine-tuning? What about other pre-training methods for dense retrievers (for an overview see https://arxiv.org/abs/2112.07577)? Is the proposed pre-training method actually better than other pre-training strategies?\nI think the 3 additions (pre-training, ICoL, hybrid retrieval with BM25) could be better separated / evaluated more individually. It would help to see what are the contributing factors for the improved performance. So far it appears that switching to a hybrid approach with BM25 made the large difference in performance.\nHaving a more clear separation in their evaluation would help to assess what is really relevant for future methods. \ncomments,_suggestions_and_typos\n- Line 231: DaPI was concurrently also proposed by Liu et al (which they call Mirror-BERT https://arxiv.org/abs/2104.08027, published April 16; SimCSE was published April 18) -Line 430: You say that the batch size for each GPU is 4,096, so 16*4k = 64k in total for your server. But how do you get a batch of 4,096 on a single 32GB GPU? A batch larger than 100 examples results usually in an out-of-memory error when using huggingface transformers with a distilbert model. Did you use some special off-loading?\n-Maybe I might missed it in the paper: Did you specify your max sequence length for pre-training / fine-tuning? I.e. do you use 512 word pieces or do you pre-train on shorter paragraphs?  -Table 4: The heading \"w/o ICT (SimCSE 2021c)\" looks confusing, you would think that ICT was proposed by SimCSE 2021c) -Table 4: Adding SimCSE to pre-training appears to bring little effect, performance improves from 43.4 -> 43.8. Could this improvement just be due to randomness? How does the performance change for the fine-tuning setting when pre-training was without SimCSE (i.e. just ICT pre-training followed by supervised MS MARCO training)?  -Table 5: The three settings, I assume these are the hybrid approaches with BM25. How is the performance only for the dense retrievers without BM25? ", "label": [[257, 306, "Eval_pos_1"], [308, 382, "Eval_pos_2"], [384, 432, "Eval_pos_3"], [434, 507, "Eval_pos_4"], [509, 617, "Major_claim"], [700, 745, "Jus_neg_1"], [746, 808, "Eval_neg_1"], [1190, 1269, "Eval_neg_2"], [1270, 1442, "Jus_neg_2"], [1873, 1952, "Eval_neg_3"], [1953, 2257, "Jus_neg_3"], [2258, 2387, "Eval_neg_4"], [2388, 2694, "Jus_neg_4"]]}
{"id": 158, "review": "- Strengths:  - The paper is clearly written and well-structured.   - The system newly applied several techniques including global optimization to end-to-end neural relation extraction, and the direct incorporation of the parser representation is interesting.\n - The proposed system has achieved the state-of-the-art performance on both ACE05 and CONLL04 data sets.\n - The authors include several analyses.\n- Weaknesses:  - The approach is incremental and seems like just a combination of existing methods.    - The improvements on the performance (1.2 percent points on dev) are relatively small, and no significance test results are provided.\n- General Discussion: - Major comments:  - The model employed a recent parser and glove word embeddings. How did they affect the relation extraction performance?\n - In prediction, how did the authors deal with illegal predictions?\n- Minor comments:  - Local optimization is not completely \"local\". It \"considers structural correspondences between incremental decisions,\" so this explanation in the introduction is misleading.\n - Points in Figures 6 and 7 should be connected with straight lines, not curves.\n - How are entities represented in \"-segment\"?\n - Some citations are incomplete. Kingma et al. (2014) is accepted to ICLR, and Li et al. (2014) misses pages. ", "label": [[16, 65, "Eval_pos_1"], [190, 258, "Eval_pos_2"], [369, 406, "Eval_pos_3"], [424, 506, "Eval_neg_1"], [512, 547, "Eval_neg_2"], [548, 575, "Jus_neg_2"], [576, 597, "Eval_neg_2"], [897, 1015, "Jus_neg_3"], [1016, 1070, "Eval_neg_3"], [1203, 1233, "Eval_neg_4"], [1234, 1310, "Jus_neg_4"]]}
{"id": 160, "review": "- Strengths: The macro discourse structure is a useful complement to micro structures like RST. The release of the dataset would be helpful to a range of NLP applications.\n- Weaknesses: 1. Providing more comparisons with the existed CDTB will be better. \n2. The \u201cprimary-secondary\u201d relationship is mentioned a lot in this paper, however, its difference with the nuclearity is unclear and not precisely defined. \n3. The experiment method is not clearly described in the paper.\n- General Discussion: ", "label": [[13, 95, "Eval_pos_1"], [96, 171, "Eval_pos_2"], [189, 253, "Eval_neg_1"], [258, 410, "Eval_neg_2"], [415, 475, "Eval_neg_3"]]}
{"id": 162, "review": "paper_summary\nThe paper proposes a new dataset for reading comprehension with multi-span answers, MultiSpanQA. \nIn addition, a new model for MultiSpanQA is introduced.\nThe **dataset** is created using a re-annotation of questions with multiple answers from Natural Questions. \nThis re-annotation process consists of two phases: -First, each instance is given one of four categories: (1) Good example, (2) Bad question, (3) Bad answer span(s), and (4) Bad QA pair.\n-Second, instances are categorized to different \"semantic structures\" (for example, whether all answer spans are needed to correctly answer a question, or each span is an answer on its own). \nAn expanded version of the dataset also includes single-span and unanswerable questions for better simulation of real-world scenarios, The **proposed model** is a sequence tagging module (BIO), augmented by three modules: -Semantic structure prediction -Span number prediction -Span adjustment module that combines the predicted number of spans with preliminary predicted spans.\nThe authors demonstrated that their proposed model improves over single-span models (generalized for multi-span QA) and over vanilla sequence tagging models. \nsummary_of_strengths\n- The problem of multi-span QA is very important and helps deviating from the limiting single-span setting -The dataset is potentially useful for the QA community -To the best of my knowledge, the 5-way semantic annotation scheme proposed by the authors is novel (and I found it elegant and important to better understand the task) -The proposed model improves over all baselines \nsummary_of_weaknesses\n1. Section 4 (Models) misses some details about the proposed model. For example, what is the exact inference procedure over $O_p$ (personally I prefer equations over textual descriptions) 2. Evaluation metrics: This subsection is difficult to read and not rigorous. \ncomments,_suggestions_and_typos\nComments & questions: -Abstract: The sentence in lines 12-17 (\"After multi-span re-annotation, MultiSpanQA consists of over a total of 6,0000 multi-span questions in the basic version, and over 19,000 examples with unanswerable questions, and questions with single-, and multi-span answers in the expanded version\") is cumbersome and can be made clearer.  -Do you perform re-annotation for the expanded dataset as well? The text now says \"..and applying the same preprocessing\" (line 355) - this point can be made more clear.\n-What is the inference procedure for single-span (v1)? Is the prediction a long span like in training?\nTypos: -Line 47: \"constitinga\" -> \"consisting\" -Line 216: \"classifies\" -> \"classify\" ", "label": [[1217, 1321, "Eval_pos_1"], [1323, 1377, "Eval_pos_2"], [1380, 1477, "Eval_pos_3"], [1483, 1545, "Eval_pos_4"], [1548, 1594, "Eval_pos_5"], [1621, 1685, "Eval_neg_1"], [1686, 1805, "Jus_neg_1"], [1809, 1883, "Eval_neg_2"]]}
{"id": 164, "review": "This paper describes a new collection for multi-hop Question Answering (QA). The new collection represents an important contribution to the community given that: 1) there are few datasets of this type, 2) other datasets are smaller and 3) this dataset contains explanations about how to obtain the answer. Besides, the process for obtaining the dataset is mostly automatic (it only requires to create some logical rules). So, I think this paper would help to advance the current state of the art in multi-hop QA systems.\nI only miss a deeper analysis of errors in Section 5.3. I think it is important to manually analyze the errors in order to detect why human performance is below 90. The authors claim that it could be because some questions could be unanswerable given the supporting information. I think it is important to know this feature of the dataset before releasing it to the research community.\nI also think that some of the inferred relations can be explicit in some text. For example, a text could contain that A is the grandmother/grandchildren of B (e.g. grandchildren\u00a0of\u00a0Queen Victoria). How do you check that a multi-hop inference is required? With the experiments using single-hop?\nI give below some details comment per section: Section 2.2: - Please, include an example for each type of question (or at least for type 4) Section 3.1: - \"entity. We used only a summary from each Wikipedia article as a paragraph that describes the entity\": How do you obtain such a summary?\nSection 4: - \"the system understand several logical rules.\" -- > \"the system understandS several logical rules.\"\nSection 5.3: - \"which might be because the mismatch information between Wikipedia and Wikidata makes questions unanswerable\": Before publishing the paper, the authors should check this information. It is important to know if come questions are unanswerable.\n- \"We conjecture that there are two main reasons why the score of the evidence generation task was low.\" : Again, I think you should perform a deeper analysis beyond these conjectures for the camera-ready version.\nSection 6:  - Include the main disadvantages of datasets for bullet \"Multi-hop questions in MRC domain\"  (they are already given in the Intro, so maybe you can refer to that section) - Appendix A6 seems to be empty. Maybe you should include a reference to algorithm 1 in such section. ", "label": [[77, 160, "Eval_pos_1"], [162, 305, "Jus_pos_1"], [306, 421, "Jus_pos_2"], [422, 520, "Eval_pos_2"], [521, 576, "Eval_neg_1"], [577, 906, "Jus_neg_1"], [907, 985, "Eval_neg_2"], [986, 1200, "Jus_neg_2"]]}
{"id": 165, "review": "paper_summary\nThe authors present an approach for the task of discourse dependency parsing (DDP), i.e. the task of identifying the structure and relationship between EDUs in a document. They explore different contextualized representations for DDP in a Sentence-First parsing framework, where a complete discourse tree is built up sentence by sentence. In addition, the authors propose a novel method for relation identification that exploits the writing patterns that people typically use to organize discourses. Experiments show that the proposed approaches outperform the state of the art on an English and a Chinese dataset.\nCONTRIBUTIONS: (1) The authors formulate the task of relation identification in a novel sequence labeling paradigm to take advantage of the inherent structural information in the discourse. \n(2) The authors develop an approach for contextualized EDU representations to dynamically capture the information needed for the DDP task at different text granularity levels. \n(3) The authors show empirical success of their approach. \nsummary_of_strengths\n- Overall, the paper is clear in its objectives and methodology followed. The work is well structured, easy to read and follow.\n-The approach is well motivated and addresses a problem that is relevant to the community.\n-The proposed approach is tested with reasonable models and appropriate experiments. The experimental results are promising, demonstrating the effectiveness of the proposed method. \nsummary_of_weaknesses\n- Lack of illustrative examples regarding the model outputs.\n-In the evaluation, I'm missing a detailed error analysis. It would be very enlightening to add a manual error analysis to figure out the most prevalent mistakes of the different approaches. \ncomments,_suggestions_and_typos\n- l. 25/26: \"... can benefit many downstream examples\" List/Cite some examples.\n-l. 40: \"... previous studies have shown the benefit...\" Citation?\n-l. 419: stmodel udy -> model study -l. 567: We -> we ", "label": [[1078, 1150, "Eval_pos_1"], [1151, 1204, "Eval_pos_2"], [1206, 1295, "Eval_pos_3"], [1297, 1380, "Eval_pos_4"], [1381, 1476, "Eval_pos_5"], [1502, 1560, "Eval_neg_1"], [1562, 1619, "Eval_neg_2"], [1620, 1751, "Jus_neg_2"]]}
{"id": 167, "review": "paper_summary\nThe paper suggests an alternative to attention that achieves similar results but with less electricity used. \nsummary_of_strengths\nThe results seem convincing. The work seems novel and the contribution and writing clear. \nsummary_of_weaknesses\nThe method helps in (theoretical?) energy consumption, but does not improve speed as GPUs do not benefit from this change. \ncomments,_suggestions_and_typos\nfixed from last version ", "label": [[145, 173, "Eval_pos_1"], [174, 194, "Eval_pos_2"], [199, 233, "Eval_pos_3"], [258, 381, "Eval_neg_1"]]}
{"id": 168, "review": "- Strengths: 1) an interesting task, 2) the paper is very clearly written, easy to follow, 3) the created data set may be useful for other researchers, 4) a detailed analysis of the performance of the model.\n- Weaknesses: 1) no method adapted from related work for a result comparison 2) some explanations about the uniqueness of the task and discussion on limitations of previous research for solving this problem can be added to emphasize the research contributions further.  - General Discussion: The paper presents supervised and weakly supervised models for frame classification in tweets. Predicate rules are generated exploiting language-based and Twitter behavior-based signals, which are then supplied to the probabilistic soft logic framework to build classification models. 17 political frames are classified in tweets in a multi-label classification task. The experimental results demonstrate the benefit of the predicates created using the behavior-based signals. Please find my more specific comments below: The paper should have a discussion on how frame classification differs from stance classification. Are they both under the same umbrella but with different levels of granularity?\nThe paper will benefit from adding a brief discussion on how exactly the transition from long congressional speech to short tweets adds to the challenges of the task. For example, does past research rely on any specific cross-sentential features that do not apply to tweets? Consider adapting the method of a frame classification work on congressional speech (or a stance classification work on any text) to the extent possible due to its limitations on Twitter data, to compare with the results of this work.\nIt seems \u201cweakly supervised\u201d and \u201cunsupervised\u201d \u2013 these two terms have been interchangeably used in the paper (if this is not the case, please clarify in author response). I believe \"weakly supervised\" is the more technically correct terminology under the setup of this work that should be used consistently throughout. The initial unlabeled data may not have been labeled by human annotators, but the classification does use weak or noisy labels of some sort, and the keywords do come from experts. The presented method does not use completely unsupervised data as traditional unsupervised methods such as clustering, topic models or word embeddings would.   The calculated Kappa may not be a straightforward reflection of the difficulty of frame classification for tweets (lines: 252-253), viewing it as a proof is a rather strong claim. The Kappa here merely represents the annotation difficulty/disagreement. Many factors can contribute to a low value  such as poorly written annotation guidelines, selection of a biased annotator, lack of annotator training etc. \n(on top of any difficulty of frame classification for tweets by human annotators, which the authors actually intend to relate to). \n73.4% Cohen\u2019s Kappa is strong enough for this task, in my opinion, to rely on the annotated labels.  Eq (1) (lines: 375-377) will ignore any contextual information (such as negation or conditional/hypothetical statements impacting the contributing word) when calculating similarity of a frame and a tweet. Will this have any effect on the frame prediction model? Did the authors consider using models that can determine similarity with larger text units such as perhaps using skip thought vectors or vector compositionality methods?   An ideal set up would exclude the annotated data from calculating statistics used to select the top N bi/tri-grams (line: 397 mentions entire tweets data set has been used), otherwise statistics from any test fold (or labeled data in the weakly supervised setup) still leaks into the selection process. I do not think this would have made any difference in the current selection of the bi/tri-grams or results as the size of the unlabeled data is much larger, but would have constituted a cleaner experimental setup.   Please add precision and recall results in Table 4.  Minor: please double check any rules for footnote placements concerning placement before or after the punctuation. ", "label": [[16, 36, "Eval_pos_1"], [40, 90, "Eval_pos_2"], [94, 151, "Eval_pos_3"], [155, 207, "Eval_pos_4"], [225, 284, "Eval_neg_1"], [288, 476, "Eval_neg_2"], [1022, 1120, "Eval_neg_2"], [1121, 1200, "Jus_neg_2"], [1201, 1367, "Eval_neg_3"], [1368, 1710, "Jus_neg_3"], [1711, 1882, "Eval_neg_4"], [1883, 2369, "Jus_neg_4"], [2371, 2550, "Eval_neg_5"], [2551, 2911, "Jus_neg_5"]]}
{"id": 169, "review": "paper_summary\nThe paper presents a new benchmark for Pre-trained Language Model (PLM) knowledge probing in the biomedical domain called MedLAMA and a new probing approach called Contrastive-Probe.\nThe dataset is based on the UMLS metathesaurus. It is composed of a set of 19 handpicked relations with 1k instances per relation. Based on these relations, the authors devised 19 prompts.\nThe paper also presents a new method for probing PLMs, called Contrastive-Probe. The method is a 2-step approach which begins by a light pre-training phase based on a cloze-stype self-retrieving task. Then the actual probing is performed using MedLAMA. Contrastive-Probe do not rely on the MLM head of the PLM being probed. The authors start by encoding the prompt and the available entities (extracted from UMLS). Then the 10 closest entity representations are selected with a Nearest Neighbor Search. One of the advantage of this approach is the possibility to easily include answers with multiple tokens.\nThe paper apply MedLAMA using Contrastive-Prove and other approaches on different PLMs (general and biomedical domain) and show that their approach allow to better measure the biomedical knowledge included in the models. \nsummary_of_strengths\nThe paper is building on previous work in the relatively new domain of PLM knowledge probing (most citations are after 2020). The authors manage to take some distance with the domain and present a interesting overview of the current literature. They build upon this literature and present a new benchmark + a new method for knowledge probing.\nPrincipal strengths of the paper: 1. The paper present an in-depth state-of-the-art related to PLM knowledge probing. \n2. There is an extensive comparison to the literature. \n3. There is an interesting and extensive analysis of the proposed approach. \n4. The authors investigates how much knowledge is stored in each Transformer layer. \n5. The paper include robustness tests (sensitivity to the random seed). \nsummary_of_weaknesses\n1. According to table 8, the authors performed 500 training steps during the rewire phase. Why is this hyper-parameter not optimized? It seems by looking at Figure 4 and 9, that the best performance append before reaching this point. \ncomments,_suggestions_and_typos\nNone ", "label": [[1363, 1481, "Eval_pos_1"], [1482, 1579, "Eval_pos_2"], [1617, 1697, "Eval_pos_3"], [1702, 1753, "Eval_pos_4"], [1758, 1830, "Eval_pos_5"]]}
{"id": 170, "review": "paper_summary\nThis paper presents a method to improve text generation by refining the target output while decoding. The idea is that at each decoding step t, the model predicts not only the current word but re-processes and predicts previous ones (in a window of k-words), so some generation errors can be corrected. \nsummary_of_strengths\n-   The idea of simultaneously refining and decoding is sound. It has the potential to improve text generation with low overhead of resources.\n-  The results on three tasks show that their model works well in comparison to a standard model, however, comparisons with similar methods are missing. \nsummary_of_weaknesses\n- Comparison of results with equivalent methods described on related work are missing. For example, post-editing or multiple pass decoders.\n-The general idea is well described but several details are not clear in the paper (refer to comments for details). This information is important for understanding and reproducibility, and it is necessary for properly reviewing the work. \ncomments,_suggestions_and_typos\n1 . Line 050: A common way to address the issue is using beam search.\n2. Lines 076-079 Question: Do you replace a word independent of the context? For example, if you replace w_i, what happens with w_{i-1} and w_{i+1}. This could create incoherent text. I will assume that it is better to replace the whole phrase instead of independent words. \n3. Lines 197-198 Question: How do you calculate the joint probability P(y_1, y_2, ..| y_{<i}, X) if this part is not autoregressive?  I suggest adding the equation to the paper for clarity.\n4. Line 197 Question: What it means the \u201c0\u201d? \n5. Lines 185-191 Question: Same question as in (2).  It is not clear if you replace the whole phrase or each word? \n6. Lines 199-207. The decoding algorithm mixing greedy and beam search should be added as pseudocode for clarity. \n7. Equation 12. I assume that P_g and P_r shared the same parameters, so their calculation varies only for the use of different masks. Does this imply making two passes of the decoder? Please clarify these points in the paper. \n8. As far as I understand, the refining mask is used only for training. Is this correct? ", "label": [[343, 401, "Eval_pos_1"], [402, 481, "Jus_pos_1"], [485, 579, "Eval_pos_2"], [580, 634, "Eval_neg_1"], [660, 744, "Eval_neg_2"], [745, 797, "Jus_neg_2"], [799, 880, "Eval_neg_3"], [881, 1035, "Jus_neg_3"]]}
{"id": 171, "review": "paper_summary\nThis paper analyses whether task-specific transformer models (i.e., fine-tuned BERT models) are successful at brain encoding tasks. The authors compare the performance of encoding models across a wide range of NLP tasks and discuss the differences between encoding fMRI stimuli from reading vs. from listening to stories. They also analyse the results for various language specific brain regions. \nsummary_of_strengths\nThe paper provides initials answers to an important open questions the field of brain encoding tasks from language stimuli. \nThe methodology seems appropriate and sound. \nThe paper has a well written introduction that summarizes the current state of the field. The paper clearly distinguished between brain encoding and decoding tasks for clarification. \nsummary_of_weaknesses\nThis is a revision of a previously submitted paper. I have read the author response and the new version of the paper. The weaknesses stated in my original review have been addressed and the writing has been improved.\nMy main concern was the reasoning behind the conclusions: It is very tricky to reach clear conclusions between reading and listening from two such different datasets. The differences might be due to the stimulus presentation, but could also arise from countless other factors such as experimental conditions, the text domain of the stimuli or the number of voxels. \nThis is clearly difficult to test due to the limited availability of such datasets, but therefore requires a careful discussion. While this still is a risk to the conclusions that are drawn, it is now described in the discussion. \ncomments,_suggestions_and_typos\n- Please put the equation in section 4.3 on separate lines for better readability -Capitalization of subsection titles is inconsistent. ", "label": [[433, 557, "Eval_pos_1"], [558, 602, "Eval_pos_2"], [604, 693, "Eval_pos_3"], [694, 786, "Eval_pos_4"], [1027, 1083, "Eval_neg_1"], [1085, 1521, "Jus_neg_1"]]}
{"id": 172, "review": "paper_summary\nThe authors propose to add a confidence estimating network, acting as a gating unit that allow ground-truth probability to be merged in to the decoder probability. The confidence network will produce a loss by adding up the gating values, which is combined with cross-entropy loss in training with a hyperparameter lambda. \nsummary_of_strengths\n- In Table 1, Conf outperforms TP (model probability) for all language pairs.\n-proposed an lambda annealing schedule to avoid training the confidence network as an finetuning step \nsummary_of_weaknesses\n- In Table 1, although Conf outperforms TP, but the advantage of D-Conf comparing to D-TP is modest.  Given these numbers are Pearson's correlation, practically if the correlation cannot be improved by around 10% in absolute term, it's unlikely people will adopt it for estimating confidence scores given the extra cost on model training, inference cost and hyperparameter optimization.\n- The hyper-parameter is crucial for the proposed loss in this paper. If lambda is too strong, then no confidence will be learned, c_t is always 1. If lambda is too small, perhaps the model will always produce c_t = 0 to peak the ground truth. However, critically, I was not able to find information about how the authors found/optimized the hyper-parameter. \ncomments,_suggestions_and_typos\nMany mistakes, for example Eq.6 does not have the summation. And also grammar/spelling mistakes. ", "label": [[564, 662, "Eval_neg_1"], [664, 948, "Jus_neg_1"], [951, 1192, "Jus_neg_2"], [1193, 1307, "Eval_neg_2"], [1341, 1355, "Eval_neg_3"], [1356, 1437, "Jus_neg_3"]]}
{"id": 173, "review": "paper_summary\nThe paper addresses the problem of privacy for document embeddings. The authors define a strong sentence-level privacy for documents and propose deepcandidate, an unsupervised embedding technique to generate sentence-private document embeddings. Experiments on sentiment analysis and text classification were conducted to show the usefulness and effectiveness of the proposed method. \nsummary_of_strengths\n- The paper is theoretically sound. There are a lot of mathematical notations in the paper but the authors defined them clearly, explained in detail, also showed the some necessary derivations (in Appendix) to help the readers to understand.\n-Experimental results show SentDP performs much better on stronger privacy guarantee, empirically proved the previous theoretical statements.\n-The authors proposed a novel approach from a strong privacy definition for document embeddings, which is significant different from the existing works. \nsummary_of_weaknesses\n- It would be nice to show more experimental results of the proposed method from more aspects such as an ablation study or parameter analysis.\n-There is still a clear gap between the private embeddings and non-private ones. For some baseline approaches the embedding performances are basically random guessing. The paper failed to explain this tradeoff and potential solutions. \ncomments,_suggestions_and_typos\n- One question: x' is sampled within certain distance from x in the embedding space, does that mean they are semantically/syntactically similar? If so, how much high-level concepts from the documents can be hided? ", "label": [[422, 455, "Eval_pos_1"], [456, 661, "Jus_pos_1"], [663, 803, "Eval_pos_2"], [805, 956, "Eval_pos_3"], [982, 1073, "Eval_neg_1"], [1074, 1121, "Jus_neg_1"], [1124, 1203, "Eval_neg_2"], [1204, 1357, "Jus_neg_2"]]}
{"id": 174, "review": "paper_summary\nThe paper introduces a framework for learning a taxonomy using cognitive data, e.g., fMRI data. The authors focus on the NLP domain and compare the performance of this method with the previously used visual Taskonomy method (Saaty, 1987, Zamir et al., 2018). They find that their method performs better than random and that it provides taxonomy results that seem intuitive. While the method doesn't provide performance as high as the one in Zamir et al. (2018), it requires less compute and might therefore function as a potential alternative. \nsummary_of_strengths\nThe authors take an influential taxonomy idea from the visual domain and explore it in the domain of NLP. The learned taxonomy is based on cognitive data, providing an interesting approach to the task. \nsummary_of_weaknesses\nIn my view, the paper would benefit from refocusing on exploring its main contributions. First of all, the abstract states that the method results in \"competitive [performance] to the Analytic Hierarchy Process (Saaty, 1987) used in visual Taskonomy (Zamir et al., 2018)\". When inspecting the results, the performance is better than random but still far from the performance of the AHP method. The core contribution, therefore, seems to be that their method doesn't require expensive transfer learning between tasks. This point should be highlighted and explained further: Where does this become especially relevant and to what extent does it matter? The authors write that more advantages over AHP will be shown later (line 533ff) but it remains unclear to me when and where we will learn about those.\nSecondly although the CogTaskonomy framework is introduced as a combination of two separate methods, there is no analysis how important their combination is. CRA seems to be more important than CNM and in combination with the BERT model, their combination barely matters. Is there an intuition why one would be more important than the other? Similarly, a comparison of which cognitive data is used might be interesting as well (i.e., fMRI vs. eyetracking data, for instance).\nFinally, the taxonomy itself could be an interesting contribution but its exploration remains very limited. The authors visualize the taxonomy learned by one component of the CogTaskonomy framework (CNM) which also seems intuitive (Figure 4). However, it remains unexplored whether this is the same for all tested methods or even just what it would be for the complete CogTaskonomy framework.\nOverall, I found the paper slightly difficult to follow. Making the contribution more pronounced and how this specifically connects to the results and future opportunities might help with this. A more direct method comparison with respect to data needed, training pipeline and other important points of distinction would have also helped me. There were also a couple of conclusions that didn't follow for me from the data which I'll specify in the comments and suggestions. \ncomments,_suggestions_and_typos\n*Conclusions that didn't quite follow for me:* - line 511ff: I can't follow why a higher performance of TinyBERT over BERT suggests that TinyBERT \"mak[es] sentence representations more relevant to individual tasks [through knowledge distillation] and hence result[s] in better task similarity estimation.\"\n-line 551ff: \"RSA is able to be adapted to NLP task structure detection\" seems to be already argued with a single configuration where it outperforms random.  -line 572ff: \"This is consistent with our previous finding in the main results that TinyBERT (with KD) captures more task-relevant knowledge than BERT for task relation detection.\" This is argued based on the finding that \"with a small number of cognitive signals (voxels), TinyBERT for CNM can achieve a good task ranking score\" (line 566ff). Is there a way to make this more intuitive?\n*General comments:* In the contributions (line 110), the authors state that the \"CNM is able to learn stable task relations\" (line 128f). However, this is only one of two components of the the proposed CogTaskonomy framework. What about the CRA?\nThe phrase \"sentence-level textual stimuli of cognitive data\" (e.g., line 259f) is very obscure. I recommend expanding on it once to clarify.\nI didn't understand the paragraph 4.1 Cognitive Dataset. Specifically, it's not intuitive to me why the voxels had to be randomly selected (line 400f).\nI think it would be useful to mention to the reader that high scores are undesirable in the TRS metric (e.g., Table 1 and Figure 3).\nThe authors try to make an argument across architectures but only based on BERT architectures which seems a weak generalizability claim. I would suggest either weakening the phrasing or providing more models.\n*Typos:*  - abstract: CogTaxonomy -> CogTaskonomy -line 83f: clarify that these terms are introduced by the authors and are not pre-existing parts -line 265: concepts -line 557: more robust ", "label": [[686, 781, "Eval_pos_1"], [805, 893, "Eval_neg_1"], [894, 1607, "Jus_neg_1"], [1608, 1765, "Eval_neg_2"], [1766, 2083, "Jus_neg_2"], [2084, 2191, "Eval_neg_3"], [2192, 2476, "Jus_neg_3"], [2477, 2533, "Eval_neg_4"], [2534, 2950, "Jus_neg_4"]]}
{"id": 175, "review": "The paper describes an MT training data selection approach that scores and ranks general-domain sentences using a CNN classifier. Comparison to prior work using continuous or n-gram based language models is well done, even though  it is not clear of the paper also compared against bilingual data selection (e.g. sum of difference of cross-entropies). \nThe motivation to use a CNN instead of an RNN/LSTM was first unclear to me, but it is a strength of the paper to argue that certain sections of a text/sentence are more important than others and this is achieved by a CNN. However, the paper does not experimentally show whether a BOW or SEQ (or the combination of both( representation is more important and why. \nThe textual description of the CNN (one-hot or semi-supervised using pre-trained embeddings)  is clear, detailed, and points out the important aspects. However, a picture of the layers showing how inputs are combined would be worth a thousand words.\nThe paper is overall well written, but some parentheses for citations are not necessary (\\citet vs. \\citep) (e.g line 385).\nExperiments and evaluation support the claims of the paper, but I am a little bit concerned about the method of determining the number of selected in-domain sentences (line 443) based on a separate validation set: -What validation data is used here? It is also not clear on what data hyperparameters of the CNN models are chosen. How sensitive are the models to this?\n-Table 2 should really compare scores of different approaches with the same number of sentences selected. As Figure 1 shows, the approach of the paper still seems to outperform the baselines in this case.  Other comments: -I would be interested in an experiment that compares the technique of the paper against baselines when more in-domain data is available, not just the development set.\n-The results or discussion section could feature some example sentences selected by the different methods to support the claims made in section 5.4.\n-In regards to the argument of abstracting away from surface forms in 5.4: Another baseline to compare against could have been the work of Axelrod, 2015, who replace some words with POS tags to reduce LM data sparsity to see whether the word2vec embeddings provide an additional advantage over this.\n-Using the sum of source and target classification scores is very similar to source & target Lewis-Moore LM data selection: sum of difference of cross-entropies. A reference to this work around line 435 would be reasonable.\nFinally, I wonder if you could learn weights for the sum of both source & target classification scores by extending the CNN model to the bilingual/parallel setting. ", "label": [[130, 217, "Eval_pos_1"], [218, 306, "Eval_neg_1"], [307, 350, "Jus_neg_1"], [353, 574, "Eval_pos_2"], [575, 714, "Jus_pos_2"], [716, 867, "Eval_pos_3"], [868, 965, "Eval_neg_2"], [966, 1000, "Eval_pos_4"], [1001, 1053, "Eval_neg_3"], [1054, 1088, "Jus_neg_3"], [1090, 1149, "Eval_pos_4"], [1150, 1303, "Eval_neg_4"], [1304, 1662, "Jus_neg_4"]]}
{"id": 176, "review": "The paper presents an application of Pointer Networks, a recurrent neural network model original used for solving algorithmic tasks, to two subtasks of Argumentation Mining: determining the types of Argument Components, and finding the links between them. The model achieves state-of-the-art results.\nStrengths: - Thorough review of prior art in the specific formulation of argument mining handled in this paper.\n-Simple and effective modification of an existing model to make it suitable for the task. The model is mostly explained clearly.\n-Strong results as compared to prior art in this task.\nWeaknesses: - 071: This formulation of argumentation mining is just one of several proposed subtask divisions, and this should be mentioned. For example, in [1], claims are detected and classified before any supporting evidence is detected. \nFurthermore, [2] applied neural networks to this task, so it is inaccurate to say (as is claimed in the abstract of this paper) that this work is the first NN-based approach to argumentation mining.\n-Two things must be improved in the presentation of the model: (1) What is the pooling method used for embedding features (line 397)? and (2) Equation (7) in line 472 is not clear enough: is E_i the random variable representing the *type* of AC i, or its *identity*? Both are supposedly modeled (the latter by feature representation), and need to be defined. Furthermore, it seems like the LHS of equation (7) should be a conditional probability.\n-There are several unclear things about Table 2: first, why are the three first baselines evaluated only by macro f1 and the individual f1 scores are missing? \nThis is not explained in the text. Second, why is only the \"PN\" model presented? Is this the same PN as in Table 1, or actually the Joint Model? What about the other three?\n-It is not mentioned which dataset the experiment described in Table 4 was performed on.\nGeneral Discussion: - 132: There has to be a lengthier introduction to pointer networks, mentioning recurrent neural networks in general, for the benefit of readers unfamiliar with \"sequence-to-sequence models\". Also, the citation of Sutskever et al. (2014) in line 145 should be at the first mention of the term, and the difference with respect to recursive neural networks should be explained before the paragraph starting in line 233 (tree structure etc.).\n-348: The elu activation requires an explanation and citation (still not enough well-known).\n-501: \"MC\", \"Cl\" and \"Pr\" should be explained in the label.\n-577: A sentence about how these hyperparameters were obtained would be appropriate.\n-590: The decision to do early stopping only by link prediction accuracy should be explained (i.e. why not average with type accuracy, for example?).\n-594: Inference at test time is briefly explained, but would benefit from more details.\n-617: Specify what the length of an AC is measured in (words?).\n-644: The referent of \"these\" in \"Neither of these\" is unclear.\n-684: \"Minimum\" should be \"Maximum\".\n-694: The performance w.r.t. the amount of training data is indeed surprising, but other models have also achieved almost the same results - this is especially surprising because NNs usually need more data. It would be good to say this.\n-745: This could alternatively show that structural cues are less important for this task.\n-Some minor typos should be corrected (e.g. \"which is show\", line 161).\n[1] Rinott, Ruty, et al. \"Show Me Your Evidence-an Automatic Method for Context Dependent Evidence Detection.\" EMNLP. 2015.\n[2] Laha, Anirban, and Vikas Raykar. \" An Empirical Evaluation of various Deep Learning Architectures for Bi-Sequence Classification Tasks.\" COLING. 2016. ", "label": [[314, 412, "Eval_pos_1"], [414, 502, "Eval_pos_2"], [503, 541, "Eval_pos_3"], [543, 596, "Eval_pos_4"], [611, 737, "Eval_neg_1"], [738, 1037, "Jus_neg_1"], [1039, 1099, "Eval_neg_2"], [1101, 1484, "Jus_neg_2"], [1486, 1532, "Eval_neg_3"], [1534, 1817, "Jus_neg_3"], [1934, 1995, "Eval_neg_4"], [1996, 2365, "Jus_neg_4"]]}
{"id": 177, "review": "This paper proposes a neural-styled topic model, extending the objective of word2vec to also learn document embeddings, which it then constrains through sparsification, hence mimicking the output of a topic model.\nI really liked the model that the authors proposed, and found the examples presented by the authors to be highly promising. What was really missing from the paper, however, was any empirical evaluation of the model -- evaluation entirely falls back on tables of examples, without any indication of how representative the examples are, or any attempt to directly compare with standard or neural topic models. Without empirical evaluation, it is impossible to get a sense of the true worth of the model, making it very hard to accept the paper. Some ideas of how the authors could have achieved this: (1) use the topic representation of each document in a supervised document categorisation setup to compare against a topic model with the same topic cardinality (i.e. as an indirect evaluation of the quality of the representation); or (2) through direct evaluation over a dataset with document similarity annotations (based on pairwise comparison over topic vectors).\nIt's fantastic that you are releasing code, but you have compromised anonymity in publishing the github link in the submitted version of the paper (strictly speaking, this is sufficient for the paper to be rejected outright, but I leave that up to the PCs) Other issues: - how did you select the examples in Figures 3-6? presenting a subset of the   actual topics etc. potentially reeks of cherry picking.\n- in Section 2.2.1 you discuss the possibility of calculating word   representations for topics based on pairwise comparison with each word in   the vocabulary, but this is going to be an extremely expensive process for a   reasonable vocab size and number of topics; is this really feasible?\n- you say that you identify \"tokens\" using SpaCy in Section 3.1 -- how? You   extract noun chunks (but not any other chunk type), similarly to the Section   3.2, or something else? Given that you go on to say that you use word2vec   pre-trained embeddings (which include only small numbers of multiword   terms), it wasn't clear what you were doing here.\n- how does your model deal with OOV terms? Yes, in the experiments you report   in the paper you appear to train the model over the entire document   collection so it perhaps isn't an immediate problem, but there will be   contexts where you want to apply the trained model to novel documents, in   which case the updating of the word2vec token embeddings is going to mean   that any non-updated (OOV, relative to the training collection) word2vec   embeddings are not going to be directly comparable to the tuned embeddings.\n- the finding that 20 topics worked best over the 20 Newsgroups corpus wasn't   surprising given its composition. Possibly another (very simple) form of   evaluation here could have been based on some information-theoretic   comparison relative to the true document labels, where again you would have   been able to perform a direct comparison with LDA etc.\n- a couple of other neural topic models that you meed to compare yourself with   are: Cao, Ziqiang, Sujian Li, Yang Liu, Wenjie Li, and Heng Ji. \" A Novel Neural Topic Model and Its Supervised Extension.\" In AAAI, pp. 2210-2216. 2015.\nNguyen, Dat Quoc, Richard Billingsley, Lan Du, and Mark Johnson. \" Improving Topic Models with Latent Feature Word Representations.\" Transactions of the Association for Computational Linguistics 3 (2015): 299-313.\nShamanta, Debakar, Sheikh Motahar Naim, Parang Saraf, Naren Ramakrishnan, and M. Shahriar Hossain. \" Concurrent Inference of Topic Models and Distributed Vector Representations.\" In Machine Learning and Knowledge Discovery in Databases, pp. 441-457. Springer International Publishing, 2015.\nLow-level things: line 315: \"it's similarity\" -> \"its similarity\" line 361: what does it mean for the \"topic basis\" to be affected (and the \"are\" is awkward here) - in the caption of Figure 5, the examples should perhaps be \"terms\" rather   than \"words\" - the reference formatting is all over the place, e.g. \"Advances in ...\",   \"Advances in Neural ...\", Roder et al. is missing the conference name, etc. ", "label": [[214, 265, "Eval_pos_1"], [270, 336, "Eval_pos_2"], [338, 428, "Eval_neg_1"], [432, 1180, "Jus_neg_1"], [2763, 2875, "Eval_neg_2"], [2875, 3118, "Jus_neg_2"]]}
{"id": 178, "review": "The paper introduces BioMedBert, a BERT-like model trained on BREATHE, a very large bio-medical dataset (10Bwords). The model is trained started from BERT_Large model and then fine-tuned on some of the original task datasets (Squad 1 and Squad 2) but also more interestingly on bio-medical task datasets (NER, Relation Extration, Question-Answering). Evaluations are provided for these tasks that show that BioMedBert outperforms BERT and also that BioMedBert is state-of-the-art on QA tasks over several BioASQ datasets. \nThe paper also shows the interest of BioMedBert (versus other generic embeddings) to improve information retrieval of medical papers by reranking the list of documents returned by Elastic Search. \nThe paper does not introduce new techniques or notions but is clear and well written. And BioMedBert should be an interesting resource for health applications which shows there are a room for domain-specific BERT-like models when there are enough data for training. However, the paper mentions that it is still better to start from the general domain BERT, and it could also be interesting to check how much specific domain data is really necessary to add in order to achieve good results. Also, in order to test the stability of BioMedBert (on general domain), as done for Squad, it would be nice to get the performance of BioMedBert on the other original BERT-tasks In table 5 about Relation Extraction, I was a bit surprised to observe that recall decreases strongly when using BioMedBert. Do you have some explanation ? ", "label": [[720, 805, "Eval_pos_1"], [806, 878, "Eval_pos_2"], [879, 984, "Jus_pos_2"], [1426, 1512, "Eval_neg_2"]]}
{"id": 179, "review": "paper_summary\nThis paper concerns compounds and elaborate expressions (EEs) in Hmong, Lahu, and Chinese. An elaborate expression is an expression ABAC (A B1 A B2 in the paper but i prefer ABAC to indicate the identity of the words so swill use that) like \u201cpeople pile people lump\u201d fo ra throng of people. It starts with the prior observation that there is a phonological hierarchy that dictates the ordering of the B and C words (or for compounds the A and B words). It then fits SVMs/decision trees/etc to see if you can predict the ordering based on phonology, which is successful as a classifier.  It then looks to see if you can predict (out of sample) based on a neural embedding model. This classifier also works very well. \nsummary_of_strengths\nIt is nice to see a paper investigating a specific linguistic phenomenon of theoretical interest in a non-English, non-Indo-European language. There seems to be a rich prior literature on EEs in Hmong, Lahu, and Chinese and this paper contributes to that literature. \nsummary_of_weaknesses\nThe paper is framed as pitting, to some extent, distributional information against the phonological hierarchy. But these kinds of phonological constraints tend to be soft and violable. So I was not convinced by the takeaway of the first set of experiments using the phonological classifiers. What does this add on top of just examining the distributional patterns of phonological features in the compounds and EEs?  The motivation for the neural experiment was also a bit confusing to me. Essentially, it shows that you can predict the ordering of B and C in ABAC based on the training data, because B and C occur in that same ordering elsewhere in the corpus. This doesn\u2019t seem particularly informative as a result.\nAs the paper points out, there seem to be two routes to learning this information: (soft?) phonological constraints and the distributions in texts. But I\u2019m not fully convinced that this work breaks ground theoretically or methodologically. \ncomments,_suggestions_and_typos\nMight want to check out work by Emily Morgan and Roger Levy on binomial pairs, predicting the ordering of items in a phrase like \u201cpeanut butter and jelly\u201d. ", "label": [[752, 894, "Eval_pos_1"], [895, 1018, "Eval_pos_2"], [1042, 1226, "Jus_neg_1"], [1227, 1333, "Eval_neg_1"], [1458, 1530, "Eval_neg_2"], [1531, 1758, "Jus_neg_2"], [1907, 1998, "Eval_neg_3"]]}
{"id": 180, "review": "paper_summary\nThe paper discusses length bias for textual matching models, shows that models are prone to this bias, gives an explanation why, and proposes a solution to correct it. \nsummary_of_strengths\n- Identification of a new bias - Solution (adversarial training) to address it - Investigation of IR and \"NLP\" model settings \nsummary_of_weaknesses\n- This is the (n+1)st paper on discussing biases in models and datasets and it's not clear to me whether this specific bias hasn't been discovered before - not a single 2021 paper cited - models investigated (ESIM, etc.) are a bit old, no novel model is included - the explanation via probing is a bit trivial (this probing has been criticized btw., as unreliable [1]); it's also not clear to me why BERT performs best in the probing, but is least prone to the length diversion bias, questioning the apparent explanation - in l.237, authors write \"except for one combination\", but there are quite a few negative signs in Table 3, indicating that adversarial training is less often helpful - the losses from length bias are often small, e.g., 1 percentage point for BERT [1] https://aclanthology.org/2020.conll-1.8.pdf \ncomments,_suggestions_and_typos\n- Textual matching is also relevant for evaluation metrics [2], where similar biases (e.g., lexical overlap) are discovered. It would be interesting to extend this analysis and also consider models from such communities [2] https://aclanthology.org/2021.emnlp-main.701/ ", "label": [[355, 506, "Eval_neg_1"], [509, 722, "Jus_neg_1"], [723, 873, "Eval_neg_2"], [1044, 1088, "Eval_neg_3"], [1089, 1122, "Jus_neg_3"]]}
{"id": 182, "review": "paper_summary\nThis paper pursues a line of research about 'prompting', a method that leverages the prediction capability of pre-trained language models to help perform few-shot classification.  The paper explores one of the research topics in this line of research: how to find good mappings from the original task labels (e.g., positive vs negative) to tokens that can be predicted by the language model (the `label words', e.g., \"great\", \"perfect\" vs \"terrible\", \"awful\").  More specifically, the authors propose a simpler method to use multiple labels words; and a selection method that makes sure that no label word is shared by multiple labels. \nExperiments are presented, with comparison to baselines and earlier methods (Shick et al., 2020; Gao et al., 2021), followed by analyses and a discussion. \nThis is a second revised version. \nsummary_of_strengths\n- The authors propose a simple method to use multiple label words per class by summing their language model probabilities, and does not need to perform fine-tuning during label-word search.  Gao et al. (2021) instead only keep the best label word for each class, and perform fine-tuning multiple times during label-word search.\n-Tested on seven classification tasks in the BLUE benchmark, the method performs better in general than earlier prompting methods on few-shot experiments (n=16 examples in each class in train and in dev).\n-Each experiment is performed five times, mean and standard deviation are provided.\n-The Analysis and Discussion sections count three pages including tables and figures, and show example label words, examine the impact of deduplication, multiple labels, and label mapping, the impact of the number of training examples, and attempt to analyze why this methods works. \nsummary_of_weaknesses\n- (minor) Studying the correlation between inter-class JS divergence and performance is a good point.  However, I found its results not entirely convincing because it depends on the presence of two of the six tasks: CoLA and SST-2.  CoLA is a quite different task, and the authors also report the correlation without this task, which becomes much smaller.  At that point the presence of a positive correlation entirely depends on the presence of SST-2.  Or to phrase it differently, no such correlation is observed in the subset of tasks {QQP, MRPC, QNLI, RTE}. \ncomments,_suggestions_and_typos\n- The authors have taken into account all my previous comments, and plan to look into the correlation point above. ", "label": [[1889, 1941, "Eval_neg_1"], [1942, 2348, "Jus_neg_1"]]}
{"id": 183, "review": "paper_summary\nThis paper describes modifications made to the ReClor data set and ELECTRA, a system built to perform machine reading comprehension (MRC) on that data set, so that both the data and the system handle uncertainty in the selection of multiple-choice answers and abstain from providing an answer when the question posed is unanswerable.  ReClor is augmented to contain four variations of the same content/question/answer-choice triplets but with three versions of the same triplet containing a \"None of the above\" answer choice.  Then ELECTRA is modified to produce confidence scores for its answer selections using one of several metrics, the best-performing of which is expected entropy. \nsummary_of_strengths\nThe authors of this paper are correct in that most multiple-choice data sets like these don't include a \"None of the above\" option, and that most standardized tests, especially those that are adaptive (e.g. GRE) don't penalize test-takers for not answering a given question.  In order to mimic that, data sets need uncertainty built into them, which can be provided using the augmentation strategy here, if needed. \nsummary_of_weaknesses\nI think many members of our community will find this work rather niche.  It might be better suited for BEA.\nThe authors build on top of a system called ELECTRA which is a shared task entry that has not yet been released, or even published.  Therefore, it's difficult to understand the modifications made to ELECTRA described in this paper.\nThe authors should consider doing evaluations using other data sets besides ReClor to see if their augmentation strategy generalizes well. \ncomments,_suggestions_and_typos\nYour abstract is way too long.  You can shorten it starting at the sentence, \"This paper investigates both of these issues by making use of predictive uncertainty\", remove everything until you get to \"It is shown\", then replace \"uncertainty. It is shown\" with \"uncertainty, and shows\".  Also, in the last sentence of your abstract, \"it is shown that\" should be \"we show that\".\nOn line 050, replace \"consistently observed\" with \"been used to create\".\nOn lines 066-067, remove \", and of course...\" to the end of the sentence.\nOn lines 074-076, you can remove everything prior to \"answer uncertainty\" and begin this sentence with \"Answer uncertainty\" for clarity.\nOn line 192, insert \"give\" between \"and\" and \"no\".\nOn line 194, \"discourages\" should just be \"discourage\".\nOn line 234, \"increase demand\" should be \"increased demand\".\nOn line 273, \"ensembled-based\" should just be \"ensemble-based\".\nOn lines 338-339, change \"with unanswerable examples too\" to \"that also contain unanswerable examples\".\nIn Table 2, can you clarify what \"Paper\" and \"Others\" are?  Which papers do these come from?\nOn line 466, insert \"and\" in front of \"only\".\nCan you provide some examples content/question/answer-choice triplets from ReClor?  What is ReClor's domain? ", "label": [[723, 760, "Eval_pos_1"], [761, 996, "Jus_pos_1"], [1161, 1232, "Eval_neg_1"], [1234, 1268, "Jus_neg_1"], [1269, 1400, "Jus_neg_2"], [1402, 1500, "Eval_neg_2"], [1673, 1703, "Eval_neg_3"], [1705, 1958, "Jus_neg_3"]]}
{"id": 184, "review": "Review, ACL 2017, paper 256: This paper extends the line of work which models generation in dialogue as a sequence to sequence generation problem, where the past N-1 utterances (the \u2018dialogue context\u2019) are encoded into a context vector (plus potential other, hand-crafted features), which is then decoded into a response: the Nth turn in the dialogue. As it stands, such models tend to suffer from lack of diversity, specificity and local coherence in the kinds of response they tend to produce when trained over large dialogue datasets containing many topics (e.g. Cornell, Opensubtitles, Ubuntu, etc.). Rather than attempting to produce diverse responses using the decoder, e.g. through word-by-word beam search (which has been shown not to work very well, even lose crucial information about grammar and valid sequences), or via a different objective function (such as in Li et. al.\u2019s work) the authors introduce a latent variable, z, over which a probability distribution is induced as part of the network. At prediction time, after encoding utterances 1 to k, a context z is sampled, and the decoder is greedily used to generate a response from this. The evaluation shows small improvements in BLEU scores over a vanilla seq2seq model that does not involve learning a probability distribution over contexts and sampling from this.\nThe paper is certainly impressive from a technical point of view, i.e. in the application of deep learning methods, specifically conditioned variational auto encoders, to the problem of response generation, and its attendant difficulties in training such models. Their use of Information-Retrieval techniques to get more than one reference response is also interesting.  I have some conceptual comments on the introduction and the motivations behind the work, some on the model architecture, and the evaluation which I write below in turn: Comments on the introduction and motivations\u2026.  The authors seem not fully aware of the long history of this field, and its various facets, whether from a theoretical perspective, or from an applied one.\n1. \u201c[ the dialogue manager] typically takes a new utterance and the dialogue context as input, and generates discourse level decisions.\u201d          This is not accurate. Traditionally at least, the job of the dialogue manager is to select actions (dialogue acts) in a particular dialogue context. \nThe                    action chosen is then passed to a separate generation module for realisation. Dialogue management is usually done in the context of task-based systems which are goal driven. The dialogue manager is to choose actions which are optimal in some sense, e.g. reach a goal (e.g. book a restaurant) in as few steps as possible. See publications from Lemon & Pietquin, 2012, Rieser, Keizer and colleagues, and various publications from Steve Young, Milica Gasic and colleagues for an overview of the large literature on Reinforcement Learning and MDP models for task-based dialogue systems.\n2. The authors need to make a clear distinction between task-based, goal-oriented dialogue, and chatbots/social bots, the latter being usually no more than a language model, albeit a sophisticated one (though see Wen et. al. 2016). What is required from these two types of system is usually distinct. \nWhereas the former is required to complete a task, the latter is, perhaps only required to keep the user engaged. Indeed the data-driven methods that have been used to build such systems are usually very different. \n3. The authors refer to \u2018open-domain\u2019 conversation. I would suggest that there is no such thing as open-domain conversation - conversation is always in the context of some activity and for doing/achieving something specific in the world. And it is this overarching goal, the overarching activity, this overarching genre, which determines the outward shape of dialogues and determines what sorts of dialogue structure are coherent. Coherence itself is activity/context-specific. Indeed a human is not capable of open-domain dialogue: if they are faced with a conversational topic or genre that they have never participated in, they would embarrass themselves with utterances that would look incoherent and out of place to others already familiar with it. \n(think of a random person on the street trying to follow the conversations at some coffee break at ACL). This is the fundamental problem I see with systems that attempt to use data from an EXTREMELY DIVERSE, open-ended set of conversational genres (e.g. movie subtitles) in order to train one model, mushing everything together so that what emerges at the other end is just very good grammatical structure. Or very generic responses.  Comments on the model architecture: Rather than generate from a single encoded context, the authors induce a distribution over possible contexts, sample from this, and generate greedily with the decoder. It seems to me that this general model is counter intuitive, and goes against evidence from the Linguistic/Psycholinguistic literature on dialogue: this literature shows that people tend to resolve potential problems in understanding and acceptance very locally - i.e. make sure they agree on what the context of the conversation is - and only then move on with the rest of the conversation, so that at any given point, there is little uncertainty about the current context of the conversation. The massive diversity one sees results from the diversity in what the conversation is actually trying to achieve (see above), diversity in topics and contexts etc, so that in a given, fixed context, there is a multitude of possible next actions, all coherent, but leading the conversation down a different path.\nIt therefore seems strange to me at least to shift the burden of explaining diversity and coherence in follow-up actions to that of the linguistic/verbal/surface contexts in which they are uttered, though of course, uncertainty here can also arise as a result of mismatches in vocabulary, grammars, concepts, people\u2019s backgrounds etc. But this probably wouldn\u2019t explain much of the variation in follow-up response.  In fact, at least as far as task-based Dialogue systems are concerned, the challenge is to capture synonymy of contexts, i.e. dialogues that are distinct on the surface, but lead to the same or similar context, either in virtue of interactional and syntactic equivalence relations, or synonymy relations that might hold in a particular domain between words or sequences of words (e.g. \u201cwhat is your destination?\u201d = \u201cwhere would you like to go?\u201d in a flight booking domain). See e.g. Bordes & Weston, 2016; and Kalatzis, Eshghi & Lemon, 2016 - the latter use a grammar to cluster semantically similar dialogues.\nComments on the evaluation: The authors seek to show that their model can generate more coherent, and more diverse responses. The evaluation method, though very interesting, seems to address coherence but not diversity, despite what they say in section 5.2: The precision and recall metrics measure distance between ground truth utterances and the ones the model generates, but not that between the generated utterances themselves (unless I\u2019m misunderstanding the evaluation method). \nSee e.g. Li et al. who measure diversity by counting the number distinct n-grams in the generated responses.\nFurthermore, I\u2019m not sure that the increase in BLEU scores are meaningful: they are very small. In the qualitative assessment of the generated responses, one certainly sees more diversity, and more contentful utterances in the examples provided. But I can\u2019t see how frequent such cases in fact are.\nAlso, it would have made for a stronger, more meaningful paper if the authors had compared their results with other work, (e.g. Li et. al) that use very different methods to promote diversity (e.g. by using a different objective function). The authors in fact do not mention this, or characterise it properly, despite actually referring to Li et. al. 2015. ", "label": [[1336, 1401, "Eval_pos_1"], [1402, 1705, "Jus_pos_1"], [1923, 2079, "Eval_neg_1"], [2080, 6727, "Jus_neg_1"], [6854, 6984, "Eval_neg_2"], [6986, 7321, "Jus_neg_2"], [7322, 7395, "Eval_neg_3"], [7397, 7620, "Jus_neg_3"], [7621, 7742, "Eval_neg_4"], [7743, 7976, "Jus_neg_4"]]}
{"id": 187, "review": "- Strengths: Clear description of methods and evaluation Successfully employs and interprets a variety of evaluations Solid demonstration of practicality of technique in real-world interactive topic modeling - Weaknesses: Missing related work on anchor words Evaluation on 20 Newsgroups is not ideal Theoretical contribution itself is small  - General Discussion: The authors propose a new method of interactive user specification of topics called Tandem Anchors. The approach leverages the anchor words algorithm, a matrix-factorization approach to learning topic models, by replacing the individual anchors inferred from the Gram-Schmidt algorithm with constructed anchor pseudowords created by combining the sparse vector representations of multiple words that for a topic facet. The authors determine that the use of a harmonic mean function to construct pseudowords is optimal by demonstrating that classification accuracy of document-topic distribution vectors using these anchors produces the most improvement over Gram-Schmidt. They also demonstrate that their work is faster than existing interactive methods, allowing interactive iteration, and show in a user study that the multiword anchors are easier and more effective for users.\nGenerally, I like this contribution a lot: it is a straightforward modification of an existing algorithm that actually produces a sizable benefit in an interactive setting. I appreciated the authors\u2019 efforts to evaluate their method on a variety of scales. While I think the technical contribution in itself is relatively small (a strategy to assemble pseudowords based on topic facets) the thoroughness of the evaluation merited having it be a full paper instead of a short paper. It would have been nice to see more ideas as to how to build these facets in the absence of convenient sources like category titles in 20 Newsgroups or when initializing a topic model for interactive learning.\nOne frustration I had with this paper is that I find evaluation on 20 Newsgroups to not be great for topic modeling: the documents are widely different lengths, preprocessing matters a lot, users have trouble making sense of many of the messages, and naive bag-of-words models beat topic models by a substantial margin. Classification tasks are useful shorthand for how well a topic model corresponds to meaningful distinctions in the text by topic; a task like classifying news articles by section or reviews by the class of the subject of the review might be more appropriate. It would also have been nice to see a use case that better appealed to a common expressed application of topic models, which is the exploration of a corpus.\nThere were a number of comparisons I think were missing, as the paper contains little reference to work since the original proposal of the anchor word model. \nIn addition to comparing against standard Gram-Schmidt, it would have been good to see the method from Lee et. al. (2014), \u201cLow-dimensional Embeddings for Interpretable Anchor-based Topic Inference\u201d. I also would have liked to have seen references to Nguyen et. al. (2013), \u201cEvaluating Regularized Anchor Words\u201d and Nguyen et. al. (2015) \u201cIs Your Anchor Going Up or Down? Fast and Accurate Supervised Topic Models\u201d, both of which provide useful insights into the anchor selection process.\nI had some smaller notes: -164: \u2026entire dataset -164-166: I\u2019m not quite sure what you mean here. I think you are claiming that it takes too long to do one pass? My assumption would have been you would use only a subset of the data to retrain the model instead of a full sweep, so it would be good to clarify what you mean.\n-261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\n-337: Usenet should be capitalized -338-340: Why fewer than 100 (as that is a pretty aggressive boundary)? Also, did you remove headers, footers, and/or quotes from the messages?\n-436-440: I would have liked to see a bit more explanation of what this tells us about confusion.\n-692: using tandem anchors Overall, I think this paper is a meaningful contribution to interactive topic modeling that I would like to see available for people outside the machine learning community to investigate, classify, and test hypotheses about their corpora.\nPOST-RESPONSE: I appreciate the thoughtful responses of the authors to my questions. I would maintain that for some of the complimentary related work that it's useful to compare to non-interactive work, even if it does something different. ", "label": [[13, 56, "Eval_pos_1"], [57, 117, "Eval_pos_2"], [118, 207, "Eval_pos_3"], [222, 258, "Eval_neg_1"], [259, 299, "Eval_neg_2"], [300, 340, "Eval_neg_3"], [1244, 1285, "Major_claim"], [1287, 1415, "Eval_pos_4"], [1417, 1500, "Eval_pos_5"], [1507, 1571, "Eval_neg_4"], [1572, 1630, "Jus_neg_4"], [1631, 1724, "Eval_pos_5"], [1936, 2051, "Eval_neg_5"], [2053, 2671, "Jus_neg_5"], [2672, 2829, "Eval_neg_6"], [2831, 3319, "Jus_neg_6"], [3369, 3416, "Eval_neg_7"], [3417, 3642, "Jus_neg_7"], [3653, 3829, "Jus_neg_8"], [3833, 3898, "Eval_neg_8"], [4203, 4441, "Major_claim"]]}
{"id": 188, "review": "paper_summary\nThe paper addresses a fundamental shortcoming in evaluating conversational question answering (CQA) models based on the gold-annotated question-answer pairs. The existing evaluation procedure ignores the impact of the model\u2019s output on upcoming exchanges, thereby detracting from real-world scenarios. Specifically, when a model flounders, the conversation is likely to be progressed differently than what is already annotated. The current evaluation mechanism does not account for these cases.\nThe paper first highlights this problem by using human annotators to converse with 4 CQA models, trained on QuAC. After collecting QA conversations, another group of annotators check the validity of conversations by (1) checking whether the question at each turn is valid and answerable, and (2) verifying whether the models\u2019 answers are correct. This experiment reveals that human evaluation substantially differs from gold-answer evaluation when the relative performance of the 4 models is compared using each evaluation method.\nThe paper finds that using predicted answers while retaining existing questions suffers from 3 drawbacks: unresolved coreference, incoherence, and change in the correct answer. Since the most frequent issue was related to coreference, the paper proposes a question-rewrite method to amend references by replacing them with the original entities, mentioned in the history. This method is empirically shown to be most aligned with human evaluation when compared to other evaluation methods.\nFinally, the paper suggests 3 critical areas that future models should focus on: (1) ability to detect question dependencies, (2) ability to detect unanswered questions, (3) evaluation under real-world settings. \nsummary_of_strengths\n- The paper focuses on an interesting problem and the insights provided in this work can be of interest to the community and spark future research.\n-The analysis of CQA models is much-needed in the literature. Once we understood where our models fall short, we can make meaningful progress in building more effective CQA models.\nOverall, I think the paper is a solid work. \nsummary_of_weaknesses\n- My main concern with this paper is the proposed question-rewrite strategy. I\u2019m not convinced this method covers a sufficiently large number of questions because out of 100 passages (or \\~3.7K questions based on Table 4 in the appendix), 23% become invalid (\\~850 questions), out of which 44% are attributed to unresolved coreference (\\~375 questions). As reported at L373, the proposed method rewrites 12% of the questions for all models (\\~45 questions per model), which hardly has a significant effect.\n-I don\u2019t understand why generating question-in-context lags behind the question-rewrite method. The question-rewrite method may make a question ungrammatical (as pointed out in $\\S$B) because the original entity can be a clause or the coreference resolution tool may make mistakes. On the other hand, the question-replace model is trained to generate contextualized questions and its output is more likely to be grammatically correct. \ncomments,_suggestions_and_typos\nL238-L241: The sentence seems to be repeated but with different agreements. ", "label": [[1765, 1808, "Eval_pos_1"], [1813, 1910, "Eval_pos_2"], [1912, 1972, "Eval_pos_2"], [1973, 2091, "Jus_pos_2"], [2092, 2135, "Major_claim"], [2161, 2235, "Eval_neg_1"], [2236, 2665, "Jus_neg_1"], [2667, 2761, "Eval_neg_2"], [2762, 3100, "Jus_neg_2"]]}
{"id": 190, "review": "paper_summary\nThe paper presents a rational-centric framework with human-in-the-loop to boost model out-of-distribution performance in few-shot learning scenarios. The proposed approach uses static semi-factual generation and human corrections to decouple spurious associations and bias models towards generally applicable underlying distributions, which enables fast and accurate generalization. Experimental results shows the superiority of the proposed approach on in-distribution and out-of-distribution settings, especially for few-shot learning scenarios. \nsummary_of_strengths\n- Improving out-of-distribution model performance specially in few-shot learning settings is an important problem of real-life need in the NLP community.\n-The proposed approach is simple and can be applied for NLP tasks where rationales can be easily identified and annotated. For instance, in sentiment analysis and text classification tasks.\n-The proposed approach provides cost savings in comparison to alternative approaches for data augmentation, and provides strong out-of-distribution as well as in-distribution performance in few-shot learning settings. \nsummary_of_weaknesses\n- The proposed approach is not fully automatic, and still requires human annotations for identifying rationales and correcting errors from the static semi-factual generation phase. While this annotation effort could be less significant that other data augmentation methods, it still presents a significant cost overhead.\n-It is not clear how to generalize this approach to other NLP tasks aside from sentiment analysis and text classification. For instance, it is not clear how to generalize this approach to other sequence-to-sequence tasks like machine translation.\n-Identifying rationales is not a simple problem, specifically for more complicated NLP tasks like machine translation. \ncomments,_suggestions_and_typos\nThe paper is well organized and easy to follow. Figure 2 is a bit cluttered and the \"bold\" text is hard to see, perhaps another color or a bigger font could help in highlighting the human identified rationales better. ", "label": [[586, 737, "Eval_pos_1"], [739, 860, "Eval_pos_2"], [861, 927, "Jus_pos_2"], [929, 1035, "Eval_pos_3"], [1040, 1144, "Jus_pos_3"], [1171, 1216, "Eval_neg_1"], [1217, 1489, "Jus_neg_1"], [1491, 1612, "Eval_neg_2"], [1613, 1855, "Jus_neg_2"], [1889, 1936, "Eval_pos_4"]]}
{"id": 191, "review": "paper_summary\nA comprehensive set of experiments are conducted which suggests that baseline wide MLPs with bag-of-words (BoW) input can perform just as well as recently popular graph-based models, such as TextGCN and HeteGCN, in topical text categorization. In addition to wide MLP, sequential BERT and DistilBERT models are fine-tuned, which overall yield state-of-the-art results on 5 well-known text categorization datasets. Parameter counts of the models as well as runtime are also compared, showing the effectiveness of wide MLP with BoW models. \nsummary_of_strengths\n- A comprehensive comparison of models on text categorization datasets demonstrating the effectiveness of a relatively simple baseline model -Consideration of model size, runtime performance in addition to accuracy metrics -Clear description, detailed explanations, acknowledgement of potential limitations, good insights \nsummary_of_weaknesses\n- Limited to multi-class topical text categorization, which is while important, is ultimately just one task -Fair amount of repetition, which is sometimes helpful but can also be a bit distracting (e.g., the difference between inductive learning and transductive learning is explained a couple of times, same with how they used BERT vs. DistilBERT) \ncomments,_suggestions_and_typos\n- Can parameter counts/training time for graph-based models be provided?\n-less classes -> fewer -Minaee et al. (2021) can be cited: Deep Learning\u2013based Text Classification: A Comprehensive Review -Dataset characteristics discussed  (line 359-376) can be combined with Table 2.\n-Missing period (line 224).\n-Sentence line 315-319 is ungrammatical. ", "label": [[576, 714, "Eval_pos_1"], [798, 895, "Eval_pos_2"], [921, 1026, "Eval_neg_1"], [1028, 1115, "Eval_neg_2"], [1116, 1267, "Jus_neg_2"]]}
{"id": 192, "review": "paper_summary\nThis paper proposes an extension of the pseudo-labeling method named PLATE for summarization distillation, extensive experiments on three datasets suggest the effectiveness of the proposed method versus vanilla sequence distillation. Ablation studies and empirical analysis reveal the performance gain may come from the diverse cross-attention distribution of the teacher model and concise and abstractive summaries generated from the teacher model. \nsummary_of_strengths\nThe paper is well-motivated and clearly presented;  The authors extensively study the effect of Pseudo-labeling with Larger Attention TEmperature on various summarization tasks. Substantial ablation studies and empirical analyses are also provided for revealing the performance gain over baseline methods, which may help future studies understand how to conduct effective summarization distillation. \nThe findings of the diverse cross attention pattern, the conciseness, and abstractiveness of produced summarization pseudo-labels as well as the attention focus of the teacher model help future studies develop better summarization models. \nsummary_of_weaknesses\nFrom Table 6, the difference of attention pattern results in a great difference in novel-n-grams ratios and length in the generated teacher summarization, but this does not necessarily translate to a better student summarization system in Table 2, further explanation or better ablation studies might be needed to understand how the length, novel-n-grams ratio, the different dataset will affect the distillation effects.  Given that this paper focuses on sequence distillation, wondering how the beam size, length penalty of the teacher model will affect the sequence distillation results? \ncomments,_suggestions_and_typos\nIt may be good to know if a better teacher model (BART-large, PEGASUS-large) produces better pseudo-labels for summarization distillation. ", "label": [[486, 536, "Eval_pos_1"], [538, 663, "Eval_pos_2"], [664, 791, "Eval_pos_3"], [792, 885, "Jus_pos_3"], [1149, 1396, "Jus_neg_1"], [1397, 1570, "Eval_neg_1"]]}
{"id": 195, "review": "This paper proposes to use an encoder-decoder framework for keyphrase generation. Experimental results show that the proposed model outperforms other baselines if supervised data is available.\n- Strengths: The paper is well-organized and easy to follow (the intuition of the proposed method is clear). It includes enough details to replicate experiments. Although the application of an encoder-decoder (+ copy mechanism) is straightforward, experimental results are reasonable and support the claim (generation of absent keyphrases) presented in this paper.\n- Weaknesses: As said above, there is little surprise in the proposed approach. Also, as described in Section 5.3, the trained model does not transfer well to new domain (it goes below unsupervised models). One of the contribution of this paper is to maintain training corpora in good quantity and quality, but it is not (explicitly) stated.\n- General Discussion: I like to read the paper and would be pleased to see it accepted. I would like to know how the training corpus (size and variation) affects the performance of the proposed method. Also, it would be beneficial to see the actual values of p_g and p_c (along with examples in Figure 1) in the CopyRNN model. From my experience in running the CopyNet, the copying mechanism sometimes works unexpectedly (not sure why this happens). ", "label": [[206, 252, "Eval_pos_1"], [253, 300, "Jus_pos_1"], [302, 354, "Eval_pos_2"], [355, 557, "Eval_pos_3"], [572, 637, "Eval_neg_1"], [638, 727, "Eval_neg_2"], [728, 763, "Jus_neg_2"], [922, 987, "Major_claim"]]}
{"id": 196, "review": "paper_summary\n**What is the task?**\nDebiased contrastive learning framework for unsupervised sentence representation learning **What has been done before?**\nPrevious works (contrastive learning based baselines) mostly utilize in-batch negatives to learn the uniformity, but the randomly negative sampling strategy may lead to sampling bias, such as false negatives and anisotropy representations. Different from these methods, the proposed framework adopts an in-stance weighting method for punishing false negatives and a gradient-based algorithm for generating noise-based negatives towards the most nonuniform points. In this way, the influence of false negatives can be alleviated and the model can better learn the uniformity. It finally reduces the sampling bias and improves the model performance.\n**What are the main contributions of the paper? How many of them are novel?**\n-First attempt to reduce the sampling bias in contrastive learning of unsupervised sentence representations.\n-Presented a new framework DCLR to alleviate the influence of sampling bias.\n-Experiments on 7 semantic textual similarity tasks show that our approach is more effective than competitive baselines on semantic textual similarity (STS) tasks using BERT and RoBERTa **What are the key techniques used to tackle this task?**\nThe core idea is to improve the random negative sampling strategy for alleviating the sampling bias problem.\n- A noise-based negatives generation strategy to reduce the bias caused by the anisotropy PLM-derived representations - initialize new negatives based on a Gaussian distribution and iteratively update these negatives by non-uniformity maximization.\n- An instance weighting method to reduce the bias caused by false negatives - similarity between original sentence and each negative **What are the main results? Are they significant?**\nExperiments on 7 semantic textual similarity tasks show that our approach is more effective than competitive baselines on semantic textual similarity (STS) tasks using BERT and RoBERTa \nsummary_of_strengths\n- First attempt to reduce the sampling bias in contrastive learning of unsupervised sentence representations.\n- Well-written and easy to read paper - Claims well-supported by ablation analysis and experimental results \nsummary_of_weaknesses\nSee comments below \ncomments,_suggestions_and_typos\n- It is unclear which evaluation set(s) were used for Figure 3 - In section 6.1, it is unclear why and how negative sampling (DCLR) has to change. It seems DCLR is independent of the positive data augmentation strategy used.\n- Why was STS-B and SICK-R chosen for figure 4-6 and not STS-avg ?\n- How does DCLR compare with SimCSE on figure 5? ", "label": [[2176, 2211, "Eval_pos_1"], [2214, 2282, "Eval_pos_2"], [2359, 2419, "Eval_neg_1"], [2422, 2503, "Eval_neg_2"], [2504, 2581, "Jus_neg_2"]]}
{"id": 197, "review": "paper_summary\nThis paper explores whether intermediate pre-training on visual knowledge can improve performance on commonsense reasoning tasks. The visual knowledge can be categorized into textual knowledge from captions and knowledge from image-caption pairs. Authors first study which types of knowledge can benefit which tasks. Besides, the authors also explore which knowledge transfer methods are beneficial for the downstream tasks. Experimental results show that visual knowledge helps tasks related to physical commonsense, especially in few-shot settings. Also, models can be further improved with the help of knowledge distillation and contrastive learning methods. \nsummary_of_strengths\n1. Meaningful analysis: It is a natural idea to study how vision modality can help textual tasks. And intermediate pre-training on visual knowledge is a great choice to evaluate the role of visual knowledge in the process. \n2. Comprehensive experiments: Authors attempt to reproduce the most recent methods including VidlanKD and Vokenization, and perform analysis on various aspects which can influence the transferability. I'm impressed by the hard work of authors. \nsummary_of_weaknesses\n1. The effect of different corpus: The sizes of different text corpus are different. But in the current experimental setup, authors didn't consider this factor. I'm also a bit confusing about why the authors choose MSCOCO as the caption dataset instead of the larger Conceptual Captions. \n2. Lack of knowledge probing tasks: Since the paper mainly studies how to evaluate the \"knowledge\" transfer, it is natural to consider adopting knowledge probing task such as LAMA (Petroni et al., 2019) to evaluate whether models already study the knowledge. \ncomments,_suggestions_and_typos\nMost comments are mentioned in the weakness part. I suggest authors add the knowledge probing tasks in the updated version and conduct some qualitative analysis to show if the models indeed learn the visual knowledge and which kinds of knowledge they learn more. ", "label": [[701, 720, "Eval_pos_1"], [722, 920, "Jus_pos_1"], [925, 951, "Eval_pos_2"], [952, 1164, "Jus_pos_2"], [1192, 1222, "Eval_neg_1"], [1224, 1476, "Jus_neg_1"], [1481, 1512, "Eval_neg_2"], [1514, 1736, "Jus_neg_2"]]}
{"id": 198, "review": "paper_summary\nThe paper proposes a neural model computing local coherence on the basis of entities, by constraining the input to noun phrases and proper names. This allows modeling the notion of focus. This approach is presented as more linguistically sound, it outperforms previous models in applications and leads to better explainability. \nsummary_of_strengths\nThe paper introduces a new model for computing local coherence that outperforms previous models on three applications. The approach is well-motivated and the authors provide a clear picture of how the paper relates to previous work. The methodology appears sound and is described in detail.  Besides the overall results in the end tasks, through further analyses, the authors show 1) that their method leads to more interpretable explanations about model behavior, and 2) patterns underlying the model's behavior. \nsummary_of_weaknesses\nIn the introduction, the authors say that computing coherence on the basis of words or subwords is incorrect from a linguistic perspective and that their approach is more sound. However, they do not directly state why this is the case. One way of achieving this would be, for instance, by commenting on an example, which would also serve as an introduction to the concept of discourse coherence. This would improve the readability of the paper, especially for a reader that is not so familiar with the research area. \ncomments,_suggestions_and_typos\nThe font in Figure 1 is very small. ", "label": [[483, 513, "Eval_pos_1"], [518, 595, "Eval_pos_2"], [597, 654, "Eval_pos_3"], [901, 1136, "Eval_neg_1"], [1137, 1418, "Jus_neg_1"]]}
{"id": 199, "review": "paper_summary\nThis seems to be a primarily theoretical ML paper that describes the _softmax bottleneck_ problem in neural LMs, which results from the fact that a single hidden state embedding cannot be close to the embeddings of all possible candidate next words under certain conditions. The paper then proposes an improvement for the traditional softmax use in neural LMs, namely the _multi-facet softmax_ (MFS). This is compared to an existing MoS approach of Yang et al., as well as some other baselines. The modeling experiments show that the described problem indeed occurs in the artificially designed data sets, and that the proposed MFS solution solved this problem. In addition, MFS yields better results than the baselines in terms of language modeling perplexity and in the task of answering ambiguous questions. \nsummary_of_strengths\nThe paper describes what seems to me like an important issue that may affect most/many of the existing LMs, and the results seem to be consistent across the presented tasks. \nsummary_of_weaknesses\nThe paper is very dense, with a lot of theoretical material, may be more suitable for a ML conference or a journal rather than *ACL conference. Having said that, I lack expertise in ML and may not have a good feel for where this kind of studies belong. \ncomments,_suggestions_and_typos\n- Line 036 - \"probabilities of becoming the next word\" reads awkward to me -Line 046 - should be \"man\" instead of \"king\"?\n-Line 166 - typo, \"thoery\" -Line 214 - theorem 2 could benefit from providing an intuitive explanation -Line 280 - typo, \"there existS\" ", "label": [[847, 954, "Eval_pos_1"], [959, 1020, "Eval_pos_2"], [1044, 1187, "Eval_neg_1"]]}
{"id": 200, "review": "paper_summary\nThe paper proposed a semantic and syntactic disentanglement approach based on Siamese Semantic Distangelement Model ($S^2DM$). This semantic dissociation from syntax is learned in representation obtained from the pre-trained models (mBERT and XLM-100). This decoupling model reduces the \u2018violation of syntactic constraint\u2019 and advances the answer-span boundary detection performance (for Question Answering (QA) tasks) in zero-shot cross-lingual transfer for low-resource languages. The proposed approach uses two-step training (with labeled parallel and task-specific datasets) and six linguistically inspired training losses in two combinations (models).  The Experiments are conducted on one task (QA), three datasets (MLQA, XQuAD, TyDiQA), and across nine languages. Extensive experiments and an ablation study are conducted to support the paper's claim. \nsummary_of_strengths\n1. The paper is well written and easy to follow. \n2. The proposed work is linguistically inspired and intuitive. The work or its variants (unsupervised version) may be extended to other cross-lingual/multilingual applications. \n3. The proposed approach's strength is mathematical analysis in section 3.3 and empirical analysis. The systematic study has been conducted and whenever required, the suitable argument and experiment are conducted, respectively. \n4. The paper's visual presentations (Figure-3, Figure-5, and Figure-6) are informative and thoughtful. \nsummary_of_weaknesses\n1. One problem I can notice is the scalability issue of the proposed method.  As mentioned in section 4.3, step-01 of training required labeled parallel sentences, limiting the proposed approaches to the few high-resource languages. \n2. The proposed model lacks a comparison with baseline(s) from the literature. For example, a baseline that uses external knowledge like LAKM (Yuan et al. 046 (2020); line-46) or Liang et al. (2021; line-50) can be taken. It is not necessarily the proposed model that should outperform these baselines but to get an idea of where this approach stands compared to previous work. Other possibilities to use the more popular pre-trained model like XLM-R or mT5 (encoder only). \n3. Many of the reported scores (particularly for mBERT) in Table-2,3 & 4 are close to baseline; the statistical significance test is recommended for reliability \ncomments,_suggestions_and_typos\n1. It is hard to read numbers in Figure-3 and Figure-5 with a printed copy. \n2. In Line 492, the referred table number is incorrect. \n3. Author(s) can plan to investigate the unsupervised approach as a future extension of this work which will be more promising. ", "label": [[897, 943, "Eval_pos_1"], [948, 1007, "Eval_pos_2"], [1008, 1121, "Eval_pos_3"], [1126, 1222, "Eval_pos_4"], [1223, 1351, "Jus_neg_4"], [1356, 1455, "Eval_pos_5"], [1482, 1555, "Eval_neg_1"], [1557, 1711, "Jus_neg_1"], [1716, 1791, "Eval_neg_2"], [1792, 2186, "Jus_neg_2"], [2190, 2283, "Eval_neg_3"], [2284, 2348, "Jus_neg_3"]]}
{"id": 201, "review": "paper_summary\nThis papers presents a new dataset for evaluating model performance on figurative language. The dataset is Wino-grad style: it contains figurative phrases with divergent meanings, allowing assessment of whether models can correctly identify the appropriate interpretation of figurative phrases. This dataset is composed via crowdsourcing, with extensive expert validation. An analysis of the dataset is presented, including some linguistic analysis of what types of figuration and syntactic structures are present, as well as analysis of what kinds of metaphors. They then evaluate two types of models on the proposed task of identifying the correct meaning. Additional prompting-based improvements are proposed, as well as some analysis of how the models can work for generating interpretations. Comprehensive error analysis is performed, with a particular eye for where model performance and human performance differ, offering some interesting insights into the difficulty of processing metaphoric language. \nsummary_of_strengths\nThe paper is clear and well-written, and presents a strong background knowledge of the linguistic aspects of metaphor. It is framed particularly well wrt. the background literature, and I personally found the authors attention to the linguistic aspects of the proposed dataset to be encouraging. The proposed dataset seems very well thought out, and the extensive expert checks indicate that the resulting dataset is of high quality. This kind of resource is currently valuable not just for the inspection of LMs as proposed, but also for paraphrasing and generation tasks, and I see this dataset being a very valuable resource. The analysis performed is good, and the difference between the auto-regressive and masked language models is an interesting finding. \nsummary_of_weaknesses\nThere is of course a fair amount of creativity in the pairs, which may yield some strange inferences. There's something about these paired sentences with the sarcastic reading (\"bright as coal\", \"easy as kindergarten for a newborn\") that seem particuarly forced: I wouldn't expect these sentences to be very common in everyday language, and then perhaps LM performance is expected, as they wouldn't have seen these things before. I feel they're a strange kind of sentence with a somewhat difficult semantics even for humans. For the pilot pair, I'm not sure I would get the intended meanings for the difference between \"ballet dancer\" and \"modern dancer\" - I feel the \"dancer\" alone could yield either reading. So there may be quirks in the dataset that make it tricky to interpret exactly what's going on with model performance.\nI'm curious about the strong performance of the RoBERTa model - could it be that this kind of classification task isn't particularly difficult, if we give the model enough training and data? I think the paper shows clear difficulties in the generation and inference aspects, but perhaps classification is getting better. \ncomments,_suggestions_and_typos\nIt should be noted that there are also lots of recent work on novel, challenging NLI sets, which the authors seem aware of. There are some recent and contemporary works that also deal with figurative language (Chakrabarty 2021, \"Figurative Language in RTE\", Poliak 2018, https://aclanthology.org/D18-1007/) the authors should be aware of: in my mind, this work provides a novel, high-quality dataset, they could benefit from clarifying how their work is distinct from these.\nA possible question: this work seems focused on starting with metaphors and getting literal interpretations. I think there are also significant applications the other way, particularly in metaphor generation, where the literal interpretations can be used as input and the metaphors as output. ", "label": [[1046, 1082, "Eval_pos_1"], [1087, 1163, "Eval_pos_2"], [1165, 1227, "Eval_pos_3"], [1232, 1340, "Eval_pos_4"], [1342, 1391, "Eval_pos_5"], [1396, 1479, "Eval_pos_6"], [1480, 1619, "Jus_pos_7"], [1624, 1674, "Eval_pos_7"], [1675, 1706, "Eval_pos_8"], [1711, 1807, "Jus_pos_8"], [1831, 2541, "Jus_neg_1"], [2542, 2660, "Eval_neg_1"], [3139, 3353, "Eval_neg_2"], [3354, 3489, "Jus_neg_2"]]}
{"id": 202, "review": "paper_summary\nThis paper proposes NoisyTune, which is a matrix-wise perturbing method by adding different uniform noises according to the standard deviations of different parameter matrices. \nsummary_of_strengths\nA simple method \nsummary_of_weaknesses\nThere are some questions: Q1. Even simple, I doubt its effectiveness. The experimental results of both GLUE and XTREAMRE show that the improvements are range from 0.2 to 0.8 (Avg score), which seems not significant. \nBesides, please report the standar\u00ad\u00add deviations of 5 times runs in Table 1 and Table 2.\nQ2. The over-fitting problem is more serious in large models, so the experiment of applying NoisyTune on larger PLMs will be more convincing.\nQ3.  The right Fig of Fig3 makes me confusing, which is not consistent with the claim \u201cthe relative change of L1-norms becomes smaller when NoisyTune is applied\u201d.\nQ4. Missing the most relevant work, \u00ad\u00ad\u00adraise a child in Large Language Model: Towards Effective and Generalizable Fine-tuning. Xu et al., EMNLP 2021. It masks out the gradients of the parameters during the backward process. The difference between NoisyTune and above-mentioned paper is using masking or noisy to perturb parameters. Thus I am very interested to see which method (masking or noisy) will have better benefits. \ncomments,_suggestions_and_typos\nSee above ", "label": [[213, 228, "Eval_pos_1"], [282, 321, "Eval_neg_1"], [322, 468, "Jus_neg_1"], [562, 619, "Jus_neg_2"], [620, 699, "Eval_neg_2"], [705, 746, "Eval_neg_3"], [747, 862, "Jus_neg_3"], [867, 898, "Eval_neg_4"], [899, 1286, "Jus_neg_4"]]}
{"id": 204, "review": "This paper proposes a method for recognizing lexical entailment (specifically, hypernymy) in context. The proposed method represents each context by averaging, min-pooling, and max-pooling its word embeddings. These representations are combined with the target word's embedding via element-wise multiplication. The in-context representation of the left-hand-side argument is concatenated to that of the right-hand-side argument's, creating a single vectorial representation of the input. This input is then fed into a logistic regression classifier.\nIn my view, the paper has two major weaknesses. First, the classification model used in this paper (concat + linear classifier) was shown to be inherently unable to learn relations in \"Do Supervised Distributional Methods Really Learn Lexical Inference Relations?\" ( Levy et al., 2015). Second, the paper makes superiority claims in the text that are simply not substantiated in the quantitative results. In addition, there are several clarity and experiment setup issues that give an overall feeling that the paper is still half-baked.\n= Classification Model = Concatenating two word vectors as input for a linear classifier was mathematically proven to be incapable of learning a relation between words (Levy et al., 2015). What is the motivation behind using this model in the contextual setting?\nWhile this handicap might be somewhat mitigated by adding similarity features, all these features are symmetric (including the Euclidean distance, since |L-R| = |R-L|). Why do we expect these features to detect entailment?\nI am not convinced that this is a reasonable classification model for the task.\n= Superiority Claims = The authors claim that their contextual representation is superior to context2vec. This is not evident from the paper, because: 1) The best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\n2) This experiment uses ready-made embeddings (GloVe) and parameters (context2vec) that were tuned on completely different datasets with very different sizes. Comparing the two is empirically flawed, and probably biased towards the method using GloVe (which was a trained on a much larger corpus).\nIn addition, it seems that the biggest boost in performance comes from adding similarity features and not from the proposed context representation. This is not discussed.\n= Miscellaneous Comments = - I liked the WordNet dataset - using the example sentences is a nice trick.\n- I don\u2019t quite understand why the task of cross-lingual lexical entailment is interesting or even reasonable.\n- Some basic baselines are really missing. Instead of the \"random\" baseline, how well does the \"all true\" baseline perform? What about the context-agnostic symmetric cosine similarity of the two target words?\n- In general, the tables are very difficult to read. The caption should make the tables self-explanatory. Also, it is unclear what each variant means; perhaps a more precise description (in text) of each variant could help the reader understand?\n- What are the PPDB-specific features? This is really unclear.\n- I could not understand 8.1.\n- Table 4 is overfull.\n- In table 4, the F1 of \"random\" should be 0.25.\n- Typo in line 462: should be \"Table 3\" = Author Response = Thank you for addressing my comments. Unfortunately, there are still some standing issues that prevent me from accepting this paper: - The problem I see with the base model is not that it is learning prototypical hypernyms, but that it's mathematically not able to learn a relation.\n- It appears that we have a different reading of tables 3 and 4. Maybe this is a clarity issue, but it prevents me from understanding how the claim that contextual representations substantially improve performance is supported. \nFurthermore, it seems like other factors (e.g. similarity features) have a greater effect. ", "label": [[550, 597, "Major_claim"], [598, 836, "Eval_neg_1"], [837, 954, "Eval_neg_2"], [955, 1086, "Eval_neg_3"], [1087, 1652, "Jus_neg_1"], [1653, 2520, "Jus_neg_2"]]}
{"id": 206, "review": "paper_summary\nThe paper conducts a large-scale human evaluation of human-machine conversations using four state-of-the-art CQA systems on the QuAC dataset. The motivation behind this study is that using gold history does not generalize well to real-world applications. In terms of model ranking, the study shows disagreement between human and gold history evaluations, which is expected, given that humans are interactive and will have followup questions based on model's outputs. An intuitive solution is using predicated history, however, this invalidates some questions. To this end, the paper also proposes a simple question re-writing mechanism based on co-reference resolution to correct invalid questions when the predicted history is used. The paper reports many interesting findings. \nsummary_of_strengths\nThe paper is well-written and easy to follow.\nThe motivation is clear and the experimental setup is sound The human evaluation of human-machine conversations and the findings in the paper are insightful and would improve how CQA models are evaluated, and how future CQA datasets are constructed. \nsummary_of_weaknesses\nIt seems that the proposed re-writing approach makes re-written questions context-independent and self-contained (i.e., easier for the models). For example, \"What songs were in Parade\" could be answered without history information. Moreover, the proposed mechanism seems to kill the naturality of a conversation. Can you please elaborate on this?\nWith human evaluation (shown in Table 2), the number of unanswerable questions is higher across all models. I wonder if this is an artefact of humans conversing with models or the fact that humans (annotators) did not have access to the full passage. \ncomments,_suggestions_and_typos\nAs a followup work, it would be insightful to conduct a similar study on different CQA datasets ", "label": [[748, 792, "Eval_pos_1"], [815, 860, "Eval_pos_2"], [861, 884, "Eval_pos_3"], [889, 920, "Eval_pos_4"], [921, 1110, "Eval_pos_5"], [1134, 1277, "Eval_neg_1"], [1278, 1365, "Jus_neg_1"], [1366, 1446, "Eval_neg_2"]]}
{"id": 207, "review": "paper_summary\nThe paper introduces a new fact-checking dataset, containing claims from a variety of fact-checking websites. No annotations are performed. Instead, the dataset combines each claim with its review written by a human fact-checker (working for the respective fact-checking site) and all background articles mentioned in the review. The main task to be solved using the dataset is claim verification based on background articles. The full dataset contains over 30k claims.\nThe authors experiment with various baselines models for solving this task in a two-step approach. The first is sentence retrieval with the goal to retrieve those sentences from all background articles that belong to background articles of a claim. The authors compare TF-IDF with a BERT architecture, where the latter shows better performance. In a second step, the top retrieved sentences are used for veracity prediction. Here, a Bi-LSTMs, hierarchical attention networks, and RoBERTa are compared, with the RoBERTa model outperforming the others.\nThe authors also show that there is a time-dependent topic shift in the dataset, which affects performance. That is, if the model was to be applied to new data collected in the future, a drop in performance is to be expected due to the topic shift. \nsummary_of_strengths\n-\tA large new dataset for fact-checking is introduced that will be made publicly available. \n-\tA two-step approach with various different baseline models is evaluated on the new dataset. \n-\tAn analysis regarding data-shift is performed, which is very relevant for the real-world application of models trained on the dataset. \n-\tThe paper is well written and easy to follow. \nsummary_of_weaknesses\n-\tThe authors claim that this is the first fact-checking dataset considering background documents. However, both the FEVEROUS dataset (\u201cFEVEROUS: Fact Extraction and VERification Over Unstructured and Structured information\u201d by Aly et al. 2021) and the UKP Snopes Corpus (\u201cA Richly Annotated Corpus for Different Tasks in Automated Fact-Checking\u201d by Hanselowski et al. 2019) consider background documents. Particularly the latter dataset considers links found in the review articles just as in the WatClaimCheck datast, the only difference being that only the Snopes website is used. \n-\tGiven that the main contribution of the paper is the introduction of a new dataset, more analysis of the dataset would have been useful. For example, is there any overlap of claims, background documents, or (more generally) topics between the claims extracted from the various different sources? \ncomments,_suggestions_and_typos\n-\tIt would be interesting to know if there is a particularly reason that the authors frame the retrieval task (stage 1) as sentence retrieval rather than as document retrieval. \n-\tIt was not completely clear to me whether the retrieval model is trained only on the new dataset or if the model pre-trained on the QA task is further fine-tuned on the dataset. \n-\tIn addition to the two datasets mentioned in the \u201cWeakness\u201d section there are others that could be useful to compare against, see e.g. \u201cAutomated fact-checking: A survey\u201d by Zeng et al. 2021 or \u201cA Richly Annotated Corpus for Different Tasks in Automated Fact-Checking\u201d by Hanselowski et al. 2019 for further references. \n-\tIn Table 1, it is stated that the Vlachos and Riedel 2014 dataset has no Evidence, however in the paper it is stated that \u201cwe also collected the sources used by the journalists in the analysis provided for the verdict. Common sources include tables with statistics and reports from governments, think tanks and other organisations, available online\u201d. \n-\tIn Section 4.2.2, is it correct that for each claim-true_sentence_from_review pair, n=number_of_claims incorrect sentences are chosen? Furthermore, are these incorrect sentences chosen from one other review article or each sentence from a different review article? \n-\tIn Section 4.2.2, it is unclear what the \u201ctop scoring\u201d sentences (line 417) are. Do you choose a certain number of sentences or do you define a cut-off threshold? \n-\tThe fact that the claimant is included in the veracity prediction hints at the authors\u2019 assumption that certain claimants are more or less likely to make true/false claims. It would be useful here to investigate this relationship between claimant and veracity as well as to exclude the claimant from the prediction (only use claim and evidence) to figure out what the weight of the claimant in this prediction is (maybe the model does not actually learn to predict the veracity from the evidence but rather from the claimant). \n-\tIn Section 4.3.3, how do you deal with the RoBERTa token limit? \n-\tIn Section 5.1 it would be useful to report the total number of sentences in the test set so that the reader has an idea of the difficulty of the retrieval task. \n-\tIn Table 3 the authors report the performance based on the review of a claim as the upper bound. However, it seems to me that a better upper bound would be the prediction based on true background articles. \n-\tIt would be interesting to also report or mention the class-wise F1 scores for veracity prediction and to analyse the confusion between the classes as a confusion between the True and False is much more severe than between True (or False) and the Partially True (False) labels. ", "label": [[1496, 1631, "Eval_pos_1"], [1634, 1680, "Eval_pos_2"], [1705, 1801, "Eval_neg_1"], [1802, 2287, "Jus_neg_1"], [2290, 2426, "Eval_neg_2"], [2427, 2585, "Jus_neg_2"], [2980, 3105, "Eval_neg_3"], [3106, 3299, "Jus_neg_3"], [3925, 4005, "Eval_neg_4"], [4006, 4087, "Jus_neg_4"], [4091, 4263, "Eval_neg_5"], [4264, 4617, "Jus_neg_5"], [4950, 5058, "Eval_neg_6"]]}
{"id": 208, "review": "paper_summary\nThis work proposes to improve lexically constrained non-autoregressive translation by incorporating constraints during training and utilizing the source-side alignment during training and inference. The main motivation is that even for rare constraints, the context is not necessarily rare, and aligning source-side context can be beneficial for the model's performance. The results on En-De in-domain and out-of-domain data, as well as a comparison with other constrained models show that the proposed model a) improves the quality b) does not increase latency \nsummary_of_strengths\n- Paper contains an extensive comparison with previous works on lexically constrained NAT, as well as with AR Transformer model -ACT shows improvements on domain-specific datasets and on a constrained portion of in-domain data -Incorporated constraints do not result in latency increase \nsummary_of_weaknesses\n- No comparison across languages (only De target language) -While testing on the full En-De test set, the improvements are minor \ncomments,_suggestions_and_typos\n*Typos:* -Table 1: htten --> hatten -line 155: An --> a -line 166: For NATs --> seems redundant, an NAT --> a -Figure 1: I am confused by frequency buckets. Does the x-axis represent the **inverse** frequency of constraint? Otherwise, I am genuinely confused. And while sampling self-constrains, do you sample from the word buckets based on the sentence itself or do you consider ALL possible words (from all sentences) in the bucket?  *Additional questions:* -During training you sample 0-3 reference tokens as constraints, since the BPE is used, does it mean that it might be the case that only part of the word is considered as a constraint? Or do you handle such a situation in some way?\nThanks! ", "label": [[600, 725, "Eval_pos_1"], [727, 884, "Eval_pos_2"], [910, 966, "Eval_neg_1"], [968, 1036, "Eval_neg_2"], [1181, 1226, "Eval_neg_3"], [1227, 1504, "Jus_neg_3"]]}
{"id": 209, "review": "paper_summary\nThe paper proposes an encoder-decoder-based framework for document re-ranking. Instead of using a dual-encoder framework, the paper regards documents and the input and queries as output, and uses the generative likelihood of queries to retrieve documents. For compressing pre-computed document embeddings, the paper proposes to Funnel Transformer. Empirical results on MS MARCO dataset show that the proposed method achieves the best effectiveness-efficiency trade-off. \nsummary_of_strengths\n1. The idea of using the generative likelihood of queries to retrieve documents seems to be novel.\n2. The proposed method ED2LM archives a better MRR-Latency trade-off on MS MARCO dataset. \nsummary_of_weaknesses\n1. The paper says \"The normalised scores from the true and false tokens are combined as in (Nogueira et al., 2020).\" ( line 249) It's unclear to the reviewer how the scores are combined in the paper. Besides, the paper does not show how the cross-entropy loss and query likelihood loss affect the model performance, which requires an ablation study.\n2. According to the Bayes' theorem, $p(d|q) \\propto \\frac{p(q|d)}{p(q)}$. However, the $p(q)$ term, i.e., the language modeling likelihood of queries is modeled in the paper, which makes the proposed method not mathematically sound. \ncomments,_suggestions_and_typos\n- ", "label": [[509, 604, "Eval_pos_1"], [608, 694, "Eval_pos_2"], [847, 917, "Eval_neg_1"], [1071, 1243, "Jus_neg_2"], [1243, 1299, "Eval_neg_2"]]}
{"id": 210, "review": "- Strengths: 1. The presentation of the paper, up until the final few sections, is excellent and the paper reads very well at the start. The paper has a clear structure and the argumentation is, for the most part, good. \n2. The paper addresses an important problem by attempting to incorporate word order information into word (and sense) embeddings and the proposed solution is interesting.\n- Weaknesses:  1. Unfortunately, the results are rather inconsistent and one is not left entirely convinced that the proposed models are better than the alternatives, especially given the added complexity. Negative results are fine, but there is insufficient analysis to learn from them. Moreover, no results are reported on the word analogy task, besides being told that the proposed models were not competitive - this could have been interesting and analyzed further. \n2. Some aspects of the experimental setup were unclear or poorly motivated, for instance w.r.t. to corpora and datasets (see details below). \n3. Unfortunately, the quality of the paper deteriorates towards the end and the reader is left a little disappointed, not only w.r.t. to the results but with the quality of the presentation and the argumentation.\n- General Discussion: 1. The authors aim \"to learn representations for both words and senses in a shared emerging space\". This is only done in the LSTMEmbed_SW version, which rather consisently performs worse than the alternatives. In any case, what is the motivation for learning representations for words and senses in a shared semantic space? This is not entirely clear and never really discussed in the paper. \n2. The motivation for, or intuition behind, predicting pre-trained embeddings is not explicitly stated. Also, are the pre-trained embeddings in the LSTMEmbed_SW model representations for words or senses, or is a sum of these used again? If different alternatives are possible, which setup is used in the experiments? \n3. The importance of learning sense embeddings is well recognized and also stressed by the authors. Unfortunately, however, it seems that these are never really evaluated; if they are, this remains unclear. Most or all of the word similarity datasets considers words independent of context. \n4. What is the size of the training corpora? For instance, using different proportions of BabelWiki and SEW is shown in Figure 4; however, the comparison is somewhat problematic if the sizes are substantially different. The size of SemCor is moreover really small and one would typically not use such a small corpus for learning embeddings with, e.g., word2vec. If the proposed models favor small corpora, this should be stated and evaluated. \n5. Some of the test sets are not independent, i.e. WS353, WSSim and WSRel, which makes comparisons problematic, in this case giving three \"wins\" as opposed to one. \n6. The proposed models are said to be faster to train by using pre-trained embeddings in the output layer. However, no evidence to support this claim is provided. This would strengthen the paper. \n7. Table 4: why not use the same dimensionality for a fair(er) comparison? \n8. A section on synonym identification is missing under similarity measurement that would describe how the multiple-choice task is approached. \n9. A reference to Table 2 is missing. \n10. There is no description of any training for the word analogy task, which is mentioned when describing the corresponding dataset. ", "label": [[16, 136, "Eval_pos_1"], [137, 168, "Eval_pos_2"], [173, 218, "Eval_pos_3"], [224, 349, "Eval_pos_4"], [354, 391, "Eval_pos_5"], [410, 597, "Eval_neg_1"], [598, 861, "Jus_neg_1"], [866, 938, "Eval_neg_2"], [939, 1003, "Jus_neg_2"], [1008, 1217, "Eval_neg_3"], [1564, 1631, "Eval_neg_4"], [1636, 1737, "Eval_neg_5"], [2051, 2157, "Eval_neg_6"], [2158, 2241, "Jus_neg_6"], [2373, 2463, "Eval_neg_7"], [2690, 2732, "Eval_neg_8"], [2733, 2850, "Jus_neg_8"], [2855, 3014, "Eval_neg_9"]]}
{"id": 211, "review": "paper_summary\nThe paper extensively explores the ways to extend GECTOR model by using larger transformers of different kind, model distillation and ensembling. It allows to achieve new SOTA on BEA2019 test. The main results are: + new SOTA achieved by using an ensemble of large pretrained Transformer models + a single model trained on data annotated by a large ensemble model outperforms standard GECTOR training + Edit majority ensembling works better than probability ensembling \nsummary_of_strengths\n+ new SOTA on BEA 2019 test + a better ensembling method proposed + a nice study on knowledge distillation + the paper is clearly written \nsummary_of_weaknesses\n- Judging the paper for a workshop, I find practically no weaknesses. However, I have minor concerns whether the contribution is significant enough for people outside GEC community -seems to be an extension of earlier published work https://s3.eu-central-1.amazonaws.com/ucu.edu.ua/wp-content/uploads/sites/8/2021/04/Improving-Sequence-Tagging-Approach-for-Grammatical-Error-Correction-Task-.pdf  The authors should properly highlight the contribution made on top of it. \ncomments,_suggestions_and_typos\n- Please explain better how knowledge distillation is performed. Do you mean that you find erroneous sentences in unannotated data and then correct them by a strong ensemble to obtain the target sentence? ", "label": [[507, 532, "Eval_pos_1"], [535, 570, "Eval_pos_2"], [573, 611, "Eval_pos_3"], [614, 642, "Eval_pos_4"], [736, 846, "Eval_neg_1"], [847, 1136, "Jus_neg_1"], [1172, 1234, "Eval_neg_2"], [1235, 1374, "Jus_neg_2"]]}
{"id": 212, "review": "paper_summary\nEntropy estimates are a fundamental quantity of interest for answering data-driven information-theoretic questions. However, it is known that the estimation of entropy from raw data can be quite challenging knowing that the MLE estimator in expectation underestimates entropy and the power-law governing linguistic data cannot be effectively captured by such estimates. This study is an empirical comparison of various entropy estimators on natural language data and fills an important research gap. \nsummary_of_strengths\nStrengths: 1) The paper addresses an important research gap in the computational linguistics literature. \n2) It studies a set of 6 different estimators of entropy and their empirical comparisons using both synthetic data and natural language data. \n3) The authors have also replicated recent results that used the plug-in estimator to approximate entropy to show how the results overrepresent the actual effects compared to what could have been when using a power-law sensitive entropy estimator. \nsummary_of_weaknesses\nThe paper doesn't have any weaknesses. \ncomments,_suggestions_and_typos\nSome typos to correct.\nLine 045: challening -> challenging Line 310: \"See Fig 2\" could be written in parentheses, instead of making it a separate sentence. ", "label": [[383, 513, "Eval_pos_1"], [550, 640, "Eval_pos_2"], [645, 783, "Eval_pos_3"], [1056, 1094, "Major_claim"]]}
{"id": 213, "review": "paper_summary\nThis paper propose a novel encoder-decoder model for language generation. It revises the potential errors by considering the representation of  the target future context and generates the next target token synchronously. Despite a slight reduction in decoding speed compared to the Transformer baselines in three machine translation tasks, the proposed model is superior to competitive comparison systems in translation quality and efficiency using the same number of parameters as the Transformer. And the improvement is also demonstrated in other three language generation task, including summarization, storytelling and simultaneous machine translation. \nsummary_of_strengths\nI really enjoyed reading this paper. The research question is clearly stated, and the experimental results are fascinating to go through. \nsummary_of_weaknesses\nIt is not clear when deletion operation is used in the refinement period. \ncomments,_suggestions_and_typos\n1. How to ensure that the consecutive refined BEP tokens can be valid whole words after de-BPE operation? \n2. I suggest show the the effect of the proposed model with beam search during the refinement stage for the translation quality, although with inferior inference speed. \n3. Although achieving an improvement, what are the main advantages of this proposed over some simple and effective methods without additional parameters and speed reduction, such as back-translation and R-drop? \n4. The ratio between the generation and the refinement probabilities in the whole training objective is 1 in the equation 12. Is it the optimal setting? ", "label": [[693, 729, "Major_claim"], [730, 770, "Eval_pos_1"], [775, 830, "Eval_pos_2"], [854, 927, "Eval_neg_1"]]}
{"id": 214, "review": "paper_summary\nThe paper proposed an expert-guided heuristic linguistic adversarial augmentation that simulates the overlapping or ambiguous categories or label shift where a word or phrase can have different entity labels in different scenarios. They also proposed a challenging dataset consisting of 1000 high-quality examples to facilitate benchmarking the out-of-distribution performance of NER models.  With limited expert-guided adversarial augmented training data, the NER model\u2019s performance was boosted on their challenge set and out-of-domain set. \nsummary_of_strengths\nThey proposed a novel and valuable adversarial augmentation method that simulates the common problem of NER models facing real cases. The augmentation method boosted the performance of models on the challenge and out-of-domain sets. They empirically showed that mixing the original and guided-adversarial examples can further enhance the generalization ability of NER models consistently. \nsummary_of_weaknesses\nThere are two main concerns from me . The first one is that how to generate the adversarial training set with code is not clearly discussed and  the detail of adversarial training set is not be provided. The second concern is that the reason of excluding 25% of the word phrases said in the last paragraph in 4.1 and the reason of using first 50 examples from the OntoNotes test set is not be fully discussed. \ncomments,_suggestions_and_typos\n1.  For the Appendix H section, it should be reorganized which is difficult to follow. ", "label": [[579, 712, "Eval_pos_1"], [713, 811, "Eval_pos_2"], [812, 968, "Eval_pos_3"], [991, 1028, "Eval_neg_1"], [1029, 1400, "Jus_neg_1"]]}
{"id": 215, "review": "paper_summary\nThis paper introduces a dataset intended for structured multi-document summarisation, based on annotated meta-reviews from recent ICLR conferences along with the corresponding reviews and ratings of the papers. The data is thoroughly described and analysed, and a series of experiments are carried out comparing a range of baselines to a system based on Transformers. The paper presents the results from a particular configuration, while the appendix gives more details of the parameter space and other settings that were explored. A human evaluation was also carried out, which indicated that the proposed method produced output that was rated more highly than the baseline systems. The authors intend to release the data and code when the paper is published. \nsummary_of_strengths\nThe main strength of this paper is the quality of the dataset, which is of a good size and appears to have high-quality, reliable annotations. This dataset, together with the code for the baseline and Transformer-based systems, can provide an extremely useful resource for other researchers working in this and similar areas.\nThe paper itself is well written throughout, and the inclusion of the appendix giving details of the other settings that were explored is a useful extra. \nsummary_of_weaknesses\nThe \"Related Work\" section is quite brief: it cites a number of papers, but mostly only in a list of references with very little context. It would help to give a more thorough comparison, particularly to the most similar approaches such as the Bhatia et al. (2020) paper which also addressed meta-reviews. \ncomments,_suggestions_and_typos\nSee above for my main comments.\nThe font in Table 7 in particular is EXTREMELY small, particularly with the highlighting which makes it even harder to read. Similar issues also apply to the axes of the graph in Figure 5. ", "label": [[225, 271, "Eval_pos_1"], [797, 859, "Eval_pos_1"], [860, 939, "Jus_pos_1"], [940, 1122, "Eval_pos_2"], [1123, 1167, "Eval_pos_3"], [1172, 1277, "Eval_pos_4"], [1300, 1341, "Eval_neg_1"], [1343, 1605, "Jus_neg_1"]]}
{"id": 216, "review": "paper_summary\nThis paper investigates an interesting problem of automatically generating high-cognitive-demand educational questions for children\u2019s storybooks. To this end, this paper proposes a three-stage framework: 1) learn to predict the question type distribution, 2) extract salient question-worthy events from the text, and 3) generate educational questions based on the outputs from steps 1) and 2). Both automatic and human evaluation validates the effectiveness of the proposed method. \nsummary_of_strengths\n- A well-motivated task worthy of study by the QG community. It is practically meaningful to generate educational questions for children's storybooks and it could be applied to many real-world educational applications.  -The proposed framework is straightforward but logically sound.  -The authors conducted comprehensive evaluation, including the automatic evaluation and human evaluation.  -The paper is fairly easy to follow. \nsummary_of_weaknesses\n- This work is a bit incremental since it is basically an improved model of Yao et al., 2021 for FairytaleQA.  -Each component of the framework is based on existing works. For example, BERT for learning question type distribution, BART for summary generation, and question generation. Therefore, I don't see many technical contributions from this paper.  -The performance improvement over Yao et al., 2021 also seems a bit incremental in terms of BERTScore and human evaluation results. \ncomments,_suggestions_and_typos\nPutting together all the strengths and weaknesses I believe the NLP and QG community will benefit from this work. However, at the same time, it does leave things to be desired. Nonetheless, I am slightly inclined to accept this work.  Some minor suggestions & questions:  - Line 516: \"We can see that our method has a better estimation of the distribution of question types, which is closer to the distribution of the ground-truth.\" Does this count as an advantage of the proposed model? I think a perfect fit of FairytaleQA's distribution of question types shows that the model can better overfit the FairytaleQA dataset, but this may hurt its generalizability to other datasets.  - In Line 136, the authors missed some related works about multi-sentence / multi-document QG, for example:    - Pan et al. Semantic Graphs for Generating Deep Questions. ACL, 2020. \n  - Xie et al. Exploring Question-Specific Rewards for Generating Deep Questions. COLING, 2020. \n  - Tuan et al. Capturing Greater Context for Question Generation. AAAI, 2020. ", "label": [[520, 578, "Eval_pos_1"], [578, 736, "Jus_pos_1"], [804, 851, "Eval_pos_2"], [852, 908, "Jus_pos_2"], [911, 947, "Eval_pos_3"], [972, 1002, "Eval_neg_1"], [1003, 1079, "Jus_neg_1"], [1081, 1254, "Jus_neg_2"], [1255, 1323, "Eval_neg_2"], [1324, 1457, "Eval_neg_3"], [1490, 1723, "Major_claim"]]}
{"id": 217, "review": "paper_summary\nThis paper proposed to extend the monolingual entity representation model (LUKE) to the multilingual version and further conduct experiments on several cross-lingual tasks. The experimental results show the superiority of incorporating pre-trained multilingual entity representations. Besides, the authors also had detailed analysis of the benefits of proposed methods. Overall the work is solid with simple ideas and good results. I believe the released entity representation model would be useful for multilingual knowledge-related tasks. \nsummary_of_strengths\n1. Well-written paper and good presentation for the method, tasks as well as analysis.\n2. The method is effective by incorporating entity representations and the performance got improved for various cross-lingual transfer tasks.  3. If the pre-trained model is released, it would benefit more research about utilizing multilingual entity knowledge by either feature extraction or extra entity features. \nsummary_of_weaknesses\n1. The idea is a bit incremental and simply the extension of previous monolingual LUKE. \n2.For the language-agnostic characters of entity representations, the paper has weak analysis on the alignment of entity representations. \ncomments,_suggestions_and_typos\nThe authors could add more analysis about the multilingual alignment of entity representations and it would be better to have visualizations or case studies for different types of languages such as language family. We are also interested in whether entities from low-resourced languages are well aligned with the high-resourced ones. ", "label": [[384, 445, "Major_claim"], [580, 663, "Eval_pos_1"], [810, 980, "Eval_pos_2"], [1006, 1035, "Eval_neg_1"], [1036, 1091, "Jus_neg_1"], [1158, 1230, "Eval_neg_2"]]}
{"id": 218, "review": "paper_summary\nThis paper claims the Distinct\u2019s bias that tends to pose higher penalties over longer sequences, and then fixes the bias by calculating the expectation of distinct tokens of a random text with the same length, and divide the origin Distinct value by it. They provide theoretical evidence to the formula, and do experiments on the dialog generation task to prove that the newDistinct correlates better with human evaluations. \nsummary_of_strengths\n1. As the paper mentioned, the idea is inspired by psychological linguistics, making it more convincing. \n2. This paper gives math analysis and derivation of the formula, making it more solid. \n3. The experiments prove the efficiency of the new Distinct. \nsummary_of_weaknesses\n1. In the results in Table 2, there is a large score gap between the newdistinct and original distinct for the system of AdaLab, please give more explanations. \n2. In Page 7, line 517~523, C can\u2019t be so large that the limitation method can get a good enough result. Perhaps you need more accurate math to show that when C is not so big, for example, when C is only 5x or 10x of V, the derivative of NewDistinct is still bigger than the original Distinct. Besides, the conclusion \u201cthe bigger C is, the slower the original Distinct increases\u201d is also right for NewDistinct. \ncomments,_suggestions_and_typos\nThis paper is well written and is elegant. ", "label": [[742, 867, "Jus_neg_1"], [868, 899, "Eval_neg_1"], [1344, 1387, "Eval_pos_1"]]}
{"id": 219, "review": "paper_summary\nThis paper developed a new dataset, GlobalWOZ, a multilingual extension of the English MultiWOZ to about 20 languages through machine translation (to save cost) and replacing entities occurring in the English dataset with local entities that are most likely to be used in a foreign language (in the country where the language is spoken). They considered the following 3 use cases: (1) a foreign language speaker uses ToD in the foreign-language country (F&F), or (2) an English country (F&E), (3) an English speaker uses ToD in a foreign-language country (E&F), which are different from the traditional E&E use case where an English speaker uses ToD in an English-speaking country.  To ensure that the dataset captures more local entities in the foreign countries of interest, first, English-specific entities are replaced with a set of general-purpose placeholders, followed by the translation of the sentences (without entities), and lastly, ontologies containing definitions of dialogue acts, local entities and their attributes in the target language countries are used for placeholder replacement. The dataset created consists of high-quality test sets in 3 languages (Chinese, Spanish, and Indonesia) that have been post-edited by professional translators after machine translation. The test sets of the remaining 17 languages are machine translated.  For the experiments, they compare the performance of their approach to zero-shot evaluation when transferring from English and using translated english dataset for training. \nsummary_of_strengths\n- They introduced a very important dataset over 30 languages, they also compare the quality of the dataset that has been machine-translated with the one machine translated and post-edited by professional translators on 3 languages (Chinese, Spanish, and Indonesia).  - The paper is well-written and the experiments are quite interesting. \nsummary_of_weaknesses\n- The translation quality for low-resource languages should be discussed. For example, some low-resource languages like Swahili may give poor performance if they have not been trained on dialogue texts. \ncomments,_suggestions_and_typos\nNA ", "label": [[1837, 1862, "Eval_pos_1"], [1867, 1906, "Eval_pos_2"], [1931, 2002, "Eval_neg_1"], [2003, 2132, "Jus_neg_1"]]}
{"id": 221, "review": "paper_summary\nThis paper focuses on quality estimation of machine translation systems. Using a sentence level QE system, they use feature attribution in a sentence level QE system to determine the errorfull  spans of words in a target translation that most contribute to the QE model score. This enables them to define a semi-supervised training set for a word-level QE method.\nIn addition, they propose to use the word-level QE task as a new benchmark for evaluating explainability methods.\nThey explore 4 models for feature attribution for sentence level QE method: LIME, Information Bottleneck, Integrated Gradients, and Attention. \nThey use 4 metrics: AUC, Average Precision, Recall at top K, and Accuracy at top 1. \nAll the metrics they define cannot handle MT target sentences that are all correct or all wrong as the authors acknowledge. hence they exclude such sentences form further analysis. A serious issue! for example over 56% of Ro-En sentences are discarded.\nFor sentence-level QE they use a SOTA method called TransQuest (using XLM-R base). \nThey experiment with the MTQE-PE data set that has both Direct Assessment (DA) with a scale of 0-100 of MT sentences by humans and Post Editing of these sentences. \nThey only keep sentences with DA score below 70 for the DA task.\nThe major results per the authors are in Table 2. It shows that 3 language pairs that some of the semi-supervised methods outperform the glass-box supervised approach (though it is worse uniformly and significantly than the black box Micro TransQuest approach.)\nThe authors do not provide significance tests or standard deviations of the metrics making it hard to judge if these results are meaningful for discriminating between the approaches. \nsummary_of_strengths\nThe paper proposes using feature attribution methods to a extend a sentence-level  QE system to a word-level QE system.\nThey identify a good data set MTQE-PE for analyzing these methods. They study effectively 4 methods for word-level QE with semi-supervised approach and 2 fully supervised word-level QE methods across 3 language pairs. They analyze a high error subset and a low error subset of the MTQE-PE test set. They report results using multiple metrics for word-level QE. \nsummary_of_weaknesses\n1. The fact that the 4 metrics cannot handle perfectly correct MT output or 100% error full MT output is a major weakness particularly for the proposed type of use case which is identifying where the errors are in the MT system output. I recommend the authors define metrics that enable evaluation on the whole test set.  For example one can use an F-measure like metric on all the words in the test set. It can be done either by using a threshold to define an error detection and then measuring Precision and recall of all error events (words) a micro-F or if no threshold is desired for every word a mean square error is measured between system probability of an error and the gold truth (again a micro measure over all word positions) for all sentences in the test set). \n  2. By selecting higher error rate sentences, metrics like accuracy at 1 are measuring more of the tail of the distribution since one is counting if the top scoring event is indeed an error ( a sentence has many errors typically).\n3. Also, given that all 4 metrics are averaged over the test instances, at least a standard deviation should be given to give a sense of the variability of the 4 metrics for a test condition.\n4. Another weakness is that the conclusion of superiority of feature attribution methods is somewhat unreliable given that they hold for RoEn and NeEn but not EsEn (using AP as indicated in the paper). \ncomments,_suggestions_and_typos\nno major comments. ", "label": [[721, 902, "Eval_neg_1"], [902, 918, "Eval_neg_1"], [919, 973, "Jus_neg_1"], [1550, 1733, "Eval_neg_2"], [2262, 2380, "Eval_neg_3"], [2380, 2494, "Jus_neg_3"], [3461, 3569, "Eval_neg_4"], [3570, 3660, "Jus_neg_4"]]}
{"id": 222, "review": "paper_summary\nThis paper introduces a multimodal DST task. The paper's contributions are 3 folds:  1. defines the task which pertains to state tracking over a multi-turn conversation between the system and the user wherein the user asks questions and the system provides answers from a video scene. \n2. provides a synthetic dataset (DVD-DST) for the task. \n3. VDTN: which is a multimodal transformer-based model that combines video and dialog features to answer the user's questions. \nThey test VDTN on their synthetic dataset against baselines to showcase that VDTN does better at the multimodal DST task. \nsummary_of_strengths\n1. Interesting task definition - traditional DST systems do not quite deal with references to multiple intra-domain objects (which is a harder task to track states over IMO). This task catches up on that distinction. \n2. The paper is well-written (extensive analysis in appendix) and tries to cover a large ground of work (defining task + providing synthetic dataset + novel model architecture) \nsummary_of_weaknesses\n- Overall, I feel that the paper is trying to do a lot of things at one go. The authors have covered a large ground of work but maybe it's too much to put into a single paper and isn't doing justice to each of the contributions. Example, given that this is a new task - I would want to know more details about the task - how is it different/why is it beneficial etc. ( the authors have covered these aspects, but briefly). \nGetting into the details of weaknesses: -One of the major weaknesses of this paper is the use of synthetic datasets. Also, the choice of baseline (DVD) doesn't seem super clear. Based on the examples, it seems to be a very \"dumb\" baseline. Some insight into the chosen baseline would be helpful.\nHowever, despite these weaknesses it is still a comprehensive paper. \ncomments,_suggestions_and_typos\nnil ", "label": [[632, 659, "Eval_pos_1"], [661, 846, "Jus_pos_1"], [850, 875, "Eval_pos_2"], [876, 908, "Jus_pos_2"], [908, 950, "Eval_pos_3"], [951, 1023, "Jus_pos_3"], [1047, 1275, "Eval_neg_1"], [1276, 1413, "Jus_neg_1"], [1511, 1587, "Eval_neg_2"], [1588, 1648, "Eval_neg_3"], [1649, 1766, "Jus_neg_3"]]}
{"id": 224, "review": "paper_summary\nThe paper proposes a new benchmark for the task of analogical reasoning, framing it as two tasks over a new corpus. The tasks consist of a multiple-choice question answering and the generation of explanations related to the query and each candidate answer. In the experimental part, the benchmark is used to test a set of neural language models, whose performances are not satisfactory and highlight the difficulty of this benchmark.\nThe paper is well written and easy to understand. The background section provides all the necessary material to understand and contextualize the task. \nThe task itself is very interesting, well-motivated, and offers a new difficult challenge to the NLP community. \nThe experimental part and the analysis of the result are satisfactory.\nThe dataset creation process seems to have some weak points but it is still acceptable if some additional details are provided. It is not clear whether the authors plan to release this dataset publicly or whether it will remain private. \nsummary_of_strengths\nThe task is well designed and motivated. It provides a difficult challenge that sets a high bar for current models and that will be very useful to evaluate future NLP-based reasoning systems. \nsummary_of_weaknesses\nAdditional details regarding the creation of the dataset would be helpful to solve some doubts regarding its robustness. It is not stated whether the dataset will be publicly released. \ncomments,_suggestions_and_typos\n1) Additional reference regarding explainable NLP Datasets: \"Detecting and explaining unfairness in consumer contracts through memory networks\" (Ruggeri et al 2021) 2) Some aspects of the creation of the dataset are unclear and the authors must address them.  First of all, will the author release the dataset or will it remain private? \nAre the guidelines used to train the annotators publicly available? \nHaving a single person responsible for the check at the end of the first round may introduce biases. A better practice would be to have more than one checker for each problem, at least on a subset of the corpus, to measure the agreement between them and, in case of need, adjust the guidelines. \nIt is not clear how many problems are examined during the second round and the agreement between the authors is not reported. \nIt is not clear what is meant by \"accuracy\" during the annotation stages.\n3) Additional metrics that may be used to evaluate text generation: METEOR (http://dx.doi.org/10.3115/v1/W14-3348), SIM(ile) (http://dx.doi.org/10.18653/v1/P19-1427).\n4) Why have the authors decided to use the colon symbol rather than a more original and less common symbol? Since the colon has usually a different meaning in natural language, do they think it may have an impact?\n5) How much are these problems language-dependent? Meaning, if these problems were perfectly translated into another language, would they remain valid? What about the R4 category? Additional comments about these aspects would be beneficial for future works, cross-lingual transfers, and multi-lingual settings.\n6) In Table 3, it is not clear whether the line with +epsilon refers to the human performance when the gold explanation is available or to the roberta performance when the golden explanation is available? \nIn any case, both of these two settings would be interesting to know, so I suggest, if it is possible, to include them in the comparison if it is possible.\n7) The explanation that must be generated for the query, the correct answer, and the incorrect answers could be slightly different. Indeed, if I am not making a mistake, the explanation for the incorrect answer must highlight the differences w.r.t. the query, while the explanation for the answer must highlight the similarity. It would be interesting to analyze these three categories separately and see whether if there are differences in the models' performances. ", "label": [[448, 497, "Eval_pos_1"], [498, 599, "Jus_pos_1"], [600, 712, "Eval_pos_2"], [1043, 1083, "Eval_pos_2"], [1084, 1235, "Eval_pos_2"], [1258, 1378, "Eval_neg_2"], [1379, 1433, "Jus_neg_2"], [1644, 1734, "Eval_neg_3"], [1736, 1813, "Jus_neg_3"], [1814, 1882, "Jus_neg_3"], [1883, 2379, "Jus_neg_3"], [3434, 3565, "Eval_neg_4"], [3565, 3901, "Jus_neg_4"]]}
{"id": 226, "review": "paper_summary\nThis paper proposes a contrastive learning framework to learn phrase representations in an unsupervised way. Cluster-assisted contrastive learning is proposed to reduce noisy in-batch negatives by selecting negatives from clusters. Extensive experiments on entity clustering and topical phrase mining show the effectiveness of the proposed methods. Case studies are also provided that demonstrate coherent and diverse topical phrases can be founded by UCTopic without supervision. \nsummary_of_strengths\n- The paper is well-written and clearly presented.  -The proposed cluster-assisted contrastive learning objective is well-motivated and effective when finetuning the encoder on the target task further. Extensive experimental results are provided to show the significance of the proposed UCTopic versus baseline methods.  -Detailed discussion is also provided for different datasets to further analyze the effectiveness of the proposed method with respect to informativeness, diversity of the constructed phrases, and the source of phrase semantics. \nsummary_of_weaknesses\n- The details of how to apply K-Means methods to obtain the pseudo labels when using the CCL and how the number of clusters will affect the final performance is missing. \ncomments,_suggestions_and_typos\n- Given that sentence length might affect in Table 1, additional statistics of the pre-trained sentence length versus the might be good to provide.  -Could the author provide a more detailed description of how to construct the pre-training phrases and how many pre-training phrases are present and how these phrases overlapped with the downstream tasks? ", "label": [[519, 567, "Eval_pos_1"], [570, 718, "Eval_pos_2"], [719, 835, "Jus_pos_2"]]}
{"id": 227, "review": "paper_summary\nThis paper focuses on the task of phoneme discovery. It proposes a statistical definition of phonemes (through their conditional distributions over word labels) and introduces a novel neural network, information quantizer, which makes use of this statistics to create a phoneme inventory from a set of words. The proposed model is reported to outperform the existing state-of-the-art speech recognition models. \nsummary_of_strengths\nThe paper is well-written, the approach (specifically, defining phonemes as distributions over word labels) is rather elegant, the numerical results seem to be rather strong. \nsummary_of_weaknesses\nThe amount of material (in particular, mathematical proofs) presented goes beoynd an 8-page *ACL paper, in my opinion.\nThe method is defined as \"semanti-driven\", but I think this is very misleading: the approach doesn't directly deal with semantics, but rather with word labels. If the approach was truly semantic, the model wouldn't be able to distinguish between complete synonyms.\nI do not have enough expertise to evaluate the novelty of the actual model presented. \ncomments,_suggestions_and_typos\n- Line 100: why is this a class of neural networks, and not a neural network?\n-The conclusion is extremely short, the paper could benefit from some discussion.\n-Line 476: typo, compare our models _to_ -Line 510: Figure 8 is mentioned, but the text doesn't say this is actually in the Appendix.\n-Table 3 is labelled as Figure 3. As a result, Figure 4 should be Figure 3, etc.\n-Line 431: I couldn't understand which n-grams were excluded: unigrams, bigrams, but also bigrams+trigrams? What exactly is the latter type? ", "label": [[447, 473, "Eval_pos_1"], [474, 572, "Eval_pos_2"], [574, 620, "Eval_pos_3"], [645, 667, "Eval_neg_2"], [668, 704, "Jus_neg_2"], [705, 763, "Eval_neg_2"], [764, 843, "Eval_neg_1"], [844, 1028, "Jus_neg_1"]]}
{"id": 229, "review": "paper_summary\nThis paper is about character-level machine translation. The first part is a survey of 1) existing methods to enhance character-level translaiton and 2) methods used for WMT submissions in the past. The second part are experiments on character-level MT that compare different ways of encoding and decoding sequences of characters. \nsummary_of_strengths\n1) Comprehensive literature review: -I believe that the survey of existing literature is very comprehensive and covers most important works in this area. This survey in itself is valuable for the MT research community.\n2) Experiments have reasonable methodology: -In my opinion, the techniques that are compared (e.g. Lee-style, Canine, etc.) are adequately chosen.\n-The evaluation procedures (choice of metrics, several training runs to report variance, confidence intervals) are also adequate.\n-I think that the large systems trained on WMT data for EN-CS and EN-DE are representative of the capabilities of current systems.\n3) The findings are novel and actionable: -The paper does indeed provide novel evidence for why WMT submissions rarely have character-level systems.\n-While previous works have claimed that character-level MT has higher domain robustness, this paper shows that this is not the case, and the findings are backed up by more reliable data.\n-A further general conclusion is that most works on char-level MT focused on enhancing encoding, while decoding is under-explored. I think that is a reasonable and straightforward suggestion for future work. \nsummary_of_weaknesses\n1) Questionable usefulness of experiments on small datasets -As the paper itself states in the beginning, a possible weakness of earlier works is that their experiments were conducted on small datasets. In such cases it is unclear whether conclusions also apply to current MT systems trained on large datasets.\n-This criticism also applies to this paper under review, since many experiments are conducted using IWSLT data. I would like the paper to at least acknowledge this weakness.\n-It is questionable whether the outcome of the IWSLT experiments can be used to sub-select experimental conditions to try on the larger WMT data sets.\n2) The paper is too dismissive of MBR decoding, without much evidence -The paper implies that MBR is always worse than other decoding methods and that \"beam search with length normalization is the best decoding algorithm\".\n-I would change the wording to be more careful about describing the MBR results. Sampling-based MBR decoding is very new in MT and has a lot of potential to be more optimized in the future. It is simply not well optimized yet. For instance, very recent published works such as https://arxiv.org/abs/2111.09388 show that MBR improves considerably if a learned metric like COMET is used as the utility function for MBR.\n-I also take issue with this sentence: \"Sampling from character-level models leads to very poor translation quality that in turn also influences the MBR decoding that leads to much worse results than beam search.\" I believe that the quality of sampling is not necessarily indicative of the ability of MBR to pick a good translation from a pool of samples. \ncomments,_suggestions_and_typos\n1) Suggestion for the general literature review: -The only kind of work I would add are proposals to learn an ideal segmentation (instead of fixing the segmentation before starting the training). One paper I think would make sense is: https://www.cl.uni-heidelberg.de/~sokolov/pubs/kreutzer18learning.pdf (also has character-level MT in the title).\n2) Questions: -How many samples are the \"sample\" results in Table 2 based on? I believe this could be a point estimate of just 1 random sample. It is expected that the quality of one sample will be low, but the variance will also be very high. It would be better to show the mean and standard deviation of many samples, or at least clarify how exactly this result is produced.\n3) Figures and presentation: -I believe that the color scheme in tables is confusing at times. In Table 1, I think it is confusing that a deeper shade of red means better results. In Table 2 it is non-intuitive that the first row is CHRF and the second row is COMET - and the table is quite hard to read.\n4) Typos: -\"and thus relatively frequent occurrence of out-of-vocabulary tokens.\" - a word is missing here -\"The model shrinks character sequences into less hidden states\" -> \"fewer hidden states\" -\"does not apply non-linearity\" -> \"does not apply a non-linearity\" ", "label": [[404, 585, "Eval_pos_1"], [589, 629, "Eval_pos_2"], [630, 993, "Jus_pos_2"], [997, 1035, "Eval_pos_3"], [1037, 1538, "Jus_pos_3"], [1564, 1620, "Eval_neg_1"], [1621, 2196, "Jus_neg_1"], [2197, 2267, "Eval_neg_2"], [2268, 3194, "Jus_neg_2"]]}
{"id": 230, "review": "paper_summary\nIn this paper, the author proposes a deep-learning based inductive logic reasoning method. The method firstly extracts query-related (candidate-related) information. Then, the method conducts logic reasoning among the filtered information by inducing feasible rules that entail the target relation. The reasoning process is accomplished via attentive memories with novel differentiable logic operators. \nsummary_of_strengths\n1. Combining deep learning with ILP is interesting. The authors introduce a smooth connection between deep representation learning with logic reasoning by associating distributed representations with discrete logic predicates and their probabilistic evaluations. \n2. The research described in the paper seems technically sound and correct. \nsummary_of_weaknesses\n1. The experiments are not convincing. For example, the authors use the development dataset to evaluate the results. Test data is not publicly available. The reason is insufficient. In addition, four different query relations is small. It is hard to convince the effectiveness of DILR. \n2. This work is not clearly written. For example, the second paragraph of section 5.1 should be rewritten and divided into more than one paragraph. \ncomments,_suggestions_and_typos\n1. What is DNNs, in Section 1 Introduction ? \n2. The rule is \"if A is in B and B is part of country C, then A is in country C\". How to solve the example by DILR? Specifically, how to process the example by Hierarchical Attentive Reader and Multi-hop Reasoner ? \n3. BERT is a strong baseline model from Table 1. Do you explain why DILR-BERT is stronger than BERT in detail? ", "label": [[441, 490, "Eval_pos_1"], [706, 779, "Eval_pos_2"], [805, 840, "Eval_neg_1"], [841, 1088, "Jus_neg_1"], [1092, 1125, "Eval_neg_2"], [1126, 1269, "Jus_neg_2"]]}
{"id": 231, "review": "paper_summary\nThe paper is essentially an exploration of the design space for seq2seq architectures that produce code from natural language. The paper explores the use of pretrained language models for the encoder as well as the use of an output vocabulary that guarantees grammatically valid code after a mapping is applied, but using Early parser actions as the output. The latter is one of the key contributions of the paper, allowing the comparison of different architectures with direct decoding into code and decoding into grammatical operations.\nThe paper is valuable as a replication and validation of previous work. It finds that the use of copying directly from the language input is the biggest contributor to performance, with grammatical outputs being another strong factor. \nsummary_of_strengths\nThis paper validates previous work and makes it clear which components are important to pay attention to when building a text to code model. This will be valuable guidance for future research and implementation. The paper has a detailed discussion of the different settings including a qualitative discussion (which unfortunately had to be moved into the appendix). The paper makes very good use of detailed examples to make it clear what is being done. \nsummary_of_weaknesses\n- The number of datasets used is relatively small, to really access the importance of different design decisions, it would probably be good to use further datasets, e.g., the classical GeoQuery dataset.\n-I would have appreciated a discussion of the statistical properties of the results - with the given number of tests, what is the probability that differences are generated by random noise and does a regression on the different design decisions give us a better idea of the importance of the factors?\n-The paper mentions that a heuristic is used to identify variable names in the Django corpus, however, I could not find information on how this heuristic works. Another detail that was not clear to me is whether the BERT model was fine tuned and how the variable strings were incorporated into the BERT model (the paper mentions that they were added to the vocabulary, but not how). For a paper focused on determining what actually matters in building a text to code system, I think it is important to be precise on these details. \ncomments,_suggestions_and_typos\nIt would take some time to implement your task for other corpora, which potentially use different programming languages, but it might be possible to still strengthen your results using bootstrapping. You could resample some corpora from the existing two and see how stable your results are. \nIf you have some additional space, it would also be interesting to know if you have discuss results based on types of examples - e.g., do certain decisions make more of a difference if there are more variables?\nTypos: - Page 1: \"set of value\" -> \"set of values\" \"For instance, Orlanski and Gittens (2021) fine-tunes BART\" -> \"fine-tune\" -Page 2: \"Non determinism\" -> \"Non-Determinism\" ", "label": [[553, 624, "Major_claim"], [810, 950, "Eval_pos_1"], [951, 1021, "Eval_pos_2"], [1176, 1264, "Eval_pos_3"], [1289, 1337, "Eval_neg_1"], [1338, 1399, "Jus_neg_1"], [1400, 1451, "Eval_neg_1"]]}
{"id": 233, "review": "paper_summary\nThe paper aims to find the minimum spanning arborescences in an entity-mention graph to get better mention and entity representations by modeling the coreference relationships. The experimental results demonstrate that the approach can improve the performance of entity liking and the learned representations can be used for better entity coreference discovery. \nsummary_of_strengths\n1. The idea is quite simple but effective, we do not need to add new layers or parameters to boost task performance. \n2. The experiment results show the effectiveness of their proposed method on two datasets for candidate generating, linking accuracy, and entity coreference. \nsummary_of_weaknesses\n1. Some parts of the paper are not clear. \n    1). In Section 2.1 line 165-170, the paper provides some explanation for the definition of the f function, but it is still not clear why such a setting is well-suited for coreference. Suggesting provide more intuitive examples to demonstrate it. \n    2). Section 2.1 line 188-203, it is understandable to use special tokens as input for encoding, but the paper does not clarify which token is used as the representation of a mention span (is it [SATART] or [END]?), and which one is used for the entity representation (is it [CLS]?). \n2. The motivation for using the pruning for the graph is not clear. Why not just use the complete graph for training? \ncomments,_suggestions_and_typos\nAs mentioned in the weakness. ", "label": [[401, 440, "Eval_pos_1"], [441, 515, "Jus_pos_1"], [699, 739, "Eval_neg_1"], [744, 1278, "Jus_neg_1"], [1281, 1346, "Eval_neg_2"]]}
{"id": 234, "review": "paper_summary\nThis paper explores adapting the Chinese GPT for pinyin input.  Their task settings are that the input of the model includes a sequence of Chinese characters as the context and a sequence of pinyin (perfect pinyin or abbreviated pinyin), and the output is a sequence of Chinese characters. The number and the pronunciation of output characters should be consistent with the input pinyin, and the meaning of output should fit the input context.\nTo adapt the Chinese GPT for pinyin input, they propose two methods. The first one is concatenating pinyin input to the context of  Chinese characters. The second one is adding a pinyin embedding layer at the bottom of GPT. Since the model needs to select the best one from characters pronounced with the same pinyin in the inference stage, they propose pinyin-constrained training.\nFor datasets, they use the common benchmark PD dataset for training and evaluation. They also propose a new WD dataset that contains 15 domains. Their results show that their approach improves the performance of abbreviated pinyin across all domains. They also conducted a series of additional experiments to understand the importance of pinyin context and pinyin constrained training. \nsummary_of_strengths\nThis paper is well-written and well-organized; the readers could easily understand their methods and statements. The additional experiments are sufficient, which help the readers understand their method deeply. Moreover, the new evaluating dataset proposed in this paper is also very useful; it enables future work to evaluate their models across several domains. \nsummary_of_weaknesses\nThe main weakness is that the novelty of the proposed method is somewhat limited. The pinyin-enhanced pre-trained models and processing the abbreviated pinyin have existed in previous works, and this paper only adapted them on a new pre-trained model. \ncomments,_suggestions_and_typos\nI am not sure why the paper used a probability of 50% to sample a target sequence with less than five words (in line 318). Is that because people tend to input short sequences in real-world situations? If so, it would be better to explain the reasons. ", "label": [[1249, 1295, "Eval_pos_1"], [1295, 1361, "Jus_pos_1"], [1362, 1459, "Jus_pos_1"], [1460, 1540, "Eval_pos_2"], [1541, 1613, "Jus_pos_2"], [1636, 1717, "Eval_neg_1"], [1718, 1888, "Jus_neg_1"]]}
{"id": 235, "review": "paper_summary\nThe paper explores zero-short cross-lingual transfer (through fine-tuning) for the task of part-of-speech tagging, as a case-study. The authors evaluate the effect of several factors such\u00a0as the LDND distance between the source and target languages, matching language families, matching writing systems and the pre-training of the source and target languages. In addition, the paper\u00a0proposes insights on the selection of the source language when transferring to a target language of little or no annotated data. Moreover, the authors promise the release of 65 fine-tuned language models. The paper shows that pre-training on the target language and the LDND distance between the source and target languages\u00a0have the biggest impact on the cross-lingual performance. They also show that English is not necessarily the optimal source language as usually perceived\u00a0by the community. \nsummary_of_strengths\n- The paper investigates an interesting question, especially with the recent increasing interest  in multilingual NLP. While the paper focuses on part-of-speech tagging as a case-study, the conclusions might be generalizable to other cross-lingual tasks such as named-entity recognition and dependency parsing.\n-The paper proposes an in-depth analysis, both quantitative and qualitative, that studies several factors that impact the cross-lingual performance. \nsummary_of_weaknesses\n- The claims in the paper are too strong for cross-lingual transfer, as there are other cross-lingual techniques that are out of the scope of the paper such as annotation projection. Instead, it should be explicitly mentioned that the findings only apply to zero-shot model transfer through fine-tuning.\n-There is a strong bias towards Indo-European languages, and thus it's hard to generalize the findings on what a good source language is.\n-There is no discussion about the impact of word order, as expected from the introduction, other than in Table 1. \ncomments,_suggestions_and_typos\n- Make it clear that the paper focuses on zero-shot model transfer through fine-tuning instead of generalizing the findings on cross-lingual transfer.\n-It would be valuable to investigate the reason behind the high impact of the LDND distance.\n-I would recommend removing singletons from Figure 4.\n-Add statistical significance tests in Section 5.\n-Cite Pires et al., 2019 in the 2nd paragraph in the introduction. ", "label": [[917, 963, "Eval_pos_1"], [1400, 1465, "Eval_neg_1"], [1467, 1701, "Jus_neg_1"], [1703, 1839, "Eval_neg_2"]]}
{"id": 237, "review": "paper_summary\nThe paper discusses the introduction into direct speech translation models of contrastive learning methods that aim at reducing the difference between the encoded representations of speech and text with the same content. The main goal is to increase the transfer learning benefits of cross-modal systems, trained both with textual and audio data, and hence obtaining better performance especially in ST, leveraging large parallel textual corpora, which are easily available on the contrary of ST data. The effectiveness of the proposed approach is validated in different data conditions, comparing it with several previous works and with a thorough analysis targeted to validate the claim that similarity of the representations of the same sentences with different modalities is actually increased. \nsummary_of_strengths\nThe approach proposed in the paper is interesting and the results reported are promising. The main strength of the paper in my opinion is the analysis section on the modality, which demonstrates that the proposed method actually brings the benefits it was introduced for. Moreover, it motivates a possible future adoption of the proposed architecture in application scenarios where the similarity between the representations of textual and audio versions of the same content is needed. \nsummary_of_weaknesses\nThe main problem of the paper in my opinion is related to the data Augmentation: I have not been able to understand exactly which are the data augmentation methods used, and their impact on the scores (line 461-470 were quite obscure to me) and how this affects the comparison with other methods. In other words, the higher scores obtained with respect to other works cited in Table 1 are due to the contrastive learning method or to the different data augmentation methods? The comparison with some of the reported works is already not fair due to the different data condition (the usage of external speech data). It would have been better to compare with the other architectures/works repeating their experiments with the same data conditions (including data augmentation methods).\nThe reproducibility of this work would not be easy, since the code is not open sources and some details of the experimental settings are not very clear:  - line 184-186: the 2 convolutions are said to have stride 4, and that in total they shrink the input by a factor of 4. The two claims are contrasting. I guess the 2 convolutions actually have stride 2. 4 is the kernel size maybe? \n - line 236-251: the same letters (u and v) are used for different things and v is sometimes treated as a variable, sometimes as a function. This section could be more clear with a simpler notation. \n - line 342-344: Why using 10k as vocabulary size? Most of the works in literature (and also cited in table 1) use 8k (eg. Wang et al. 2020a), as also suggested in https://aclanthology.org/2020.amta-research.13/. In addition, it is not clear to me on which data the vocabulary was built: on MuST-C or on the WMT corpus? \ncomments,_suggestions_and_typos\nWhy did you introduce a parallelism between neural networks and the human brain in the introduction? This parallelism is questionable, as there are contrasting feelings and opinions about it. I am not sure it is adding anything to the motivation of the work, but it may rise concerns.\nIn the related works, among the many multi-task frameworks the addition of the CTC loss is not cited.\nIn lines 370-374, the paper claims that MT performance is not affected, but there is no evidence of it in the paper: the MT results are missing.\nIn the caption of table 2, what does strong mean? There are stronger cascade systems (e.g. you cited Bentivogli et al., 2020, whose cascade scores 28.8).\nIn line 152 the citation format is not correct. \nIn line 162, \"in\" -> \"into\". \nIn line 172, \"fout\" -> \"four\" In line 264 I guess a part of the sentence is missing: how is the contrastive loss computed? ", "label": [[835, 884, "Eval_pos_1"], [888, 924, "Eval_pos_2"], [925, 1010, "Eval_pos_3"], [1011, 1321, "Jus_pos_3"], [1344, 1425, "Eval_neg_1"], [1425, 2127, "Jus_neg_1"], [2128, 2179, "Eval_neg_2"], [2180, 2279, "Jus_neg_2"]]}
{"id": 239, "review": "paper_summary\nThe paper is a resubmission of previous work that looked at neutralising framing bias in news stories. The authors here propose a new task called neutral summary generation where the aim is to create an unbiased summary of potentially biased stories covering one event. To measure the bias of the generated summaries, the authors use lexical estimates of polarity which is shown to correlate with relative framing bias. Finally, a new model is proposed that uses titles in addition to article summaries as a way of focusing on the framing used by each side. The results show that the suggested approach is promising but a lot of challenges remain. \nsummary_of_strengths\n- The paper introduces a new task, namely neutral summary generation from multiple biased articles. The new task is more sensible compared to the originally proposed one. Overall the revised paper (and the authors' response) has addressed a lot of the issues in the original submission.\n-The dataset is a useful resource for studying framing bias and multi-view summarisation.\n-The use of polarity as a proxy for framing bias is an interesting research direction. \nsummary_of_weaknesses\n- A number of claims from this paper would benefit from more in-depth analysis.\n-There are still some methodological flaws that should be addressed. \ncomments,_suggestions_and_typos\n### Main questions/comments Looking at the attached dataset files, I cannot work out whether the data is noisy or if I don't understand the format. The 7th example in the test set has three parts separated by [SEP] which I thought corresponded to the headlines from the three sides (left, center, right). However, the second and third headlines don't make sense as stand-alone texts. Especially the third one which states \"Finally, borrowers will receive relief.\" seems like a continuation of the previous statements.  In addition to the previous question, I cannot find the titles, any meta-information as stated in lines 187-189, nor VAD scores which I assumed would be included.\nPart of the motivation is that scaling the production of neutral (all-sides) summaries is difficult. It would be good to quantify this if possible, as a counterargument to that would be that not all stories are noteworthy enough to require such treatment (and not all stories will appear on all sides).\nAllsides.com sometimes includes more than one source from the same side -- e.g. https://www.allsides.com/story/jan-6-panel-reportedly-finds-gaps-trump-white-house-phone-records has two stories from Center publishers (CNBC and Reuters) and none from the right. Since the inputs are always one from each side (Section 3.2), are such stories filtered out of the dataset?  Of the two major insights that form the basis of this work (polarity is a proxy for framing bias and titles are good indicators of framing bias) only first one is empirically tested with the human evaluation presented in Section 5.1.3. Even then, we are missing any form of analysis of disagreements or low correlation cases that would help solidify the argument. The only evidence we have for the second insight are the results from the NeuS-Title system (compared to the NeuSFT model that doesn't explicitly look at the titles), but again, the comparison is not systematic enough (e.g. no ablation study) to give us concrete evidence to the validity of the claim.\nRelated to the previous point, it's not clear what the case study mentioned in Section 4.2 actually involved. The insights gathered aren't particularly difficult to arrive at by reading the related literature and the examples in Table 1, while indicative of the arguments don't seem causally critical. It would be good to have more details about this study and how it drove the decisions in the rest of the paper. In particular, I would like to know if there were any counterexamples to the main points (e.g. are there titles that aren't representative of the type of bias displayed in the main article?).\nThe example in Table 3 shows that the lexicon-based approach (VAD dataset) suffers from lack of context sensitivity (the word \"close\" in this example is just a marker of proximity). This is a counterpoint to the advantages of such approaches presented in Section 5.1.1 and it would be interesting to quantify it (e.g. by looking at the human-annotated data from Section 5.1.3) beyond the safeguard introduced in the second paragraph (metric calibration).\nFor the NeuS-Title model, does that order of the input matter? It would be interesting to rerun the same evaluation with different permutations (e.g. center first, or right first). Is there a risk of running into the token limit of the encoder?\nFrom the examples shared in Table 3, it appears that both the NeuSFT and NeuS-Title models stay close to a single target article. What strikes me as odd is that neither chose the center headline (the former is basically copying the Right headline, the latter has done some paraphrasing both mostly based on the Left headline). Is there a particular reason for this? Is the training objective discouraging true multi-headline summarisation since the partisan headlines will always contain more biased information/tokens?\n### Minor issues I was slightly confused by the word \"headline\" to refer to the summary of the article. I think of headlines and titles as fundamentally the same thing: short (one sentence) high-level descriptions of the article to come. It would be helpful to refer to longer text as a summary or a \"headline roundup\" as allsides.com calls it. Also there is a discrepancy between the input to the NeuS-Title model (lines 479-486) and the output shown in Table 3 (HEADLINE vs ARTICLE).\nSome citations have issues. Allsides.com is cited as (all, 2021) and (Sides, 2018); the year is missing for the citations on lines 110 and 369; Entman (1993) and (2002) are seemingly the same citation (and the 1993 is missing any publication details); various capitatlisation errors (e.g. \"us\" instead of \"US\" on line 716) It's not clear what the highlighting in Table 1 shows (it is implicitly mentioned later in the text). I would recommend colour-coding the different types of bias and providing the details in the caption. ", "label": [[972, 1060, "Eval_pos_1"], [1062, 1148, "Eval_pos_2"], [1172, 1250, "Eval_neg_1"], [1252, 1320, "Eval_neg_2"], [3373, 3482, "Eval_neg_3"], [3483, 3978, "Jus_neg_3"]]}
{"id": 240, "review": "paper_summary\nPaper proposes quantization of generative PLMs coming from GPT and BART family. To overcome the challenges of homogenous word representations with quantized model which can impact conditional models such as GPT, authors present two modifications 1./ Word level contrastive distillation loss. 2./ Module aware quantization that depends on the magnitude of the layer to incorporate more adaptability.  Empirical results on various tasks show that proposed method outperforms the state of-the-art compression methods on generative  PLMs across next utterance prediction and summarization problems. With comparable performance with the full-precision models, paper present 14.4\u00d7 and 13.4\u00d7 compression rates on GPT-2 and BART, respectively. \nsummary_of_strengths\nPaper proposes quantization of generative PLMs coming from GPT and BART family. To overcome the challenges of homogenous word representations with quantized model which can impact conditional models such as GPT, authors present two modifications 1./ Word level contrastive distillation loss. 2./ Module aware quantization that depends on the magnitude of the layer to incorporate more adaptability.  Lot of earlier work focusses on BERT based model. Thorough analysis and proposal to mitigate gaps for generative models makes compelling case for future research where the problem is harder than the BERT counter-part and opens avenues for generative tasks by quantizing large PLMs.\nEmpirical results on various tasks show that proposed method outperforms the state of-the-art compression methods on generative  PLMs across next utterance prediction and summarization problems. With comparable performance with the full-precision models, paper present 14.4\u00d7 and 13.4\u00d7 compression rates on GPT-2 and BART, respectively. Thorough ablation studies for the two proposed directions. \nsummary_of_weaknesses\nLot of interesting ideas presented int he paper. It would be useful to incorporate latency aspects of the proposed methods. Additionally, it seems that GPT is used for utterance and BART for summarization but it will be good to add results for both the models across the tasks to see their impact. \ncomments,_suggestions_and_typos\nThere is lot of content and tables. Perhaps moving Related work higher might be useful for readers. Additionally adding results for both QuantGPT and QuantBART for both the tasks for completeness. ", "label": []}
{"id": 241, "review": "paper_summary\nThe paper introduces a token attribution analysis method for transformer based models building upon previous norm-based and rollout aggregation methods. The proposed approach incorporates all the components of the encoder block except feed forward layers to compute contribution of each token in a sentence. The work provides a detailed quantitative analysis of each component's role, layer-wise evaluation and achieves better results than previous methods on three classification datasets. \nsummary_of_strengths\n1. Paper is well written and easy to follow in general.\n2. Related work is adequately covered.\n3. The background section builds the analysis method quite well citing previous methods wherever necessary.\n4. The major strength and novelty of the paper lies in the detailed analysis, analyzing role of each component and comparison with weight-based, norm-based and gradient based methods. Sub-sections in section 4.5 can be quite relevant for the interpretability research area in general and relevant downstream application areas. \nsummary_of_weaknesses\n1. Although the work is important and detailed, from the novelty perspective, it is an extension of norm-based and rollout aggregation methods to another set of residual connections and norm layer in the encoder block. Not a strong weakness, as the work makes a detailed qualitative and quantitative analysis, roles of each component, which is a novelty in its own right.\n2. The impact of the work would be more strengthened with the proposed approach's (local and global) applicability to tasks other than classification like question answering, textual similarity, etc. ( Like in the previous work, Kobayashi et al. (2020)) \ncomments,_suggestions_and_typos\n1. For equations 12 and 13, authors assume equal contribution from the residual connection and multi-head attention. However, in previous work by Kobayashi et al. (2021), it is observed and revealed that residual connections have a huge impact compared to mixing (attention). This assumption seems to be the opposite of the observations made previously. What exactly is the reason for that, for simplicity (like assumptions made by Abnar and Zuidema (2020))?\n2. At the beginning of the paper, including the abstract and list of contributions, the claim about the components involved is slightly inconsistent with the rest. For instance, the 8th line in abstract is \"incorporates all components\", line 73 also says the \"whole encoder\", but on further reading, FFN (Feed forward layers) is omitted from the framework. This needs to be framed (rephrased) better in the beginning to provide a clearer picture.\n3. While FFNs are omitted because a linear decomposition cannot be obtained (as mentioned in the paper), is there existing work that offers a way around (an approximation, for instance) to compute the contribution? If not, maybe a line or two should be added that there exists no solution for this, and it is an open (hard) problem. It improves the readability and gives a clearer overall picture to the reader.\n4. Will the code be made publicly available with an inference script? It's better to state it in the submission, as it helps in making an accurate judgement that the code will be useful for further research. ", "label": [[529, 582, "Eval_pos_1"], [624, 685, "Eval_pos_2"], [686, 729, "Jus_pos_2"], [1082, 1298, "Eval_neg_1"], [1455, 1651, "Eval_neg_2"]]}
{"id": 242, "review": "paper_summary\nThis paper propose to use an additional loss to automatically learn model confidence for NMT models. Experiments show that the learned confidence can better reflect the quality the model output and using the learned confidence can result in better BLEU score. \nsummary_of_strengths\n1. The work is clearly motivated, the authors aim to learn a better calibrated confidence for NMT models, which is useful in many practical scenarios. \n2. Experiments indicate the effectiveness of the learned confidence. \nsummary_of_weaknesses\n1. Previous studies find that the mis-calibration problem is closely related with model size (Guo et al., 2019; Wang et al., 2020). In this work, the authors did not conduct experiments using larger NMT models, making the results less convincing. \n2. Table 1 shows that the proposed method only brings marginal improvements in terms of Pearson\u2019s correlation. \n3. Table 2 shows that the learned confidence score can result in better BLEU score. To my knowledge, model confidence can affect the BLEU score with difference beam sizes. I wonder wether the proposed method can improve BLEU in larger beam size (i.e., beam size = 100). Moreover, in Table 2, the authors did not compare the proposed method with graduated LS (Wang et al., 2020), which is a closely related baseline. \ncomments,_suggestions_and_typos\nIn Table 4, I wonder whether \"the model probability\" is estimated using MC dropout. Since MC dropout is very useful in OOD detection. Please provide more details. ", "label": [[298, 329, "Eval_pos_1"], [450, 517, "Eval_pos_2"], [543, 671, "Jus_neg_1"], [672, 787, "Eval_neg_1"]]}
{"id": 243, "review": "paper_summary\nThe authors present LexGLUE, a benchmark for legal language understanding. Their goal is that in releasing this benchmark, it will be easier to measure progress on legal language understanding, which currently has relatively fragmented resources and evaluations. They also release several baselines on the benchmark they release. \nsummary_of_strengths\nThis is a well written paper that's motivation is quite clear. \nThe datasets/benchmarks gathered should be useful to researchers working on legal language understanding. \nsummary_of_weaknesses\nNone of the datasets presented are actually new, rather these are just new splits and a packaging of existing resources into a benchmark. \nThe tasks require input texts of drastically different lengths, which a single model might not be very good at. \nThe baselines are a little bit old and arbitrary (no encoder-decoder models and only one type of hierarchical modeling). \ncomments,_suggestions_and_typos\nLexGLUE feels like kind of an odd name for a legal focused dataset 2 Line 231-232: s/access/assess 3 It feels odd to try to make only have english a desirable property of the benchmark, instead of a limitation 3.2  what does it mean articles cannot be violated? \nare these cases normally split chronologically? While this ensures a lack of overlap, it seems possible that each court produces a slightly different distribution of decisions. Is this type of out of distribution evaluation intended?\n4.2 Why are all of the models tested encoder-only? Why no encoder-decoder models like T5? \nBERT-like models etc. not that good at storing sentence encoding representations in the CLS token (see e.g., Fast, Effective, and Self-Supervised: Transforming Masked Language Models into Universal Lexical and Sentence Encoders (Liu et al 2021) or Condenser: a Pre-training Architecture for Dense Retrieval (Gao & Callan 2021), so difficult to conclude these baselines are particularly strong. In general, hierarchical modeling seems to be its own task area, where both different pretraining techniques might produce better sentence encodings and different methods of hierarchical modeling may produce better overall results. \nThis baseline seems to combine tasks of various different lengths \u2013 do you expect the same models to be good at both? Is it even important for one model to be good at both?\n5.2: feels a bit hard to say that your result clearly favors the legal models -- 13 total results, 6 of which the legal models do the best Why is there no \u201coverall\u201d score reported, like there is with GLUE? ", "label": [[366, 429, "Eval_pos_1"], [430, 536, "Eval_pos_2"], [811, 859, "Eval_neg_1"], [859, 932, "Jus_neg_1"]]}
{"id": 244, "review": "paper_summary\nThis paper introduces a new image analysis task aiming to identify the time and location of a given image. The authors demonstrate that solving the task requires extracting visual and textual information and searching in a knowledge base. The details of the data collecting process, and an analysis of the collected dataset is provided. The authors study human performance and the performances of several CLIP-based models and show that there is a large gap between machine performance and human performance. \nsummary_of_strengths\nBoth the task and the dataset are new. The task requires extracting and reasoning with visual and textual information in the input image, and it is a non-trivial task to collect the dataset. The large gap between human and machine performance suggests that this is a challenge task to solve. \nsummary_of_weaknesses\n- The usefulness of the proposed task is not clearly justified. In the abstract it is briefly mentioned that the proposed task helps with subsequent tasks, but the authors did not provide further discussions and references. Also, it seems that time and location can only be inferred for certain types of images (e.g., news images), and therefore it is not clear how this can be a generally useful task.\n-The authors demonstrated that the proposed task requires some reasoning capability to solve, but they only evaluated basic vision-language retrieval models that do not seem to have the corresponding reasoning capability. There is also not too much analysis of the experiment result, especially on the result that shows time prediction is completely off. \ncomments,_suggestions_and_typos\n- The authors need to provide a compelling argument about the practical value of the proposed task and demonstrate that incorporating the corresponding reasoning capability improves the model performance.\n-It might be useful to provide the chance accuracy so that the readers can have a better idea of how well the baselines perform.\n-Considering that the task can also be formulated as a classification problem, it might be useful to consider a standard softmax classifier. This will also give us an idea about how much performance one can get without transfer learning. Also, the time prediction seems to be completely off; the softmax classifier might be useful for diagnosing the problem. ", "label": [[862, 923, "Eval_neg_1"], [924, 1262, "Jus_neg_1"], [1264, 1485, "Eval_neg_2"], [1485, 1618, "Eval_neg_3"], [1653, 1855, "Eval_pos_4"]]}
{"id": 245, "review": "paper_summary\nThe paper proposes a novel model called R^2G which combines neural initial retrieval and reranking into a BART-based sequence- to-sequence generation model.\nFurthermore they ensemble different first stage retrieval methods, combining neural retrieval and lexical retrieval with BM25. \nThey propose a novel variation of knowledge distillation to train the initial retrieval, reranker and generation using only ground truth on the target sequence output.\nThey compare their novel model on the KILT leaderboard and demonstrate effectiveness gains compared to relevant related work. \nsummary_of_strengths\nThe paper proposes an extension of the RAG architecture by including another lexical index.\nI like the discussion of the different solutions for end-to-end training of the system and also the explanations why certain approaches do not work and the other ones do.\nI also think that the extension of the RAG system with a re-ranking approach is interesting for the task. \nsummary_of_weaknesses\nI am wondering how novel the application of the knowledge distillation from the reranker as teacher model to provide labels to the DPR model is, see Hofst\u00e4tter et al.\"Improving Efficient Neural Ranking Models with Cross-Architecture Knowledge Distillation\" (https://arxiv.org/abs/2010.02666).\nFurthermore I am missing an ablation of training the reranker with the combined scores of the positive passages (line 277) in comparison to the standard way the reranker is trained in Nogueira et al.. I would also appreciate an ablation study of combining DPR and BM25 results with reranking in comparison to established solutions like combination with Reciprocal Rank Fusion (see Chen et al. \"Out-of-Domain Semantics to the Rescue! Zero-Shot Hybrid Retrieval Models\") or combining the scores with a linear combination of the BM25 and DPR score (see Karpuhkin et al. \"Dense passage retrieval for open-domain question answering\") I am also wondering why only 5 of 11 existing datasets of the KILT leaderboard are selected, I would be interested why you exactly choose these datasets. \ncomments,_suggestions_and_typos\nI think the overall writing is clear and understandable however I thought it could be better organized in some sections: Line 207-2018 describes dense retrieval in the section about re-ranking, I think dense retrieval should be described beforehand and not in the section \"Reranking\" The formulas in Line 310 are not described and embedded in the text, I think describing them in the neighbouring text would make clear why the formulas are there. ", "label": [[707, 877, "Eval_pos_1"], [878, 984, "Eval_pos_2"], [1007, 1150, "Eval_neg_1"], [1300, 1500, "Eval_neg_2"], [1500, 1928, "Eval_neg_3"], [1929, 2083, "Eval_neg_4"]]}
{"id": 246, "review": "paper_summary\nThis paper aims to find the best strategy to select training examples for few-shot transfer. They evaluated 5 data selection methods (data cross-entropy, predictive entropy, gradient embedding, loss embedding) on 3 tasks (POS tagging, NER, NLI) from 20 languages (categorized into 3 groups) . Their final results showed embedding based data selection methods consistently outperforms other strategies. \nsummary_of_strengths\nComprehensive empirical studies. Experiments seems to be very solid. Paper is clearly written. \nsummary_of_weaknesses\nAlthough the experiments are very comprehensive, this paper lacks technical novelty. The optimal data selection strategy also seems to offer limited improvement over random data selection. \ncomments,_suggestions_and_typos\nCan you provide more insights about why loss embedding based data selection is better for sequential labeling task while gradient embedding is better for classification task? Why embedding based methods are better than other selection methods?\nFor method selection, details should be included in the main paper rather than appendix (2.1 and 2.2 all refer to Appendix B). ", "label": [[471, 506, "Eval_pos_1"], [507, 533, "Eval_pos_2"], [605, 641, "Eval_neg_1"]]}
{"id": 247, "review": "paper_summary\nThe paper introduces the Pragmatic Rational Speaker framework that allows a speaker to take the listener\u2019s disparities into account when communicating. This framework is designed with a disparity adjustment layer into the working memory of the speaker, which would lead a speaker to adjust their lexical and semantic language while speaking, on top of the long-term memory. Through this, the working memory would be constantly updated to modify the speaker\u2019s language based on the different listeners.  The paper\u2019s authors create a dataset to imitate the two types of disparity, including \u201cvocabulary limitation\u201d and \u201cvisual access\u201d. This is then used in experimentation which initially follows the Rational Speech Act (which this paper also extends), and the models are evaluated along the performance (measures the accuracy of the collaborative game), efficiency (measures time used for model training), and transparency (shift in vocabulary that is used in accordance with different speakers). In the end, the authors notice that the framework designed increases the game performance and shifts language to accommodate the listener\u2019s understanding. The paper list several limitations, however, of the study that need future work, including how the generated captions do not entirely and accurately represent the image at hand due to various interactions within the image itself. \nsummary_of_strengths\nThe paper is thorough and describes a novel method to understanding the listener\u2019s disparity as the speaker communicates. This is also the first model that proves the shift in semantic and lexical dialogue of the speaker, when other studies do not provide this, and the empirical results clearly show this. The paper introduces both a dataset and collaborative game that allows the models to be tested in a simulated environment. In turn, this will be especially helpful to ACL readers.\nThe Related Work section is also especially helpful and introduces the concepts covered in the paper in an easy-to-follow format. It clearly shows the Rational Speaker Act, the previous work that the paper extends, and introduces why the authors of the paper applied the disparity adjustment layer to the working memory. \nsummary_of_weaknesses\nThe methodology used in the paper to test the models is interesting, as the speaker and listener simulation is done in a collaborative game. However, how well does this game model the complexities of real-life communication and the speaker? The authors also list that the image captioning model does not entirely represent the image most accurately, which could bring concerns upon the \u201ctransparency\u201d measurement of this simulation. Moreover, speakers adjust their thinking to accommodate listener disparity in many ways, some of which come from incoming people or the environment that they are in (this is acknowledged in the Conclusion and Future Work section). A lack of including this in the simulation may be a cause of concern. \ncomments,_suggestions_and_typos\nLine 122: \u201cis a is a\u201d -> \u201cis a\u201d ", "label": [[1418, 1468, "Eval_pos_1"], [1540, 1679, "Eval_pos_2"], [1725, 1847, "Jus_pos_3"], [1848, 1904, "Eval_pos_3"], [1905, 1956, "Eval_pos_4"], [1960, 2034, "Jus_pos_4"]]}
{"id": 248, "review": "paper_summary\nThis paper proposes a new Consistency and Robustness Evaluative Test Suite (CARETS) for the VQA task.  This test suite differs from previous work by balancing question generation to create pairs of instances to test models and focusing on six capabilities, rephrasing, ontological, order, visual obfuscation, attribute antonym, and negation.\nThe paper conducts many experiments in some state-of-the-art VQA systems in the proposed dataset to reveal existing major issues. \nsummary_of_strengths\n1. The motivation of this paper is clear and novel, as well as easy to follow.\n2. The new dataset can be very useful for the VQA community and greatly promote progress in this field. \nsummary_of_weaknesses\n1. This paper mainly compares some experimental results of existing models on proposed new data, I would suggest exploring a potential solution based on the proposed new data.\n2. How do the templates generate? \ncomments,_suggestions_and_typos\nPlease check some typo before publication: Line435-436: \"their weight initailiza-tions\" -> initailizations Line 613: \"their comprehension of different visuo-linguistic\" -> visual-linguistic ", "label": [[511, 586, "Eval_pos_1"], [590, 691, "Eval_pos_2"], [717, 889, "Eval_neg_1"]]}
{"id": 249, "review": "paper_summary\nThe paper proposes a contrastive learning-based method for phrase representations and topic mining. Results show that the method achieves the best performance on topic mining and phrase representations. The proposed model can extract more diverse phrases. \nsummary_of_strengths\n1. The paper applies unsupervised contrastive learning to topic modeling, which is suitable for the unsupervised task. Results show that the proposed model can extract more diverse phrases. \n2. The authors also find that in the finetuning process, in-batch negative samples have a bad influence on the performance. So they propose a topic-assist contrastive learning method to reduce noises and turn the original finetuning process into a topic-specific finetuning process. \n3. Experiemnts results show that the proposed model achieves good performance on several datasets. \nsummary_of_weaknesses\n1. It seems that the biggest contribution of the paper is to apply contrastive learning to topic modeling, which is of limited novelty. \n2. Batch is a sampling method, so what is the major difference between batch and the proposed one? This limits the performance significantly. \ncomments,_suggestions_and_typos\nSuggestions: 1. It is a little bit confusing in the assumptions in section 1. First of all, \u201cThe phrase semantics are determined by their context.\u201d. As for the examples in Figure 1, the semantic of phrase \u201c United States\u201d is fixed and not influenced by the context. I think the writers want to express: if we mask \u201cUnited States\u201d, we can still infer the mask phrase by its context. \n2. Writing should be strengthened. ", "label": [[892, 995, "Jus_neg_1"], [996, 1025, "Eval_neg_1"]]}
{"id": 251, "review": "paper_summary\nThis paper presents and empirically confirms the \"prompt waywardness hypothesis\" in the context of continuous prompt tuning. The hypothesis is that for any arbitrary text there exists a continuous prompt whose discrete projection is said arbitrary text *and* whose resulting performance is almost as good as the best possible continuous prompt. Therefore, the paper suggests, it is difficult to evaluate continuous prompts in a discrete space because there always exist good continuous prompts that map to random or misleading discrete text.\nA little more formally, the hypothesis is that for any downstream task and arbitrary text $p_d$ of length L, there exists some continuous prompt $\\tilde{p}_c$ of length L such that $\\tilde{p}_c$ results in a test loss close to that of the best continuous prompt $p^*_c$ of length L, and yet $\\tilde{p}_c$ projects to $p_d$ (when projected into discrete space).\nThe authors empirically confirm this hypothesis by showing that for five downstream classification tasks they are able to find continuous prompts which project to certain arbitrary sentences (at least very closely, as evaluated by F1) but whose resulting accuracy is almost as good as that of the optimal prompt (as evaluated by the difference in test accuracy). These experiments use the GPT-2-large model (except when model size is ablated), and projection is defined as follows: continuous prompts are projected to discrete space by finding the token with the nearest-neighbor embedding (where token embeddings are taken to be GPT-2's embedding matrix). Finally, the paper discusses implications of the prompt waywardness hypothesis, the main implication being that continuous prompts may never be interpretable by being projected to discrete language. \nsummary_of_strengths\n1. The paper is clearly written and easy to follow. \n2. The experimental results are thorough and convincing; the paper provides a strong analysis section evaluating the effects of hyperparameters, model size, and prompt length. \n3. The paper discusses implications of the hypothesis: the difficulty of interpreting discrete prompts and the untrustworthiness of discrete projections of continuous prompts. \n4. The conclusion that encouraging continuous prompts to project to semantically accurate instructions (Appendix B) provides interesting insight into the function of continuous prompts. \nsummary_of_weaknesses\nThe implications discussed in the paper apply *if* one were to try to project continuous prompts to discrete space using GPT-2's (or any other) embedding matrix, but, as far as I am aware, no one has attempted to interpret continuous prompts in this way. The paper could be strengthened by more explicitly discussing *why* we should be concerned with this particular method of prompt interpretation: 1. Do the authors think we should be especially concerned about nearest-neighbor, embedding matrix projections because that's how language model outputs and word2vec are calculated? This seems to be the case, but could be made more explicit. It's not clear why the use of an embedding matrix at the output layer of language models implies this would be a popular way of discretizing continuous prompts. \n2. Is it because the particular discrete projection studied is the simplest that the authors believe it might be used in the future? \n3. Do the authors think their hypothesis generalizes to other methods of discretely interpreting prompts? If so, that could also be made more explicit. \ncomments,_suggestions_and_typos\n1. Space permitting, it may be beneficial to move the finding of Appendix B to the main paper as I found this interesting. \n2. Line 246 is difficult to parse. Possibly split this into two sentences. ", "label": [[1798, 1847, "Eval_pos_1"], [1851, 1904, "Eval_pos_2"], [1904, 2024, "Jus_pos_2"], [2205, 2388, "Eval_pos_3"], [2666, 2810, "Eval_neg_1"]]}
{"id": 252, "review": "paper_summary\nThis paper introduces a new dataset FAIRYTALEQA with text extracted from fairy tales, claimed to be appropriate for K-8th grade, with human annotations of QA pairs. A hybrid approach is proposed for generating QA pairs. Heuristic rules are applied to the design of the model for answer extraction (e.g., extracting named entities and noun chunks as potential answers and semantic roles of verbs as annotated in Propbank). These heuristics help in identifying answers of a specific type appropriate for narratives (identifying characters, setting, plot etc). Then a model is fine tuned on this dataset to generate questions. The model is compared with models fine-tuned with other datasets. The generated QA pairs are ranked via a classification task that decides if a QA pair is more similar to a ground truth pair or not. Automated evaluation metrics include calculating a ROUGE-L score. Human evaluation metrics are reported on three evaluation questions for 70QA pairs. \nsummary_of_strengths\nOverall this paper is tackling the issue of domain specific QA generation, specifically comprehension of narrative fictional passages for K-8.  This is an important area with a high potential for impact and problem has been adequately been described. Below I list the major strengths of the paper. However, there are sever issues with the paper that need to be addressed.  + Substantial effort for creating new dataset of fairytales annotated with QA pairs. \n+ Interdisciplinary work with education experts which leads to introducing a specific set of comprehension questions appropriate for the fiction narrative genre. \n+ Human annotation and evaluation \nsummary_of_weaknesses\n-p. 3 the authors point out that general purpose QA are not appropriate for the education domain and that they solve this problem by developing a new dataset targeting students K-8th grade. This is a big range of grades where fundamental reading comprehension strategies and abilities apply. Reading comprehension questions for Kindergarteners are vastly different from 8th grade middle schoolers. \n-The authors overgeneralize when making claims about the problem they solve in many places. I advise the authors to be precise with their claims.  The current work does not solve the problem of QA in education. It makes a contribution for asking specific type of basic reading comprehension questions for the genre  fiction fairy tale genre. \n-It is not clear if the improved results reported in Table 3 are statistically significant. It would be useful to add a discussion as to whether the reported improvement was within expectation given the effort for annotating a specialized dataset. \n-p.5 It is not clear how the proposed classification task can be extended for ranking QA pairs beyond the specific dataset. The method relies on having available ground truth pairs. \ncomments,_suggestions_and_typos\np.3 Casual Relationship --> Causal Relationship p.7/8 The human evaluation questions that are reported are 3  (readability, question relevancy, answer relevancy) but the intercoder reliability is evaluated on 4 dimensions. Which is the fourth dimension. Also, can you provide more details about how you computed Krippendoff's alpha? The reported agreement for 5 annotators on questions of relevancy is surprisingly high. Maybe it would be helpful to explain why you didn't choose answer accuracy. ", "label": [[1153, 1259, "Eval_pos_1"], [1693, 1979, "Eval_neg_1"], [1980, 2086, "Jus_neg_1"], [2088, 2178, "Eval_neg_2"], [2234, 2429, "Jus_neg_2"], [2683, 2802, "Eval_neg_3"], [2803, 2861, "Jus_neg_3"]]}
{"id": 253, "review": "paper_summary\nThis paper addresses some key issues of the Entity Set Expansion (ESE) problem; from the evaluation metrics and dataset perspective. The authors mentioned that existing datasets do not consider entities that belong to multiple concepts, are non-named and vaguely attached to the concepts they belong to. They show how their benchmark datasets capture these properties. They also argue that MAE at top-20, which is a standard evaluation metric for ESE approaches, is not sufficient to understand their effectiveness. They propose MAE. at gold-k as an alternative. \nsummary_of_strengths\nThe paper proposes three benchmark datasets that are helpful for evaluating ESE approaches. The authors focus on three key issues of ESE datasets and address those in constructing their own datasets. \nsummary_of_weaknesses\nThe study is not well motivated. The paper does not consider the application perspective of ESE approaches and the arguments about evaluation metrics is not strong. The description of the dataset construction process is inadequate. \ncomments,_suggestions_and_typos\nWhat is the motivation behind applying ESE methods on user-generated text? Why would someone apply set expansion approaches to construct a dictionary from customer reviews? For example, a customer might mention that the room type of a hotel is small. How does that help in constructing a dictionary for the room type concept?   \u201cWe first select concepts for the seed by referring to the features on the corresponding websites, to ensure their relevance for immediate downstream tasks.\u201d - This is how the authors define the relevance for immediate downstream tasks. But, I worry if ESE has application in such scenarios. In fact, ESE is a useful mechanism when a concept is complicated enough to be expressed as a query. The concepts selected by the authors in this paper are well-defined by the site owners.  The argument about the new metric MAP at gold-k is not solid. From the retrieval perspective MAP at top-20 still makes sense. If someone is interested to retrieve all the entities belonging to a concept, they can look at the top-20 retrieved entities, annotate them, and then get more seeds to train a model to retrieve more. Based on this iterative process, one can find all the relevant entities belonging to a concept. There is a Total Recall track at the Text Retrieval Conference (TREC) that tries to achieve this. That\u2019s why we should not disregard MAP at top-20, but we can explore the performance of a method at gold-k for reference. How does the map at gold-k makes sense from the application perspective? If an ESE approach returns a ranked list of entities, at which depth a user should go to find all the relevant entities? The user does not have any idea about gold-k and could only go to some pre-defined depth such as 20. Different depths could be explored considering how much effort a user is interested to put in, but MAP at top-20 should never be ignored.  \u201cExisting evaluation metrics tend to overestimate the real-world performance of ESE methods and may be unreliable for evaluating concepts with large entity sets.\u201d \u2014 I do not agree with this takeaway because of the above statement.  The effectiveness of ensemble approaches is not surprising.  The performance difference between CGExpan and LM-Base (proposed by the authors) is striking even if the underlying techniques are very similar. Can the authors provide more insight into this? Why does LM-base perform so well compared to CGExpan in user-generated datasets? I wonder if there could be hyperparameters for CGExpan that are not well-tuned.  I find the dataset description to be inadequate. Specifically, the source of the data is not clearly mentioned. ", "label": [[822, 854, "Eval_neg_1"], [855, 986, "Jus_neg_1"], [987, 1054, "Eval_neg_2"]]}
{"id": 254, "review": "paper_summary\nThis paper tries to answer the question: `what kinds of synthetic data contributes to BT performance? ` The authors identify two factors that affect the performance: quality and importance. The authors propose a simple yet effective method to generate synthetic data for better translation performance. \nsummary_of_strengths\nThe authors analyzed the BT data both theoretically and practically. \nDefined two important factors: quality, importance, from a theoretical perspective, to better understand the BT performance. \nProposed a simple yet effective method to improve BT performance. \nsummary_of_weaknesses\n1. It's not intuitive that the `importance` and `quality` are mutually exclusive. I assume the `importance` somehow related to the diversity of BT data, and improve the robustness of MT model to noisy input. \n2. About the experiments, I realized that all experiments are carried out on xx to en directions, while en to xx directions are missing, which makes the results less convincing. \n3. Line 254. The authors utilize GPT to estimate the importance of x, which is reasonable. In such scenario, comparing with a GPT-based (pretrained) translation baseline would be more fair since additional information was introduced. \ncomments,_suggestions_and_typos\nSee weaknesses ", "label": [[627, 832, "Eval_neg_1"], [836, 968, "Jus_neg_2"], [970, 1010, "Eval_neg_2"]]}
{"id": 255, "review": "paper_summary\nThis paper introduces a new method called CrossAligner which is used to transfer knowledge from English (or a high-resource language) to some other language without the presence of data using a zero-shot approach. Several methods that used CrossAligner  such as training techniques for machine translation, dialog classification, and more along with a detailed error analysis are introduced as intrinsic evaluations of the method. \nsummary_of_strengths\nThe paper presents statistical rigor on the technique it presents as novel, CrossAligner. \nThe mathematics are sound and the approach makes sense. \nThe qualitative findings are well presented and a deep research analysis covering the architecture, gradient losses, and intent and entity classification is performed. \nAll metrics and results are gotten from previous work and others are introduced in a sound manner, based on investigative findings. \nsummary_of_weaknesses\nIn some cases, this paper goes overboard with the analysis in what could seem as an attempt to salvage good results. \nThere are several findings presented on several, almost non-related, tasks and the related work section, while interesting seems like overkill. \nSome of the more important works on translation multi-lingual transfer are omitted. \nThe findings are not outperforming the current state of the art in several tasks and in those tasks where the CrossAligner does outperform the state of the art, it is not bot much (1 point on F-score more or less). \nWhile the statistical rigor is fun to read, I am not sure that the overall findings are novel enough for an ACL paper. \nDespite the statistical rigor and attention paid on the details of error especially from Figure 2 where weighted losses are combined, the error analysis makes a lot of generic claims that do not seem to be based on the statistical proof. \ncomments,_suggestions_and_typos\nLine 048 - There is an assumption that it \"can\" be done based on non-relevant results. \nLine 058 - The intro here talks about alignment but then there seems to be a reference to Question Answering (QA), I understand that the case is not that but the writing here seems to be a little convoluted. \nLines 068 - 080 - Are these lines referring back to the QA idea or is this now a translation problem, how did translation become part of a QA problem? Is it due to the desire to show evidence that the aligner works? Please explain better. \nLine 081 - The related work is a little long, you could mention the two types of transfer and cite some work in a few lines, not this many pages. It is not bad what you have done but not as important I think. Also, you may want to mention the latest low-resource XLM transfer findings from AmericasNLP (Mager et. al) for example to help strengthen this some. \nLine 185 - You are repeating some here. \nLine 173 - borne --> born Line 171-175 - Is there a percentage amount that was observed previously? \nLine 178 - Please explain what you mean by \"losing\" their positional information during translation. \nLine 188 - Algorithm 1, line 14, while it may be obvious, \"y\" is never introduced or explained, is it the labels? \nLine 188 - Algorithm 1, line 25, gradients are explained and shown as a method call, but the main technique (CA) is not really in the algorithm, is the novel idea the logloss gradient transfer? If so, you may want to separate it and show it better. \nLine 238 - \"forms a positive pair\", how is this pair formed by the -log on cosine similarity, doubtful but please explain more. \nLine 286 - Here the datasets point back to QA, but translation is covered, what is this paper about, QA? Intent Classification? Entity Classification? Or, translation? Or all? The line is now unclear. \nLine 310 - A \"minimalist\" setup, you may want to remove some of the related work and expand this, how can your work be reproduced off of these settings? \nLines 320 - 330 - Now, it looks like we are combining tasks again, the other sections here, however, are good. \nLines 381 - 410 - The gains here do not represent a major novelty, in several cases there are losses, which is fine but discouraging. \nLines 430 - 432 - Has this type of loss been proposed in the past for interpretability? If so, please cite. \nLines 441 - 453 - I am not convinced that doing this really helps show the performance, it almost seems as if you are mining to find a difference. Could you not show easily an f-score as you have already done? I understand that it probably isn't the case but the deep dive here is not warranted really in my opinion. \nSection 6.1 - Error Analysis summary, several issues, \"don't carry important sentence level semantics\" - this is not clear, please explain; \"culture and vernacular\" - how did you get that out of your findings?; \" The limits of machine translation\" - please see the AmericasNLP paper mentioned before and cite it as it is important; \"Finally, there were no substantial qualitative...\" - this paper does not show that really. \nSection 7 - The conclusion is weak as far as the results are concerned at a high level. I would suggest that the technique focused on more high level ways of measuring for gains. ", "label": [[467, 557, "Eval_pos_1"], [558, 614, "Eval_pos_2"], [615, 783, "Eval_pos_3"], [954, 1056, "Eval_neg_1"], [1057, 1201, "Eval_neg_2"], [1547, 1622, "Major_claim"], [2442, 2475, "Eval_neg_3"], [2477, 2790, "Jus_neg_3"], [4996, 5071, "Eval_neg_4"]]}
{"id": 256, "review": "paper_summary\nThis paper proposes DAR, which improves dense retrieval models with MixUp interpolations and dropout perturbation. MixUp (Zhang+ 2018) was initially proposed for image models, which creates augmented examples by linearly interpolating both the representations and labels between positive-negative sample pairs.  This work applies this technique to dense  text retrieval, and augments the original contrastive loss with soft binary cross-entropy losses. Furthermore, this paper also explores perturbing the representations with 0.1 dropout rate, and this technique can be combined with the MixUp strategy.\nExperiment on the DPR framework shows that DAR significantly outperforms vanilla DPR and its combinations with different data augmentation strategies (QA,DA,AR). The improvement is more prominent on unlabeled documents (not seen during training).  Oblation study shows that perturbation is only marginally helpful by itself, but a lot more helpful when combined with MuxUp.  Experiment on the ANCE framework shows that combining ANCE with DAR significantly improves MRR, and marginally improves R@1k. \nsummary_of_strengths\nThis work is well motivated from the need to generalize unseen documents, and shows good empirical results. \nsummary_of_weaknesses\nHowever, I find it less motivated by the computational cost of other data augmentation approaches, which are not really that expensive as shown in Table 7. \ncomments,_suggestions_and_typos\nIt seems that the proposed document augmentation approach is not mutually exclusive to other approaches such as query generation (QA, DA, AR in Table 1), and we should evaluate the combined performance of them with DAR. ", "label": [[1142, 1250, "Eval_pos_1"], [1282, 1429, "Eval_neg_1"]]}
{"id": 257, "review": "paper_summary\nThis paper concerns the task of temporal grounding, which is framed as the mapping of temporal expressions (morning, noon, afternoon, evening, night) to their start and end time hours in the day in different languages (and cultures). To achieve such a mapping,  different approaches are proposed: a) a corpus-based approach where start and end time are (indirectly) inferred from the distribution of time references in Wikipedia data; b) two language-model (LM) methods where start and time can be either i) directly predicted by multilingual BERT, or ii) indirectly estimated from the predictions of multilingual BERT. Such methods are evaluated against newly created gold standard temporal annotations for 4 languages (Italian, Hindi, English and Portuguese). Additionally, the most successful extractive corpus-based method is tested on other  (although unlabeled) 24 languages. The paper also includes several discussions on cultural/temporal differences across languages and additional analyses on the proposed methods. \nsummary_of_strengths\nThe paper contributes to a niche of research by proposing an interesting take on the study of time expressions in NLP.  Specifically, the cross-lingual and cross-cultural focus of this paper provides several insights on the challenges of temporal commonsense that can inform research on the topic by highlighting -- often neglected in NLP -- language variability and speakers (time) experiences. This, in particular, emerges in sections 2.3, 5.2, 5.3 but also in 7.   Besides being reported as a quite informative process (section 2), the creation and consequent release of temporal gold standard annotations for 4 different languages can be a useful resource for the community.  The paper is reasonably well-written and pleasant to read. \nsummary_of_weaknesses\n(1) It is claimed that this paper \u201cpropose a task\u201d, \u201creframe a research question\u201d. It thus seems to suggest that this work addresses temporal grounding as a *new* task. However, information about its utility and scenarios of applicability is not really elaborated (just a generic sentence at l.60) and are rather left to the reader to imagine. Also, the related work is presented as more of a description of previous studies rather than a critical \u201cmapping\u201d. As a consequence, it is hard to situate the proposed methods and insights and how they differ from/ and contribute to the topic. I would suggest being more specific and explicit about its contributions and how it relates to previous work.  (2) I see some issues with the proposed methods and evaluation: 2a) Figure 2 shows how different annotators (for the same language) referred to different time ranges for the same part of the day. In fact, Hindi annotations suggest that night can span till 9 am, thus overlapping with the time range for the morning, too. Then, at lines 195-99, the extractive approach is defined as to infer *non-overlapping* time ranges. Finally, in Figure 3, the gold standard annotations are represented as non-overlapping time spans among parts of the day. It is however unmentioned how a clear-cut distinction across parts of the day with definite time spans was defined to create such a final benchmark.  2b) Section 3.1: the extractive corpus-based approach relies on *English* Wikipedia data. No information about the number of time expressions needed is however provided. Also, I wonder whether by relying on English time expressions -- which are then automatically translated into other languages -- a strong western bias is induced as well as translation errors: was there someone with the needed language competence to verify the soundness of such translations? This part should be better documented, for instance in the supplementary material. \n  2c) The translation error issue presents itself also in section 3.2. Footnote number 5 mentions that BERT queries are based on the template \u201cThe <morning> starts at <9.00>\", and that such sentences are automatically translated into other languages. In a language with grammatical gender such as Italian, however, the translation of \u201cthe\u201d would vary depending on the mentioned part of the day (e.g. the afternoon: IL pomeriggio; the morning: LA mattina). Is this something that has been adapted? If not could it have skewed BERT predictions?\n2d) In section 4.2. the proposed approaches are compared against a baseline based on the \u201cgreeting method\u201d by Vilares and G\u03ccmez-R\u03ccdriguez (2018). Their work however does not really seem to serve well as a baseline since it does not entail a *method*. Rather, they created a corpus to conduct an analysis on the semantics of greetings. Thus their goal is different from the one pursued in this paper, and their corpus here is simply employed to test the extractive method (3.1) on a different source of data.  (3) Although the paper is reasonably well written and pleasant to read, some turns are taken for granted and not really justified. For instance, the analysis in section 5 seems to concern the 24 unlabeled languages, this is however left unstated. Also, for instance, in 5.1 the analysis is carried out for Italian only, but the choice for such a decision is not given. \ncomments,_suggestions_and_typos\nAs a general comment, I find that this paper seems to be a bit scattered, or better, it alternatively shifts from a culturally-situated focus to the strive for \u201clanguage-agnostic\u201d approaches. Besides the fact that I am having some doubt as to whether such approaches grant the definition of language-independent method (it does not seem to be very effective on all languages, it largely relies on *English* data and concepts, e.g. noon), I think that such shifts bring the paper out of focus at times and may affect its narrative. Note that this is more  more of an opinionated point of view (which could be discarded and I am thus not putting in the weaknesses section), but I think the paper should enhance the former, linguistically and culturally grounded perspective, which in my opinion is also the main strength of this work.  Few minor suggestions: - footnote1. move it at line 58 -footnote2: specify HIT acronym -the related work would be maybe put to better use after the introduction Finally, as per https://2021.aclweb.org/ethics/Ethics-review-questions/, information about the compensation of annotators via Amazon Mechanical Turk has to be provided. ", "label": [[1061, 1179, "Eval_pos_1"], [1181, 1456, "Jus_pos_1"], [1529, 1739, "Eval_pos_2"], [1741, 1800, "Eval_pos_3"], [1905, 2166, "Eval_neg_1"], [2282, 2410, "Jus_neg_2"], [2411, 2522, "Eval_neg_2"], [2526, 2585, "Eval_neg_3"], [2586, 5184, "Jus_neg_3"], [5217, 5408, "Eval_neg_4"], [5409, 5641, "Eval_neg_5"], [5655, 5747, "Jus_neg_5"]]}
{"id": 258, "review": "paper_summary\nThe paper introduces a new Chinese semantic role labeling (SRL) dataset, MuPAD, that covers 6 domains (product comment, web fiction, law, medical etc.). The dataset is motivated by the fact that most Chinese SRL research has been on the news domain. A frame-free annotation scheme was adopted to avoid difficulty in determining frames of novel predicates. A total of 24 semantic role labels, including core and non-core ones, are used in the annotation. In addition to annotation procedure and quality control, the paper provides analysis on the annotations, including consistency on predicates/arguments, accuracy of annotators, and distribution of labels. Finally, the paper shows cross-domain transfer learning experiment results. The authors use a multi-task learning (MLT) approach, adding an auxiliary task of parsing a previous dataset with a different scheme but sharing the embeddings and some model parameters. On average across the domains, this approach outperforms the baseline by a large margin. They also showed that using contextualized word representation from BERT can further boost the performance, but the gap between MLT and baseline become smaller. \nsummary_of_strengths\n1. The dataset is in a reasonably large scale. It is constructed under solid methodology and the process is documented in detail. Moreover, performance drop in domain shift is a prominent issue for learning-based approaches to NLP, so this multi-domain dataset has great potential to drive future research towards this important direction. \n2. The authors conducted systematic experiments on cross-domain transfer learning. The results illustrate the performance drop in domain shift and demonstrate how much we can get by applying several standard techniques (MTL, contextualized embeddings). These are informative for understanding the status of the task (Chinese SRL) so are worth sharing with the research community. \n3. In addition to methodology and experiments, the paper is also comprehensive in reviewing related datasets and approaches. The whole paper is organized well and is easy to read. It also does a good job explaining some Chinese-specific linguistic phenomena which may have brought challenges to the annotators. \nsummary_of_weaknesses\n1. The paper covers little qualitative aspects of the domains, so it is hard to understand how they differ in linguistic properties. For example, I think it is vague to say that the fantasy novel is more \u201ccanonical\u201d (line 355). Text from a novel may be similar to that from news articles in that sentences tend to be complete and contain fewer omissions, in contrast to product comments which are casually written and may have looser syntactic structures. However, novel text is also very different from news text in that it contains unusual predicates and even imaginary entities as arguments. It seems that the authors are arguing that syntactic factors are more significant in SRL performance, and the experimental results are also consistent with this. Then it would be helpful to show a few examples from each domain to illustrate how they differ structurally. \n2. The proposed dataset uses a new annotation scheme that is different from that of previous datasets, which introduces difficulties of comparison with previous results. While I think the frame-free scheme is justified in this paper, the compatibility with other benchmarks is an important issue that needs to be discussed. It may be possible to, for example, convert frame-based annotations to frame-free ones. I believe this is doable because FrameNet also has the core/non-core sets of argument for each frame. It would also be better if the authors can elaborate more on the relationship between this new scheme and previous ones. Besides eliminating the frame annotation, what are the major changes to the semantic role labels? \ncomments,_suggestions_and_typos\n- In Sec. 3, it is a bit confusing why there is a division of source domain and target domain. Thus, it might be useful to mention explicitly that the dataset is designed for domain transfer experiments.\n-Line 226-238 seem to suggest that the authors selected sentences from raw data of these sources, but line 242-244 say these already have syntactic information. If I understand correctly, the data selected is a subset of Li et al. (2019a)\u2019s dataset. If this is the case, I think this description can be revised, e.g. mentioning Li et al. (2019a) earlier, to make it clear and precise.\n-More information about the annotators would be needed. Are they all native Chinese speakers? Do they have linguistics background?\n-Were pred-wise/arg-wise consistencies used in the construction of existing datasets? I think they are not newly invented. It is useful to know where they come from.\n-In the SRL formulation (Sec. 5), I am not quite sure what is \u201cthe concerned word\u201d. Is it the predicate? Does this formulation cover the task of identifying the predicate(s), or are the predicates given by syntactic parsing results?\n-From Figure 3 it is not clear to me how ZX is the most similar domain to Source. Grouping the bars by domain instead of role might be better (because we can compare the shapes). It may also be helpful to leverage some quantitative measure (e.g. cross entropy).\n-How was the train/dev/test split determined? This should be noted (even if it is simply done randomly). ", "label": [[1209, 1336, "Eval_pos_1"], [1337, 1438, "Jus_pos_2"], [1439, 1547, "Eval_pos_2"], [1801, 1928, "Eval_pos_3"], [1931, 2053, "Jus_pos_4"], [2054, 2107, "Eval_pos_4"], [2109, 2239, "Jus_pos_4"], [2266, 2395, "Eval_neg_1"], [2396, 3129, "Jus_neg_1"], [3133, 3299, "Eval_neg_2"], [3300, 3540, "Jus_neg_2"]]}
{"id": 259, "review": "paper_summary\nThis paper studies efficiently processes long input sequences for conditional language generation with Transformer. In addition to local attentions where each token attends to its neighbors within a limited distance, their proposed efficient efficient attention mechanism dynamically constructs global tokens by aggregating representations for input tokens that are divided into even blocks (Transient Global Attention). The global tokens can be attended by all input tokens and thus enable long-range interaction. They adopt the design of T5 for other components of the Transformer and pre-train the model using the gap sentence prediction objective by Pegasus. The resulting model, LongT5, outperforms existing models designed for long sequences on long document summarization with arXiv, PubMed, BigPatent, and MediaSum. On NaturalQuestion and TriviaQA, their largest model also outperforms existing models. Their additional experiments show that increasing the input length and model size usually lead to improved performance. \nsummary_of_strengths\n- Their pre-trained models are helpful for the research community.\n-Their proposed efficient Transformer model (LongT5) performs better than the model with the original attention mechanism (T5) across different input lengths. It also achieves better performance on different datasets than results reported by existing work. \nsummary_of_weaknesses\nWhile I appreciate their effort in building pre-trained models for long sequences, I do have the following concerns: -The results by other models (Table 2, Table 4 bottom) are taken from the original papers whose experiments might not be conducted under the same computational limit (i.e., GPU/TPU memory). It is likely that the improved performance is due to the authors' better computational resource that allows longer inputs. Also, it is unclear if techniques that can reduce memory usage (e.g., gradient checkpointing) are used by the authors in this paper to reach longer inputs.\n-In Table 4 and Figure 2, due to the lack of T5 models pre-trained with the Gap Sentence Prediction objective, It is unclear the better performance by LongT5 is due to the Transient Global Attention or the Gap Sentence Prediction pre-training objective.\n-There lacks some ablation studies for the Transient Global Attention (e.g., is it necessary to use all global tokens?). \ncomments,_suggestions_and_typos\n- Would be better if the input lengths used by different comparisons are listed. ", "label": [[1069, 1133, "Eval_pos_1"], [1532, 1696, "Eval_neg_3"], [1720, 1999, "Jus_neg_3"], [2001, 2109, "Jus_neg_1"], [2111, 2253, "Eval_neg_1"], [2254, 2323, "Eval_neg_2"], [2323, 2375, "Jus_neg_2"]]}
{"id": 260, "review": "paper_summary\nThis paper focuses on transitioning from chit to task-oriented dialogues. This is important for triggering business opportunities.\nSpecifically, the authors propose a framework to automatically generate dialogues, which start from open-domain social chatting and then gradually transit to task-oriented dialogs.\nThe human evaluation shows that the automatically generated dialogues have a reasonable quality with natural conversation \ufb02ows from a business point of view. \nsummary_of_strengths\n1. Combining Chit-Chat and Task-Oriented Dialogues is a less studied direction. \n2. Generating dialogs automatically can be useful in the industry. \nsummary_of_weaknesses\n1. From the perspective of research, since the released dataset is automatically generated without further manual revision or annotation, it is hard to say that this work proposes a new research task. Furthermore, the two difficult problems (implicit intent detection and transition utterance generation) deserve a more closer study. \n2. From the perspective of models, the proposed method consists of a list of sub-models. The whole process is too trivial. \ncomments,_suggestions_and_typos\nThe authors target an important problem that tries to combine the open-domain dialog and task-oriented dialog. Specifically, they provide a  clear objective for this combination, i.e. triggering the business opportunities. There are at least three basic sub-problems. \n(1) How to capture the interaction between the two types of dialogs? \n(2) How to determine the turning point? \n(3) How to transit properly?\nAll of these sub-problems need to be treated more systematically and carefully.  Other suggestions. \n(1) The introduction is a little wordy and sometimes confusing. For example, in line 78, \"the conversation starts without any speci\ufb01c goal\". This is confusing, since if a user has no specific goal, why does he or she chat with a salesperson? \n(2) This paper is highly related to dialog recommendation. In line 505-512, the authors claim \"such systems is to only make entity recommendations instead of tasks...\". From my point of view, there is considerable overlap between \"make entity recommendations\" and \"transferring from chit-chat to task-oriented dialogues and completing a task the user may want\". Specifically, in Liu 2020, they have also combined chitchat and task-oriented dialog, and achieved chit to task-oriented transition. Then, the only difference lies is complete the task, which is not the focus of this paper. ", "label": [[590, 654, "Eval_pos_1"], [714, 813, "Jus_neg_1"], [814, 877, "Eval_neg_1"], [879, 1011, "Eval_neg_2"], [1015, 1135, "Eval_neg_3"], [1682, 1741, "Eval_neg_4"], [1742, 1920, "Jus_neg_4"]]}
{"id": 261, "review": "paper_summary\nThis paper proposed a new machine learning model fairness metric based on the prediction sensitivity. The paper also showed its theoretical connection to group fairness and individual fairness. \nsummary_of_strengths\n- The paper proposed a new fairness metric accumulated prediction sensitivity to measure the model prediction change with respect to the change of the protected features.\n-The paper showed theoretical relations between accumulated prediction sensitivity and group/individual fairness.\n-The evaluation is conducted on multiple datasets. The author explained the reason to only compare with CF baseline. \nsummary_of_weaknesses\nIt seems that the main focus in the experiment is \"gender-ness\" of words (e.g., he, she, etc.) based on the description and qualitative examples. How about many other cases? For example: -Same words with different senses in different sentences: \" book an Italian trip for me\" vs \"i want an Italian trip book\" -Different verbs/prepositions: \"she takes the flight to new york city\" vs \"she works in the flight to new york city\" \ncomments,_suggestions_and_typos\nPlease see the above reviews. ", "label": []}
{"id": 262, "review": "paper_summary\nThis paper works on multimodal misinformation detection, particularly in the context of \"miscaptioned\" images on Twitter. The authors collect a large multimodal data of 884k tweets on the topics of Climate Change, COVID-19, and Military Vehicles. Next, a misinformation detection method, based on CLIP embeddings is proposed. The effectiveness of this approach is demonstrated on (a) a simulated dataset mismatched image captions and (b) real-world data of misinformation. \nsummary_of_strengths\n- The paper is very well written and structured.\n-The collected dataset could be useful for multimodal research.\n-The authors demonstrate strong gains over the baseline approach of detection using zero-shot CLIP.\n-Experiments are mostly rigorous (for a short paper). \nsummary_of_weaknesses\n- My main criticism is that the \"mismatched\" image caption dataset is artificial and may not capture the kind of misinformation that is posted on platforms like Twitter. For instance, someone posting a fake image of a lockdown at a particular place may not just be about a mismatch between the image and the caption, but may rather require fact-checking, etc. Moreover, the in-the-wild datasets on which a complementary evaluation is conducted are also more about mismatched image-caption and not the real misinformation (lines 142-143). Therefore, the extent to which this dataset can be used for misinformation detection is limited. I would have liked to see this distinction between misinformation and mismatched image captions being clear in the paper.\n- Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.   - Since this is a new dataset, I would have liked to see evaluation of more models (other than just CLIP). But given that it is only a short paper, it is probably not critical (the paper makes enough contributions otherwise) \ncomments,_suggestions_and_typos\n- Table 4 and Table 5: Are the differences statistically significant? ( especially important because the hNews and Twitter datasets are really small) -Lines 229-240: Are the differences between the topics statistically significant? ", "label": [[511, 557, "Eval_pos_1"], [723, 775, "Eval_pos_2"], [801, 968, "Eval_neg_1"], [969, 1336, "Jus_neg_1"], [1337, 1433, "Eval_neg_1"], [1433, 1555, "Eval_neg_1"], [1558, 1652, "Eval_neg_2"], [1653, 1935, "Jus_neg_2"]]}
{"id": 263, "review": "paper_summary\nThis work provides a broad and thorough analysis of how 8 different model families and varying model sizes for a total of 28 models perform on the oLMpics benchmark and the psycholinguistic probing datasets from Ettinger (2020). It finds that all models struggle to resolve compositional questions zero-shot and that attributes such as model size, pretraining objective, etc are not predictive of a model's linguistic capabilities. \nsummary_of_strengths\n- The work is well-motivated and clear -A vast selection of models are investigated \nsummary_of_weaknesses\n- While the findings are interesting, there is little to no qualitative analysis to provide insight into why these effects might occur -I would expect an analysis such as this to have at least 3 runs with varying random seeds per model to give greater confidence in the model's abilities. A growing body of work indicates that models' linguistic abilities can vary considerably even across initialisations -The exploration and prompt design to adapt the GPT models to the tasks at hand is quite limited \ncomments,_suggestions_and_typos\nN/A ", "label": [[470, 507, "Eval_pos_1"], [613, 709, "Eval_neg_1"], [711, 862, "Eval_neg_2"], [864, 981, "Jus_neg_2"], [982, 1078, "Eval_neg_3"]]}
{"id": 264, "review": "paper_summary\nThis paper tries to simplify the process of prompt engineering for few-shot learning. Specifically, this paper tries to resolve two specific aspects of prompt engineering - 1. How to chose the right prompt? \n2. How to ensure efficient deployment for prompt tuned networks Choosing the right prompt? \nPrompt engineering while successful, requires significant tuning to identify the right prompt. While AutoPrompt methods exist, they start showing benefits only at scale. This paper shows that the complexity around prompt choices can be easily managed by finetuning on Null prompt Efficient deployment? \nFinetuning on null prompt leads to creation of a new model for every task. The authors further use BitFit to reduce the per task parameters for efficient deployment. \nsummary_of_strengths\nThe results in the paper show some interesting insights that have not been covered by prior work -  1. Show that parameter efficient fine-tuning techniques like BitFit and Adaptors can work for prompt-tuning in few-shot regime. For example, the original BitFit paper uses the entire training dataset for finetuning. This paper uses 2*K where K=4,8,16 examples from each downstrean tasks for prompt-tuning. Showing BitFit and other parameter efficient techniques work in such few-shot scenarios is an interesting experiment that the community will find useful 2. The ability of null prompts with finetuning to simplify prompt engineering for LM to the scale of 300M parameters will also be a technique of interest of various practitioners in this domain \nsummary_of_weaknesses\nKey weakness - 1. Results covered in existing literature - The key insight of stability after prompt fine-tuning is already covered in prior work (https://aclanthology.org/2021.naacl-main.208.pdf). The authors acknowledge this work and call it simultaneous publication. However, the paper first appeared in Mar 2021 on arxiv and was published at NAACL in Jun 2021. Given that ACL guidelines sets contemporaneous publication limit to 3 months from submission (https://www.aclweb.org/adminwiki/index.php?title=ACL_Policies_for_Submission,_Review_and_Citation), I am not sure if their claims of simultaneous publication is still valid.\n2. I think the writing of the paper can be improved. The paper does not lay down its claims concretely or can be confusing at times. Some examples below -       1. Given that the results in this paper may not scale to larger LM, I think the paper should mention the LM targeted explicitly in Abstract. As a reader, I was looking to understand how this paper compares to AutoPrompt techniques, given that they are also driven by the motivation of simplifying prompt engineering. While the authors compare with this work, they dont talk about it till section 5 (lines 86-88 dont give enough context). \n     2. It can be unclear at times as to what they mean by standard fine-tuning - finetuning all parameters across entire dataset or fine-tuning all parameters for a sampled dataset. This can create confusion while reading the paper 3. Unclear if the results are overfitting to the dataset sampled- In a few shot scenario, its very easy to overfit to the examples sampled. How do these results vary when we sample a different set of \"K\" samples from the dataset? How does it change with a skewed label distribution in the sampled dataset? Experiments around these questions help understand the generalizability of the technique.\n4. The authors talk about null prompt based fine-tuning being more robust (line 91), but do not define robustness in their paper nor show results justifying it.\nQuestions for the authors: 1. What is the value of k in Table 2? \n2. In line 84, when you say \"better few-shot accuracy than standard fine-tuning\" did you mean fine-tuning over the entire dataset or fine-tuning in the few shot scenario? \ncomments,_suggestions_and_typos\nNA ", "label": [[805, 901, "Eval_pos_1"], [908, 1558, "Jus_pos_1"], [1598, 1637, "Eval_neg_1"], [1640, 1727, "Jus_neg_1"], [2216, 2266, "Eval_neg_2"], [2267, 3669, "Jus_neg_2"]]}
{"id": 265, "review": "paper_summary\nIn this paper, the author proposed XLM-E, a cross-lingual ELECTRA-style pretrained LM, which merges the benefits of the XLM and Electra model. Also, the authors introduce a gated relative position bias which the author claim is helpful to the pretrained model. In the experiments, they show their method achieves better results on most cross-lingual transferability tasks and show XLM-E requires less computation resource than previous methods. Moreover, they compare the cross-lingual alignment ability of XLM-E to several baseline models.\nAlthough the paper is complete and well written, the proposed method is not that exciting. Although I gave him 3.5 points, if I could, I would give him a score between 3-3.5. ( maybe 3.25) \nsummary_of_strengths\n1. The paper is well written and easy to follow. \n2. The experiments are complete but missing some baseline. \n3. The proposed method is much efficient than previous methods. \nsummary_of_weaknesses\n1. It is well known that ELECTRA-style is efficient and the TLM-based model is helpful to the cross-lingual representation. It is no surprise that combining both methods would work. \n2. The TLM-based pre-trained method required translation pairs. I understand most of the baseline required translation pairs, too. However, instead of using translation pairs, I would like to see more research try to achieve better cross-lingual transferability without using translation pairs. \n3. There is no comparison between the usual relative position bias and gated relative position bias. \ncomments,_suggestions_and_typos\n1. missing baseline in Fig.3 and Fig.4: I would like to see the comparison including InfoXLM an XLM-Align. \n2. missing baseline in table 5, 6: I would like to see the comparison including InfoXLM and XLM-Align. \n3. I'm interested in the difference between the results of using usual relative position bias and gated relative position bias. ", "label": [[555, 744, "Major_claim"], [769, 815, "Eval_pos_1"], [819, 875, "Eval_pos_2"], [879, 940, "Eval_pos_3"], [966, 1085, "Jus_neg_1"], [1086, 1145, "Eval_neg_1"]]}
{"id": 266, "review": "paper_summary\nThe present paper investigates the Word-in-Context task in a few-shot setting. Utilising a prompt-based approach, rather than optimising models to predict the expected label (i.e. whether two word senses are different), this paper proposes to predict (embeddings of) words synonymous to those in question and compare the distances of these embeddings. Models optimised in this way reach the performance of models finetuned on the whole training set and the approach is argued to generalise to some other tasks. \nsummary_of_strengths\nThe method is surprisingly simple, the results are convincing as they significantly improve upon the state-of-the-art on the WiC task in the few-shot setting. The paper shows that the methodology is applicable to some other few-shot tasks as well. Claims are substantiated and backed by evidence. \nsummary_of_weaknesses\nThe main weakness of the paper is that it's very terse, partly due to the 4-page limit of a short submission. This leads to potentially important information being omitted, see detailed comments below. \nSome of the mentioned points could be easily alleviated by utilising the additional page that is provided upon acceptance in a venue. \ncomments,_suggestions_and_typos\nThe following are points where i would like to see further clarifications: (NB: I do not have access to the forum/comments of the previous submission so some of my comments might have been addressed earlier.)\n- Why are SST-2 and SICK-E chosen as representative tasks to show that the proposed method generalises to other few-shot settings? Other papers seem to go for the full SuperGLUE suite. Similarly, for those tasks, why is Autoprompt chosen as the only reference approach to compare against? Admittedly, the difference in performance to other approaches (at least for the SST-2 task) appears to not differ too much.  - It is great to see that confidence scores were reported for the obtained results, but how exactly are they calculated?\n- The high-level description helps to understand the approach intuitively, but a more detailed (e.g. mathematical) formulation, for example in the appendix, would be helpful as well. Similarly, the figure is supposed to help to understand the problem better, but I find it confusing in two ways: First, the figure is too abstract for me. Maybe having more text labels would help. Second, depicting sentiment analysis, it does not align well with the main contribution of the paper, improvements on the WiC task. Maybe reworking the figure to depict the WiC task would help with both problems.\n- Qualitative error analysis in the Appendix is great, but the manuscript so far lacks a more detailed analysis of the results. One could wonder, for the WiC task, are the errors always due to models predicting \"matched\" for \"not matched\" GT? Is this similar to other approaches? For example, a by-class accuracy breakdown could answer some of these questions. ", "label": [[547, 608, "Eval_pos_1"], [609, 705, "Jus_pos_1"], [867, 922, "Eval_neg_1"], [977, 1039, "Eval_neg_1"], [1237, 2935, "Jus_neg_1"]]}
{"id": 267, "review": "paper_summary\nThe paper describes an approach for improving  Math Word Problem (MWP) solving. \nDue to the non-distinction of MWP patterns, current methods, which are based on neural networks, perform poorly. The authors  propose a contrastive learning method to assist  the model to correctly separate confused patterns. The proposed methods outperform  two baselines in both monolingual and multilingual settings. \nsummary_of_strengths\n1) The paper is written well and is a pleasure to read (although some details are missing) 2) The paper presents detailed results, which outperform the two baselines in both monolingual and multilingual settings. \n3)  The coupling of both semantic encoding (Bert encoder) and contrastive learning is  a key contribution and strength of the paper, in my opinion. \nsummary_of_weaknesses\n1) Lack of interpretability: There could be more of a discussion of why \"semantic encoder understands semantics in lower layers and gathers the prototype equations in higher layers\".  This aspect could be discussed in more detail. consequently, the paper leaves many questions open while not giving definite answers about others. \n2) It will be interesting to how this method scales with respect to more complex mathematical questions. \n3) The authors have not motivated their choice of (Bert ) as the sole semantic encoder in their experimental settings. There are battery of models to chose from. \ncomments,_suggestions_and_typos\nThe paper could be further improved by including more discussion about interpretability as it difficult to explain the model's  behavior. ", "label": [[440, 491, "Eval_pos_1"], [654, 799, "Eval_pos_2"], [825, 850, "Eval_neg_1"], [851, 1152, "Jus_neg_1"], [1262, 1377, "Eval_neg_2"], [1454, 1591, "Eval_neg_3"]]}
{"id": 268, "review": "paper_summary\nThis paper proposes NoisyTune, which is a method to add noise using the standard deviation of matrix-wise parameters during fine-tuning pretrained language models. The main motivation of using NoisyTune is in line with other work (e.g., by Chen et al. 2020 and Lee et al. 2020) on avoiding large deviation of parameters from the pretrained model to avoid overfitting during finetuning. Experimental results on XTREME and GLEU benchmark datasets suggest that NoisyTune gives small improvement consistently on various settings. \nsummary_of_strengths\n- Simplicity of the method: A matrix-wise standard deviation of parameters is only necessary to use this method -Comparison with a simple baseline of adding a global noise -Parameter matrix-wise distribution is useful for adding noise -Experimental results with four different pretrained language models \nsummary_of_weaknesses\n- The performance gain by using NoisyTune on XTREME and GLEU is very small (less than 1 point in accuracy or F1 score).  -No standard deviation scores are reported, and the comparison of the performance gain vs. standard deviation is not reported.\n-The methods compared in the experiments are only with using global noise. \ncomments,_suggestions_and_typos\n### Comments -I suggest further experimenting with various hyperparameters and would the addition of NoisyTune be complimentary to hyperparameter tuning.\n-Since the motivation of this paper is avoiding overfitting during fine-tuning, I would be very curious to see NoisyTune in few-shot settings, where overfitting is more of an issue.\n-[Minor] For Figure 4, preferable to have legends outside the plot.\n### Questions to the Authors -Are different hyperparameters (e.g., drop out rate) explored? Would it be possible that tuning one of the hyperparameters leads to larger gains than using NoisyTune?\n-What is the standard deviation across multiple runs for e.g., Table 1 and Figure 2? ", "label": [[891, 963, "Eval_neg_1"], [964, 1008, "Jus_neg_1"], [1011, 1136, "Eval_neg_2"], [1400, 1477, "Jus_neg_3"], [1479, 1580, "Eval_neg_3"]]}
{"id": 269, "review": "paper_summary\nThis paper compares three entropy estimators with the simplest plug in entropy estimator on linguistic data of CELEX corpora and synthetic data.  The authors compared the quality of estimators on unigram distribution, information study on the association between gendered inanimate nouns and their modifying adjectives, and finally with similarity between grammatical gender partitions between languages. \nAuthors conclude that other estimators are more reliable than the plugin estimator. \nsummary_of_strengths\nThis paper investigates some entropy estimators whose application to linguistic data have not appeared in the literature. \nsummary_of_weaknesses\n1. References Although, the authors claim that this is the first literature, there have been abundant application of entropy estimates on linguistic data. Since Brown's Computational Linguistics paper 1983, some related papers have appeared in ACL milieu, but the main debate appeared elsewhere, especially, discussed in the journal of MDPI Entropy, some were even archived in the book forms. Those include multiple articles about the bias of plugin entropy, therefore, authors are recommended to go through them.  Authors are requested to correctly situate this work within the large history of studies on bias of plug in entropy, which is no more than one kind of work to apply entropy estimate to linguistic data.\n2. Soundness of this paper The authors claim at the bottom of p.2, about the plugin estimator,     by Jensen's inequality, this estimator is, in expectation,     a lower bound on the true entropy. \nHowever, by information theory, any entropy estimator is destined to be no more than an upper bound of the true entropy. In Appendix A, the author draws conclusion by mixing the true entropy and the estimated entropy, where plugin entropy is no more than an estimated entropy. Please see the definition of cross entropy.\n3. Experiments This paper only reports the partial and summarized output for each experiment. For example, only the \"bests\" were mentioned, but authors are recommended to show the concrete performances of each estimator.  To investigate the quality of entropy estimates, much more fundamental analysis are conducted within the previous literature, before going on to concrete application such as described in 4.2 and 4.3.  Such includes tests on other corpora than CELEX, how bias depends on the length of corpora etc. For tips, please see the previous litterature. \ncomments,_suggestions_and_typos\np. 3 first paragraph, \"should\" replicates just below formula (7). ", "label": [[674, 825, "Eval_neg_1"], [826, 1387, "Jus_neg_1"], [1391, 1414, "Eval_neg_2"], [1415, 1906, "Jus_neg_2"], [1909, 1921, "Eval_neg_3"], [1921, 2473, "Jus_neg_3"]]}
{"id": 271, "review": "paper_summary\nThis paper proposes a trainable subgraph retriever (SR) that acts as a plug-in module to enhance subgraph-oriented knowledge base question answering (KBQA) frameworks. The authors decouple the subgraph retriever from the reasoner based on the probabilistic formalization of KBQA. The subgraph retriever constructs the subgraph by expanding paths from topic entities and merging the trees. The authors explore weak supervision and distant supervision for training the subgraph retriever and also propose an end-to-end training method to jointly learn the retriever and reasoner. Extensive experiments show that the proposed method shows significant improvement over existing subgraph-oriented KBQA models. \nsummary_of_strengths\n1. \tThe proposed method is very novel. The decoupled retriever and reasoner are substantially different from existing works. Different training strategies and an end-to-end variant are also studied. \n2. \tThe proposed method shows significant improvement over previous methods. \n3. \tThe experiments are very comprehensive and solid. \n4. \tThe paper is well-written and easy to follow. \n5. \tCode is provided and well-organized. \nsummary_of_weaknesses\n1. \tWhile the proposed method is more effective than the previous methods, the efficiency (e.g. training and inference speed) is not compared. \ncomments,_suggestions_and_typos\nQuestion: 1. \tFor the end-to-end variant, are the retrieving and the reasoning also intertwined as in previous works? Any intuition on why the proposed method is better?\nTypos and Formatting Issues: 1. \tLine 53: the publication year is missing from the citation for WebQSP. \n2. \tThe caption of the table should be put on top. \n3. \tLine 435: a space is missing between \u201cstatement.\u201d and \u201cSR\u201d. \n4. \tLine 576: proves -> prove. ", "label": [[745, 779, "Eval_pos_1"], [780, 940, "Jus_pos_1"], [945, 1018, "Eval_pos_3"], [1023, 1073, "Eval_pos_4"], [1078, 1123, "Eval_pos_2"], [1129, 1166, "Eval_pos_5"], [1193, 1332, "Eval_neg_1"]]}
{"id": 272, "review": "paper_summary\nThe general motivation of this paper is to provide tools that analyze pairs of review and rebuttal in the peer-review process of scientific work and assess Area Chairs in making sense of this discussion to reach better-informed decisions of acceptance or rejection. For this goal, the main contribution of this paper is a dataset of peer-reviews annotated with discourse labels and relations between the rebuttal and review pairs. The proposed discourse scheme attempts to unify several other discourse labels proposed in previous works. In my opinion, this scheme is complete and well organized on different levels of granularity. Although limited, the paper additionally presents some descriptive analyses of the collected annotations. \nsummary_of_strengths\n- The task is relevant to the community, and efforts towards easing the life of area chairs are appreciated.\n- The constructed discourse labels scheme is nicely detailed and complete.\n- The annotated dataset could benefit future research on discourse analysis of the peer review process \nsummary_of_weaknesses\n- The analysis section is limited and confusing in some parts: \t- Section 4.1 describes only the annotation results, without any further discussion of the implications.\n\t- In Section 4.2, based on Figure 2, the authors conclude that rebuttal sentences are not aligned with review sentences. However, what I see is completely different. The majority of review-rebuttal pairs happen to have a positive correlation of more than 50% unless I am missing something here!\n\t- Section 4.3 wasn't clear for me. I didn't understand how the figure supported the claim that authors often interpret reviews in a way that supports their argumentative goals. \ncomments,_suggestions_and_typos\n- There is missing work of Cheng et al. 2021 (Argument Pair Extraction via Attention guided Multi-Layer Multi-Cross Encoding) - It wasn't clear to me why the authors needed to build their own software for the annotation task. I think the task is similar to any other discourse labeling and relation identification task, and there are a lot of open-source tools that can be used for such annotations.\n- In the introduction, the authors mention something about annotators' training and calibration process, but this wasn't elaborated upon later. I guess some details on this process should be presented in the paper.\n- In Table 6, the header contains abbreviations. It is good to point out what do they mean. ", "label": [[776, 882, "Eval_pos_1"], [885, 957, "Eval_pos_2"], [960, 1061, "Eval_pos_3"], [1087, 1147, "Eval_neg_1"], [1149, 1727, "Jus_neg_1"], [1887, 1985, "Eval_neg_2"], [1986, 2159, "Jus_neg_2"], [2162, 2303, "Eval_neg_3"]]}
{"id": 274, "review": "paper_summary\nThis paper proposes a moderate-sized vision-and-language model for low-resource learning of several vision-and-language tasks. Specifically, the proposed methods includes first pre-training a sequence to sequence transformer based model with prefix language modeling and masked language modeling. Then, the pre-trained model can be adapted to several vision-and-language tasks (e.g., VQAv2) in a zero-shot or few-shot manner with hand-craft prompts. Empirical results show that the proposed methods could achieve comparable results to PICa on VQA tasks. This paper also analyzes the effect of using different hand-craft prompts/noisy prompts for zero-shot/few-shot performance and the effect of two pre-training objectives on different tasks. \nsummary_of_strengths\n1. The proposed method is effective regarding zero-shot and few-shot performance, achieving comparable performance to larger vision-and-language models (i.e., PICa) on VQA tasks. On most tasks, the proposed method gets large improvement compared with VL-T5, which has almost the same architecture (same parameter number). \n2. Analysis shows that different prompt design will affect the zero-shot performance a lot. \n3. Interesting analysis demonstrating the effect of different pre-training objectives on different tasks, suggesting that the zero-shot and few-shot performance on downstream tasks can get better if including similar pre-training objective. \nsummary_of_weaknesses\n1. The few-shot performance is evaluated over 5 randomly sampled different training and dev splits, it's better to also report the performance variance across 5 splits. If the variance is high, it might be better to include an analysis of how to pick training/dev data for few-shot learning / what kind of data will be more helpful for the performance. \n2. The proposed method gets large improvement compared with VL-T5, which has almost the same architecture. I'm also interested in how much of the improvement comes from different pre-training objective and how much of the improvement comes from better hand-craft prompt design (if we consider VL-T5 as using dataset-specific prefixes as hand-craft prompt). \ncomments,_suggestions_and_typos\nTypos: Line 166: \"or significantly improves\", no \"or\"?\nSuggestions: Related paper: Learning to Prompt for Vision-Language Models. Zhou et al. ", "label": [[1198, 1436, "Eval_pos_1"], [1462, 1627, "Eval_neg_1"], [1628, 1812, "Jus_neg_1"], [1919, 2089, "Eval_neg_2"]]}
{"id": 275, "review": "paper_summary\nThis paper focuses on the task of Conversational Question Answering (CQA), which envisions a back-and-forth question answering scenario with an information-seeking goal. The work shows how the most common way of automatically evaluating CQA systems (on a popular instantiation of the task with one specific English dataset) is empirically different from how humans would evaluate the systems in an actual real-world scenario. The models use a dataset of human-human conversations and evaluation is commonly done by using the 'gold-standard' conversational history. But the human-machine conversations, which would result when the models are deployed or used, represent a fundamentally different distribution from human-human conversations. This means the current evaluation paradigm does not accurately capture the use-case and therefore, tracking modeling improvement with that evaluation paradigm will not be meaningful. The work's primary contribution is showing this discrepancy by conducting a human evaluation study where CQA systems get to interact with humans - the result human-machine conversational QA dataset is also a contribution. Further analysis shows how automated evaluation differs from human evaluation, and what human evaluation tells about current CQA modeling strategies including insights for future work. The work also puts forth a new automated evaluation paradigm that mirrors human evaluation results more closely than the existing evaluation strategy of using the gold answers. \nsummary_of_strengths\n- The paper highlights a fundamental discrepancy between the dominant automated evaluation paradigm and human judgment of CQA models when situated in the real-world scenario - this exposition is meaningful for the CQA community in order to take stock of whether modeling improvements can be claimed with an evaluation strategy that seems to be quite broken.  - Further, the exposition of this problem in automated evaluation is carried out rigorously by conducting a user study where humans interact with different CQA models - and the resulting human-machine dataset is another important contribution. The work does also shows the different ways in which the problem with the current automated evaluation paradigm manifests itself.  - The difference in the distribution of human-human and human-machine conversations is made clear, and what that means for current automated model evaluation (such as models being ranked differently in terms of automated and human evaluation) is explained well.  - The insights derived for different modeling strategies and how they fare on human evaluation can help better understand differences and relative strengths and weaknesses of different CQA models. These insights can inform future work on CQA modeling and some solid recommendations are indeed put forth in this work.  - The paper is written clearly and organized well. The main ideas and contributions, and their importance, are presented early and quite clearly. \nsummary_of_weaknesses\n1. The case made for adopting the proposed strategy for a new automated evaluation paradigm - auto-rewrite (where the questions that are not valid due to a coreference resolution failure in terms of the previous answer get their entity replaced to be made consistent with the gold conversational history) - seems weak. While the proposed strategy does seem to do better in terms of being closer to how humans evaluated the 4 models (all in the context of one specific English dataset), it is not clear how the proposed strategy -      a) does better than the previously proposed strategy of using model-predicted history (auto-pred). Looking at the comparison results for different evaluations - in terms of table 1, there definitely does not seem to be much difference between the two strategies (auto-rewrite and auto-pred). In fig 5, for some (2/6) pairs, the pred-history strategy has higher agreement than the proposed auto-rewrite strategy while they are all at the same agreement for 1/6 pairs.      b) gets to the fundamental problem with automated evaluation raised in the paper, which is that \"when placed in realistic settings, the models never have access to the ground truth (gold answers) and are only exposed to the conversational history and the passage.\" The proposed strategy seems to need gold answers as well, which is incompatible with the real-world use case. The previously proposed auto-pred strategy, however, uses only the questions and the model's own predictions to form the conversational history - which seems to be more compatible with the real-world use case.  In summary, it is not clear why the proposed new way of automatically evaluating CQA systems is better or should be adopted as opposed to the previously proposed automated evaluation method of using a model's predictions as the conversational history (auto-pred), and the comparison between the results for these two automated strategies seems to be a missing exploration and discussion. \ncomments,_suggestions_and_typos\nQuestions to the authors (which also act as suggestions):  Q1. - Line 151: \"four representative CQA models\" - what does representative mean here? representative in what sense? In terms of types or architectures of models? This needs clarification and takes on importance because the discrepancy, in terms of how models get evaluated on human vs automated evaluation, depends on these four models in a sense.  Q2. Line 196: \"We noticed that the annotators are biased when evaluating the correctness of answers\" - are any statistics on this available?  Q3. Section 3.1: For Mechanical Turk crowdsourcing work, what was the compensation rate for the annotators? This should be mentioned, if not in the main text, then add to appendix and point to it in the main text. Also, following the work in Card et al (2020) (\"With Little Power Comes Great Responsibility.\") - were there any steps taken to ensure the human annotation collection study was appropriately powered? ( If not, consider noting or discussing this somewhere in the paper as it helps with understanding the validity of human experiments) Q4. Lines 264-265: \"The gap between HAM and ExCorD is significant in Auto-Gold\" - how is significance measured here?\nQ5. Lines 360-364: \"We determine whether e\u2217_j = e_j by checking if F1(s\u2217_{j,1}, s_{j,1}) > 0 ....  .... as long as their first mentions have word overlap.\" Two questions here -      5a. It is not clear why word overlap was used and not just an exact match here? What about cases where there is some word overlap but the two entities are indeed different, and therefore, the question is invalid (in terms of coreference resolution) but deemed valid? \n    5b. How accurate is this invalid question detection strategy? In case this has not already been measured, perhaps a sample of instances where predicted history invalidates questions via unresolved coreference (marked by humans) can be used to then detect if the automated method catches these instances accurately. Having some idea of how well invalid question detection happens is needed to get a sense of if or how many of the invalid questions will get rewritten.  Comments, suggestions, typos:  - Line 031: \"has the promise to revolutionize\" - this should be substantiated further, seems quite vague.  -Line 048: \"extremely competitive performance of\" - what is 'performance' for these systems? ideally be specific since, at this point in the paper, we do not know what is being measured, and 'extremely competitive' is also quite vague.  -The abstract is written well and invokes intrigue early - could potentially be made even better if, for \"evaluating with gold answers is inconsistent with human evaluation\" - an example of the inconsistency, such as models get ranked differently is also given there.  -Line 033: \"With recent development of large-scale datasets\" -> the* recent development, but more importantly - which languages are these datasets in? And for this overall work on CQA, the language which is focused on should be mentioned early on in the introduction and ideally in the abstract itself.  -Line 147: \"more modeling work has been done than in free-form question answering\" - potential typo, maybe it should be \"maybe more modeling work has been done 'in that'\" - where that refers to extractive QA?\n-Line 222: \"In total, we collected 1,446 human-machine con- versations and 15,059 question-answer pairs\" - suggestion: It could be reasserted here that this dataset will be released as this collection of conversations is an important resource and contribution and does not appear to have been highlighted as much as it could.  -Figure 2: It is a bit unintuitive and confusing to see the two y-axes with different ranges and interpret what it means for the different model evaluations. Can the same ranges on the y-axes be used at least even if the two metrics are different? Perhaps the F1 can use the same range as Accuracy - it would mean much smaller gold bars but hopefully, still get the point across without trying to keep two different ranges in our head? Still, the two measures are different - consider making two side-by-side plots instead if that is feasible instead of both evaluations represented in the same chart.  -Lines 250-252: \"the absolute numbers of human evaluation are much higher than those of automatic evaluations\" - saying this seems a bit suspect - what does absolute accuracy numbers being higher than F1 scores mean? They are two different metrics altogether and should probably not be compared in this manner. Dropping this, the other implications still hold well and get the point across - the different ranking of certain models, and Auto-Gold conveying a gap between two models where Human Eval does not.  -Line 348: \"background 4, S\u2217 - latex styling suggestion, add footnote marker only right after the punctuation for that renders better with latex, so - \"background,\\footnote{} ...\" in latex.\n-Footnote 4: 'empirically helpful' - should have a cite or something to back that there.  -Related Work section: a suggestion that could make this section but perhaps also the broader work stronger and more interesting to a broader audience is making the connection to how this work fits with other work looking at different NLP tasks that looks at failures of the popular automated evaluation strategy or metrics failing to capture or differing significantly from how humans would evaluate systems in a real-world setting. ", "label": [[1719, 1770, "Eval_pos_1"], [1770, 1900, "Jus_pos_1"], [2075, 2145, "Eval_pos_2"], [2279, 2538, "Eval_pos_3"], [2542, 2856, "Eval_pos_4"], [2860, 3004, "Eval_pos_5"], [3030, 3345, "Eval_neg_1"], [3562, 5008, "Jus_neg_1"]]}
{"id": 276, "review": "paper_summary\nThis paper focus on table pretraining via spreadsheet formula to realize numerical reasoning. Concretely, it proposes two effective pretraining tasks: numerical reference prediction and numerical calculation prediction. At the same time, formula mask language model is also employed to get better representation of the spreadsheet formula. Experimental results show that the numerical-reasoning-aware pretraining make the TUTA (tree-based transformer for table pretraining) outperforms SOTA methods on formula prediction, question answering and cell \nsummary_of_strengths\n1. The paper is well-written and easy to follow.\n2. The numerical-reasoning-aware pretraining outperforms SOTA by large margin in three representative tasks.\n3. Analysis is very detailed to further clarify the effectiveness of pretraining. \nsummary_of_weaknesses\nMost of the experiments focus on spreadsheet related task including formula prediction (NCP task helps) and cell type classification (NRP tasks helps). For Table QA task,  it would be better to conduct experiments on more comprehensive datasets like WikiTableQuestion (TableQA), TabFact (Table-base fact verification) following TaPEx. \ncomments,_suggestions_and_typos\nQuestions: 1. For table-text reasoning, why NRP task + formula-based prompt improve this reasoning ability? \n2. In Table 2, what about the result of  SpreadsheetCoder under 20% train set? \n3. L174, is there an [SEP] exists between columns, like the delimiter among value cells?\nSuggestions: 1. A little introduction of 'Formula', 'Sketch', 'Range', 'table hierarchies' in session2 will lead to better understanding, which are often used in the latter part. \n2. The order of terms in L288-L290 should be aligned with Figure 2. \n3. In Figure 2, It would be better to replace [ RANDOM \u2026\u2026 TOKENS \u2026\u2026 from \u2026\u2026 VOCAB ] with an specific example. \n4. While the error analysis is conducted in Appendix, there is no case study to make the improvement more concretely.  Typos: 1. Table#1 in Figure 1: (C3-B3)/B3 -> (D3-C3)/C3 ", "label": [[589, 634, "Eval_pos_1"], [638, 743, "Eval_pos_2"], [747, 825, "Eval_pos_3"]]}
{"id": 277, "review": "paper_summary\nThis paper proposed how to adapt the graph-based semantic parser PERIN to the task of structured sentiment analysis, aiming to analyze a polar expression, an optional holder, an optional sentiment target, and sentiment polarity. Based on PERIN, the weighted bipartite graph between all queries and nodes is applied to indicate the prior ordering of the graph. The proposed method advances the-state-of-the art method on 4 out of 5 standard benchmark sets. \nsummary_of_strengths\n1) This work applies the graph-based semantic parser PERIN for structured sentiment analysis with task-specific adaption, which refines the parallel queries process and gold nodes mapping. \n2) The paper is clearly written and well structured. Given the page limitation of a short paper, this paper still provides abundant information. \n3) The results of performance evaluation are quite convincing for the comparison with state-of-the-art methods and complete experimental settings. \nsummary_of_weaknesses\n1) Lacking innovation is the main weakness of this paper, though proven the usefulness of PERIN and the refinement. \n2) Due to the page limitation, this work lacks detailed analysis on Node-centric encoding, Labeled-edge encoding and Opinion-tuple encoding, which are the essential design towards SSA. \ncomments,_suggestions_and_typos\n1) In Figure 2, please indicate STEP 1,2,3,4 in the corresponding places in the diagram for better illustration. \n2) It would be better to present a few case studies for readers. ", "label": [[684, 734, "Eval_pos_1"], [735, 827, "Jus_pos_1"], [831, 889, "Eval_pos_2"], [1001, 1055, "Eval_neg_1"], [1145, 1300, "Eval_neg_2"]]}
{"id": 278, "review": "paper_summary\nThis paper proposes an ELECTRA-style tasks for the purpose of cross-lingual pretraining. The resulting pretrained models are competitive against previous  pretrained models. Extensive experiments were conducted to show the effectiveness and efficiency of the proposed approach. \nsummary_of_strengths\nFrom the experiments we can see that the resulting pretrained models have some advantages. \nsummary_of_weaknesses\n1. The proposed approach to pretraining has limited novelty since it more or less just follows the strategies used in ELECTRA. \n2. It is not clear whether baselines participating in the comparison are built on the same datasets that are used to build XLM-E. \ncomments,_suggestions_and_typos\n1. From the results in Table 1, we can see that XLM-E lags behind baselines in \"Structured Prediction\" tasks while outperforms baselines in  other tasks. Any possible reason for such a phenomenon?\nSome typos and grammatical errors. \n1. \" A detailed efficiency analysis in presented in Section 4.5\". Here \"in\" --> \"is\" 2. \" XLM-E substantially outperform XML on both tasks\". Here \"outperform\" --> \"outperforms\" 3. \"... using parallel corpus\" --> \"using parallel corpora\" ", "label": [[314, 405, "Eval_pos_1"], [431, 487, "Eval_neg_1"], [488, 555, "Jus_neg_1"], [559, 686, "Eval_neg_2"]]}
{"id": 279, "review": "paper_summary\nThis paper advocates the importance of cultural awareness when designing an NLP solution. The authors of this paper identify four dimensions of culture associated with the NLP systems, i.e., linguistic form, common ground, aboutness, and values. Then, the authors propose the strategies to work on cross-cultural NLP, which cover three areas, namely data collection, model training, and translation. \nsummary_of_strengths\nSeveral reasons to accept this paper in ACL special theme track: + Understanding the challenges of cross-cultural and multicultural NLP can improve the NLP systems to serve the users in a more inclusive world. This paper raises the issues, surveys existing strategies, and highlights directions for the future development of cross-cultural NLP. \n+ When discussing the four cultural factors, the authors cite the various examples from different culture around the world (not Western centric only). For example: Latin America, Middle East (section 2); India (section 4); and Africa (section 6).    + Most recommendations, regarding data curation, annotation, translation, and model bulding, are sensible and formulated in systematic way. \nsummary_of_weaknesses\n- The author describes that the four components of culture are represented orthogonally (Figure 1). In fact, most of these components depend on each other. \ncomments,_suggestions_and_typos\nLine 338, \"..towards shifting the current culture into a hopefully more equitable one\". What is meant by \"current culture\"?\nLine 642, \"..more diverse cross-cultural representation\". What is the concrete implementation of this idea when building an NLP model?\nIn which NLP tasks is cultural bias a prominent issue and needs to be prioritized to address? ", "label": [[436, 499, "Major_claim"], [503, 645, "Eval_pos_1"], [646, 781, "Jus_pos_1"], [784, 904, "Eval_pos_2"], [933, 1029, "Jus_pos_2"], [1034, 1172, "Eval_pos_3"]]}
{"id": 280, "review": "paper_summary\nThis paper presents an extension of the CrowS-pairs dataset (Nangia et al., 2016) for French. CrowS-pairs consists of about ~1500 English sentence pairs, where one sentence of each pair shows a certain type of social bias, and the other is an unbiased version of it. In the extension, each sentence has been translated by two authors of the paper, taking care of adjusting the bias properly to French contexts/culture. Moreover, 212 new French sentence pairs have been added in a crowdsourcing process, in which also the translations and the bias types were checked.\nIn experiments on the dataset, the paper analyzes how much bias different BERT-based language models show for French (in comparison to English). Several insights into the problems and solutions of translating biased sentences are given afterwards. \nsummary_of_strengths\n1. Social bias has largely been studied for English only so far. The dataset contributes to enabling multilingual social bias research, and it seems to be constructed thoroughly in different respects. I have little doubt that the data is of reasonable quality. The dataset is promised to be made available.\n2. The paper's discussion convinced me of the quality of the translation, even though I was skeptical at first whether translation can work here. The examples and statistics in Tables 1 and 2, along with the descriptions in the text, emphasize the effort that was put into obtaining sentences that make sense for French (such as adjusting names, culture-related concepts, etc.).\n3. The experimental results give useful insights into the behavior of the language models with respect to social bias in French text. With my given but limited knowledge of French, I was able to follow the discussion well, also because the examples are well-chosen and not too complex. Without any knowledge of French, some details may be hard to assess for readers, but I don't think this anyhow avoidable for such a topic. \nsummary_of_weaknesses\n1. The paper raises two hypotheses in lines 078-086 about multilinguality and country/language-specific bias. While I don't think the hypotheses are phrased optimally (could they be tested as given?), their underlying ideas are valuable. However, the paper actually does not really study these hypotheses (nor are they even mentioned/discussed again). I found this not only misleading, but I would have also liked the paper to go deeper into the respective topics, at least to some extent.  2. It seemed a little disappointing to me that the 212 new pairs have _not_ been translated to English (if I'm not mistaken). To really make this dataset a bilingual resource, it would be good to have all pairs in both languages. In the given way, it seems that ultimately only the French version was of interest to the study - unlike it is claimed initially.\n3. Almost no information about the reliability of the translations and the annotations is given (except for the result of the translation checking in line 285), which seems unsatisfying to me. To assess the translations, more information about the language/translation expertise of the authors would be helpful (I don't think this violates anonymity). For the annotations, I would expect some measure of inter-annotator agreement.\n4. The metrics in Tables 4 and 5 need explanation, in order to make the paper self-contained. Without going to the original paper on CrowS-pairs, the values are barely understandable. Also, information on the values ranges should be given as well as whether higher or lower values are better. \ncomments,_suggestions_and_typos\n- 066: social contexts >> I find this term misleading here, since the text seems to be about countries/language regions.\n- 121: Deviding 1508 into 16*90 = 1440 cases cannot be fully correct. What about the remaining 68 cases?\n- 241: It would also be good to state the maximum number of tasks done by any annotator.\n- Table 3: Right-align the numeric columns.\n- Table 4 (1): Always use the same number of decimal places, for example 61.90 instead of 61.9 to match the other values. This would increase readability.  - Table 4 (2): The table exceeds the page width; that needs to be fixed.\n- Tables 4+5 (1): While I undersand the layout problem, the different approaches would be much easier to compare if tables and columns were flipped (usually, one approach per row, one metric per column).  - Tables 4+5 (2): What's the idea of showing the run-time? I didn't see for what this is helpful.\n- 305/310: Marie/Mary >> I think these should be written the same.\n- 357: The text speaks of \"53\", but I believe the value \"52.9\" from Table 4 is meant. In my view, such rounding makes understanding harder rather than helping.\n- 575/577: \"1/\" and \"2/\" >> Maybe better use \"(1)\" and \"(2)\"; confused me first. ", "label": [[916, 1051, "Eval_pos_1"], [1052, 1111, "Eval_pos_2"], [1161, 1230, "Eval_pos_2"], [1304, 1536, "Jus_pos_2"], [1540, 1670, "Eval_pos_3"], [1671, 1962, "Jus_pos_3"], [2232, 2474, "Eval_neg_1"], [2479, 2579, "Eval_neg_2"], [2601, 2835, "Jus_neg_2"], [2839, 3028, "Eval_neg_3"], [3029, 3266, "Jus_neg_3"], [3270, 3360, "Eval_neg_4"], [3361, 3560, "Jus_neg_4"]]}
{"id": 283, "review": "paper_summary\nThis paper addresses a text classification task: assigning ICD-9-CM codes to a clinical text.  This is a difficult task because only short spans of the input text are relevant to a given target code, hence the use of attention mechanisms in recent work.  In the present work, multiple synonyms are used in the attention mechanism in order to attend to all descriptions of a code.  Furthermore, a biaffine transformation is learnt to help compare synonym representations to the representation of the input text for a given code. \nThat combination of using multiple synonyms in the attention mechanism and a biaffine transformation in the architecture is novel to the best of my knowledge.  On the MIMIC-III ICD9 coding dataset (full and top-50 codes), both sub-methods are shown to improve results over using a single term per code or learning independent code representations as in previous work. \nThe paper finally explores the space of synonym representations. \nsummary_of_strengths\n- The introduction of multiple synonyms and of the biaffine transformation in the architecture is motivated.\n-Each are shown to bring improvements.\n-Improves over previous state of the art, represented by five published systems.\n-Short exploration of the resulting synonym representation space. \nsummary_of_weaknesses\n- Experiments are performed only once, with no test of multiple random seeds.  Performing experiments N times and reporting mean and standard deviation is good practice.\n-Tests should be performed to check whether the observed differences are significant. \ncomments,_suggestions_and_typos\n- The basic principle of using multiple synonyms of ICD terms is quite common in earlier work: using the presence of ICD in the UMLS makes it straightforward to obtain more terms for the same ICD code, and terms can then be used to help concept extraction and normalization from text.  In a way, it is therefore surprising that recent work using deep learning methods does not use this simple resource. \n  - Note that in the related task of entity linking, for instance BioSyn (Sung et al., ACL 2020) uses multiple synonyms for each concept.\n- How did you sample the synonyms when not all of them are selected: fully randomly? \n  - what if less synonyms are available than the required number $m$?  Does this happen in the performed experiments?\n- More synonyms probably increase the memory and time requirements: can you say more about the complexity of the method relative to the number of synonyms?\n- \"Deep learning methods usually treat this task as a multi-label classification problem\": this is the case more generally of supervised machine learning methods, and is not specific to deep learning methods.\n- \"A d-layer bi-directional LSTM layer with output size h /is followed by/ word embeddings to obtain text hidden representations H.\" -> I assume that the LSTM takes word embeddings as input instead?\n- \"We notice that the macro F1 has large variance in MIMIC-III full setting\": please explain what you mean by variance here.  This does not seem to be visible in Table 1.\n@inproceedings{Sung:ACL2020,     title = \"Biomedical Entity Representations with Synonym Marginalization\",     author = \"Sung, Mujeen  and       Jeon, Hwisang  and       Lee, Jinhyuk  and       Kang, Jaewoo\",     booktitle = \"Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics\",     month = jul,     year = 2020,     address = \"Online\",     publisher = \"Association for Computational Linguistics\",     url = \"https://aclanthology.org/2020.acl-main.335\",     doi = \"10.18653/v1/2020.acl-main.335\",     pages = \"3641--3650\" } ", "label": [[1001, 1107, "Eval_pos_1"], [1109, 1146, "Eval_pos_2"], [1147, 1227, "Eval_pos_3"], [1319, 1394, "Eval_neg_1"], [1396, 1486, "Jus_neg_1"], [1488, 1573, "Eval_neg_2"]]}
{"id": 286, "review": "paper_summary\nThis paper focuses on data to text generation from RDF triples and attribute value pairs. The task involves generating a couple of sentences from a set of triples. As far as the traditional generation pipeline is concerned, it does not include any content selection or document planning. All the triples must be verbalized although their order must be determined and which ones should be grouped (aggregated together) to give rise to a single sentence. Most state of the art models are trained or fine-tuned on datasets developed specifically for this task. The authors argue that training on in-domain datasets leads to overfitting. They propose to render the RDF triples into natural language, aka facts, using domain-specific templates, and then develop general modules which perform the text rewriting operations (ordering, aggregation) required to render the clunky fact-based sentences into fluent text. The rewriting modules are trained on a synthetic corpus which the authors create from the English Wikipedia. \nsummary_of_strengths\n- The paper is well written, and the proposed approach intuitive and conceptually simply.\n- The Wikifluent corpus will be of use for other text-to-text tasks, such as simplification or even document compression.\n- The results are rather encouraging for a zero-shot model.\n- Evaluation is through, and includes ablations and experiments with automatic metrics and humans \nsummary_of_weaknesses\nI am not sure there are many weaknesses as such. Although conceptually simple the approach relies on several tools to create the Wikifluent corpus (e.g., decontextualization, sentence splitting) and domain expertise to turn the trips into facts, so even though it zero shot wrt generation task itself, it is not data-lean at all.  For this reason, I would have liked to see an assessment of how noisy these intermediate steps are. \ncomments,_suggestions_and_typos\n- Please give us some statistics on the templates, how many they are per dataset, how they were developed (by one or several humans, was there an algorithm? Is the process deterministic or is the any ambiguity?). Btw what you call templates in the Appendix should be facts (which you get by application of your templates in the input).  - In the introduction you should give more credit to Myrossef et al. As far as I am aware they were the first who proposed to convert data-to-text generation to text-to-text generation (their model is not zero-shot, and you have enough of a contribution here to give them appropriate credit).  -Perhaps the following dataset is of use for your coreference replacement task: https://arxiv.org/abs/2102.05169 - I would have been curious to see what happens if you applied your approach in a non zero shot manner, i.e., if you fine tuned (using your templates and the Wikifluent corpus) - The paper contains detailed comparison to related systems, it would have been useful to organize these more meaningfully (i.e., fully supervised vs zero-shot, fine-tuned or not), such groupings would help the reader follow the discussion of your results  - ", "label": [[1057, 1082, "Eval_pos_1"], [1088, 1144, "Eval_pos_2"], [1269, 1326, "Eval_pos_3"], [1329, 1350, "Eval_pos_4"], [1356, 1425, "Jus_pos_4"], [1497, 1776, "Jus_neg_1"], [1796, 1879, "Eval_neg_1"]]}
{"id": 287, "review": "paper_summary\nThe paper proposes a new pairwise ranking approach for the task of automatic readability assessment (ARA). The proposed approach is compared against other modeling paradigms used in the ARA literature, namely classification and regression across 3 languages (English, French and Spanish). The experiments show that the approach works competitively in monolingual settings and gets strong results in cross-corpus and zero shot cross-lingual settings. The paper also releases a new parallel bilingual ARA dataset for future research. \nsummary_of_strengths\n1. Clear motivation for need of such a technique with cross-corpus compatibility being a realistic desired property for ARA approaches.\n2. Well defined research questions, adequately answered through experiments.  3. Thorough experimentation, spanning 3 languages and different modeling approaches.\n4. New dataset (and the first of its kind) for a relatively understudied task of ARA. \nsummary_of_weaknesses\n1. Not clear if the contribution of the paper are sufficient for a long *ACL paper. By tightening the writing and removing unnecessary details, I suspect the paper will make a nice short paper, but in its current form, the paper lacks sufficient novelty.  2. The writing is difficult to follow in many places and can be simplified. \ncomments,_suggestions_and_typos\n1. Line 360-367 are occupying too much space than needed.  2. It was not clear to me that Vikidia is the new dataset that was introduced by the paper until I read the last section :)   3. Too many metrics used for evaluation. While I commend the paper\u2019s thoroughness by using different metrics for evaluation, I believe in this case the multiple metrics create more confusion than clarity in understanding the results. I recommend using the strictest metric (such as RA) because it will clearly highlight the differences in performance. Also consider marking the best results in each column/row using boldface text.  4. I suspect that other evaluation metrics NDCG, SRRR, KTCC are unable to resolve the differences between NPRM and the baselines in some cases. For e.g., Based on the extremely large values (>0.99) for all approaches in Table 4, I doubt the difference between NPRM\u2019s 0.995 and Glove+SVMRank 0.992 for Avg. SRR on NewsEla-EN is statistically significant.  5. I did not understand the utility of presenting results in Table 2 and Table 3. Why not simplify the presentation by selecting the best regression based and classification based approaches for each evaluation dataset and compare them against NPRM in Table 4 itself?  6. From my understanding, RA is the strictest evaluation metric, and NPRM performs worse on RA when compared to the baselines (Table 4) where simpler approaches fare better.  7. I appreciate the paper foreseeing the limitations of the proposed NPRM approach. However, I find the discussion of the first limitation somewhat incomplete and ending abruptly. The last sentence has the tone of \u201cdespite the weaknesses, NPRM is useful'' but it does not flesh out why it\u2019s useful.  8. I found ln616-632 excessively detailed for a conclusion paragraph. Maybe simply state that better metrics are needed for ARA evaluation? Such detailed discussion is better suited for Sec 4.4 9. Why was a classification based model not used for the zero shot experiments in Table 5 and Table 6? These results in my opinion are the strongest aspect of the paper, and should be as thorough as the rest of the results.  10. Line 559: \u201clower performance on Vikidia-Fr compared to Newsela-Es \u2026\u201d \u2013 Why? These are different languages after all, so isn\u2019t the performance difference in-comparable? ", "label": [[570, 703, "Eval_pos_1"], [707, 738, "Eval_pos_2"], [739, 779, "Eval_pos_3"], [785, 809, "Eval_pos_4"], [811, 866, "Jus_pos_4"], [870, 953, "Eval_pos_5"], [979, 1059, "Major_claim"], [1060, 1168, "Jus_neg_1"], [1174, 1230, "Eval_neg_1"], [1235, 1308, "Eval_neg_2"]]}
{"id": 288, "review": "paper_summary\nThe paper proposes an approach with an information-theoretic solution for the out-of-vocabulary (OOV) entity recognition problem. Specifically, the proposed approach contains two mutual information based training objectives: i) Information maximization between context and entity surface forms. 2) Superfluous information minimization, which entails minimizing task-non-specific information. Both these objectives are modeled after the information bottleneck principle in information theory science.\nThe paper leverages several experimental datasets and baselines and in the end demonstrates the robustness and effectiveness of the proposed solution. \nsummary_of_strengths\n1) The additional of the information theoretic approach within the current state-of-the-art for NER extraction i.e. SpanNER architecture is not just creative but an elegant engineering solution. It seems to extend the state-of-the-art incrementally, while showing a nice understanding of addressing precisely some problems or information leakages that can help boost NER for OOV NER.\n2) The solution proposed tackles a relevant advancement of the NER problem in the NLP community specifically OOV NER.\n3) From this paper, one can gain new insights into the OOV NER problem in terms of both the problem itself and the solution.  4) One can also gain a surveyistic birds-eye-view on the state-of-the-art systems and available datasets for the task.\n5) The experiments while they could have been better structured/organized (see my comments in \u201cSummary of Weaknesses\u201d) are still informative, and offer sound and meaningful observations. \nsummary_of_weaknesses\n1) I would have liked to have seen a better application domain motivation for why the OOV problem in NER is so important.  2) The paper has the following phrase explaining the problem that it wishes to address: \u201centail preventing the model from rote memorizing the entity names or exploiting biased cues via eliminating entity name information.\u201d However, majority of the paper and indeed the method itself seems to then emphasis the following specific problem, i.e. \u201celiminating task-specific nuisances.\u201d I think maybe the former line should then either be removed or somewhere it state whether it is a hypothesis of the authors that \u201celiminating task-specific nuisances\u201d indeed means \u201cpreventing the model from rote memorizing the entity names or exploiting biased cues via eliminating entity name information.\u201d Perhaps the authors would need another whole paper to analyze what \u201celiminating task-specific nuisances\u201d w.r.t. the proposed approach precisely means. So I would not say this as a weakness of this paper. However, is the phrase \u201cpreventing the model from rote memorizing the entity names or exploiting biased cues via eliminating entity name information\u201d is used, it would help the reader if a connection is made to the actual solution which is mostly referred to as \u201celiminating task-specific nuisances.\u201d\n3) I also wonder if the authors would be willing to rephrase \u201cnuisances\u201d in \u201celiminating task-specific nuisances?\u201d Maybe as \u201cnoise?\u201d\n4) Table 4. Curious why the ablation results are not shown on the CONLL 2003 dataset even though the method seems to have a maximum impact on it? Further, the CONLL 2003 \u2013 OOV data in particular seems quite relevant in this experimental setting. Can the authors provide these results? Otherwise the empirical analysis would seem incomplete to me by missing a seeming very relevant detail.  5) Not a weakness of this work as such. But maybe as a suggestion for future work. Lin et al [1] described four categories of artificially simulating OOV scenarios in NER. An empirical analysis with the method proposed in this work over their dataset or at least a dataset using their OOV synthesis methodology would be very interesting to have particularly on this problem of OOV NER.\n6) The authors did not mention MINER code will be publicly released if the paper were accepted.\n---------------- References ---------------- [1] Lin, Hongyu, et al. \"A Rigorous Study on Named Entity Recognition: Can Fine-tuning Pretrained Model Lead to the Promised Land?.\" Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2020. \ncomments,_suggestions_and_typos\nTable 1 in this paper corresponds exactly to Table 3 Vanilla Baseline in Lin et al. [1]. However, Lin et al. mention using the ACE 2005 dataset (paragraph 1 in section 2.1 in Lin et al. [1]) and this paper says it uses the CONLL 2003 dataset. Two concerns here: 1) the dataset used in this paper must be clarified \u2013 how do the authors get the exact same results?; 2) if indeed the experimental datasets are the same, I am not sure if it is okay to replicate the exact same table across papers.\n-------------------------------- Comments on Structure -------------------------------- Line 194: Should it be section 3.4? The subsection organization seemed confusing.\n--------- Typos --------- The paper has several typos and grammar errors more toward the latter half of the paper. I list a few of the typos here. But this paper if accepted needs better proofreading.\nLine 83: \u201ccus via eliminating\u201d -> \u201ccues via eliminating\u201d Line 123: \u201cshotcut learning problem\u201d -> \u201cshortcut learning problem\u201d Line 217: \u201cwe can subdividing\u201d -> \u201cwe can subdivide\u201d Line 334: \u201cstatic results\u201d -> statistic?\nLine 420: \u201cthe the span classification\u201d ---------------- References ---------------- [1] Lin, Hongyu, et al. \"A Rigorous Study on Named Entity Recognition: Can Fine-tuning Pretrained Model Lead to the Promised Land?.\" Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2020. ", "label": [[690, 881, "Eval_pos_1"], [882, 1070, "Jus_pos_1"], [1074, 1188, "Eval_pos_2"], [1192, 1313, "Eval_pos_3"], [1318, 1433, "Eval_pos_4"], [1437, 1621, "Eval_pos_5"], [1646, 1765, "Eval_neg_1"], [3107, 3240, "Eval_neg_2"], [3241, 3483, "Jus_neg_2"]]}
{"id": 289, "review": "paper_summary\nThis paper proposes to extract word-level quality information for machine translated text from a state-of-the-art sentence-level quality estimation (QE) model, based on pre-trained contextual embeddings, fine-tuned in a semi-supervised way, i.e. using only sentence-level scores for fine-tuning. The main research question the authors focus on is, do QE models infer sentence-level translation quality scores based on word-level errors, which would provide insights on the model decision making (rationales) and alleviate the need of costly error-annotated word-level data. \n  To answer this question, an empirical work is conducted by comparing several interpretability related approaches, so-called feature attribution methods, as well as a supervised and an unsupervised approach, along with a random classification baseline. Experiments include three translation directions towards English and are performed on a publicly available dataset. Results show the superiority of two approaches, namely integrated gradients and attention weights, based on several metrics calculated on a test corpus where sentences containing only or no errors were ignored.\nThe analysis of these results is rather succinct, but proposes nonetheless a few interesting observation nuggets. First, more useful error-related word-level information is encoded at deeper layers of the model, second, both source and target sentences appear to have an impact on feature attribution, third, frequency of words in the machine translation training corpus and sentence-level QE seem to be in a relation of some sort. \nsummary_of_strengths\nOverall, this paper is very well written, with a clear hypothesis and straightforward experimental setup.  The semi-supervised word-level translation errors extraction paradigm is, to the best of my knowledge, relatively novel when using contextual embedding models. The approach proposed by the authors is thus interesting for the research community and could pave the way towards more cost effective translation QE, fine-grained methods for explaining QE models outputs, etc.\nWhile the experimental results do not necessarily emphasize which approach would allow for the best word-level QE results, which is not the scope of the study, they offer a comparison point for future work and validate the use of semi-supervised methods for word-level QE. The analysis provides a few insights on where the information is encoded in the model, as well as the importance of source and target sentences for QE modelling.\nThe use of publicly available resources, including the dataset and the various implementations, ensure a reasonable level of reproducibility. \nsummary_of_weaknesses\nThe main weaknesses of this paper are the empirical validation of the method proposed by the authors and the lack of deep analysis. The main findings are limited to applying to QE a set of existing interpretability oriented methods already applied generally to NLP tasks.  For the empirical validation of the method, the evaluation does not reflect a general scenario in machine translation where both perfect and fully erroneous translations exist. Additionally, as shown later in the analysis, there could be a relation between word frequency distributions in the machine translation training data and the ability of the QE model to rely on this information to estimate the sentence-level translation quality. This phenomenon should be investigated further, for instance by replacing the random baseline by a frequency based classifier, as a starting point. The various approaches used as feature attribution methods, along with the current experimental setup, do not offer a clear understanding of what the QE model does unfortunately.\nFor the analysis section, extracting a useful QE signal at deeper layers of the pre-trained model is somewhat expected, but is still interesting to find. However, we need more insights on what kind of information is encoded at each layer, by the use of probes for instance, at a sentence-level basis. As Figures 2 and 3 indicate, variability in feature attribution between layers show that some words, in some context (sentences in this case), receive different scores given an approach. What are the characteristics of these words and sentences? Do feature attribution methods agree or diverge given words in context? \ncomments,_suggestions_and_typos\nIt would be interesting to see how relevant to word-level QE is the information encoded at different layers of the model without fine-tuning for sentence-level QE, by using simple classifiers as probes for instance. \n  The feature attribution method based on attention could easily be the focus of the study, whether by using the method proposed by (Kobayashi et al., 2020, \u201cAttention is Not Only a Weight:Analyzing Transformers with Vector Norms\u201d) as a comparison point, by analysing attention heads separately the same way it is conducted per layer, etc. ", "label": [[1633, 1664, "Eval_pos_1"], [1666, 1729, "Jus_pos_1"], [1731, 1890, "Eval_pos_2"], [1891, 1974, "Eval_pos_3"], [1979, 2101, "Jus_pos_3"], [2261, 2374, "Eval_pos_4"], [2375, 2536, "Eval_pos_5"], [2537, 2679, "Eval_pos_6"], [2702, 2833, "Eval_neg_1"], [2833, 4360, "Jus_neg_1"]]}
{"id": 290, "review": "paper_summary\nThe paper presents a new dataset for image retrieval in Language and Vision research, and some first results computed with state-of-the-art models in L&V (CLIP and Vilbert). The idea of the data collection is to present human annotators with a set of images, out which one is the target that needs to be described. The image sets are sampled from existing resources of static images, and videos. The data analysis in the paper shows that the datasets contain interesting and challenging phenomena for language-and-vision reasoning. The experiments with CLIP and Vilbert show that existing models are clearly not up to human performance on this task. \nsummary_of_strengths\nThis paper is an interesting, relevant, well-executed, and well-written resource paper. The dataset is well-motivated, the collection is described clearly and the tested models and baselines are perfectly reasonable. The data set will be of great use to the community and foster further research in challenging language phenomena in the area of L&V. \nsummary_of_weaknesses\nThe only thing I can complain about is the rather general framing of the dataset in the introduction. To me, it seems that \"contextual descriptions\" is a bit too vague and generic as a term for describing the contribution. The contexts explored in this work are very specific distractor-like types of contexts where an image appears in combination with extremely similar images. This is close to the classical referring expression setting that investigates descriptions of objects in the context of similar distractor objects. The resulting expressions may be called discriminative descriptions/unambiguous references or something along these lines. \ncomments,_suggestions_and_typos\nMy suggestion would be to mention the connection to referring expressions somewhere in the paper and to think about refining the intro/notion of context a bit.\nYu, Licheng, et al. \"Modeling context in referring expressions.\" European Conference on Computer Vision. Springer, Cham, 2016. \nMao, Junhua, et al. \"Generation and comprehension of unambiguous object descriptions.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2016. ", "label": [[686, 773, "Eval_pos_1"], [774, 803, "Eval_pos_2"], [805, 840, "Eval_pos_3"], [844, 902, "Eval_pos_4"], [903, 1036, "Eval_pos_5"], [1059, 1160, "Eval_neg_1"], [1161, 1281, "Jus_neg_1"]]}
{"id": 291, "review": "paper_summary\nThis paper dealt with a interchange intervention training with a distillation technique. They propose to train a student model to become a causal abstraction of a teacher model, which is faithful with a simpler causal structure. Their experimental results show that the proposed method improved some tasks compared with a standard distillation method. \nsummary_of_strengths\n+ Distillation method with a causal abstraction is novel approach. \n+ They experimented the proposed method with various types of tasks. \nsummary_of_weaknesses\n+ They did not show the detailed method (of distillation training procedure) in the paper. They should include them in main text not in appendix. \n+ They should explain why the proposed model can deal with a causal abstraction. At least they should show some samples and analyze them. \n+ They should compare among the proposed methods and analyze these results. \ncomments,_suggestions_and_typos\n+ Performance of each teacher model should be shown in Table 1. \n+ They can use the following paper: Knowledge Distillation from Internal Representations [Aguilar, AAAI20] ", "label": [[390, 454, "Eval_pos_1"], [458, 525, "Eval_pos_2"], [550, 638, "Eval_neg_1"], [639, 694, "Jus_neg_1"], [697, 774, "Eval_neg_2"], [776, 833, "Jus_neg_2"], [836, 910, "Eval_neg_3"]]}
{"id": 292, "review": "paper_summary\nThis paper proposes a routing strategy for multitask machine translation. For a given language or domain, only a fixed amount of nodes is activated at each layer and the model learns which nodes should be activated. They experiment on both multilingual and multidomain settings, and the experimental results demonstrate that they can achieve comparable and sometimes marginally better performance than adapter-based methods with fewer parameters. \nsummary_of_strengths\n1. The general idea is intuitive and makes sense, and the paper proposes a technically sound way of implementing it. \n2. The empirical results are good as they can achieve comparable performance with an adapter-based method with fewer parameters. \n3. The paper is well-organized. \nsummary_of_weaknesses\nPrevious comments:  1. There is little analysis of their methods. Sensitivity analysis of their hyper-parameters ($n_d$, $k$, $\\tau$), some qualitative analysis are required. \n2. Requiring partitioning nodes into groups and a fixed number of nodes to be activated at each layer makes their method less flexible. \n3. More baselines should be included (e.g. Li et al., (2020), Gong et al., (2021a, b) as mentioned in the paper).\nAfter reading the response: The added sensitivity analysis -Thanks for adding the sensitivity analysis on $k$! However, more analyses are still required as in my previous comment (e.g. the effect of $n_d$, the use of the annealing strategy for $\\tau$, analysis of the translation outputs, ...).\nThe flexibility of their approach -The method requires a fixed number of nodes to be activated at each layer for all the tasks. This is inflexible because intuitively, harder tasks require more nodes to process while simpler tasks may only need a few nodes. However, their method cannot take this into account, and thus the method is not very flexible.\nThe added baseline -It'd be better to compare with published baselines (e.g. [1, 2]). The baselines share similar claims as the author(s) mentioned in the rebuttal. I agree that this paper doesn't need to obtain state-of-the-art performance, however, the comparisons are necessary so that other researchers can know which methods to use in practice. Therefore, comparisons are required even if the results are not favorable to their method.\n[1] Lin et al., Learning Language-Specific Sub-network for Multilingual Machine Translation, ACL 2021. \n[2] Li et al., Deep transformers with latent depth, NeurIPS 2020. \ncomments,_suggestions_and_typos\nThe added significance tests -The added significance tests are questionable. First, many of the results (e.g. most of the results in Table 5) are not significantly better than the Transformer baseline. Second, it is better to compare with the best baseline model (e.g. the HMGD Transformer in Table 3) rather than the vanilla Transformer baseline. Third, in Table 3 the author(s) claim that the average BLEU score is significantly better than the baseline using compare-mt, but more details are required as compare-mt does not support comparing average numbers. ", "label": [[486, 531, "Eval_pos_1"], [536, 600, "Eval_pos_2"], [604, 634, "Eval_pos_3"], [635, 730, "Jus_pos_3"], [734, 763, "Eval_pos_4"], [808, 851, "Eval_neg_1"], [852, 961, "Jus_neg_1"], [965, 1098, "Eval_neg_2"], [1102, 1134, "Eval_neg_3"], [1137, 1210, "Jus_neg_3"], [2535, 2581, "Eval_neg_4"], [2582, 3067, "Jus_neg_4"]]}
{"id": 293, "review": "paper_summary\nThis paper tries to use co-citations in full-text papers to learn an aspect matching model with fine-grained aspect representation of paper abstracts in sentence level for fine-grained retrieval, and it also achieves to compute document-level distance with a multi-vector document similarity model for abstract level retrieval. \nsummary_of_strengths\nAdvantages of this paper are as follows. \n\u2022 Co-citations used in this paper seems to be a new data source for fine-grained retrieval task. \n\u2022 The proposed ASPIRE model uses co-citing sentences as textual supervision signals to train fine-grained retrieval models. \n\u2022 This paper gets good performance across several datasets. \nsummary_of_weaknesses\nWeaknesses of the paper: \u2022 The mix of different approaches and tasks leads to a confusion for readers. \n\u2022 The logic flow of the paper needs to be improved. \n\u2022 Not sure why the proposed model work for the experimental datasets, since those datasets do not have such textual supervision as co-citing sentences. \n\u2022 There are no comparison and contrast between proposed approaches and baselines. \ncomments,_suggestions_and_typos\nI strongly suggest to improve the narrative of the paper. ", "label": [[407, 503, "Eval_pos_1"], [631, 689, "Eval_pos_2"], [739, 815, "Eval_neg_1"], [818, 868, "Eval_neg_2"], [871, 937, "Eval_neg_3"], [939, 1021, "Jus_neg_3"], [1024, 1104, "Eval_neg_4"], [1137, 1193, "Eval_neg_5"]]}
{"id": 294, "review": "paper_summary\nThis paper proposes a text-to-SQL parser that can better utilize given alignments between text and SQL queries. Empirically, the proposed parser achieves the new state-of-the-art performance on the SQUALL dataset. \nsummary_of_strengths\nThe usage of alignments as contexts is creative and coupled well with BERT-based encoding. The training method of noisy alignment augmentation is also interesting, and the way I view this is that it's an efficient way of \"marginalizing\" over all possible noisy alignments. \nsummary_of_weaknesses\nMy main complaints are 1) the presentation of this work is a bit misleading to me, 2) I'm not convinced by the main arguments of this work.\nFor 1), it's not completely clear from early sections that this work focus on the setting where the gold alignments are given. I don't think this setting is the default text-to-SQL in the current literature, so it'd be better to clarify this as early as possible.  For 2), the main arguments of the proposed system are that attention cannot capture alignments well because it's at the token level. But isn't the sequence tagging model used is also at the token level?  The second argument is that attention is prone to overfitting. I'm not convinced that we should attribute the overfitting of current parsers to attention mechanisms, and this paper does not provide any justification.  Maybe the authors should emphasize their way of incorporating alignments (alignments as context, and alignment augmentation), which I think are the main contribution of this work. \ncomments,_suggestions_and_typos\nThe two-stage process is very related to [1]. In general, the paper perhaps should also mention the line of work that treat alignments as latent variables, e.g., [2, 3] - [1] Compositional Generalization via Semantic Tagging -[2] Span-based semantic parsing for compositional generalization -[3] Learning semantic parsers from denotations with latent structured alignments and abstract programs ", "label": [[250, 297, "Eval_pos_1"], [302, 340, "Eval_pos_2"], [341, 412, "Eval_pos_3"], [414, 523, "Jus_pos_3"], [572, 627, "Eval_neg_1"], [632, 685, "Eval_neg_2"], [686, 949, "Jus_neg_1"], [950, 1553, "Jus_neg_2"]]}
{"id": 296, "review": "paper_summary\nThis work reports on the results of a cross-document misinformation detection system using event graph reasoning. Recognizing the need for automatic detection of real versus fake news, they train a system for event detection at two levels \u2013 document level and event level. To do this, they created their own fake data, apply cross-document coreference, group data into three sets based on event \u201cclusters\u201d (topically related events), and report event detection results. Their approach is as follows:  1.) \tCreation of a within-document knowledge graph based on IE (\u201cIE\u201d is never defined. I assume it stands for information extraction)  2.) \tCreation of a cross-document knowledge graph, connecting each cross-document event cluster to \u201cevents in the cluster\u201d  3.) \tConnection of document nodes with all events and entities in the document.\n4.) \tEncoding of cross-document knowledge graphs via graph neural networks and use of event and document level events to detect misinformation at both levels.  Their results outperform HDSF and GROVER at the document level and random guessing and logistic regression heuristics at the event level. \nsummary_of_strengths\n1. \tOverall, work seems valid and logical. It improves upon current misinformation detection. \n2. \tFigures are extremely helpful \nsummary_of_weaknesses\n1. \tMissing some details to lend full credibility. This pipeline seems to be fully automated. Were humans involved at any level and were automatic annotations, such as coreference resolution, subjected to human sanity checks?  This paper states that, \u201cWe employ a cross-document event coreference resolution system (Lai et al., 2021; Wen et al., 2021) to identify clusters of events from multiple documents that refer to the same real-world events.\u201d Was any coreference spot-checked? What is the reliability of the system employed? It next states that, \u201cFor each event cluster, we add a node to represent the overall information of the real-world complex event corresponding to the cluster.\u201d How was this added? Automatically or by hand? If automatically, what was the reliability of the nodes themselves? If by humans, what was the agreement? These details may impact your final results. This is followed by, \u201cThen, an edge is added between each event node and corresponding cluster node to allow reasoning among cross-document coreferential events.\u201d How is the edge added? Finally, how were your event structures pre-defined? I assume they differ by event, how did you determine the trigger, arg1, and arg2? Were there ever more than two arguments?\n2. \tJustification of the need to create fake data is not fully motivated. You state that collection of annotation is time consuming and expensive but that these datasets are of higher quality, both of which are very true. You further state that you follow previous work that created datasets by manipulating knowledge graphs. While references are provided, no mention of validation of such manipulation is provided to satisfy a reader as to the of created fake news nor is mention made as to the heterogeneity or homogeneity of created fake news. How generalizable is this to real fake news? \ncomments,_suggestions_and_typos\n1)\tMany acronyms not defined, such as \u201cIE\u201d and \u201cRNN\u201d 2)\tMissing references for some claims that are presented as if they are common knowledge. For example, it is claimed that \u201cconspiracy theories tend to be closely related to each other and convey highly similar information because they share the same biases or aim to manipulate readers in the same way.\u201d However, your rational for creating your own fake news is the lack of sufficient existent data. Considering prior work on fake information detection, it is not always the case that conspiracy theories and/or fake news in general is similar between authors. The claim may very well be true for specific type of data. However, work validating the claim must be included. \n3)\tYou state that you improve upon current state of the art work by 7 points. Did the state of the art work use real or fake training data? What are your results on real data? \n4)\tHeuristics used as baseline for event-level detection seem valid. However, multiple publications from 2021 report similar work and could potentially be used as a baseline (ex. COVID-19 misinformation detection via identification claims and within-document events and entities.) Many such resources are not based on coreference resolution so may not be applicable. However, comparison to a different method would seem to bolster your findings. \n5)\tGrammatical error in \u201cthe two real news compliment each other\u201d, \u201cin the first news\u201d, and \u201cthe two fake news contradict the real news with different details\u201d ", "label": [[1178, 1203, "Eval_pos_1"], [1204, 1216, "Eval_pos_2"], [1217, 1268, "Eval_pos_3"], [1273, 1303, "Eval_pos_4"], [1330, 1376, "Eval_neg_1"], [1377, 2576, "Jus_neg_1"], [2581, 2650, "Eval_neg_2"], [2651, 3169, "Jus_neg_2"], [3258, 3345, "Eval_neg_3"], [3345, 3928, "Jus_neg_3"], [4173, 4279, "Eval_neg_4"], [4387, 4552, "Jus_neg_4"]]}
{"id": 297, "review": "paper_summary\nThis paper proposes a model to rank pronunciations of a word given a sequence of graphemes. A limited set of potential pronunciations are generated based on orthographic rules. The model takes a sequence of graphemes and a sequence of phones, and produces a score. The approach enumerated the exponentially many sequences generated by the orthographic rules, and find the sequence of phones that achieves the highest score. Empirically, the set of potential pronunciations are not prohibitively large, and is feasible to enumerate. The performance of the model is on par with many other approaches. \nsummary_of_strengths\nI was one of the reviewers in the previous round. I advocated acceptance and I still do. The greatest strength is the simplicity of the approach. The approach does not work for all languages, but it is still valuable for many, such as Thai. \nsummary_of_weaknesses\nThe paper is easy to follow, but the presentation can be improved. Below is a list of minor points, and they can be fixed rather easily.\n- The paper does not defend itself well enough. Instead of claiming \"both training and predicting processes are language independent,\" it should openly acknowledge that the approach won't be applicable to certain language orthography that is not very syllabic. This is addressed in the conclusion, but should appear early.\n- The training loss is not explicit. The description in section 3 is not detailed enough.\n- The paragraph and the table right above section 4.1 are good to have, but they feel cherry-picked. The paper should consider discussing other benefits, such as how easy it is to add new words to the model.\n- Section 4.1 is weak, if not hurting the paper. There should be a deterministic mapping (or something very close to deterministic) for Japanese Hiragana, and the results definitely do not help the claim that the approach is language independent. The approach would not be effective if we apply it to Japanese Kanji or Mandarin Chinese. \ncomments,_suggestions_and_typos\nThough there is a accompanying repository, I still think it's worth talking about the model architecture, training hyperparameters, and the training loss.\nThe model is an energy-based model. Given the limited space, there probably isn't much to be said, but it might be worth mentioning so that the readers are aware of this. ", "label": [[684, 723, "Major_claim"], [724, 780, "Eval_pos_1"], [827, 876, "Eval_pos_2"], [899, 927, "Eval_pos_3"], [928, 1035, "Eval_neg_1"], [1038, 1994, "Jus_neg_1"]]}
{"id": 298, "review": "paper_summary\nThis paper studies the problem of lifelong learning in the context of language model pretraining. The goal is to continuously learn new pretraining data domains for a language model such that we do not need to retrain a new one from scratch. They propose a method called ELLE, which aims to expand the current model's width and depth upon a new data source is available. A few related techniques are also applied to further boost performance, e.g. function recovering finetuning, domain specific tokens. Overall, the method is valid with plenty of experimental results, yet its practicality remains unclear. \nsummary_of_strengths\n1. This paper studies lifelong learning for model pretraining. This setup is less studied in lifelong learning literature. \n2. The proposed method works well on all experiments considered, with an astonishing amount of analysis (in the appendix). \n3. The paper is easy to follow and well written. \nsummary_of_weaknesses\nA critical limitation is how practical the method is given the nature of model expansion. This method is studied under a still relatively synthetic setup. There are a lot of problems that I can imagine for real-world large scale models such as GPT3. To mention a few: (1) How should we decide when to expand the model? In reality, these models are trained on O(100B) data, covering a significant portion of the whole internet. So when do we know a new data source emerges? How to detect this? ( 2) Large scale models are trained using sharding systems of thousands of accelerators, it is a challenging technical question to expand them in a way described in this paper. ( 3) Recent works have shown it is possible to continue training language models for either language understanding [1] or additional applications such as code synthesis [2]. Therefore, perhaps these simple approaches are sufficient enough for good generalization.\n[1] Finetuned Language Models Are Zero-Shot Learners. Wei et al., 2021. \n[2] Program Synthesis with Large Language Models. Austin et al., 2021. \ncomments,_suggestions_and_typos\nHow is the prompt token computed at inference time? Is it included using nearest neighbor search or not included at all? ", "label": [[771, 831, "Eval_pos_1"], [832, 871, "Eval_pos_2"], [895, 941, "Eval_pos_3"], [964, 1053, "Eval_neg_1"], [1054, 1118, "Jus_neg_1"], [1119, 1212, "Eval_neg_2"], [1232, 2042, "Jus_neg_2"]]}
{"id": 299, "review": "paper_summary\nThis paper presents a new domain-specific language model for the area of political violence and conflicts. General-purpose language models, like BERT, do not cover important domain-specific vocabularies, as a result, those models do not perform well in many downstream tasks. This paper addresses this limitation and proposes ConfliBERT, which is trained on text data from the area of social unrest, political violence, etc. ConfliBERT has two variations \u2013 (1) built from scratch, and (2) trained on top of pre-trained BERT. Experimental results show that either version performs better than BERT in several tasks related to this domain. \nsummary_of_strengths\n+ This paper proposes a new language model for a domain. As the authors claim there are not any previous language models specific to this area. While this may boost research in this area, this new model can also help in developing new research questions. \n+ There is a lack of a large text corpus specific to this area. Addressing this issue, the authors develop this model by merging several existing datasets and creating a large dataset that is suitable to train a model like BERT. This corpus will be an important resource to conduct text data-driven research in this area. \n+ The paper evaluates its claims through a variety of experiments. The experiments are designed to include several types of problems, such as binary and multiclass classification, sequence labeling, on the other hand, each problem has been tested on several datasets. In all these experiments, the ConfliBERT has consistently outperformed the performance of the BERT model. These results demonstrate the power of this model across a diverse set of problems and corpora (across the world) and the need for a domain-specific language model in this context. \n+ The work is reproducible, where the paper furnishes details about the implementation, datasets, and hyperparameters and also the system configuration for training. \nsummary_of_weaknesses\n-\tWhile this model is one of its kind in this area, the language model\u2019s scope is too narrow to have a wide range of applications. Other similar BERT-based models (e.g. BioBERT, SciBERT) has a wider coverage. The authors should strengthen the claim of the importance of this model by discussing important problems in this area and how this language model will be essential in addressing a wide variety of problems in this domain. \n-\tAlthough ConfliBERT is a novel language model, the concept, methodology, implementation follows the BERT model. While it can contribute to the area of political science and computational social science in general, it is not clear how this work contributes to the area of NLP. \n-\tThe paper lacks a sound discussion to explain certain observations. Although, the experimental results unanimously show that for problems related to this domain ConfliBERT is a better choice but it is not clear which version of ConfliBERT is better, SCR or Cont. On many occasions, we see that the Cont version (built on top of BERT) performs better than SCR. This raises two questions \u2013 under what conditions Cont is more likely to perform better and given that Cont is trained on top of BERT, what are the features of BERT that are important in this case.  The paper should address the merits of BERT that are part of Cont but not of SCR. Additional experiments may help to show whether this is a data problem or a model problem. \ncomments,_suggestions_and_typos\nThe paper argues the need for a domain-specific language model for the area of political conflicts and violence. They validate this claim by showing how such a language model can improve downstream tasks in this area. The paper is clearly written and the claims are evaluated well using an extensive set of experiments. \nThe advantage and the utility of the proposed language model are clear but there remain a few gaps that are important to understand the full strength of this model. The authors should present a clear discussion on the advantages and disadvantages of using the domain-specific datasets used in the training. The authors discussed that their dataset has additional words related to terrorism that are not present in a more general-purpose corpus but are there any other types of words that enriched the training of ConfliBERT? We see that only 3 out of 9 problems are related to terrorism. Indeed, the SCR model performed better in all the tasks involving terrorism-related data. On the other hand for protest-related data, we see a mixed performance, where Cont doing better in 2 out of 3 cases. For the case of Cont, are there any disadvantages for not using a general-purpose model/data? What is the percentage of vocabulary that is present in the general dataset but not in the domain-specific data? If this percentage is large, will that create a roadblock in the wider applicability of this language model? On the other hand, having too many unrelated words in the general-purpose data can act as noise for more domain-dependent tasks? The paper will benefit from a more in-depth analysis and comparison of the datasets used against a more generic corpus. \nMinor point: On page 6, section 5.2, lines 514, 515, it is not clear how the p-values were computed. What tests were performed to compare the two cases? ", "label": [[676, 730, "Eval_pos_1"], [731, 929, "Jus_pos_1"], [932, 993, "Eval_pos_2"], [994, 1252, "Jus_pos_2"], [1255, 1319, "Eval_pos_3"], [1320, 1808, "Jus_pos_3"], [1811, 1835, "Eval_pos_4"], [1837, 1975, "Jus_pos_4"], [2049, 2128, "Eval_neg_1"], [2129, 2428, "Jus_neg_1"], [2430, 2542, "Eval_neg_2"], [2543, 2707, "Jus_neg_2"], [2710, 2777, "Eval_neg_3"], [2777, 3442, "Jus_neg_3"], [3693, 3721, "Eval_pos_5"], [3726, 3755, "Eval_neg_6"], [3756, 3795, "Jus_neg_6"], [3796, 3866, "Eval_pos_7"], [3867, 3960, "Eval_neg_4"], [3961, 4102, "Jus_neg_4"], [5036, 5156, "Eval_neg_5"]]}
{"id": 300, "review": "paper_summary\nThis paper proposes a framework for training and extract-the-generate model for the long document summarization task. The premise is that the extractor should pass on important information from the source to the generator for producing abstractive summaries. To train the entire network, there have been three losses defined (generator, oracle, and consistency), of which generator is the decoder loss, oracle is used to optimize the extractor with oracle labels, consistency is used to marginalize dynamic attention with extractor distribution. Results are promising on two long summarization datasets (GovReport and QMSum) and competitive on arXiv. \nsummary_of_strengths\n-The paper is easy-to-follow and understandable in most parts. \n-The problem is well-approached, although it has not been motivated much in the paper. \n-Results outperform the prior SOTA by a large margin on two long datasets (GovReport and QMSum). \nsummary_of_weaknesses\n-The idea makes sense for the long document summarization, but I\u2019m wondering what the others have done in this area with a similar methodology? What does the system offer over the previous extract-then-generate methodologies? This is troublesome considering that the paper does not have any Related Work section, nor experimenting other extract-then-generate with their proposed model.\n- The extract-then-generate can be re-phrased as a two-phase summarization system that can be either trained independently or within an end-to-end model. The choice of baselines is a bit picky here considering the methodology. The authors should report the performance of other similar architectures (i.e., extract-the-generate or two-phase systems) here.  - While results are competitive on arXiv, some of the baselines are composed of less parameters and obtain better performance.\n-The paper lacks in providing human analysis, which is an important part of current summarization systems as to revealing the limitations and qualities of the system that could not be captured by automatic metrics.\n- The paper misses some important experimental details such as the lambda parameters values, how the oracle snippets/sentences are picked, and etc. It could be improved. \ncomments,_suggestions_and_typos\nIn the introduction part, the authors have made this claim: \u201cWe believe that the extract-then-generate approach mimics how a person would handle long-input summarization: first identify important pieces of information in the text and then summarize them.\u201d It will be good to provide a reference for this claim. ", "label": [[688, 750, "Eval_pos_1"], [752, 782, "Eval_pos_2"], [784, 838, "Eval_neg_1"], [840, 913, "Eval_pos_3"], [960, 1103, "Eval_neg_2"], [1103, 1344, "Jus_neg_2"], [1499, 1571, "Eval_neg_3"], [1572, 1700, "Jus_neg_3"], [1704, 1828, "Eval_neg_4"], [1830, 1873, "Eval_neg_5"], [1875, 2043, "Jus_neg_5"], [2046, 2098, "Eval_neg_6"], [2099, 2191, "Jus_neg_6"]]}
{"id": 301, "review": "paper_summary\nThe paper tackles a fundamental problem of linear layer followed by a softmax. Due to the particular parameterization, there is a problem termed softmax bottleneck, where the degree of freedom for the output word distribution is limited by the dimension of the hidden vector. Building on top of this observation, the paper formally shows that when some word embeddings are linearly dependent, it becomes impossible to produce certain ranking of words. To overcome this problem, special output layers or parameterization needs to be designed. One potential solution is to use a mixture of softmax (MoS), but the paper further argues that MoS is still not expressive enough, and attribute the limitation to the reuse of a single hidden vector. The paper then proposes to use multiple hidden vectors to compute mixture of softmax, and term the approach multi-facet softmax due to the use of multiple projections. Results show that multi-facet softmax can improve over the regular mixture of softmax. \nsummary_of_strengths\nThe paper is easy to follow. The approach is very well motivated. The figures are helpful for understanding the different approaches and the proposed approach. \nsummary_of_weaknesses\nThe paper really struggles to find a case where the proposed approach is better than the regular MoS. The improvement on natural language seems to be small, and the paper needs to work on a synthetic language to show the gap between MoS and the proposed approach. The paper then looks into a QA task where the uncertainty is high to showcase the benefits of the proposed approach, but the gain seems to be small. Either natural language does not exhibit the variability that fits the theory, or the problem of modeling is not a lack of uncertainty in the output distribution but requires something else.\nThe use of the term bimodal suddenly appears in section 5, and the use of the terms hints that it is the multimodality of the distribution that causes the problem. Indeed, this has been pointed out in Chris Bishop's seminal paper, Mixture Density Networks. However, the connection between multimodality problem and the theory in the paper is missing. It would be much better to draw the connection in section 2, and signpost a little in section 5.\nThere are minor presentation issues. For example, it is not until section 2.2 did I start to understand why the problem stated in the introduction exists. it would be much better to explain the problem in words in the introduction. As another example, the figure contains too many colors, and the colors don't particularly mean anything and are a bit distracting. \ncomments,_suggestions_and_typos\nI would talk about theorem 1 informally in the introduction. I would also remove unnecessary colors in the figures.\nTheorem 2 reads a bit abstract, so it would be much better to relate the theorem with the running, queen-king-woman-man example.\nI also think that the argument about the limitation of MoS, in the second paragraph of section 3.2, is a little dense. It might be better to add more redundancy to that paragraph, using different ways to explain the same concept. ", "label": [[1033, 1061, "Eval_pos_1"], [1062, 1098, "Eval_pos_2"], [1098, 1193, "Eval_pos_3"], [1216, 1318, "Eval_neg_1"], [1318, 1479, "Eval_neg_2"], [2076, 2170, "Eval_neg_3"], [2171, 2267, "Jus_neg_3"], [2268, 2304, "Eval_neg_4"], [2305, 2632, "Jus_neg_4"]]}
{"id": 302, "review": "paper_summary\nThe paper looks at distilling models for abstractive summarization. The paper makes the claim that the most obvious way to do knowledge distillation with seq2seq models (just using the generated output from the teacher model as a target output for the student; also called pseudo-labeling), is problematic. The problems with pseudo-labeling relate to known but important to restate problems with current abstractive summarization models in general: they generally copy from the target document, and they generally only copy the leading line of the target document. The paper attributes the causes for both of these problems being at least in part due to the attention distribution for the teacher models being too sharp, often focusing most of its weight on the next available word. In order to counteract these effects, the paper proposes to raise the temperature for the attention soft max in order to smooth the attention distribution of the teacher model. The temperature scaling modifications are evaluated on three standard datasets (CNN/DM, XSum, and NYT), with slight improvements in distillation performance found across all three datasets. \nsummary_of_strengths\n- The paper is easy to read and follow - A lot of detail has been provided in the paper, making it quite easy for someone to be able to reproduce.  - Evaluations are done on reasonable datasets, against reasonable baselines, and shows promising results. Human evaluations are also provided - Abalations and additional comparisons with obvious baselines (like just using sampling in the teach model for generating pseudo labels) are done. \nsummary_of_weaknesses\n- There is unfortunately not a whole lot of new content in this paper. The proposed method really just boils down to playing with the temperature settings for the attention of the teacher model; something that is usually just considered a hyperparameter. While I have no problem with what is currently in the paper, I am just not sure that this is enough to form a long paper proposing a new method. I think if this paper couched itself as more of a 'analysis/experimental' type of paper, expanding its analysis even further (and maybe expanding its scope to other tasks besides just abstractive summarization), it could be solid contribution. Unfortunately, the paper is presented more as a 'methods' paper, with the new method being proposed simply being a change in hyperparameters. \ncomments,_suggestions_and_typos\n- What hypothesis test are you using for computing the significance in table 2 and 3?\n- Are there other tasks where temperature smoothing could be beneficial? ", "label": [[1188, 1224, "Eval_pos_1"], [1227, 1332, "Eval_pos_2"], [1336, 1379, "Eval_pos_3"], [1381, 1410, "Eval_pos_4"], [1415, 1439, "Eval_pos_5"], [1440, 1624, "Eval_pos_6"], [1649, 1717, "Eval_neg_1"], [1718, 1901, "Jus_neg_1"], [1902, 2046, "Eval_neg_2"], [2046, 2433, "Jus_neg_2"]]}
{"id": 303, "review": "paper_summary\nThis paper investigates what kind of linguistic structural knowledge is transferable during language model pre-training. It explicitly probes certain knowledge/properties by designing several synthetic languages, i.e., (1) tokens are sampled independently and uniformly; (2) tokens are sampled independently but following the Zipfian distribution; (3) tokens are sampled depending on the sentence topic; (4) flat or nesting parenthesis language (Papadimitriou and Jurafsky, 2020); (5) flat or nesting dependency language. Experimental results show that for a causal LM task, conditioned token sampling and nested dependencies provide useful information, especially for LSTM LMs. Nested dependencies are most useful to pre-train a masked LM for a dependency parsing task. \nsummary_of_strengths\n- This paper provides a novel and interesting setting to demonstrate that synthetic conditioned token sampling and synthetic nested dependencies contain knowledge that can be transferred to real language modeling or dependency parsing.\n-Many follow-up ideas could be tested along this direction. \nsummary_of_weaknesses\n- This paper brings more questions than answers -- many results are counter-intuitive or contradictory without explanation. For example: 1) Setting the vector dimension to 10 can make the entire conditional token distribution close to the Zipfian distribution. Why is that? What if the dimension is larger or smaller than 10? \n2) In Figure 2(a), why do uniform and Zipfian token sampling even hurt the perplexity comparing with random weights? \n3) In Figure 2(b), why does L1=nesting-parenthesis is significantly worse than L1=flat-parenthesis for Transformer? \n4) In Figure 2(c), why does transferring from L1=English non-significantly worse than L1=Japanese while the task language L2=English? The flexibility of the Transformer is not a convincing explanation -- if the closeness between L1 and L2 is not a good indicator of transfer performance, then how do we conclude that a synthetic language L1 is helpful because it is closer to a real language L2? \n5) In figure 3(b), why does uniform token sampling is worse than random weights by so much?\n-There some technical mistakes. \n1) The method of sentence-dependent token sampling can not be called \"random work\". In (Arora et al. 2016), $c_t$ does a slow random walk meaning that $c_{t+1}$ is obtained from $c_t$ by adding a small random displacement vector. BTW, the correct citation should be \"Arora et al. 2016. A Latent Variable Model Approach to PMI-based Word Embeddings. In TACL\". \n2) If LSTM/Transformer models are trained with a causal (auto-regressive) LM loss, then they should be decoders, not encoders. \ncomments,_suggestions_and_typos\n- Algorithm 1. How did you choose p < 0.4?\n-L395. \" the combination\" -> \"combine\" -L411 \"train the model with one iteration over the corpus\". Why only one iteration? Is the model converged?\n-After fine-tuning a LM pre-trained with conditioned token sampling (L456 \"useful inductive bias\"), you could check if embeddings of L2 have interpretable topological relations, such as analogy. ", "label": [[809, 1042, "Eval_pos_1"], [1044, 1103, "Eval_pos_2"], [1128, 1248, "Eval_neg_1"], [1248, 2176, "Jus_neg_1"], [2178, 2209, "Eval_neg_2"], [2210, 2697, "Jus_neg_2"]]}
{"id": 304, "review": "paper_summary\nThe paper advances research in Temporal KGQA through a unified time-sensitive question answering framework (TSQA). The framework involves a time-sensitive approach to learn temporal KG embeddings, and contrastive losses to improve answer retrieval, and enhance the model's ability in capturing temporal signals. The proposed architecture significantly outperforms state of the art models on the CronQuestions dataset. \nsummary_of_strengths\n1. The paper is well written and presents the simple yet subtle techniques to incorporate time-sensitive information while learning temporal KG embeddings and model training for T-KGQA. \n2. The paper performs a thorough qualitative analysis and ablation studies highlighting the efficacy of the proposed design choices on temporal KGQA. \nsummary_of_weaknesses\nNo visible weakness. One research question I hoped to see was the amount of data required to ensure the TSQA framework could be applied to other datasets. The gains in performance comes from learning the TKGE embeddings and the inclusion of the contrastive losses. It would be interesting to observe whether the gains would remain as steep if trained on a fraction of the original dataset. \ncomments,_suggestions_and_typos\n", "label": [[457, 482, "Eval_pos_1"], [487, 640, "Eval_pos_2"], [644, 791, "Eval_pos_3"], [835, 968, "Eval_neg_1"], [969, 1204, "Jus_neg_1"]]}
{"id": 305, "review": "paper_summary\nThis paper introduced Fig-QA, a Winograd-style nonliteral language understanding task consisting of correctly interpreting paired figurative phrases with divergent meanings.  The authors evaluated the performance of several language models with Fig-QA and found that the models achieve performance significantly over chance, but  largely fall beyond human performance. \nsummary_of_strengths\nThe authors opened a new perspective in the milieu of ACL with this figurative language.  They first organized its typology and presented quantitative results.  Then the authors crowdsourced 10256 metaphors and call it Fig-QA, a new benchmark task.  It can be used to evaluate the nonliteral reasoning abilities of language models, from multiple viewpoint of zero-shot, fine-tuned and prompting methods.  The overall proposal introduced a novel way to evaluate the language models, and showed how they indeed are far from human capability.\nThe discussion of the limitation of the present language models is rich and this work provides an important understanding about SOTA language models in the way different from other language model evaluation papers. \nsummary_of_weaknesses\nI guess that there are theories about figurative languages. How can section 3.2 be grounded among such theory? If he authors could ground their results in relation to such theories, I think this paper will have even larger impact. \ncomments,_suggestions_and_typos\nJust a conjecture, but I am a non-native and I am certain that I am worse than GPT in these tasks. \nI guess that a non-native human cannot do well in these tasks, in general. \n If you crowdsource the performance on Fig-QA among non-natives, it might reveal what could be  necessary to train LMs.\n-It would be good if Fig-QA is  built for  other languages as well.\n-Although I did not find any datasets nor software attached, I highly recommend Fig-QA to be publicised. ", "label": [[405, 493, "Major_claim"], [810, 885, "Eval_pos_1"], [887, 944, "Eval_pos_2"], [945, 1016, "Eval_pos_3"], [1021, 1160, "Eval_pos_4"], [1294, 1414, "Eval_neg_1"]]}
{"id": 306, "review": "paper_summary\nThis paper focuses on an approach of Non-Autoregressive Generation (NAR) for summarization tasks, it includes three parts. The first part is using search-based method to the training data, the second part is training an encoder-only model, the third part includes the training strategy and a length-control algorithm. \nsummary_of_strengths\n1. This work combines an unsupervised approach and NAR generation on a summarization task. The experiment shows some gain over baselines.  2. The experiment settings, ROUGE score and speed are detailed. \nsummary_of_weaknesses\n1. The proposed NAR model is almost an encoder part of a transformer model, papers [1,2] are used an encoder-only model for NAR. The novelty is not clear described.  [1] Non-Autoregressive Text Generation with Pre-trained Language Models [2] Fine-Tuning by Curriculum Learning for Non-Autoregressive Neural Machine Translation 2. Lack of an ablation analysis to show the gain from each change. For example, with the special blank token (the difference between this model and [1]) vs without it. \ncomments,_suggestions_and_typos\n1. The three observations taking a lot of space in the introduction are very general, the reader may want to know more about the specific issue this paper is targeted, and the contributions made in this work. ", "label": [[496, 557, "Eval_pos_1"], [710, 744, "Eval_neg_1"], [909, 973, "Eval_neg_2"], [974, 1075, "Jus_neg_2"], [1111, 1317, "Eval_neg_3"]]}
{"id": 307, "review": "paper_summary\nThe paper investigates uncertainty estimation (UE) in two NLP task: named entity recognition and text classification. Uncertainty estimation is an important research area in many machine learning fields, however, as the authors point out, there has been little attention given to UE in natural language processing. The authors explain in detail UE methods and conduct extensive experiments comparing the different methods for a Transformer model trained for the selected tasks. The authors also introduce two new modifications to the existing approaches and show that they are able to improve the existing approaches. \nsummary_of_strengths\nThe topic of uncertainty estimation is an interesting and important research area which has not received the attention it deserves in NLP. This paper provides a good overview of the different approaches and shows their relative strengths and weaknesses through extensive experiments.\nThe paper introduces some interesting modifications to the existing approaches (Mahalanobis distance and DPP Monte Carlo dropout) and the authors are able to show that the proposed modifications lead to state-of-the-art performance in most of the selected tasks. The proposed approaches are computationally cheap.\nThe authors connect their research well to previous work and, to the best of my knowledge, the related work section cites the relevant work sufficiently.\nThe paper is well written. The different sections are well structured and the explanation of technical details is clear and concise.\nAll-in-all this is a good paper. \nsummary_of_weaknesses\nThe experiments are run using only one model (ELECTRA). It would be interesting to see some comparison between different models. \ncomments,_suggestions_and_typos\nTypo/gramma: Sentence starting on line 614: \"The proposed in this work method based on the Mahalanobis distance and spectral normalization of a weight matrix...\" ", "label": [[654, 791, "Eval_pos_1"], [794, 937, "Eval_pos_2"], [938, 1017, "Eval_pos_3"], [1068, 1251, "Jus_pos_3"], [1252, 1405, "Eval_pos_4"], [1406, 1432, "Eval_pos_5"], [1433, 1538, "Jus_pos_5"], [1539, 1572, "Major_claim"], [1595, 1639, "Eval_neg_1"], [1651, 1724, "Jus_neg_1"]]}
{"id": 308, "review": "paper_summary\nThe paper proposes a minor improvement to the existing word sense disambiguation approaches. Its aim is to overcome the problem of training data imbalance (not all words have the same number of senses, and this is related to their frequencies). \nEssentially, the authors assign different weights to words during training, depending on their frequencies. Empirically, this results in improvements in WSD performance on several English datasets. \nsummary_of_strengths\nThe proposed re-weighting approach is simple and focused, and the improvements are persuasive. I really like that it is linguistically motivated (based on Zipf's law) . \nsummary_of_weaknesses\n- Although the proposed approach does bring WSD improvements, it is rather incremental. This is probably not a \"weakness\" per se, just that the paper is not an eye-opener.  - The evaluation is done on English data only, which leaves some doubts about how this would work with other languages. \ncomments,_suggestions_and_typos\nL. 135: \" linguistic law exists in many corpora\" --> probably  \"linguistic law is manifested in many corpora\"? The whole statement is a bit strange: Zipf's law is well known to be manifested not \"in many corpora\", but in _all_ human languages, and thus, in any imaginable corpus of natural language texts. ", "label": [[480, 536, "Eval_pos_1"], [541, 574, "Eval_pos_2"], [575, 625, "Eval_pos_3"], [626, 645, "Jus_pos_3"], [674, 759, "Eval_neg_1"], [847, 965, "Eval_neg_2"]]}
{"id": 309, "review": "paper_summary\nThis paper augments the standard cross-modal retrieval framework with an additional codebook. The motivation is that with the codebook, the model's behavior can be interpretable. The proposed approach improves the performance over the baseline approach by a margin. \nsummary_of_strengths\n1. I like the idea of using codebook for augmenting the cross-modal representation learning. Interpretabliity is one of the major issues in the cross-modal learning. I am glad that the author tackles this problem. \n2. The codebook update rules/policies are straightforward. It is interesting to observe that the codebook is aligned with actions in Fig. 3. \nsummary_of_weaknesses\n1. L003 mentioned the proposed framework is a self-supervised learning framework. IIUC, the model still needs cross-modal (x-modal) alignment to train. Why is the proposed framework a self-supervised learning framework? \n2. It is unclear to me how the gradient back-prop in the equation of L165? \n3. Cross-modal code matching: The key of the proposed approach is the x-modal code matching. It seems the codebook should be large enough to cover all semantic information in the dataset. I wonder how to determine the codebook space? Would the initalization of the codebook affect the performance? What is the performance under different codebook size? \n4. The proposed approach achieves higher performance than the baseline. I wonder is it due to the additional codebook or the increased model capacity? \n5. The interpretation part is a little bit confusing. Fig.3 clearly shows the codebook is aligned with the action. However, even with Fig. 3, it is still hard to interpret the output embedding of f^A_{code}. Imagine that given an embedding and a codebook, I wonder how to interpret the embedding? \ncomments,_suggestions_and_typos\nI think this paper is well-written and easy to follow. ", "label": [[305, 394, "Eval_pos_1"], [395, 516, "Jus_pos_1"], [520, 575, "Eval_pos_2"], [905, 977, "Eval_neg_1"], [1487, 1537, "Eval_neg_2"], [1538, 1813, "Jus_neg_2"], [1814, 1869, "Eval_pos_3"]]}
{"id": 310, "review": "paper_summary\nAuthors present  a novel finetuning method, BitFit, where instead of finetuning the entire model the authors finetune only the `bias\u2019 terms and the final task-specific classification layer. The authors compare BitFit to other parameter-efficient finetuning methods such as adapters and sparse params and report that BitFit performs better than previous approaches and even better than full finetuning on certain tasks. Additional analysis on the change in values of layer wise bias terms reveals that finetuning a specific subset of bias terms can also lead to marginally lower performance as compared to finetuning all bias terms. For low to medium data regimes, BitFit performs better than full finetuning. \nsummary_of_strengths\n1. Simple and effective method of finetuning that performs better than competitive baselines. For low to medium data, BitFit has better performance than fine-tuning the entire transformer model. \n2. Author perform additional analysis on the change of bias terms and find that tuning a subset of bias terms also lead to marginally lower performance than finetuning all bias terms. \nsummary_of_weaknesses\n1. Sensitivity of results to random seeds is not clear. Would be helpful to report variance for finetuning under different random seed settings. \n2. The actual strengths or limitations of BitFit are unclear. To elaborate, in addition to reporting empirical numbers on multiple tasks an analysis of the kind of examples where BitFit performs well should be added. It would be helpful for the community to understand if there is a certain type of tasks/nature of examples,  where BitFit performs better or is comparable to full fine-tuning. \ncomments,_suggestions_and_typos\nMinor typo: Line 69: hard reason --> hard to reason ", "label": [[748, 838, "Eval_pos_1"], [839, 940, "Jus_pos_1"], [1150, 1203, "Eval_neg_1"], [1204, 1292, "Jus_neg_1"], [1297, 1355, "Eval_neg_2"], [1356, 1687, "Jus_neg_2"]]}
{"id": 312, "review": "paper_summary\nThe authors propose a framework for taxonomy expansion (TE). Particularly, they explore the training of two common operations in TE (attach and merge) together. Experiments are performed on three WordNet taxonomies and the results show improvements for attaching and competitive results for merging. \nsummary_of_strengths\n- The task is quite interesting and it is important for updating resources in different languages.\n-Unlike previous approaches, the proposed model is able to give answers to two operations. In addition, the performance in both tasks are improved or got encouraging results.\n-Diverse experiments conducted along the work. \nsummary_of_weaknesses\n- Some definitions and statements are not clear or well justified.\n-Lack of clarity in the definition of the input/outputs for each subtask \ncomments,_suggestions_and_typos\n063-065 Though most of the existing studies consider the expansion a regression problem ... -> Missing a reference to support this statement 081-082 TEAM that performs both the Attach and Merge operations together -> Performs attach and merge together or is trained together?\n092 Missing reference for wordnet definition 112-114 \"The taxonomy T is arranged in a hierarchical manner directed edges in E as shown in Figure 1.\" - > It doesn't seem clear.\n115 query concept q: Is this a concept or a candidate word? Your initial examples (Page 1) mention \"mango\" and \"nutrient\" and do not seem to be concepts according to your definition.\n188 the query synset sq -> Is this a query synset or a word that belongs to the synset (concept) X. I am not sure if I understand correctly but in the case of attach, the query concept is a (d, ss): definition, synonymns included in the synset. However, it is not clear to me if it is the same for merge as it seems like the query concept is (d, ss) but ss is just the word that you are removing.\n214 and the synset is represented by the pre-trained embedding of the synonym word itself. - > It applies for merge in most cases but it is not case for attach, right? Because attach considers candidate concept (that can be composed by a synonym set) 224 comprising of the node -> that comprises the node ... 245 The GAT is trained with the whole model?\nNeeds to be reviewed by a English native speaker and some sentences need to be rewriting for improving the clarity. ", "label": [[338, 367, "Eval_pos_1"], [371, 434, "Eval_pos_2"], [436, 525, "Eval_pos_3"], [526, 609, "Eval_pos_4"], [611, 657, "Eval_pos_4"], [682, 746, "Eval_neg_1"], [748, 820, "Eval_neg_2"]]}
{"id": 313, "review": "paper_summary\nThis paper focuses generally on the task of diverse decoding from conditional language generation models. Specifically, they introduce the idea of best-first search, which unlike traditional beam search involves exploring the output space using a depth-first path completion strategy. As this results in many candidate outputs, this work also utilizes path recombination to combine similar hypotheses. The recombination methods generalize previous work to account for merge a hypothesis with candidates from earlier timesteps, and then propagates any merges back through the original lattice; these changes allow for significantly more merges to occur. The authors evaluate the performance of these methods on document summarization and machine translation. \nsummary_of_strengths\nThis paper is very well-written, and clearly articulates the issues with not only beam search, but BFS approaches generally. The methods proposed are relatively novel, and show to explore significantly more of the search space than beam search, and explores in a more principal way than random sampling-based methods. The results indicate that using these methods results in significantly more diversity in the output candidates, which based on the higher oracle ROUGE scores, has the potential to improve performance given an additional re-ranking mechanism. This seems like it could be a non-trivial step forward for decoding strategies.\nThe figures are very compelling and help visualize the mechanisms that differentiate the proposed methods from previous work. Figures 11-13 in particular are fascinating visualizations of the different paths, I greatly appreciate the authors including them in the appendix. In addition, the included error analysis is helpful in identifying and motivating future work, potentially on when to reduce the aggressiveness of path merging. \nsummary_of_weaknesses\nThe paper makes several significant claims about best-first search and recombination being much more efficient than beam search, but the discussions on actual run-time are lacking. In the appendix the authors note that the best-first search code could be optimized relatively easily, but one question that doesn\u2019t seem to be answered is: compared to the baseline approaches, how much longer does it take to generate outputs, at least as the method is implemented now?\nWhile the potential for significant improvement on quality scores is there due to the significantly higher portion of the search space explored, the authors don\u2019t actually attempt any kind of re-ranking, which leaves an open question: are we able to actually extract the best output sequence from all the generated candidates? Did the authors try any simple re-ranking mechanisms to do this?\nFor evaluating diversity, one drawback of using distinct unigrams and bigrams is that it weights all n-grams that appear the same, which does not take into account the fact that infrequent n-grams the same. Thus, it would be good to also include Ent-n, and entropy-based metric introduced by Zhang et al (2018) that does account for this. \ncomments,_suggestions_and_typos\nOne previous decoding method used the combine similar outputs was to over-generate candidates, and then perform post-decoding clustering to group similar candidates together (Ippolito et al., 2019). While the methods proposed in this paper are distinct, these methods should be mentioned in the related work.\nMissing Citations: - Generating informative and diverse conversational responses via adversarial information maximization (Zhang et al., 2018) -Comparison of Diverse Decoding Methods from Conditional Language Models (Ippolito et al., 2019) ", "label": [[794, 825, "Eval_pos_1"], [827, 918, "Jus_pos_1"], [919, 960, "Eval_pos_2"], [962, 1111, "Jus_pos_2"], [1111, 1222, "Eval_pos_3"], [1224, 1433, "Jus_pos_3"], [1434, 1559, "Eval_pos_4"], [1560, 1707, "Jus_pos_4"], [1708, 1869, "Eval_pos_5"], [1892, 2072, "Eval_neg_1"], [2072, 2359, "Jus_neg_1"], [2360, 2563, "Eval_neg_2"], [2595, 2751, "Jus_neg_2"]]}
{"id": 314, "review": "paper_summary\nThis paper attempts to address potential catastrophic forgetting, data imbalance and rare word issues, for intent detection in the medical domain. These three concerns are addressed with appropriate losses (L_pl, L_fl, L_co; Sections 3.2/3.3). Evaluations were performed against prior models, for various numbers of classes. \nsummary_of_strengths\nStrengths of the paper include its addressing of several longstanding issues in incremental learning, including the rare word problem particularly relevant to the medical domain. \nsummary_of_weaknesses\nSome concerns remain over the empirical evaluation, as detailed in the comments. \ncomments,_suggestions_and_typos\n1. The fairness of comparison against prior methods might be clarified. For example, EMR is stated to \"randomly stor[e] a few examples of old classes\" (Line 254); one might therefore expect the performance of EMR to possibly improve (significantly), as the quantity of stored examples of old classes increases.\nBasically, it is difficult to confidently evaluate the superiority of the proposed CRN over prior methods, without taking into consideration the amount of data available to each of these methods. For example, CRN implements memory by storing the top n (prototype) examples (Line 160) for each class - do the other methods such as EMR also have access to n examples?\nIn summary, the relationship between storage capacity (n, apparently maxing out at Upperbound in Figure 2) and performance appears an important factor to explore for the various memory-based models, since the memory-performance tradeoff is central to such models (as otherwise there appears no need to innovate if all old samples can just be stored). While a brief exploration is provided in Figure 3 in the Appendix, it is against a single model on a single dataset, and only up to a relatively limited range of storage (i.e. n=200) 2. For the ablation experiments in Table 2, it might be considered to include various (valid) combinations of exclusions (e.g. w/o PL and w/o FL); in particular, the performance for the baseline memory-based model (i.e. without any of the proposed extensions) would be important in benchmarking against the comparison models.\n3. The effect of the various losses towards their intended function has not been thoroughly covered. For example, it is claimed that \"The performance drops with removing Lco (\"w/o CO\"). It demonstrates that it is helpful to handle medical rare words\" (Line 292). However, this is not substantiated with an analysis of performance on actual items with rare words. These instances might be addressed.\nMinor grammatical/spelling concerns: (Line 97) \"Medica intent\" (Line 124) \"Contrastive Reply Networks\" (reply or replay?) \n(Line 161) \"...top n example(s)\" etc. ", "label": [[361, 540, "Eval_pos_1"], [563, 644, "Eval_neg_1"], [680, 2213, "Jus_neg_1"], [2217, 2314, "Eval_neg_2"], [2314, 2612, "Jus_neg_2"]]}
{"id": 315, "review": "paper_summary\nThis paper proposes an in-depth study of the impact of translationese on machine translation performance from a causal point-of-view, i.e. how the alignment of direction (from originally authored to translated text and vice-versa) of the model and data involved in the translation process influences BLEU scores. This differs from previous work which mostly focused on the alignment direction between the translation model and the test set.\nThe authors built an annotated corpus based on Europarl to control for variables such as original or translationese, content in terms of topics, and sentence length, and extracted comparable sub-corpora of aligned and unaligned directions to empirically show the following: aligned training and test directions perform best, alignment direction is also relevant when producing synthetic data with supervised training and back-translation, but the impact of data and model alignment varies among language pairs and translations when considering other factors.  The evaluation is conducted on five language pairs in ten translation directions with a test set sampled from the corpus built by the authors, as well as on the newstest2014 from WMT in two language pairs with four translation directions. \nsummary_of_strengths\nEmerging from the presence of various directions (mixed set) in WMT test sets, recent studies on the impact of translationese on machine translation were mostly limited to the test-to-model alignment impact.  The authors of this paper, however, aim at providing a deeper understanding of previously observed results, due to the fact that the relation between training and test directions alignment as measured by automatic metrics is still unclear. This is especially true with the large amount of synthetic data produced and used in machine translation nowadays.  An additional contribution, backed by further experiments, is the exploration of causal and anticausal learning and their impact on translation performances (based on an automatic metric) when various data directions are used, which appears to be positive or negative depending on the translation direction and language pair.  Finally, the claims made by the authors throughout the paper are supported by a solid set of experiments conducted on various translation directions, the paper is very well written and straightforward to follow. \nsummary_of_weaknesses\nDue to the difficulty to build a corpus as employed in this study, the amount of data used to train the machine translation systems is relatively small when compared to publicly available corpora used for the same language pairs by the research community. Thus, the conclusions drawn by the authors might be limited to low or average data settings.\nAlso, a popular method usually employed to mitigate the alignment mismatch between train and test directions is to add a tag per sample specifying the direction. Commonly used when adding back-translations to the training data, this method could contrast with the current set of experiments and provide an interesting comparison point. The question would be, is the machine translation system able to model the direction and potentially abstract from train-test or data-model alignment mismatch. \ncomments,_suggestions_and_typos\nBecause speakers at the European Parliament might not be native speakers of the language in which the transcription are written in, wouldn\u2019t the Europarl corpus contain text annotated as originals but from non-native speakers?\nThe information about the passive voice checker in English is missing.\nDark colors in tables tend to make the numbers difficult to read. ", "label": [[1841, 2166, "Eval_pos_3"], [2177, 2316, "Eval_pos_1"], [2318, 2380, "Eval_pos_2"], [2403, 2658, "Eval_neg_1"], [2659, 2751, "Jus_neg_1"]]}
{"id": 317, "review": "paper_summary\nThis paper describes a novel technique for improving knowledge-grounded conversational systems by incorporating *personal memory* into the process.  Personal memory, defined abstractly as the influence of previous personal experience, preference, and values when interpreting dialogue context and selecting follow-up dialogue, is represented in this work as user-specific utterance histories under multiple types of context.  The paper's key contributions include: -A new dataset, scraped from Reddit, providing user-specific personal memory to function as a training and testbed for this task -The first approach for introducing personal memory into knowledge-grounded conversational systems The approach, a variational method that models interdependencies between persona and knowledge using latent variables and employs dual learning to jointly optimize the relationship between dialogue context, personal memory, and knowledge, achieves strong performance on the new dataset relative to competitive baselines under both automated and human evaluation paradigms. \nsummary_of_strengths\nThe paper is for the most part clearly written, and tackles an interesting topic that is likely to be of interest to the conversational AI community.  Key strengths include: -The proposed methods are described thoroughly.  They seem to be sound and well-suited for the intended purposes.\n-A strong evaluation is conducted, comparing the proposed model to a suite of competitive baselines including a Generative Profile Memory Network (Zhang et al., 2018), Transformer Memory Network (Dinan et al., 2019), transfer learning-based Transformer model (Wolf et al., 2019), Sequential Knowledge Transformer (Kim et al., 2020), KnowledGPT (Zhao et al., 2020), a custom variation of KnowledGPT designed to incorporate personal memory, a transmiter-receiver based framework (Liu et al., 2020), and a recently published persona-based baseline (Song et al., 2021).\n-Further analyses are conducted to establish the individual contributions of model components and additional hyperparameters. \nsummary_of_weaknesses\nDespite its strengths, there are also several areas that offer opportunities for improvement in future revisions: -In general, the writing quality seems weaker in the latter part of the paper (from the experimental results onward).  Revising this section could benefit the overall clarity of findings and subsequent takeaway points.\n-Some aspects of the dataset are unclear.  For instance: (a) following ACL ethics guidelines, was approval granted by an external institutional review board to collect and publish this data?; ( b) what other requirements are there for data release (e.g., will a public link be provided, or will the data be available upon request from the authors)?; ( c) how many annotators labeled each sample (and how many annotators were there in total)?; and (d) were all samples in English?\n-Some of the presented results seem very close, particularly in the ablation study.  It is unclear whether the results in Table 5, for example, are statistically significant; if not, it may be good to temper the claims made based on these findings. \ncomments,_suggestions_and_typos\nTwo typos that could be addressed: -Line 278: The word *the* is repeated -Line 321-323: The text seems to indicate that $p_{\\theta}(Z^{p})$ is shorthand for $p_{\\theta}(Z^{p})$.  Should the second instance of this be something else? ", "label": [[1102, 1148, "Eval_pos_1"], [1154, 1251, "Eval_pos_2"], [1277, 1323, "Eval_pos_3"], [1325, 1389, "Eval_pos_4"], [1391, 1424, "Eval_pos_5"], [1426, 1953, "Jus_pos_5"], [1957, 2082, "Eval_pos_6"], [2220, 2296, "Eval_neg_1"], [2338, 2437, "Jus_neg_1"], [2439, 2480, "Eval_neg_2"], [2481, 2917, "Jus_neg_2"], [3003, 3092, "Eval_neg_3"]]}
{"id": 318, "review": "paper_summary\nThis paper provides a large new dataset written in Hanja, an old Korean language not fully understood yet by modern Korean speakers. This dataset comprises three datasets: AJD, DRS, DRRI (DRRI was proposed in this work). The authors investigate the performance of two pretrained language models (PLMs) based on BERT on this data by first continuing the pretraining of the PLMs and later finetuning them to perform a series of supervised tasks. \nThe authors considered two such models: mBERT and AnchiBERT (pretrained on ancient Chinese data). As for the tasks, the authors propose extracting labels from the dataset structure and realizing supervised learning for king prediction (era prediction), topic classification, NER, and summary retrieval. \nThe paper provides a thoughtful set of experiments evaluating the performance of these models with and without finetuning on Hanja data, including zero-shot experiments on DRRI. Later, the authors also investigate the impact of time and historical events using the best performant model (finetuned AnchiBERT). \nsummary_of_strengths\n- The paper provides a large new dataset for a low-resource language (Hanja) -The paper does an excellent job at using state-of-the-art approaches for investigating linguistic structures of the datasets, including the impact of name entities and document ages on classification -The presentation of the results is simple and yet comprehensive \nsummary_of_weaknesses\n- The paper would be easy to follow with an English-proofreading even though the overall idea is still understandable.\n-The new proposed dataset, DRRI, could have been explored more in the paper.\n-It is not clear how named entities were extracted from the datasets. \ncomments,_suggestions_and_typos\nAn English-proofreading would significantly improve the readability of the paper. ", "label": [[1097, 1171, "Eval_pos_1"], [1173, 1373, "Eval_pos_2"], [1374, 1438, "Eval_pos_3"], [1463, 1579, "Eval_neg_1"], [1581, 1656, "Eval_neg_2"], [1658, 1727, "Eval_neg_3"], [1760, 1842, "Eval_neg_4"]]}
{"id": 319, "review": "paper_summary\nThis paper digs into the evaluation of Non-AutoRegressive machine translation model and reveals several flaws in standard evaluation methodology, to provide a better understanding of the merits and weakness of NAR model.\nA more fair and detailed comparison shows that the speed gap between AR and NAR models quickly shrink in more realistic usage conditions (larger batch or multi-core CPUs), which is kind of against the common belief that NAR model is significantly faster. \nsummary_of_strengths\nThis paper shows a bigger picture in the speed evaluation of NAR models, with more settings and test-sets, we can get a better understanding of the merits and flaws of NAR models. I think it will have a positive impact on the research of NAR models.  And the experiment for comparison is solid and fair. \nsummary_of_weaknesses\nMy major concern of this paper is the innovativeness.  I think the contribution of the paper lies in more on the engineering side: results on more test-sets to avoid overfitting, speed comparison not only on 1-batch GPU setting but also on x-batch and multi-cpu settings, etc. However, while a lot of content is on engineering side, there is very few new findings on algorithm or model side: the CTC model is not new, and no new evaluation metrics is proposed. \nSo I think the paper would be better to appear in related workshop, rather than a ACL long paper. \ncomments,_suggestions_and_typos\n1. In Table 3, the COMET score is added without enough explanation: why choose this metric and how it fits into this evaluation?   And with the observation of substantial difference in COMET and BLEU score, the author suggest that NAR models may rank poorly in human evaluation, why not add the human evaluation in the paper?  If it does rank poorly, it may reveal another important feature of NAR model. ", "label": [[512, 761, "Major_claim"], [763, 816, "Eval_pos_1"], [839, 892, "Eval_neg_1"], [894, 1300, "Jus_neg_1"], [1301, 1399, "Major_claim"]]}
{"id": 320, "review": "paper_summary\nThis paper addresses cross lingual summarization task. This task is generating a summary in a target language from a source document in a source language. In other words, the cross lingual summarization is the combination of machine translation and summarization.\nSome previous studies regarded the cross lingual summarization as the combination task of machine translation and summarization, and proposed multi-task learning methods. \nIn contrast, this study regards the cross lingual summarization as a super-task of machine translation and summarization, and proposes a hierarchical model to control them. The proposed method uses two kinds of latent variables: global and local. A global corresponds to cross lingual summarization, and a local corresponds to machine translation and summarization. This paper applies conditional variational auto encoder to treat these latent variables.\nExperimental results show that the proposed method achieved better performance than previous methods in automatic and human evaluation. \nsummary_of_strengths\nThe core idea, that regards cross lingual summarization as a super-task of machine translation and summarization, seems reasonable. In addition, it is a reasonable approach to apply conditional variational auto encoder to the cross lingual summarization.\nThe proposed method achieved better performance than previous methods in automatic and human evaluation. \nsummary_of_weaknesses\nSince explanations on experiments contain unclear points, I'm not sure whether the proposed method achieved better performance than previous methods in a fair comparison. \nFor training datasets, the proposed method seems to be using only cross lingual summarization data but MS-CLS, MT-CLS, and MT-MS-CLS use machine translation, summarization, or both datasets based on descriptions in 4.2. \nI checked Appendix A but it does not contain details explicitly such as the number of sentences for each task.\nIn addition, the parameter sizes of each method might be different from each other. The proposed method share parameters of an encoder but this paper does not explain other methods explicitly. Thus, I cannot judge whether the proposed training strategy improves the performance or sharing parameters improves the performance (or other reasons for improvements). \nI recommend explaining the number of sentences and parameter sizes for each method explicitly.\nMoreover, I suppose that the proposed method requires much more training time than previous methods. If so, the authors should discuss the relation between training time and performance because a view on Green AI is important.\nFor the human evaluation, the authors used only 25 samples but the number of samples are too small. I doubt that we can conclude the effectiveness based on such small instances. \ncomments,_suggestions_and_typos\nI'm not sure why the authors describe pipeline methods in Table 1. \nZhu et al., 2019 indicated that end-to-end methods achieved better performance than pipeline methods. \nI think that the results in Table 1 are copied from their paper because scores are identical to ones of them. \nThus, I cannot understand why authors re-compare pipeline methods (with reported scores) with end-to-end methods.\nIn addition, I doubt that the above pipeline methods consist of weakly methods. \nIf the authors conduct a fair comparison, the authors should prepare strong methods for machine translation and summarization. \nFor example, initializing each model with mBART and fine-tuning them on training data of each task.\nFor explanations in Section 4.3, the authors should list each method for readability such as using \\item. ", "label": [[1063, 1194, "Eval_pos_1"], [1195, 1317, "Eval_pos_2"], [1318, 1423, "Eval_pos_3"], [1446, 1502, "Jus_neg_1"], [1504, 1617, "Eval_neg_1"], [1618, 2813, "Jus_neg_1"]]}
{"id": 322, "review": "paper_summary\nThe authors proposed Context-Aware Language Models (CALM).\nAs far as I understood, CALM changes (relabels) DB in the failed dialogue to make the dialogue successful (dialog task relabeling).\nAs dialog task relabeling is not enough to make successful performance, three additional auxiliary tricks are suggested: task-specific auxiliary loss, task pretraining, and model-based dialogue rollouts.  The proposed method is evaluated in AirDialogue dataset. The authors argue that, in the self-play scenario of AirDialogue dataset, the proposed method achieves state-of-the-art performance and achieved human-level performance. \nsummary_of_strengths\n- The idea of dialog task relabeling is novel. \nsummary_of_weaknesses\n- It is a little bit hard to understand the paper     - I think the proposed idea can be easily explained without POMDP or RL. It is also less persuasive for me that the proposed method is described from the perspective of RL. Thus, I feel the description on POMDP could be removed. \n    - Some relatively important description goes to Appendix, which makes reduced readability.\n-Motivation of the methods is less persuasive. \n    - It is weird for me that dialogue without success becomes successful dialogue by changing the contents of DB. \n    - An example of relabeled dialogue (in Appendix) would help.\n-The used dataset is not been frequently used recently, and recent TOD methods are not also been evaluated. \n    - It seems it is necessary to explain why the method could not be compared to mainstream datasets (e.g., MultiWOZ 2.0) and methodologies.\n-I feel evaluation with self-play is less confident. \n    - Though, previous works evaluate their methods by self-play, I feel that human evaluation is required for robust evaluation. \n    - I think that it is relatively easy to fit the self-play scenario. \ncomments,_suggestions_and_typos\nSee the Summary of Weaknesses ", "label": [[661, 706, "Eval_pos_1"], [730, 778, "Eval_neg_1"], [785, 1107, "Jus_neg_1"], [1109, 1154, "Eval_neg_2"], [1162, 1336, "Jus_neg_2"], [1338, 1445, "Eval_neg_3"], [1452, 1587, "Jus_neg_3"], [1588, 1641, "Eval_neg_4"], [1646, 1845, "Jus_neg_4"]]}
{"id": 323, "review": "paper_summary\nThis paper presents a data augmentation with ranking-based learning for open relation extraction. In particular, the authors argued the following contributions: 1. three types of data augmentations, including back translation as positive, entity replacing as hard negative, entity swap for semi-hard negative 2. ranking-based loss function with both euclidean distance in the feature space and KL divergence in the label space. \nThe proposed model shows the state of the art results on T-REx SPO and T-REx DS \nsummary_of_strengths\n1. Results over two datasets show significant improvements over recent state-of-the-art models. \n2. Paper contains extensive results for a short paper and is easy to read. \nsummary_of_weaknesses\n1. This paper proposed three different types of data augmentation methods. Both hard negative and semi-hard negatives focus on replacing the entity mentions. However, it is not well-explained why this can help to cluster the relational expression. Does this lead to a model which focuses on entity mention detection rather than relation patterns? From my understanding, replacing entity mentions of the same context should not largely change the relation semantics.\n2. I think back translation plays a key role in this paper. I think it is necessary to add an ablation study to exclude the hard and semi-hard negatives. In particular, how does the model perform when both formula 1 and 2 only include t and t^p and use in-batch negatives (similar to contrastive learning). \ncomments,_suggestions_and_typos\n1. It is not clear to me why hard negatives and semi-hard negatives are generated from the positive sentence rather than the original sentence. Wouldn't there exist error propagation if the back translation is noisy? \n2. It seems that T-REx is a distantly supervised dataset with no human labels. am I correct? If so, is it possible to evaluate over annotated dataset? ( For example TACRED or FewRel?) ", "label": [[548, 641, "Eval_pos_1"], [645, 717, "Eval_pos_2"], [742, 987, "Eval_neg_1"], [988, 1205, "Jus_neg_1"], [1265, 1360, "Eval_neg_2"], [1360, 1513, "Jus_neg_2"], [1548, 1680, "Eval_neg_3"], [1690, 1763, "Jus_neg_3"]]}
{"id": 324, "review": "paper_summary\nThis paper studies the discourse structure of long-form answers by analyzing two datasets: ELI5 and Natural Questions (NQ). The authors define six sentence-level discourse roles for long-form answers and ask human annotators to label a total of 1.3K examples. The authors further conduct experiments on two tasks based on the annotated corpus: automatic discourse analysis and summarizing long-form answers. \nsummary_of_strengths\n- To my belief, this is the first work that analyzes the discourse structure of long-form answers. It calls our attention that better understanding discourse structure might benefit long-form QA.  -The data annotation process is described in detail and inter-annotator agreements are fully revealed.  -It did some initial studies on how machine-generated answers and human-written answers differ in discourse structure.  -The paper is fairly easy to follow. \nsummary_of_weaknesses\nHowever, the work leaves a few things to be desired, let me order them from most to least important.\n- I am not clear about the goals of the paper. The authors argue that a better understanding of the discourse structure of long-form answers can benefit the long-form QA systems or the evaluation of long-form answers. However, this end goal is not covered in the paper. The authors did not perform further experiments to show whether a long-form QA system can benefit from that discourse-level information to generate better answers.  -The six sentence-level discourse roles defined by the authors are still coarse-grained to me. It is also not surprising that most long-form answers are consist of answer summary, auxiliary information, and examples. I did not find many motivating insights from the analysis that could potentially benefit the future work of long-form QA.  -Section 5 is good to me. It analyzes the weaknesses of current long-form QA systems in discourse structure. However, the authors only analyzed 52 examples. Such a small sample size may not be enough to make a statistically significant observation. \ncomments,_suggestions_and_typos\nPutting together all the strengths and weaknesses I believe the authors proposed a new angle to study long-form QA and the NLP community will benefit from the annotated corpus. However, there are some problems that do not allow me to accept the current version. I think there are two directions the authors can work on to improve the paper:  - Conduct more in-depth or fine-grained analysis on the discourse structure of long-form answers, which hopefully would provide more insightful observations that can benefit the long-form QA community. For example, analyzing how the discourse structure of long-form answers differs in different domains and in different question types.  -Make this paper a closed loop. That is to show the additional discourse structure information can benefit the long-form QA system in terms of either improving its performance or facilitating the evaluation of long-form answers. ", "label": [[446, 638, "Eval_pos_1"], [642, 743, "Eval_pos_2"], [746, 863, "Eval_pos_3"], [866, 902, "Eval_pos_4"], [1027, 1072, "Eval_neg_1"], [1073, 1461, "Jus_neg_1"], [1462, 1555, "Eval_neg_2"], [1555, 2037, "Jus_neg_2"], [2083, 2344, "Major_claim"]]}
{"id": 326, "review": "This paper is about named entity (genes, diseases) recognition of Chinese biomedical texts. The authors present a large collection of unannotated patents, and a small subset (5k sentences, 21 documents) manually annotated. The paper also includes experiments with named entity recognition, and relation extraction (between the named entities of interest).\nThe data set presented (released publicly) is likely to be useful for others; the NER method follows a rather standard, but reasonable, approach; and the relation extraction experiments, although preliminary, may be of interest to NLP community as well. The paper is generally well structured, and well written (see below for a few detailed comments).\nMy only (minor) reservation of the paper is that it may be slightly too specific and technical for COLING. The paper mainly is about a practical NLP problem, involving rather little (computational) linguistics.\nI have some detailed comments/suggestions below.\n- I could not fully understand the averaging. In particular, does the   averaged scores on Table 4, include 'O' label? It seems not (good),    but it would be best to explicitly state it. Related to this, is   there a reason for reporting micro-averaged scores? Clearly there is   no clear preference towards recognizing majority class(es) better   than the minority class. \n   -The NER results presented show really large variation over the cross    validation folds. With the variation at hand, I think it is very   difficult to decide the \"best\" model with certainty. Hence, I   suggest: (1) softer conclusions, and (2) maybe trying model   averaging or a similar method to reduce the variation. I definitely   like that authors reported the variation in the scores, the   suggestions should not be understood as artificially reducing it   (e.g., testing on a single validation split), or definitely not    reporting the variation. \n   -The authors report that they reserve one of the documents as 'dev   set'. However, the results presented are 5-fold CV results, and   there is no report of additional hyperparameter tuning. Hence, it   sis not clear what the purpose of the dev set is.\n- The 'mutual F1 score' used for IAA is not defined, nor there is a   reference in the paper.\n- More a question than a criticism: the authors present code-mixing as    a difficulty. Isn't code-mixing helpful for NER? I'd expect more   code mixing in named entity boundaries, hence, overall I expect it   to be helpful (although, it may be specific to the domain/task).\n- I suggest also specifying the sources (BC/HG) of the annotated data    set in Table 2 (like in Table 3).\n- Typos:     - I suggest use of more descriptive words or abbreviations in       table headings. \n    - Section 3.2, paragraph 3: \"entity type type\" -> \"entity type\"     - Suggest using (or not using) punctuation after captions       consistently. \n    - The references contain some lowercase \"named entities\", like       'bert', 'chinese' (likely BibTeX normalization issues), I       recommend a through check. ", "label": [[356, 432, "Eval_pos_1"], [434, 500, "Eval_pos_2"], [502, 609, "Eval_pos_3"], [610, 707, "Eval_pos_4"], [708, 813, "Eval_neg_1"], [815, 918, "Jus_neg_1"], [970, 1013, "Eval_neg_2"], [1014, 1342, "Jus_neg_2"], [1347, 1436, "Eval_neg_3"], [1437, 1903, "Jus_neg_3"], [1908, 1981, "Eval_neg_4"], [1982, 2159, "Jus_neg_4"], [2162, 2253, "Eval_neg_5"]]}
{"id": 328, "review": "This paper describes a multi-faceted project to provide tools to support a range of indigenous languages spoken across Canada. The paper is not analytical but rather descriptive of the various tools that have been implemented so far.  The paper is well-written and clear, and provides a nice overview of the work that has been done. There are no particularly innovative or novel solutions developed or applied. Would make a good poster.\nIn my view, the paper would have benefited from a few examples of the linguistic phenomena that are described (e.g. subsets of verb conjugations). ", "label": [[235, 271, "Eval_pos_1"], [276, 332, "Eval_pos_2"], [333, 410, "Eval_neg_1"], [411, 436, "Major_claim"], [437, 546, "Eval_neg_3"]]}
{"id": 329, "review": "The goal of the paper is to evaluate the performance of five unsupervised representation learning techniques for detecting stances of Fake News, and to understand how much hyperparameter fine-tuning is needed for the five pre-trained architectures. The models compared in the paper are BERT, RoBERTa, DistilBERT, ALBERT and XLNet, Results (shown in Table 4) show that RoBERTa achieves the best performance. \nIt is a comparative paper on hyperparameter tuning and its main contribution is to interpret how different parameter values affect the performance of the different models. These insights may benefit the community.  The main strength of the paper is the clarity: the paper is clearly presented and well-reasoned. It is easy to follow and possibly easy to replicate. To this end, the authors have set up a GitHub link, but since it is anonymised it cannot be browsed.  I would suggest the following, if the paper is accepted for publication:  1) Replace \"n't\" with full form: academic papers are still a quite formal genre. \n2) Replace the section title \"Summary\" with \"Conclusion\" and expand it. For example, say in which way the presented results can benefit the community, whether future experiments of the same kind are planned on another task or using other datasets or for another downstream task. Fine-tuning of pre-trained models is a pervasive problem: is there a way to generalize on the results presented on the paper? Or are these task- and dataset-specific? etc.   3) Update the preliminary results in Table 3. \n4) Add a table or an appendix where the hardware characteristics (computer resources) are shown (eg. CPU, GPU, Clock Speed, Ram etc). \n5) Add a table or an appendix where the the processing time of the performance (first row in Table 4) is reported for each model. ", "label": [[580, 621, "Eval_pos_1"], [623, 668, "Eval_pos_2"], [670, 874, "Jus_pos_2"], [875, 906, "Eval_neg_1"], [949, 1795, "Jus_neg_1"]]}
{"id": 330, "review": "This paper explores the ability of an LSTM to learn linguistic dependencies beyond what is seen in natural language and cast light on the flip side of the flexibility and generalizability of these models. The authors show that their LSTM learn number agreement equally well across three dataset modifications that would cause most humans trouble. They analyse the weights responsible for number agrement of the models and find that the weights of the naturally occurring structures are very similar to those of the unnatural structures.  This suggests that such models learn language differently from humans and discourages research that compare human language learning to that of an LSTM.\nStrengths: Very well written, clear and compelling. \nAn extensive introduction to the problem and the approach. \nRevealing and intriguing results Weaknesses: The authors refer to Gulordova et al. (2018) for the LSTM architecture but I miss a brief overview of the architecture in this paper.\nEdits: domain general -> domain-general (Gulordova et al., 2018)  (beginning of section 3) -> Gulordova et al. (2018) weight based -> weight-based ", "label": [[701, 742, "Eval_pos_1"], [743, 802, "Eval_pos_2"], [803, 835, "Eval_pos_3"], [848, 981, "Eval_neg_1"]]}
{"id": 332, "review": "The paper consider the relation classification problem in the presence of unseen (novel) relations in a zero-shot setting. Compared with existing approach leveraging external artificial descriptive information, the proposed approach considers external knowledge graph embedding and further the logical rule mining to improve relation representation. The approach is novel, and extensive ablation studies show the effectiveness of proposed approach.\nThe idea of leveraging knowledge is novel. The paper reads good, however it might be difficult for readers new to zero shot learning.   Comments, -Zero-shot as the backbone of the approach is not clearly described. Despite the description of related works in Sec. 2. and math equations in Eq. (2) and Eq. (3), it is still hard to get the essence and motivation of zero-shot learning given the current manuscript. As a consequence, I am unsure about what is needed for zero shot learning and what is learnt  and what is the training objective function given Eq.(2) or Eq.(3) during the training phase.\n-It might be better to provide comparison with the existing approaches (Levy et al 2017 and Obamuyide and Vlachos, 2018) leveraging different external materials given the same training set.\nSome typos, -Fig. 1 is squeezed -There is no space between \u2018(\u2019 and the preceding letter in Sec. 3.2 -Some symbols above Eq.(5) is not in math environment. ", "label": [[350, 371, "Eval_pos_1"], [377, 448, "Eval_pos_2"], [449, 491, "Eval_pos_3"], [492, 512, "Eval_pos_4"], [514, 582, "Eval_neg_1"], [596, 1049, "Jus_neg_1"]]}
{"id": 333, "review": "The paper addresses the problem of ASpect-Category Sentiment Analysis, where implicit aspects appearing in Laptop and Restaurant Reviews datasets denotes the main challange.\nThe paper is well written and clear. The use of multiple examples improves the clarity of the definitions and the analysis of results.\nThe authors provide extensive evaluation on multiple proposed approaches using two datasets.\nIn addition, the work provides nice ablation experiments in addition to focusing on the performance of the proposed techniques on the challenging portion of the dataset that includes implicit aspects. \nThus, providing interesting insight on a challenging direction.\nYet, the only weakness I see is that the baseline methods compared to Hier-*-BERT are somehow weak(Table2). \nHave you checked the main reasons/challenges in misclassifying the explicit and implicit aspects?\nI suggest placing the related work section at the beginning of the paper. ", "label": [[174, 210, "Eval_pos_1"], [211, 308, "Jus_pos_1"], [415, 667, "Eval_pos_2"], [668, 766, "Eval_neg_1"]]}
{"id": 334, "review": "The authors present a refined study on summarization evaluation metrics. The work is straightforward and well presented. The main result I would say is that we need to collect human judgements, which this, and most other attempts to assess the quality of automatic summarization techniques unfortunately does not. ", "label": [[73, 100, "Eval_pos_1"], [105, 120, "Eval_pos_2"], [121, 314, "Eval_neg_1"]]}
{"id": 335, "review": "This paper contributes a new task, dataset, and experimental results on Chinese lexical fusion resolution.\nSection 2 presents the phenomenon: lexical fusion is a phenomenon in Chinese where a combination of words (called the \"separation words\", typically a VP, it seems) is referred to anaphorically by a \"fusion word\". Fusion words are formed ad-hoc and are therefore often out-of-vocabulary; they consist of multiple (typically 2) characters, each of which corresponds to one of the separation words. Often, the characters of the fusion word are characters from the corresponding separation words, but they can also be characters that are in some purely semantic relationship with the corresponding separation word.\nSection 3 presents a machine learning model to resolve this type of anaphora. It consists of a character-wise BERT encoder, a CRF decocer for mention detection, i.e., labeling spans of characters as either separation or fusion words, and a biaffine model for anaphora resolution, i.e., to recognize which fusion word refers to which separation words. To capture semantic relationships between fusion word characters and separation words, each character is assigned an additional semantic representation that is fed into the model. This semantic representation is obtained by looking up every word that the character could be part of in this context in the HowNet lexical database, further mapping each of these words to its HowNet representation as a \"sememe graph\", and then aggregating a vector representation via sememe embeddings and graph attention networks.\nSection 4 presents a test set created by manually annotating 17,000 paragraphs from the SogouCA news corpus. It also presents an artificial training set constructed by generating possible triples of fusion word and separation words using an electronic lexicon, and using those triples as seeds for finding presumed occurrences in a large corpus. To assess the impact of sememe knowledge, the full model is compared with various degraded variants and combinations thereof: one where the expansion to characters into words is skipped and only characters (1-character words) are looked up in HowNet, one where the structure of of HowNet's sememe graphs is ignored and complete graphs between the included sememes are used instead, and one where sememe knowledge is left out entirely. It is found that the model has the highest accuracy when using the full model, confirming the usefulness of fine-grained sememe knowledge for this task. An analysis shows that in-vocabulary fusion words are resolved with greater accuracy than out-of-vocabulary ones, and that fusion word characters that are borrowed from their respective separation word are correctly mapped with much greater accuracy than ones where the relationship is purely semantic, despite using sememe information. An example of how senses with shared sememes across triples can mutually enhance their attention weights is shown. It is also found that anaphors are more difficult to resolve than cataphors, that resolution becomes more difficult as the distance between the two separation words increases, and that the model performs better when mention detection and anaphora resolution is done jointly rather than in a pipeline.\nI think it's a good and important paper, as it tackles an apparently understudied phenomenon in the Chinese language, resolution of which is important for natural language understanding. The technical choices seem well motivated and are presented fairly clearly. The promised release of the datasets will be a valuable contribution. And the analysis is fairly extensive. However, there are also several ways in which the paper could be improved: - Surely Chinese lexical fusion has been described in the linguistic literature? A reference should definitely be given. Also, the claim \"Lexical fusion is used frequently in the Chinese language\" should be backed up either by a reference or by data, e.g., from the annotation project presented here.\n-Lexical fusion should be more clearly defined. From the examples given in Section 1-2, I get the impression that the antecedent is always a VP consisting of a single-word verb and an adjacent single-word nominal object. But apparently the separation words need not be exactly two and need not be adjacent? Examples of this should be given as well, and it should be explained what possible syntactic types the antecedent can have.\n-Relatedly, the annotation guidelines should be explained. Specifically, what counts as lexical fusion for the purposes of this dataset? Are there edge cases that were excluded?\n-Relatedly, the annotation process should be explained in more detail. How many annotators checked each paragraph? What was done with paragraphs with disagreement?\n-The test set does not seem to contain any paragraphs without lexical fusion, so it is presumably rather skewed. In a more realistic setting, paragraphs without lexical fusion would not be removed from the test set.\n-I would like to see more detail on the construction of the artifical training dataset. How do you \"check if [a two-character word] can be split into two separation words\"?\n-Terminology: in my mind, the reference is from the anaphor to the antecedent, so shouldn't cataphors be called forward references and anaphors backward references, rather than the other way around?\n-Terminology: \"separation words\" sounds a bit odd, it seems to imply that these words are the result of a separation process. How about \"antecedent words\"?\nSmall stuff: - The bibliography entry for Mitkov (1999) is broken.\n-p. 4 overall -> over all -Typography: please ensure that opening parentheses (e.g., in citations) are preceded by spaces. ", "label": [[3269, 3308, "Eval_pos_1"], [3310, 3455, "Jus_pos_1"], [3456, 3531, "Eval_pos_2"], [3531, 3601, "Eval_pos_3"], [3602, 3639, "Eval_pos_4"], [3640, 3714, "Eval_neg_1"], [3715, 5532, "Jus_neg_1"]]}
{"id": 336, "review": "From a general point of view, the paper describes the effect of the teacher-forcing technique employed in training a recurrent neural network model on a sequence-to-sequence task. The teacher-forcing technique (i.e. the use of the gold-pattern to be used as input at time t) increases the exposure bias of the model (indeed the training condition is different with respect to the test condition, where the output prediction at time (t-1) is used as input at time t, hereinafter the \"student-forcing\" technique), making the model less prone to correctly predict unseen sequences. Low-resource settings perform better with the \"student-forcing\" technique, while high-resource settings benefit of the \"teacher-forcing\" technique.\nThe authors argues that the teacher-forcing technique may cause the overfitting on the training data, due to an increase of the exposure bias especially in low-resource settings. The authors tested such hypothesis on a morphological inflection task, modelled as sequence-to-sequence task at character level and employing a mixture of the two forcing methods during training.\nThe paper is clear and interesting, my main concern being the fact that the choice between the teacher and the student forcing technique remains open and highly dependant on the specific task and the size of the training data. ", "label": [[1102, 1136, "Eval_pos_1"], [1138, 1328, "Eval_neg_1"]]}
{"id": 337, "review": "The authors try to propose a holistic taxonomy for text mining features with a meta-analysis method. The proposed taxonomy is helpful for understanding different types of text features, but its novelty is limited as the paper just combines and summarizes text features used in some other studies. Moreover, the investigation process has some flaws. How did you choose the 211 papers and why? With what search terms? Do you specify any other search conditions? We can imagine that there are much more papers relating to text features. For a meta-analysis study, it is crucial to choose important and representative studies in the literature (if we cannot cover all of them). Features have also been widely studied in machine learning domain, so results from the ML community could be useful as well. For example, as to the representation dimension, nominal values (e.g. topic or domain) may exist in many text mining applications. How do we process this kind of features? It would be also expected to know how the proposed taxonomy could guide the design of a text mining application.\nSection 4, as step 3 of the investigation process, should be placed under section 3.\nTypos: 1. page 5, line 1, \"136 papers\" => \"116 papers\" 2. page 6, table 1, the last line, the last cell, \"real umber\" => \"integer\"? \n3. section 4, line 8, \"The analysis shows that researchers SOLELY use ...\" 4. section 4, line 13, \"In Figure 5\" => \"In figure 6\" ", "label": [[101, 133, "Eval_pos_1"], [134, 184, "Jus_pos_1"], [185, 212, "Eval_neg_1"], [213, 296, "Jus_neg_1"], [297, 348, "Eval_neg_2"], [349, 1083, "Jus_neg_2"]]}
{"id": 339, "review": "The paper discusses the performance of various systems for simple factoid QA over a KB (Freebase, in this case) on different datasets using various experimental settings.\nThe datasets used for these experiments include subsets of Free917, WebQSP, SimpleQuestions, and FreebaseQA that involve a single constraint with entities and predicates within FB2M.\nThe systems used for evaluation are BuboQA, Hierarchical Residual BiLSTM, KBQA-Adapter, and Knowledge Embedding-based QA.\nTop-1 accuracy numbers computed for different combinations of training/validation and test datasets show that the systems perform poorly outside of SimpleQuestions and additional experiments indicate that (1) ambiguity is similar across all datasets; (2) quality of FreebaseQA is not high; (3) data size does not account for some differences in performance (4) performance drops during the entity linking step, which also impacts the quality of relation prediction and final result.\nFurther analysis is done to explain the decrease in performance for models trained on a dataset and tested on another. Here, the larger drop in accuracy occurs for relation prediction.\nAnd, even when combining datasets (during training), performance on individual test data is less than accuracy on a single target dataset.\nStrengths: -This is an interesting evaluation with various attempts to explain the observed results. Conclusions/recommendations are provided as well. Data quality tends to be a problem that needs to be addressed, either because collected data is too simple/easy/naively collected or because it contains errors.\n-The article is well-written.\nWeaknesses: -Experiments used mostly code released by the authors of the original papers. There are some guesses/default values used here. It would be good include a note on how the performance reported here for single target datasets compares to the numbers presented by the system developers -- especially since this is done on SimpleQuestions, which is not greatly altered by the filtering of multiple-triple questions outside of FB2M. ", "label": [[1295, 1383, "Eval_pos_1"], [1384, 1434, "Eval_pos_2"], [1596, 1624, "Eval_pos_3"], [1638, 1714, "Eval_neg_1"], [1715, 2064, "Jus_neg_1"]]}
{"id": 340, "review": "This paper presents a way to parse trees (namely the universal dependency treebanks) by relying only on POS and by using a modified version of the PageRank to give more way to some meaningful words (as opposed to stop words).\nThis idea is interesting though very closed to what was done in S\u00c3\u00b8gaard (2012)'s paper. The personalization factor giving more weight to the main predicate is nice but it would have been better to take it to the next level. \nAs far as I can tell, the personalization is solely used for the main predicate and its weight of 5 seems arbitrary.\nRegarding the evaluation and the detailed analyses, some charts would have been beneficial, because it is sometimes hard to get the gist out of the tables. \nFinally, it would have been interesting to get the scores of the POS tagging in the prediction mode to be able to see if the degradation in parsing performance is heavily correlated to the degradation in tagging performance (which is what we expect).\nAll in all, the paper is interesting but the increment over the work of S\u00c3\u00b8gaard (2012) is small.\nSmaller issues: ------------------- l. 207 : The the main idea -> The main idea ", "label": [[226, 250, "Eval_pos_1"], [251, 314, "Eval_neg_1"], [315, 390, "Eval_pos_2"], [391, 451, "Eval_neg_2"], [569, 659, "Eval_neg_3"], [661, 725, "Jus_neg_3"], [726, 949, "Eval_neg_4"], [977, 1074, "Major_claim"]]}
{"id": 341, "review": "This paper is concerned with cross-lingual direct transfer of NER models using a very recent cross-lingual wikification model. In general, the key idea is not highly innovative and creative, as it does not really propose any core new technology. The contribution is mostly incremental, and marries the two research paths: (1) direct transfer for downstream NLP tasks (such as NER, parsing, or POS tagging), and (2) very recent developments in the cross-lingual wikification technology. However, I pretty much liked the paper, as it is built on a coherent and clear story with enough experiments and empirical evidence to support its claims, with convincing results. I still have several comments concerning the presentation of the work.\nRelated work: a more detailed description in related work on how this paper relates to work of Kazama and Torisawa (2007) is needed. It is also required to state a clear difference with other related NER system that in one way or another relied on the encyclopaedic Wikipedia knowledge. The differences are indeed given in the text, but they have to be further stressed to facilitate reading and placing the work in context.  Although the authors argue why they decided to leave out POS tags as features, it would still be interesting to report experiments with POS tags features similar to Tackstrom et al.: the reader might get an overview supported by empirical evidence regarding the usefulness (or its lack) of such features for different languages (i.e., for the languages for which universal POS are available at least).  Section 3.3 could contribute from a running example, as I am still not exactly sure how the edited model from Tsai and Roth works now (i.e., the given description is not entirely clear).\nSince the authors mention several times that the approaches from Tackstrom et al. (2012) and Nothman et al. (2012) are orthogonal to theirs and that they can be combined with the proposed approach, it would be beneficial if they simply reported some preliminary results on a selection of languages using the combination of the models. It will add more flavour to the discussion. Along the same line, although I do acknowledge that this is also orthogonal approach, why not comparing with a strong projection baseline, again to put the results into more even more context, and show the usefulness (or limitations) of wikification-based approaches.\nWhy is Dutch the best training language for Spanish, and Spanish the best language for Yoruba? Only a statistical coincidence or something more interesting is going on there? A paragraph or two discussing these results in more depth would be quite interesting.\nAlthough the idea is sound, the results from Table 4 are not that convincing with only small improvements detected (and not in all scenarios). A statistical significance test reported for the results from Table 4 could help support the claims.\nMinor comments: - Sect. 2.1: Projection can also be performed via methods that do not require parallel data, which makes such models more widely applicable (even for languages that do not have any parallel resources): e.g., see the work of Peirsman and Pado (NAACL 2009) or Vulic and Moens (EMNLP 2013) which exploit bilingual semantic spaces instead of direct alignment links to perform the transfer.\n- Several typos detected in the text, so the paper should gain quite a bit from a more careful proofreading (e.g., first sentence of Section 3: \"as a the base model\"; This sentence is not 'parsable', Page 3: \"They avoid the traditional pipeline of NER then EL by...\", \"to disambiguate every n-grams\" on Page 8) ", "label": [[127, 189, "Eval_neg_1"], [191, 245, "Jus_neg_1"], [246, 321, "Eval_neg_2"], [321, 485, "Jus_neg_2"], [486, 525, "Eval_pos_1"], [526, 665, "Jus_pos_1"], [666, 736, "Eval_neg_3"], [737, 2904, "Jus_neg_3"]]}
{"id": 342, "review": "This paper proposes an approach for multi-lingual named entity recognition using features from Wikipedia. By relying on a cross-lingual Wikifier, it identifies English Wikipedia articles for phrases in a target language and uses features based on the wikipedia entry. Experiments show that this new feature helps not only in the monolingual case, but also in the more interesting direct transfer setting, where the English model is tested on a target language.\nI liked this paper. It proposes a new feature for named entity recognition and conducts a fairly thorough set of experiments to show the utility of the feature. The analysis on low resource and the non-latin languages are particularly interesting.\nBut what about named entities that are not on Wikipedia? In addition to the results in the paper, it would be interesting to see results on how these entities are affected by the proposed method.  The proposed method is strongly dependent on the success of the cross-lingual wikifier. With this additional step in the pipeline, how often do we get errors in the prediction because of errors in the wikifier?\nGiven the poor performance of direct transfer on Tamil and Bengali when lexical features are added, I wonder if it is possible to regularize the various feature classes differently, so that the model does not become over-reliant on the lexical features. ", "label": [[461, 480, "Eval_pos_1"], [622, 708, "Eval_pos_2"], [765, 904, "Eval_neg_1"], [906, 1116, "Jus_neg_1"]]}
{"id": 345, "review": "This one is a tough call, because I do think that there are some important, salvageable technial results in here (notably the parsing algorithm), but the paper as a whole has very little cohesion.        It is united around an overarching view of formal languages in which a language being \"probabilistic\" or not is treated as a formal property of the same  variety as being closed under intersection or not.  In my opinion, what it  means for a formal language to be probabilistic in this view has not been  considered with sufficient rigor for this viewpoint to be compelling.\nI should note, by the way, that the value of the formal results provided mostly does not depend on the flimsiness of the overarching story.  So what we have here is not bad research, but a badly written paper.  This needs  more work.\nI find it particulary puzzling that the organization of the paper leaves so little space for elucidating the parsing result that soundness and completeness are relegated to a continuation of the paper in the form of supplementary notes.  I also find the mention of probabilistic languages in the title of the paper to be very disingenuous --- there is in fact no probabilistic reasoning in this submission.\nThe sigificance of the intersection-closure result of section 3 is also being somewhat overstated, I think.  Unless there is something I'm not understanding about the restrictions on the right-hand sides of rules (in which case, please elaborate), this is merely a matter of folding a finite intersection into the set of non-terminal labels. ", "label": [[0, 196, "Major_claim"], [720, 788, "Eval_neg_1"], [790, 812, "Major_claim"], [813, 1049, "Eval_neg_2"], [1050, 1151, "Eval_neg_3"], [1156, 1219, "Jus_neg_3"], [1220, 1327, "Eval_neg_4"], [1329, 1561, "Jus_neg_4"]]}
{"id": 346, "review": "This paper presents a novel framework for modelling symmetric collaborative dialogue agents by dynamically extending knowledge graphs embeddings. The task is rather simple: two dialogue agents (bot-bot, human-human or human-bot) talk about their mutual friends. There is an underlying knowledge base for each party in the dialogue and an associated knowledge graph. Items in the knowledge graph have embeddings that are dynamically updated during the conversation and used to generate the answers.\n- Strengths: This model is very novel for both goal-directed and open ended dialogue. The presented evaluation metrics show clear advantage for the presented model.\n- Weaknesses: In terms of the presentation, mathematical details of how the embeddings are computed are not sufficiently clear. While the authors have done an extensive evaluation, they haven't actually compared the system with an RL-based dialogue manager which is current state-of-the-art in goal-oriented systems. Finally, it is not clear how this approach scales to more complex problems. The authors say that the KB is 3K, but actually what the agent operates is about 10 (judging from Table 6).\n- General Discussion: Overall, I think this is a good paper. Had the theoretical aspects of the paper been better presented I would give this paper an accept. ", "label": [[511, 583, "Eval_pos_1"], [676, 790, "Eval_neg_1"], [791, 979, "Jus_neg_1"], [980, 1055, "Eval_neg_2"], [1056, 1163, "Jus_neg_2"], [1186, 1323, "Major_claim"]]}
{"id": 347, "review": "This paper proposes a simple attention-based RNN model for generating SQL queries from natural language without any intermediate representation. Towards this end they employ a data augmentation approach where more data is iteratively collected from crowd annotation, based on user feedback on how well the SQL queries produced by the model do. Results on both the benchmark and interactive datasets show that data augmentation is a promising approach.\nStrengths: - No intermediate representations were used.  - Release of a potentially valuable dataset on Google SCHOLAR.\nWeaknesses: - Claims of being comparable to state of the art when the results on GeoQuery and ATIS do not support it.  General Discussion: This is a sound work of research and could have future potential in the way semantic parsing for downstream applications is done. I was a little disappointed with the claims of \u201cnear-state-of-the-art accuracies\u201d on ATIS and GeoQuery, which doesn\u2019t seem to be the case (8 points difference from Liang et. al., 2011)). And I do not necessarily think that getting SOTA numbers should be the focus of the paper, it has its own significant contribution. I would like to see this paper at ACL provided the authors tone down their claims, in addition I have some questions for the authors.\n- What do the authors mean by minimal intervention? Does it mean minimal human intervention, because that does not seem to be the case. Does it mean no intermediate representation? If so, the latter term should be used, being less ambiguous.\n- Table 6: what is the breakdown of the score by correctness and incompleteness? \nWhat % of incompleteness do these queries exhibit?\n- What is expertise required from crowd-workers who produce the correct SQL queries?  - It would be helpful to see some analysis of the 48% of user questions which could not be generated.\n- Figure 3 is a little confusing, I could not follow the sharp dips in performance without paraphrasing around the 8th/9th stages.  - Table 4 needs a little more clarification, what splits are used for obtaining the ATIS numbers?\nI thank the authors for their response. ", "label": [[586, 689, "Eval_neg_1"], [712, 840, "Eval_pos_1"], [841, 923, "Eval_neg_2"], [945, 1027, "Jus_neg_2"], [1119, 1159, "Eval_pos_2"], [1160, 1242, "Major_claim"], [1859, 1889, "Eval_neg_3"], [1891, 1987, "Jus_neg_3"], [1990, 2033, "Eval_neg_4"], [2034, 2086, "Jus_neg_4"]]}
{"id": 351, "review": "Thanks for the response. I look forward to reading about the effect of incentives and the ambiguity of the language in the domain.\nReview before author response: The paper proposes a way to build natural language interfaces by allowing a set of users to define new concepts and syntax. It's an (non-trivial) extension of S. I. Wang, P. Liang, and C. Manning. 2016. Learning language games through interaction Questions: -What is the size of the vocabulary used  -Is it possible to position this paper with respect to previous work on inverse reinforcement learning and imitation learning ?\nStrengths: -The paper is well written -It provides a compelling direction/solution to the problem of dealing with a large set of possible programs while learning natural language interfaces.  Weaknesses: -The authors should discuss the effect of the incentives on the final performance ? Were other alternatives considered ?  -While the paper claims that the method can be extended to more practical domains, it is not clear to me how straightforward it is going to be. How sensitive is the method to the size of the vocabulary required in a domain ? \nWould increased ambiguity in natural language create new problems ? These questions are not discussed in the current experiments.\n-A real-world application would definitely strengthen the paper even more. ", "label": [[602, 627, "Eval_pos_1"], [629, 780, "Eval_pos_2"], [795, 877, "Eval_neg_1"], [917, 1058, "Eval_neg_2"], [1060, 1271, "Jus_neg_2"], [1273, 1347, "Eval_neg_3"]]}
{"id": 353, "review": "The paper analyzes the story endings (last sentence of a 5-sentence story) in the corpus built for the story cloze task (Mostafazadeh et al. 2016), and proposes a model based on character and word n-grams to classify story endings. \nThe paper also shows better performance on the story cloze task proper (distinguishing between \"right\" and \"wrong\" endings) than prior work.\nWhereas style analysis is an interesting area and you show better results than prior work on the story cloze task, there are several issues with the paper. \nFirst, how do you define \"style\"? Also, the paper needs to be restructured (for instance, your section \"Results\" actually mixes some results and new experiments) and clarified (see below for questions/comments): right now, it is quite difficult for the reader to follow what data is used for the different experiments, and what data the discussion refers to.\n(1) More details about the data used is necessary in order to assess the claim that \"subtle writing task [...] imposes different styles on the author\" (lines 729-732). How many stories are you looking at, written by how many different persons? And how many stories are there per person? From your description of the post-analysis of coherence, only pairs of stories written by the same person in which one was judged as \"coherent\" and the other one as \"neutral\" are chosen. Can you confirm that this is the case? So perhaps your claim is justified for your \"Experiment 1\". However my understanding is that in experiment 2 where you compare \"original\" vs. \"right\" or \"original\" vs. \"wrong\", we do not have the same writers. So I am not convinced lines 370-373 are correct.\n(2) A lot in the paper is simply stated without any justifications. For instance how are the \"five frequent\" POS and words chosen? Are they the most frequent words/POS? ( Also theses tables are puzzling: why two bars in the legend for each category?). Why character *4*-grams? Did you tune that on the development set? If these were not the most frequent features, but some that you chose among frequent POS and words, you need to justify this choice and especially link the choice to \"style\". How are these features reflecting \"style\"?\n(3) I don't understand how the section \"Design of NLP tasks\" connects to the rest of the paper, and to your results. But perhaps this is because I am lost in what \"training\" and \"test\" sets refer to here.\n(4) It is difficult to understand how your model differs from previous work. \nHow do we reconcile lines 217-219 (\"These results suggest that real understanding of text is required in order to solve the task\") with your approach?\n(5) The terminology of \"right\" and \"wrong\" endings is coming from Mostafazadeh et al., but this is a very bad choice of terms. What exactly does a \"right\" or \"wrong\" ending mean (\"right\" as in \"coherent\" or \"right\" as in \"morally good\")? \nI took a quick look, but couldn't find the exact prompts given to the Turkers. \nI think this needs to be clarified: as it is, the first paragraph of your section \"Story cloze task\" (lines 159-177) is not understandable.\nOther questions/comments: Table 1. Why does the \"original\" story differ from the coherent and incoherent one? From your description of the corpus, it seems that one Turker saw the first 4 sentences of the original story and was then ask to write one sentence ending the story in a \"right\" way (or did they ask to provide a \"coherent\" ending?) and one sentence ending the story in a \"wrong\" way (or did they ask to provide an \"incoherent\" ending)? I don't find the last sentence of the \"incoherent\" story that incoherent... If the only shoes that Kathy finds great are $300, I can see how Kathy doesn't like buying shoes ;-) This led me to wonder how many Turkers judged the coherence of the story/ending and how variable the judgements were. What criterion was used to judge a story coherent or incoherent? Also does one Turker judge the coherence of both the \"right\" and \"wrong\" endings, making it a relative judgement? Or was this an absolute judgement? This would have huge implications on the ratings.\nLines 380-383: What does \"We randomly sample 5 original sets\" mean?\nLine 398: \"Virtually all sentences\"? Can you quantify this?\nTable 5: Could we see the weights of the features?  Line 614: \"compared to ending an existing task\": the Turkers are not ending a \"task\" Line 684-686: \"made sure each pair of endings was written by the same author\" -> this is true for the \"right\"/\"wrong\" pairs, but not for the \"original\"-\"new\" pairs, according to your description.\nLine 694: \"shorter text spans\": text about what? This is unclear.\nLines 873-875: where is this published? ", "label": [[374, 419, "Eval_pos_1"], [424, 488, "Eval_pos_2"], [564, 605, "Eval_neg_1"], [607, 691, "Jus_neg_1"], [693, 706, "Eval_neg_2"], [743, 2403, "Jus_neg_2"], [2404, 3091, "Jus_neg_2"]]}
{"id": 354, "review": "- Strengths This paper deals with the issue of finding word polarity orientation in an unsupervised manner, using word embeddings.\n- Weaknesses The paper presents an interesting and useful idea, however, at this moment, it is not applied to any test case. The ideas on which it is based are explained in an \"intuitive\" manner and not thoroughly justified.  - General Discussion This is definitely interesting work. The paper would benefit from more experiments being carried out, comparison with other methods (for example, the use of the Normalized Google Distance by authors such as (Balahur and Montoyo, 2008) - http://ieeexplore.ieee.org/abstract/document/4906796/) and the application of the knowledge obtained to a real sentiment analysis scenario. At this point, the work, although promising, is in its initial phase. ", "label": [[144, 194, "Eval_pos_1"], [204, 254, "Jus_neg_2"], [256, 355, "Eval_neg_2"], [378, 414, "Eval_pos_2"], [755, 825, "Major_claim"]]}
{"id": 355, "review": "- Strengths: This paper explores a relatively under-explored area of practical application of ideas behind Bayesian neural nets in NLP tasks. With a Bayesian treatment of the parameters of RNNs, it is possible to incorporate benefits of model averaging during inference. Further, their gradient based sampling approximation to the posterior estimation leads to a procedure which is easy to implement and is potentially much cheaper than other well-known techniques for model averaging like ensembling.   The effectiveness of this approach is shown on three different tasks -- language modeling, image captioning and sentence classification; and performance gains are observed over the baseline of single model optimization.\n- Weaknesses: Exact experimental setup is unclear. The supplementary material contains important details about burn-in, number of epochs and samples collected that should be in the main paper itself. Moreover, details on how the inference is performed would be helpful. Were the samples that were taken following HMC for a certain number of epochs after burn in on the training data fixed for inference (for every \\tilda{Y} during test time, same samples were used according to eqn 5) ? Also, an explicit clarification regarding an independence assumption that p(D|\\theta) = p(Y,X| \\theta) = p(Y| \\theta,X)p(X), which lets one use the conditional RNN model (if I understand correctly) for the potential U(\\theta) would be nice for completeness.\nIn terms of comparison, this paper would also greatly benefit from a discussion/ experimental comparison with ensembling and distillation methods (\"Sequence level knowledge distillation\"; Kim and Rush, \"Distilling an Ensemble of Greedy Dependency Parsers into One MST Parser\"; Kuncoro et al.) which  are intimately related by a similar goal of incorporating effects of model averaging.\nFurther discussion related to preference of HMC related sampling methods over other sampling methods or variational approximation would be helpful.\nFinally, equation 8 hints at the potential equivalence between dropout and the proposed approach and the theoretical justification behind combining SGLD and dropout (by making the equivalence more concrete) would lead to a better insight into the effectiveness of the proposed approach.   - General Discussion: Points addressed above. ", "label": [[738, 774, "Eval_neg_1"], [775, 1468, "Jus_neg_1"], [1469, 1854, "Eval_neg_2"], [1855, 2002, "Eval_neg_3"]]}
{"id": 356, "review": "The authors presents a method to jointly embed words, phrases and concepts, based on plain text corpora and a manually-constructed ontology, in which concepts are represented by one or more phrases. They apply their method in the medical domain using the UMLS ontology, and in the general domain using the YAGO ontology. To evaluate their approach, the authors compare it to simpler baselines and prior work, mostly on intrinsic similarity and relatedness benchmarks. They use existing benchmarks in the medical domain, and use mechanical turkers to generate a new general-domain concept similarity and relatedness dataset, which they also intend to release. They report results that are comparable to prior work.\nStrengths: - The proposed joint embedding model is straightforward and makes reasonable sense to me. Its main value in my mind is in reaching a (configurable) middle ground between treating phrases as atomic units on one hand to considering their compositionallity on the other. The same approach is applied to concepts being \u2018composed\u2019 of several representative phrases.\n-  The paper describes a decent volume of work, including model development, an additional contribution in the form of a new evaluation dataset, and several evaluations and analyses performed.\nWeaknesses: - The evaluation reported in this paper includes only intrinsic tasks, mainly on similarity/relatedness datasets. As the authors note, such evaluations are known to have very limited power in predicting the utility of embeddings in extrinsic tasks. Accordingly, it has become recently much more common to include at least one or two extrinsic tasks as part of the evaluation of embedding models.\n- The similarity/relatedness evaluation datasets used in the paper are presented as datasets recording human judgements of similarity between concepts. However, if I understand correctly, the actual judgements were made based on presenting phrases to the human annotators, and therefore they should be considered as phrase similarity datasets, and analyzed as such.\n- The medical concept evaluation dataset, \u2018mini MayoSRS\u2019 is extremely small (29 pairs), and its larger superset \u2018MayoSRS\u2019 is only a little larger (101 pairs) and was reported to have a relatively low human annotator agreement. The other medical concept evaluation dataset, \u2018UMNSRS\u2019, is more reasonable in size, but is based only on concepts that can be represented as single words, and were represented as such to the human annotators. This should be mentioned in the paper and makes the relevance of this dataset questionable with respect to representations of phrases and general concepts.  - As the authors themselves note, they (quite extensively) fine tune their hyperparameters on the very same datasets for which they report their results and compare them with prior work. This makes all the reported results and analyses questionable.\n- The authors suggest that their method is superb to prior work, as it achieved comparable results while prior work required much more manual annotation. I don't think this argument is very strong because the authors also use large manually-constructed ontologies, and also because the manually annotated dataset used in prior work comes from existing clinical records that did not require dedicated annotations.\n- In general, I was missing more useful insights into what is going on behind the reported numbers. The authors try to treat the relation between a phrase and its component words on one hand, and a concept and its alternative phrases on the other, as similar types of a compositional relation. However, they are different in nature and in my mind each deserves a dedicated analysis. For example, around line 588, I would expect an NLP analysis specific to the relation between phrases and their component words. Perhaps the reason for the reported behavior is dominant phrase headwords, etc. Another aspect that was absent but could strengthen the work, is an investigation of the effect of the hyperparameters that control the tradeoff between the atomic and compositional views of phrases and concepts.\nGeneral Discussion: Due to the above mentioned weaknesses, I recommend to reject this submission. I encourage the authors to consider improving their evaluation datasets and methodology before re-submitting this paper.\nMinor comments: - Line 069: contexts -> concepts - Line 202: how are phrase overlaps handled?\n- Line 220: I believe the dimensions should be |W| x d. Also, the terminology \u2018negative sampling matrix\u2019 is confusing as the model uses these embeddings to represent contexts in positive instances as well.\n- Line 250: regarding \u2018the observed phrase just completed\u2019, it not clear to me how words are trained in the joint model. The text may imply that only the last words of a phrase are considered as target words, but that doesn\u2019t make sense.  - Notation in Equation 1 is confusing (using c instead of o) - Line 361: Pedersen et al 2007 is missing in the reference section.\n- Line 388: I find it odd to use such a fine-grained similarity scale (1-100)  for human annotations.\n- Line 430: The newly introduced term \u2018strings\u2019 here is confusing. I suggest to keep using \u2018phrases\u2019 instead.\n- Line 496: Which task exactly was used for the hyper-parameter tuning? \nThat\u2019s important. I couldn\u2019t find that even in the appendix.\n- Table 3: It\u2019s hard to see trends here, for instance PM+CL behaves rather differently than either PM or CL alone. It would be interesting to see development set trends with respect to these hyper-parameters.\n- Line 535: missing reference to Table 5. ", "label": [[727, 814, "Eval_pos_1"], [1089, 1132, "Eval_pos_2"], [1134, 1278, "Jus_pos_2"], [1293, 1404, "Eval_neg_1"], [1405, 1686, "Jus_neg_1"], [1846, 1959, "Eval_neg_2"], [1960, 2052, "Jus_neg_2"], [2647, 2831, "Jus_neg_3"], [2833, 2895, "Eval_neg_3"], [2898, 3092, "Eval_neg_4"], [3093, 3308, "Jus_neg_4"], [3311, 3408, "Eval_neg_5"], [3409, 4113, "Jus_neg_5"], [4134, 4211, "Major_claim"]]}
{"id": 357, "review": "This paper presents a  method for metaphor identification based on geometric approach. Certainly, very interesting piece of work. I enjoyed learning a completely new perspective. However, I have a few issues, I like them to be addressed by the authors. I would like to read author's response on the following issues.\n- Strengths: - A geometric approach to metaphor interpretation is a new research strand altogether.  -The paper is well written.\n-Author's claim is the beauty of their model lies in its simplicity, I do agree with their claim. But the implication of the simplicity is not been addressed in simple ways. Please refer the weakness section.\n- Weaknesses: Regarding writing =============== No doubt the paper is well-written. But the major issue with the paper is its lucidness. Indeed, poetic language, elegance is applaud-able, but clarity in scientific writing is very much needed. \nI hope you will agree with most of the stuff being articulated here: https://chairs-blog.acl2017.org/2017/02/02/last-minute-writing-advice/ Let me put my objections on writing here: -\"while providing a method which is effectively zero-shot\"..left readers in the blank. The notion of zero-shot has not been introduced yet!\n-Figure 2: most, neutral, least - metaphoric. How did you arrive at such differentiations?\n-Talk more about data. Otherwise, the method is less intuitive.\n-I enjoyed reading the analysis section. But it is not clear why the proposed simple (as claimed) method can over-perform than other existing techniques? \nPutting some examples would be better, I believe.\nTechnicality ============  \"A strength of this model is its simplicity\" - indeed, but the implication is not vivid from the writing. Mathematical and technical definition of a problem is one aspect, but the implication from the definition is quite hard to be understood. When that's the note-able contribution of the paper. Comparing to previous research this paper shows only marginal accuracy gain.\n- Comparison only with one previous work and then claiming that the method is capable of zero-shot, is slightly overstated. Is the method extendable to Twitter, let's say.\n- General Discussion: ", "label": [[87, 129, "Major_claim"], [130, 178, "Eval_pos_3"], [332, 416, "Eval_pos_2"], [419, 445, "Eval_pos_1"], [447, 543, "Eval_pos_4"], [544, 619, "Eval_neg_1"], [739, 791, "Eval_neg_1"], [792, 1220, "Jus_neg_1"], [1221, 1580, "Jus_neg_1"], [1667, 1713, "Eval_neg_2"], [1714, 1981, "Jus_neg_2"], [1984, 2105, "Eval_neg_3"]]}
{"id": 358, "review": "# Summary This paper presents an empirical study to identify a latent dimension of sentiment in word embeddings.\n# Strengths  S1) Tackles a challenging problem of unsupervised sentiment analysis.\n S2) Figure 2, in particular, is a nice visualisation.\n# Weaknesses  W1) The experiments, in particular, are very thin. I would recommend also measuring F1 performance and expanding the number of techniques compared.\n W2) The methodology description needs more organisation and elaboration. The ideas tested are itemised, but insufficiently justified.   W3) The results are quite weak in terms of the reported accuracy and depth of analysis. Perhaps this work needs more development, particularly with validating the central assumption that the Distributional Hypothesis implies that opposite words, although semantically similar, are separated well in the vector space? ", "label": [[200, 250, "Eval_pos_1"], [269, 315, "Eval_neg_1"], [316, 412, "Jus_neg_1"], [418, 486, "Eval_neg_2"], [487, 547, "Jus_neg_2"], [554, 637, "Eval_neg_3"], [638, 867, "Jus_neg_3"]]}
{"id": 361, "review": "- Strengths: i. Well organized and easy to understand ii. Provides detailed comparisons under various experimental settings and shows the state-of-the-art performances - Weaknesses: i. In experiments, this paper compares previous supervised approaches, but the proposed method is the semi-supervised approach even if the training data is enough to train.\n- General Discussion: This paper adopts a pre-training approach to improve Chinese word segmentation. \nBased on the transition-based neural word segmentation, this paper aims to pre-train incoming characters with external resources (punctuation, soft segmentation, POS, and heterogeneous training data) through multi-task learning. That is, this paper casts each external source as an auxiliary classification task. The experimental results show that the proposed method achieves the state-of-the-art performances in six out of seven datasets. \u00a0 This paper is well-written and easy to understand. A number of experiments prove the effectiveness of the proposed method. However, there exist an issue in this paper. The proposed method is a semi-supervised learning that uses external resources to pre-train the characters. Furthermore, this paper uses another heterogeneous training datasets even if it uses the datasets only for pre-training. Nevertheless, the baselines in the experiments are based on supervised learning. In general, the performance of semi-supervised learning is better than that of supervised learning because semi-supervised learning makes use of plentiful auxiliary information. In the experiments, this paper should have compared the proposed method with semi-supervised approaches.\nPOST AUTHOR RESPONSE What the reviewer concerned is that this paper used additional \u201cgold-labeled\u201d dataset to pretrain the character embeddings. Some baselines in the experiments used label information, where the labels are predicted automatically by their base models as the authors pointed out. When insisting superiority of a method, all circumstances should be same. Thus, even if the gold dataset isn\u2019t used to train the segmentation model directly, it seems to me that it is an unfair comparison because the proposed method used another \u201cgold\u201d dataset to train the character embeddings. ", "label": [[16, 30, "Eval_pos_1"], [34, 53, "Eval_pos_2"], [57, 167, "Eval_pos_3"], [185, 354, "Eval_neg_1"], [901, 951, "Eval_pos_4"], [952, 1023, "Eval_pos_5"], [1024, 1068, "Eval_neg_2"], [1069, 1661, "Jus_neg_2"]]}
{"id": 362, "review": "- Strengths:  Evaluating bag of words and \"bound\" contexts from either dependencies or sentence ordering is important, and will be a useful reference to the community. The experiments were relatively thorough (though some choices could use further justification), and the authors used downstream tasks instead of just intrinsic evaluations.\n- Weaknesses:  The authors change the objective function of GBOW from p(c|\\sum w_i) to p(w|\\sum c_i). This is somewhat justified as dependency-based context with a bound representation only has one word available for predicting the context, but it's unclear exactly why that is the case and deserves more discussion. \nPresumably the non-dependency context with a bound representation would also suffer from this drawback? If so, how did Ling et al., 2015 do it? Unfortunately, the authors don't compare any results against the original objective, which is a definite weakness. In addition, the authors change GSG to match GBOW, again without comparing to the original objective. Adding results from word vectors trained using the original GBOW and GSG objective functions would justify these changes (assuming the results don't show large changes). \nThe hyperparameter settings should be discussed further. This played a large role in Levy et al. (2015), so you should consider trying different hyperparameter values. These depend pretty heavily on the task, so simply taking good values from another task may not work well.\nIn addition, the authors are unclear on exactly what model is trained in section 3.4. They say only that it is a \"simple linear classifier\". In section 3.5, they use logistic regression with the average of the word vectors as input, but call it  a Neural Bag-of-Words model. Technically previous work also used this name, but I find it misleading, since it's just logistic regression (and hence a linear model, which is not something I would call \"Neural\"). It is important to know if the model trained in section 3.4 is the same as the model trained in 3.5, so we know if the different conclusions are the result of the task or the model changing.  - General Discussion: This paper evaluates context taken from dependency parses vs context taken from word position in a given sentence, and bag-of-words vs tokens with relative position indicators. This paper is useful to the community, as they show when and where researchers should use word vectors trained using these different decisions.  - Emphasis to improve: The main takeaway from this paper that future researchers will use is given at the end of 3.4 and 3.5, but really should be summarized at the start of the paper. Specifically, the authors should put in the abstract that for POS, chunking, and NER, bound representations outperform bag-of-words representations, and that dependency contexts work better than linear contexts in most cases. In addition, for a simple text classification model, bound representations perform worse than bag-of-words representations, and there seemed to be no major difference between the different models or context types.\n- Small points of improvement:  Should call \"unbounded\" context \"bag of words\". This may lead to some confusion as one of the techniques you use is Generalized Bag-Of-Words, but this can be clarified easily. \n043: it's the \"distributional hypothesis\", not the \"Distributed Hypothesis\". \n069: citations should have a comma instead of semicolon separating them. \n074: \"DEPS\" should be capitalized consistently throughout the paper (usually it appears as \"Deps\"). Also should be introduced as something like dependency parse tree context (Deps). \n085: typo: \"How different contexts affect model's performances...\" Should have the word \"do\". ", "label": [[14, 167, "Eval_pos_1"], [168, 340, "Eval_pos_2"], [585, 658, "Eval_neg_1"], [803, 917, "Eval_neg_2"], [1191, 1247, "Eval_neg_3"], [1248, 1465, "Jus_neg_3"], [1477, 1551, "Eval_neg_4"], [1552, 1606, "Jus_neg_4"], [2315, 2353, "Eval_pos_3"], [2354, 2458, "Jus_pos_3"]]}
{"id": 363, "review": "- Strengths: - Weaknesses: - General Discussion: This paper investigates sentiment signals in  companies\u2019 annual 10-K filing reports to forecast volatility.  The authors evaluate information retrieval term weighting models which are seeded with a finance-oriented sentiment lexicon and expanded with word embeddings. PCA is used to reduce dimensionality before Support Vector Regression is applied for similarity estimation.\nIn addition to text-based features, the authors also use non-text-based market features (e.g. sector information and volatility estimates).\nMultiple fusion methods to combine text features with market features are evaluated.\nCOMMENTS It would be interesting to include two more experimental conditions, namely 1) a simple trigram SVM which does not use any prior sentiment lexica, and 2) features that reflect delta-IDFs scores for individual features. \nAs an additional baseline, it would be good to see binary features.\nThis paper could corroborate your references: https://pdfs.semanticscholar.org/57d6/29615c19caa7ae6e0ef2163eebe3b272e65a.pdf ", "label": [[658, 726, "Eval_neg_1"], [728, 878, "Jus_neg_1"]]}
{"id": 364, "review": "- Strengths: The idea to train word2vec-type models with ngrams (here specifically: bigrams) instead of words is excellent. The range of experimental settings (four word2vec-type algorithms, several word/bigram conditions) covers quite a bit of ground. The qualitative inspection of the bigram embeddings is interesting and shows the potential of this type of model for multi-word expressions.  - Weaknesses: This paper would benefit from a check by a native speaker of English, especially regarding the use of articles. The description of the similarity and analogy tasks comes at a strange place in the paper (4.1 Datasets).  - General Discussion: As is done at some point well into the paper, it could be clarified from the start that this is simply a generalization of the original word2vec idea, redefining the word as an ngram (unigram) and then also using bigrams. It would be good to give a rationale why larger ngrams have not been used.\n(I have read the author response.) ", "label": [[13, 123, "Eval_pos_1"], [124, 252, "Eval_pos_2"], [253, 395, "Eval_pos_3"], [409, 520, "Eval_neg_1"], [521, 626, "Eval_neg_2"], [650, 871, "Eval_neg_3"], [872, 946, "Eval_neg_4"]]}
{"id": 365, "review": "- Strengths: 1. The proposed models are shown to lead to rather substantial and consistent improvements over reasonable baselines on two different tasks (word similarity and word analogy), which not only serves to demonstrate the effectiveness of the models but also highlights the potential utility of incorporating sememe information from available knowledge resources for improving word representation learning. \n2. The paper contributes to ongoing efforts in the community to account for polysemy in word representation learning. It builds nicely on previous work and proposes some new ideas and improvements that could be of interest to the community, such as applying an attention scheme to incorporate a form of soft word sense disambiguation into the learning procedure.\n- Weaknesses: 1. Presentation and clarity: important details with respect to the proposed models are left out or poorly described (more details below). Otherwise, the paper generally reads fairly well; however, the manuscript would need to be improved if accepted. \n2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes, as the authors themselves point out (more details below).\n- General Discussion: 1. The authors stress the importance of accounting for polysemy and learning sense-specific representations. While polysemy is taken into account by calculating sense distributions for words in particular contexts in the learning procedure, the evaluation tasks are entirely context-independent, which means that, ultimately, there is only one vector per word -- or at least this is what is evaluated. Instead, word sense disambiguation and sememe information are used for improving the learning of word representations. This needs to be clarified in the paper. \n2. It is not clear how the sememe embeddings are learned and the description of the SSA model seems to assume the pre-existence of sememe embeddings. This is important for understanding the subsequent models. Do the SAC and SAT models require pre-training of sememe embeddings? \n3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work. \n4. A reasonable argument is made that the proposed models are particularly useful for learning representations for low-frequency words (by mapping words to a smaller set of sememes that are shared by sets of words). Unfortunately, no empirical evidence is provided to test the hypothesis. It would have been interesting for the authors to look deeper into this. This aspect also does not seem to explain the improvements much since, e.g., the word similarity data sets contain frequent word pairs. \n5. Related to the above point, the improvement gains seem more attributable to the incorporation of sememe information than word sense disambiguation in the learning procedure. As mentioned earlier, the evaluation involves only the use of context-independent word representations. Even if the method allows for learning sememe- and sense-specific representations, they would have to be aggregated to carry out the evaluation task. \n6. The example illustrating HowNet (Figure 1) is not entirely clear, especially the modifiers of \"computer\". \n7. It says that the models are trained using their best parameters. How exactly are these determined? It is also unclear how K is set -- is it optimized for each model or is it randomly chosen for each target word observation? Finally, what is the motivation for setting K' to 2? ", "label": [[16, 415, "Eval_pos_1"], [419, 533, "Eval_pos_2"], [534, 778, "Jus_pos_2"], [796, 820, "Eval_neg_2"], [823, 1044, "Jus_neg_2"], [1048, 1237, "Eval_neg_3"], [1263, 1780, "Jus_neg_4"], [1781, 1822, "Eval_neg_4"], [1825, 2470, "Jus_neg_3"], [2473, 2686, "Jus_neg_5"], [2687, 2759, "Eval_neg_5"], [2760, 2969, "Eval_neg_5"]]}
{"id": 366, "review": "- Strengths: The paper proposes an end-to-end neural model for semantic graph parsing, based on a well-designed transition system. \nThe work is interesting, learning semantic representations of DMRS, which is capable of resolving semantics such as scope underspecification. This work shows a new scheme for computational semantics, benefiting from an end-to-end transition-based incremental framework, which resolves the parsing with low cost.\n- Weaknesses:   My major concern is that the paper only gives a very common introduction for the definition of DMRS and EP, and the example even makes me a little confused because I cannot see anything special for DMRS. The description can be a little more detailed, I think. However, upon the space limitation, it is understandable. The same problem exists for the transition system of the parsing model. If I do not have any background of MRS and EP, I can hardly learn something from the paper, just seeing that this paper is very good.\n- General Discussion:   Overall, this paper is very interesting to me. I like the DMRS for semantic parsing very much and like the paper very much. Hope that the open-source codes and datasets can make this line of research being a hot topic. ", "label": [[132, 155, "Eval_pos_1"], [460, 567, "Eval_neg_1"], [572, 615, "Eval_neg_2"], [616, 719, "Jus_neg_2"], [778, 849, "Eval_neg_3"], [850, 983, "Jus_neg_3"], [1008, 1054, "Eval_pos_2"], [1055, 1130, "Eval_pos_3"]]}
{"id": 368, "review": "This paper proposes a novel strategy for zero-resource translation where (source, pivot) and (pivot, target) parallel corpora are available. A teacher model for p(target|pivot) is first trained on the (pivot, target) corpus, then a student model for p(target|source) is trained to minimize relative entropy with respect to the teacher on the (source, pivot) corpus. When using word-level relative entropy over samples from the teacher, this approach is shown to outperform previous variants on standard pivoting, as well as other zero-resource strategies.\nThis is a good contribution: a novel idea, clearly explained, and with convincing empirical support. Unlike some previous work, it makes fairly minimal assumptions about the nature of the NMT systems involved, and hence should be widely applicable.\nI have only a few suggestions for further experiments. First, it would be interesting to see how robust this approach is to more dissimilar source and pivot languages, where intuitively the true p(target|source) and p(target|pivot) will be further apart. Second, given the success of introducing word-based diversity, it was surprising not to see a sentence n-best or sentence-sampling experiment. This would be more costly, but not much more so since you\u2019re already doing beam search with the teacher. Finally, related to the previous, it might be interesting to explore transition from word-based diversity to sentence-based as the student converges and no longer needs the signal from low-probability words.\nSome further comments: line 241: Despite its simplicity -> Due to its simplicity 277: target sentence y -> target word y 442: I assume that K=1 and K=5 mean that you compare probabilities of the most probable and 5 most probable words in the current context. If so, how is the current context determined - greedily or with a beam?\nSection 4.2. The comparison with an essentially uniform distribution doesn\u2019t seem very informative here: it would be extremely surprising if p(y|z) were not significantly closer to p(y|x) than to uniform. It would be more interesting to know to what extent p(y|z) still provides a useful signal as p(y|x) gets better. This would be easy to measure by comparing p(y|z) to models for p(y|x) trained on different amounts of data or for different numbers of iterations. \nAnother useful thing to explore in this section would be the effect of the mode approximation compared to n-best for sentence-level scores.\n555: It\u2019s odd that word beam does worse than word greedy, since word beam should be closer to word sampling. Do you have an explanation for this?\n582: The claimed advantage of sent-beam here looks like it may just be noise, given the high variance of these curves. ", "label": [[556, 583, "Eval_pos_1"], [584, 597, "Eval_pos_2"], [599, 616, "Eval_pos_3"], [621, 655, "Eval_pos_4"], [657, 804, "Eval_pos_5"], [805, 859, "Eval_neg_1"], [860, 1515, "Jus_neg_1"]]}
{"id": 369, "review": "The paper describes an extension of word embedding methods to also provide representations for phrases and concepts that correspond to words.  The method works by fixing an identifier for groups of phrases, words and the concept that all denote this concept, replace the occurrences of the phrases and words by this identifier in the training corpus, creating a \"tagged\" corpus, and then appending the tagged corpus to the original corpus for training.  The concept/phrase/word sets are taken from an ontology.  Since the domain of application is biomedical, the related corpora and ontologies are used.  The researchers also report on the generation of a new test dataset for word similarity and relatedness for real-world entities, which is novel.\nIn general, the paper is nicely written.  The technique is pretty natural, though not a very substantial contribution. The scope of the contribution is limited, because of focused evaluation within the biomedical domain.\nMore discussion of the generated test resource could be useful.  The resource could be the true interesting contribution of the paper.\nThere is one small technical problem, but that is probably just a matter of mathematical expression than implementation.\nTechnical problem: Eq. 8: The authors want to define the MAP calculation.                          This is a good idea, thought I think that a natural cut-off could be defined, rather than ranking the entire vocabulary.                          Equation 8 does not define a probability; it is quite easy to show this, even if the size of the vocabulary is infinite.  So you need to change the explanation (take out talk of a probability).\nSmall corrections: line: 556: most concepts has --> most concepts have ", "label": [[761, 790, "Eval_pos_1"], [792, 823, "Eval_pos_2"], [832, 868, "Eval_neg_1"], [869, 909, "Eval_neg_2"], [911, 970, "Jus_neg_2"], [971, 1034, "Eval_neg_3"], [1036, 1103, "Jus_neg_3"], [1106, 1226, "Eval_neg_4"], [1227, 1665, "Jus_neg_4"]]}
{"id": 370, "review": "- Strengths: The paper has a promising topic (different writing styles in finishing a story) that could appeal to Discourse and Pragmatics area participants.   - Weaknesses: The paper suffers from a convincing and thorough discussion on writing style and implications of the experiments on discourse or pragmatics. \n(1) For example, regarding \"style\", the authors could have sought answers to the following questions: what is the implication of starting an incoherent end-of-story sentence with a proper noun (l. 582)? Is this a sign of topic shift? What is the implication of ending a story coherently with a past tense verb, etc. \n(2) It is not clear to me why studies on deceptive language are similar to short or long answers in the current study. I would have liked to see a more complete comparison here. \n(3) The use of terms such as \"cognitive load\" (l. 134) and \"mental states\" (l. 671) appears somewhat vague. \n(4) There is insufficient discussion on the use of coordinators (line 275 onwards); the paper would benefit from a more thorough discussion of this issue (e.g. what is the role of coordinators in these short stories and in discourse in general? Does the use of coordinators differ in terms of the genre of the story? How about the use of \"no\" coordinators?)   (5) The authors do not seem to make it sufficiently clear who the target readers of this research would be (e.g. language teachers? Crowd-sourcing experiment designers? etc.)  The paper needs revision in terms of organization (there are repetitions throughout the text).  Also, the abbreviations in Table 5 and 6 are not clear to me.  - General Discussion: All in all, the paper would have to be revised particularly in terms of its theoretical standpoint and implications to discourse and pragmatics.\n===== In their response to the reviewers' comments, the authors indicate their willingness to update the paper and clarify the issues related to what they have experimented with. However, I would have liked to see a stronger commitment to incorporating the implications of this study to the Discourse and Pragmatics area. ", "label": [[13, 157, "Eval_pos_1"], [174, 315, "Eval_neg_1"], [319, 1782, "Jus_neg_1"]]}
{"id": 371, "review": "- Strengths: This paper presents an approach for fine-grained IsA extraction by learning modifier interpretations. The motivation of the paper is easy to understand and this is an interesting task. In addition, the approach seems solid in general and the experimental results show that the approach increases in the number of fine-grained classes that can be populated.\n- Weaknesses: Some parts of the paper are hard to follow. It is unclear to me why D((e, p, o)) is multiplied by w in Eq (7) and why the weight for e in Eq. (8) is explained as the product of how often e has been observed with some property and the weight of that property for the class MH. In addition, it also seems unclear how effective introducing compositional models itself is in increasing the coverage. I think one of the major factors of the increase of the coverage is the modifier expansion, which seems to also be applicable to the baseline 'Hearst'. It would be interesting to see the scores 'Hearst' with modifier expansion.\n- General Discussion: Overall, the task is interesting and the approach is generally solid. However, since this paper has weaknesses described above, I'm ambivalent about this paper.\n- Minor comment: I'm confused with some notations. For example, it is unclear for me what 'H' stands for. It seems that 'H' sometimes represents a class such as in (e, H) (- O, but sometimes represents a noun phrase such as in (H, p, N, w) (- D. Is my understanding correct?\nIn Paragraph \"Precision-Recall Analysis\", why the authors use area under the ROC curve instead of area under the Precision-Recall curve, despite the paragraph title \"Precision-Recall Analysis\"?\n- After reading the response: Thank you for the response. I'm not fully satisfied with the response as to the modifier expansion. I do not think the modifier expansion can be applied to Hearst as to the proposed method. However, I'm wondering whether there is no way to take into account the similar modifiers to improve the coverage of Hearst. I'm actually between 3 and 4, but since it seems still unclear how effective introducing compositional models itself is, I keep my recommendation as it is. ", "label": [[114, 164, "Eval_pos_1"], [169, 197, "Eval_pos_2"], [199, 369, "Eval_pos_3"], [383, 427, "Eval_neg_1"], [428, 1007, "Jus_neg_1"], [1030, 1062, "Eval_pos_4"], [1067, 1099, "Eval_pos_5"], [1100, 1190, "Major_claim"]]}
{"id": 372, "review": "This paper modifies existing word embedding algorithms (GloVe, Skip Gram, PPMI, SVD) to include ngram-ngram cooccurance statistics. To deal with the large computational costs of storing such expensive matrices, the authors propose an algorithm that uses two different strategies to collect counts.   - Strengths: - The proposed work seems like a natural extension of existing work on learning word embeddings. By integrating bigram information, one can expect to capture richer syntactic and semantic information.\n- Weaknesses: - While the authors propose learning embeddings for bigrams (bi_bi case), they actually do not evaluate the embeddings for the learned bigrams except for the qualitative evaluation in Table 7. A more quantitative evaluation on paraphrasing or other related tasks that can include bigram representations could have been a good contribution.\n- The evaluation and the results are not very convincing - the results do not show consistent trends, and some of the improvements are not necessarily statistically significant.\n- The paper reads clunkily due to significant grammar and spelling errors, and needs a major editing pass.\n- General Discussion: This paper is an extension of standard embedding learning techniques to include information from bigram-bigram coocurance. While the work is interesting and a natural extension of existing work, the evaluation and methods leaves some open questions. Apart from the ones mentioned in the weaknesses, some minor questions for the authors : - Why is there significant difference between the overlap and non-overlap cases? I would be more interested in finding out more than the quantitative difference shown on the tasks.\nI have read the author response. I look forward to seeing the revised version of the paper. ", "label": [[315, 408, "Eval_pos_1"], [410, 513, "Jus_pos_1"], [530, 720, "Jus_neg_1"], [721, 867, "Eval_neg_1"], [870, 924, "Eval_neg_2"], [927, 1045, "Jus_neg_2"], [1048, 1072, "Eval_neg_3"], [1073, 1119, "Jus_neg_3"], [1121, 1152, "Eval_neg_3"]]}
{"id": 374, "review": "This paper addresses the problem of disambiguating/linking textual entity mentions into a given background knowledge base (in this case, English Wikipedia).  (Its title and introduction are a little overblown/misleading, since there is a lot more to bridging text and knowledge than the EDL task, but EDL is a core part of the overall task nonetheless.)  The method is to perform this bridging via an intermediate layer of representation, namely mention senses, thus following two steps: (1) mention to mention sense, and (2) mention sense to entity.  Various embedding representations are learned for the words, the mention senses, and the entities, which are then jointly trained to maximize a single overall objective function that maximizes all three types of embedding equally.   Technically the approach is fairly clear and conforms to the current deep processing fashion and known best practices regarding embeddings; while one can suggest all kinds of alternatives, it\u2019s not clear they would make a material difference.  Rather, my comments focus on the basic approach.  It is not explained, however, exactly why a two-step process, involving the mention senses, is better than a simple direct one-step mapping from word mentions to their entities.  (This is the approach of Yamada et al., in what is called here the ALIGN algorithm.)  Table 2 shows that the two-step MPME (and even its simplification SPME) do better.  By why, exactly?  What is the exact difference, and additional information, that the mention senses have compare4ed to the entities?  To understand, please check if the following is correct (and perhaps update the paper to make it exactly clear what is going on).   For entities: their profiles consist of neighboring entities in a relatedness graph.                    This graph is built (I assume) by looking at word-level relatedness of the entity definitions (pages in Wikipedia).  The profiles are (extended skip-gram-based) embeddings.   For words: their profiles are the standard distributional semantics approach, without sense disambiguation.   For mention senses: their profiles are the standard distributional semantics approach, but WITH sense disambiguation.  Sense disambiguation is performed using a sense-based profile (\u2018language model\u2019) from local context words and neighboring mentions, as mentioned briefly just before Section 4, but without details.  This is a problem point in the approach.  How exactly are the senses created and differentiated?  Who defines how many senses a mention string can have?  If this is done by looking at the knowledge base, then we get a bijective mapping between mention senses and entities -\u2013 that is, there is exactly one entity for each mention sense (even if there may be more entities). \n In that case, are the sense collection\u2019s definitional profiles built starting with entity text as \u2018seed words\u2019?                    If so, what information is used at the mention sense level that is NOT used at the entity level?  Just and exactly the words in the texts that reliably associate with the mention sense, but that do NOT occur in the equivalent entity webpage in Wikipedia?  How many such words are there, on average, for a mention sense?                    That is, how powerful/necessary is it to keep this extra differentiation information in a separate space (the mention sense space) as opposed to just loading these additional words into the Entity space (by adding these words into the Wikipedia entity pages)?   If the above understanding is essentially correct, please update Section 5 of the paper to say so, for (to me) it is the main new information in the paper.   It is not true, as the paper says in Section 6, that \u201c\u2026this is the first work to deal with mention ambiguity in the integration of text and knowledge representations, so there is no exact baselines for comparison\u201d.  The TAC KBP evaluations for the past two years have hosted EDL tasks, involving eight or nine systems, all performing exactly this task, albeit against Freebase, which is considerably larger and more noisy than Wikipedia.  Please see http://nlp.cs.rpi.edu/kbp/2016/ .   On a positive note: I really liked the idea of the smoothing parameter in Section 6.4.2.\nPost-response: I have read the authors' responses.  I am not really satisfied with their reply about the KBP evaluation not being relevant, but that they are interested in the goodness of the embeddings instead.  In fact, the only way to evaluate such 'goodness' is through an application.  No-one really cares how conceptually elegant an embedding is, the question is: does it perform better? ", "label": [[785, 826, "Eval_pos_1"], [1079, 1256, "Eval_neg_1"], [1259, 1341, "Jus_neg_1"], [1344, 2199, "Jus_neg_1"], [2202, 2398, "Jus_neg_2"], [2400, 2440, "Eval_neg_2"], [2442, 2498, "Jus_neg_2"], [2498, 3504, "Jus_neg_2"], [3506, 4239, "Jus_neg_2"]]}
{"id": 375, "review": "- Strengths: Nicely written and understandable. \nClearly organized. Targeted answering of research questions, based on  different experiments.\n- Weaknesses: Minimal novelty. The \"first sentence\" heuristic has been in the summarization literature for many years. This work essentially applies this heuristic (evolved) in the keyword extraction setting. This is NOT to say that the work is trivial: it is just not really novel.\nLack of state-of-the-art/very recent methods. The experiment on the system evaluation vs state-of-the-art systems simply uses strong baselines. Even though the experiment answers the question \"does it perform better than baselines?\", I am not confident it illustrates that the system performs better than the current state-of-the-art. This somewhat reduces the value of the paper.\n- General Discussion: Overall the paper is good and I propose that it be published and presented.  On the other hand, I would propose that the authors position themselves (and the system performance) with respect to: Martinez\u2010Romo, Juan, Lourdes Araujo, and Andres Duque Fernandez. \" SemGraph: Extracting keyphrases following a novel semantic graph\u2010based approach.\" \nJournal of the Association for Information Science and Technology 67.1 (2016): 71-82. \n(with which the work holds remarkable resemblance in some points) Le, Tho Thi Ngoc, Minh Le Nguyen, and Akira Shimazu. \" Unsupervised Keyphrase Extraction: Introducing New Kinds of Words to Keyphrases.\" Australasian Joint Conference on Artificial Intelligence. Springer International Publishing, 2016. ", "label": [[13, 48, "Eval_pos_1"], [49, 67, "Eval_pos_2"], [68, 142, "Eval_pos_3"], [157, 173, "Eval_neg_1"], [174, 425, "Jus_neg_1"], [426, 471, "Eval_neg_2"], [472, 806, "Jus_neg_2"], [829, 904, "Major_claim"]]}
{"id": 376, "review": "The paper models the relation extraction problem as reading comprehension and extends a previously proposed reading comprehension (RC) model to extract unseen relations. The approach has two main components: 1. Queryfication: Converting a relation into natural question. Authors use crowdsourcing for this part.\n2. Applying RC model on the generated questions and sentences to get the answer spans. Authors extend a previously proposed approach to accommodate situations where there is no correct answer in the sentence.\nMy comments: 1. The paper reads very well and the approach is clearly explained.\n2. In my opinion, though the idea of using RC for relation extraction is interesting and novel, the approach is not novel. A part of the approach is crowdsourced and the other part is taken directly from a previous work, as I mention above.\n3. Relation extraction is a well studied problem and there are plenty of recently published works on the problem. However, authors do not compare their methods against any of the previous works. This raises suspicion on the effectiveness of the approach. As seen from Table 2, the performance numbers of the proposed method on the core task are not very convincing. However, this maybe because of the dataset used in the paper. Hence, a comparison with previous methods would actually help assess how the current method stands with the state-of-the-art.\n4. Slot-filling data preparation: You say \"we took the first sentence s in D to contain both e and a\". How can you get the answer sentence for (all) the relations of an entity from the first sentence of the entity's Wikipedia article? Please clarify this. See the following paper. They have a set of rules to locate (answer) sentences corresponding to an entity property in its Wikipedia page: Wu, Fei, and Daniel S. Weld. \" Open information extraction using Wikipedia.\" \nProceedings of the 48th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, 2010.\nOverall, I think the paper presents an interesting approach. However, unless the effectiveness of the approach is demonstrated by comparing it against recent works on relation extraction, the paper is not ready for publication. ", "label": [[537, 601, "Eval_pos_1"], [698, 723, "Eval_neg_1"], [725, 842, "Jus_neg_1"], [967, 1037, "Eval_neg_2"], [1038, 1396, "Jus_neg_2"], [1500, 1652, "Eval_neg_3"], [1653, 1868, "Jus_neg_3"], [2007, 2235, "Major_claim"]]}
{"id": 377, "review": "- Strengths:    - The paper states clearly the contributions from the beginning     - Authors provide system and dataset    - Figures help in illustrating the approach    - Detailed description of the approach    - The authors test their approach performance on other datasets and compare to other published work - Weaknesses:    -The explanation of methods in some paragraphs is too detailed and there is no mention of other work and it is repeated in the corresponding method sections, the authors committed to address this issue in the final version. \n   -README file for the dataset [Authors committed to add README file] - General Discussion:    - Section 2.2 mentions examples of DBpedia properties that were used as features. Do the authors mean that all the properties have been used or there is a subset? If the latter please list them. In the authors' response, the authors explain in more details this point and I strongly believe that it is crucial to list all the features in details in the final version for clarity and replicability of the paper. \n   - In section 2.3 the authors use Lample et al. Bi-LSTM-CRF model, it might be beneficial to add that the input is word embeddings (similarly to Lample et al.)    - Figure 3, KNs in source language or in English? ( since the mentions have been translated to English). In the authors' response, the authors stated that they will correct the figure. \n   - Based on section 2.4 it seems that topical relatedness implies that some features are domain dependent. It would be helpful to see how much domain dependent features affect the performance. In the final version, the authors will add the performance results for the above mentioned features, as mentioned in their response. \n   - In related work, the authors make a strong connection to Sil and Florian work where they emphasize the supervised vs. unsupervised difference. The proposed approach is still supervised in the sense of training, however the generation of training data doesn\u2019t involve human interference ", "label": [[17, 79, "Eval_pos_1"], [86, 120, "Eval_pos_2"], [126, 167, "Eval_pos_3"], [173, 209, "Eval_pos_4"], [215, 312, "Eval_pos_5"], [331, 392, "Eval_neg_1"], [397, 430, "Eval_neg_2"]]}
{"id": 378, "review": "The paper proposes a recurrent neural architecture that can skip irrelevant input units. This is achieved by specifying R (# of words to read at each \"skim\"), K (max jump size), and N (max # of jumps allowed). An LSTM processes R words, predicts the jump size k in {0, 1...K} (0 signals stop), skips the next k-1 words and continues until either the number of jumps reaches N or the model reaches the last word. While the model is not differentiable, it can be trained by standard policy gradient. The work seems to have been heavily influenced by Shen et al. (2016) who apply a similar reinforcement learning approach (including the same variance stabilization) to multi-pass machine reading.  - Strengths: The work simulates an intuitive \"skimming\" behavior of a reader, mirroring Shen et al. who simulate (self-terminated) repeated reading. A major attribute of this work is its simplicity. Despite the simplicity, the approach yields favorable results. In particular, the authors show through a well-designed synthetic experiment that the model is indeed able to learn to skip when given oracle jump signals. In text classification using real-world datasets, the model is able to perform competitively with the non-skimming model while being clearly faster.  The proposed model can potentially have meaningful practical implications: for tasks in which skimming suffices (e.g., sentiment classification), it suggests that we can obtain equivalent results without consuming all data in a completely automated fashion. To my knowledge this is a novel finding.  - Weaknesses: It's a bit mysterious on what basis the model determines its jumping behavior so effectively (other than the synthetic dataset). I'm thinking of a case where the last part of the given sentence is a crucial evidence, for instance:  \"The movie was so so and boring to the last minute but then its ending blew me away.\"  In this example, the model may decide to skip the rest of the sentence after reading \"so so and boring\". But by doing so it'll miss the turning point \"ending blew me away\" and mislabel the instance as negative. For such cases a solution can be running the skimming model in both directions as the authors suggest as future work. But in general the model may require more sophisticated architecture for controlling skimming.\nIt seems one can achieve improved skimming by combining it with multi-pass reading (presumably in reverse directions). That's how humans read to understand text that can't be digested in one skim; indeed, that's how I read this draft.  Overall, the work raises an interesting problem and provides an effective but intuitive solution. ", "label": [[844, 893, "Eval_pos_1"], [918, 956, "Eval_pos_2"], [957, 1112, "Jus_pos_2"], [1113, 1261, "Eval_pos_3"], [1263, 1336, "Eval_pos_4"], [1338, 1561, "Jus_pos_4"], [1577, 1669, "Eval_neg_1"], [1706, 2106, "Jus_neg_1"], [2556, 2653, "Major_claim"]]}
{"id": 379, "review": "- Strengths: Zero-shot relation extraction is an interesting problem. The authors have created a large dataset for relation extraction as question answering which would likely be useful to the community.\n- Weaknesses: Comparison and credit to existing work is severely lacking. Contributions of the paper don't seen particularly novel.\n- General Discussion: The authors perform relation extraction as reading comprehension. In order to train reading comprehension models to perform relation extraction, they create a large dataset of 30m \u201cquerified\u201d (converted to natural language) relations by asking mechanical turk annotators to write natural language queries for relations from a schema. They use the reading comprehension model of Seo et al. 2016, adding the ability to return \u201cno relation,\u201d as the original model must always return an answer. The main motivation/result of the paper appears to be that the authors can perform zero-shot relation extraction, extracting relations only seen at test time.\nThis paper is well-written and the idea is interesting. However, there are insufficient experiments and comparison to previous work to convince me that the paper\u2019s contributions are novel and impactful.\nFirst, the authors are missing a great deal of related work: Neelakantan at al. 2015 (https://arxiv.org/abs/1504.06662) perform zero-shot relation extraction using RNNs over KB paths. Verga et al. 2017 (https://arxiv.org/abs/1606.05804) perform relation extraction on unseen entities. The authors cite Bordes et al. (https://arxiv.org/pdf/1506.02075.pdf), who collect a similar dataset and perform relation extraction using memory networks (which are commonly used for reading comprehension). However, they merely note that their data was annotated at the \u201crelation\u201d level rather than at the triple (relation, entity pair) level\u2026 but couldn\u2019t Bordes et al. have done the same in their annotation? \nIf there is some significant difference here, it is not made clear in the paper. There is also a NAACL 2016 paper (https://www.aclweb.org/anthology/N/N16/N16-2016.pdf) which performs relation extraction using a new model based on memory networks\u2026 and I\u2019m sure there are more. Your work is so similar to much of this work that you should really cite and establish novelty wrt at least some of them as early as the introduction -- that's how early I was wondering how your work differed, and it was not made clear.\nSecond, the authors neither 1) evaluate their model on another dataset or 2) evaluate any previously published models on their dataset. This makes their empirical results extremely weak. Given that there is a wealth of existing work that performs the same task and the lack of novelty of this work, the authors need to include experiments that demonstrate that their technique outperforms others on this task, or otherwise show that their dataset is superior to others (e.g. since it is much larger than previous, does it allow for better generalization?) ", "label": [[13, 69, "Eval_pos_1"], [70, 203, "Eval_pos_2"], [218, 277, "Eval_neg_1"], [278, 335, "Eval_neg_2"], [1008, 1034, "Eval_pos_3"], [1039, 1063, "Eval_pos_4"], [1064, 1210, "Eval_neg_3"], [1218, 2421, "Jus_neg_3"], [2422, 2557, "Jus_neg_4"], [2558, 2608, "Eval_neg_4"], [2609, 2978, "Jus_neg_4"]]}
{"id": 380, "review": "- Strengths: Useful application for teachers and learners; supports fine-grained comparison of GEC systems.\n- Weaknesses: Highly superficial description of the system; evaluation not satisfying.\n- General Discussion: The paper presents an approach of automatically enriching the output of GEC systems with error types. This is a very useful application because both teachers and learners can benefit from this information (and many GEC systems only output a corrected version, without making the type of error explicit). It also allows for finer-grained comparison of GEC systems, in terms of precision in general, and error type-specific figures for recall and precision.\nUnfortunately, the description of the system remains highly superficial. The core of the system consists of a set of (manually?) created rules but the paper does not provide any details about these rules. The authors should, e.g., show some examples of such rules, specify the number of rules, tell us how complex they are, how they are ordered (could some early rule block the application of a later rule?), etc. -- Instead of presenting relevant details of the system, several pages of the paper are devoted to an evaluation of the systems that participated in CoNLL-2014. Table 6 (which takes one entire page) list results for all systems, and the text repeats many facts and figures that can be read off the table.  The evaluation of the proposed system is not satisfying in several aspects. \nFirst, the annotators should have independently annotated a gold standard for the 200 test sentences instead of simply rating the output of the system. Given a fixed set of tags, it should be possible to produce a gold standard for the rather small set of test sentences. It is highly probable that the approach taken in the paper yields considerably better ratings for the annotations than comparison with a real gold standard (see, e.g., Marcus et al. (1993) for a comparison of agreement when reviewing pre-annotated data vs. annotating from scratch). \nSecond, it is said that \"all 5 raters individually considered at least 95% of our rule-based error types to be either \u201cGood\u201d or \u201cAcceptable\u201d\". \nMultiple rates should not be considered individually and their ratings averaged this way, this is not common practice. If each of the \"bad\" scores were assigned to different edits (we don't learn about their distribution from the paper), 18.5% of the edits were considered \"bad\" by some annotator -- this sounds much worse than the average 3.7%, as calculated in the paper. \nThird, no information about the test data is provided, e.g. how many error categories they contain, or which error categories are covered (according to the cateogories rated as \"good\" by the annotators). \nForth, what does it mean that \"edit boundaries might be unusual\"? A more precise description plus examples are at need here. Could this be problematic for the application of the system?\nThe authors state that their system is less domain dependent as compared to systems that need training data. I'm not sure that this is true. E.g., I suppose that Hunspell's vocabulary probably doesn't cover all domains in the same detail, and manually-created rules can be domain-dependent as well -- and are completely language dependent, a clear drawback as compared to machine learning approaches. Moreover, the test data used here (FCE-test, CoNLL-2014) are from one domain only: student essays.\nIt remains unclear why a new set of error categories was designed. One reason for the tags is given: to be able to search easily for underspecified categories (like \"NOUN\" in general). It seems to me that the tagset presented in Nicholls (2003) supports such searches as well. Or why not using the CoNLL-2014 tagset? Then the CoNLL gold standard could have been used for evaluation.\nTo sum up, the main motivation of the paper remains somewhat unclear. Is it about a new system? But the most important details of it are left out. Is it about a new set of error categories? But hardly any motivation or discussion of it is provided. Is it about evaluating the CoNLL-2014 systems? But the presentation of the results remains superficial.\nTypos: -l129 (and others): c.f. - > cf.\n-l366 (and others): M2 -> M^2 (= superscribed 2) -l319: 50-70 F1: what does this mean? 50-70%?\nCheck references for incorrect case -e.g. l908: esl -> ESL -e.g. l878/79: fleiss, kappa ", "label": [[13, 57, "Eval_pos_1"], [122, 166, "Eval_neg_1"], [168, 194, "Eval_neg_2"], [319, 352, "Eval_pos_2"], [353, 672, "Jus_pos_2"], [673, 745, "Eval_neg_3"], [746, 1086, "Jus_neg_3"], [1393, 1469, "Eval_neg_4"], [1470, 2935, "Jus_neg_4"], [2936, 3076, "Eval_neg_5"], [3083, 3435, "Jus_neg_5"], [3436, 3502, "Eval_neg_6"], [3503, 3818, "Jus_neg_6"], [3819, 3888, "Major_claim"], [3915, 3965, "Eval_neg_7"], [3965, 4171, "Jus_neg_7"]]}
{"id": 381, "review": "This paper introduces a new approach to semantic parsing in which the model is equipped with a neural sequence to sequence (seq2seq) model (referred to as the \u201cprogrammer\u201d) which encodes a natural language question and produces a program. The programmer is also equipped with a \u2018key variable\u2019 memory component which stores (a) entities in the questions (b) values of intermediate variables formed during execution of intermediate programs. These variables are referred to further build the program.                    The model is also equipped with certain discrete operations (such as argmax or 'hop to next edges in a KB'). A separate component (\"interpreter/computer\") executes these operations and stores intermediate values (as explained before). Since the \u2018programmer' is inherently a seq2seq model, the \"interpreter/computer\u201d also acts as a syntax/type checker only allowing the decoder to generate valid tokens. For example, the second argument to the \u201chop\u201d operation has to be a KB predicate. Finally the model is trained with weak supervision and directly optimizes the metric which is used to evaluate the performance (F score). \nBecause of the discrete operations and the non differentiable reward functions, the model is trained with policy gradients (REINFORCE). Since gradients obtained through REINFORCE have high variance, it is common to first pretrain the model with a max-likelihood objective or find some good sequences of actions trained through some auxiliary objective. This paper takes a latter approach in which it finds good sequences via an iterative maximum likelihood approach. The results and discussion sections are presented in a very nice way and the model achieves SOTA results on the WebQuestions dataset when compared to other weakly supervised model.\nThe paper is written clearly and is very easy to follow.\nThis paper presents a new and exciting direction and there is scope for a lot of future research in this direction. I would definitely love to see this presented in the conference.\nQuestions for the authors (important ones first) 1. Another alternative way of training the model would be to bootstrap the parameters (\\theta) from the iterative ML method instead of adding pseudo gold programs in the beam (Line 510 would be deleted). Did you try that and if so why do you think it didn\u2019t work? \n2. What was the baseline model in REINFORCE. Did you have a separate network which predicts the value function. This must be discussed in the paper in detail. \n3. Were there programs which required multiple hop operations? Or were they limited to single hops. If there were, can you provide an example? ( I will understand if you are bound by word limit of the response) 4. Can you give an example where the filter operation would be used? \n5. I did not follow the motivation behind replacing the entities in the question with special ENT symbol Minor comments: Line 161 describe -> describing Line 318 decoder reads \u2018)\u2019 -> decoder generates \u2018)' ", "label": [[1790, 1846, "Eval_pos_1"], [1847, 1962, "Eval_pos_2"], [1963, 2027, "Major_claim"]]}
{"id": 382, "review": "- Strengths: Nice results, nice data set. Not so much work on Creole-like languages, especially English.   - Weaknesses: A global feeling of \"Deja-vu\", a lot of similar techniques have been applied to other domains, other ressource-low languages. Replace word embeddings by clusters and neural models by whatever was in fashion 5 years ago and we can find more or less the same applied to Urdu or out-of-domain parsing. I liked this paper though, but I would have appreciated the authors to highlight more their contributions and position their work better within the literature.\n- General Discussion: This paper presents a set of experiments designed a) to show the effectiveness of a neural parser  in a scarce resource scenario and b) to introduce a new data set of Creole English (from Singapour, called Singlish). While this data set is relatively small (1200 annotated sentences, used with 80k unlabeled sentences for word embeddings induction), the authors manage to present respectable results via interesting approach even though using features from relatively close languages are not unknown from the parsing community (see all the line of work on parsing Urdu/Hindi, on Arabic dialect using MSA based parsers, and so on). \nAssuming we can see Singlish as an extreme of Out-of-domain English and given all the set of experiments, I wonder why the authors didn\u2019t try the classical technique on domain-adaptation, namely training with UD_EN+90% of the Singlish within a 10 cross fold experiment ? just so we can have another interesting baseline (with and without word embeddings, with bi-lingual embeddings if enough parallel data is available). \nI think that paper is interesting but I really would have appreciated more positioning regarding all previous work in parsing low-ressources languages and extreme domain adaptation. A table presenting some results for Irish and other very small treebanks would be nice. \nAlso how come the IAA is so low regarding the labeled relations?\n***************************************** Note after reading the authors' answer ***************************************** Thanks for your clarifications (especially for redoing the IAA evaluation). I raised my recommendation to 4, I hope it'll get accepted. ", "label": [[12, 25, "Eval_pos_1"], [27, 40, "Eval_pos_2"], [42, 104, "Eval_pos_3"], [121, 150, "Eval_neg_1"], [152, 419, "Jus_neg_1"], [420, 579, "Eval_pos_4"], [1234, 1504, "Eval_neg_2"], [1505, 1655, "Jus_neg_2"], [1656, 1690, "Eval_pos_5"], [1693, 1837, "Eval_neg_3"]]}
{"id": 383, "review": "- Strengths: 1) This paper proposed a semi-automated framework (human generation -> auto expansion -> human post-editing) to construct a compositional semantic similarity evaluation data set.\n2) The proposed framework is used to create a Polish compositional semantic similarity evaluation data set which is useful for future work in developing Polish compositional semantic models.\n- Weaknesses: 1) The proposed framework has only been tested on one language. It is not clear whether the framework is portable to other languages. For example, the proposed framework relies on a dependency parser which may not be available in some languages or in poor performance in some other languages.\n2) The number of sentence pairs edited by leader judges is not reported so the correctness and efficiency of the automatic expansion framework can not be evaluated. The fact that more than 3% (369 out of 10k) of the post-edited pairs need further post-editing is worrying.  3) There are quite a number of grammatical mistakes. Here are some examples but not the complete and exhaustive list: line 210, 212, 213: \"on a displayed image/picture\" -> \"in a displayed image/picture\" line 428: \"Similarly as in\" -> \"Similar to\" A proofread pass on the paper is needed.\n- General Discussion: ", "label": [[195, 382, "Eval_pos_1"], [400, 460, "Jus_neg_1"], [461, 530, "Eval_neg_1"], [531, 689, "Jus_neg_1"], [693, 761, "Jus_neg_2"], [765, 853, "Eval_neg_2"], [855, 863, "Eval_neg_3"], [869, 949, "Jus_neg_3"], [950, 961, "Eval_neg_3"], [967, 1016, "Eval_neg_4"], [1017, 1251, "Jus_neg_4"]]}
{"id": 384, "review": "paper_summary\nThe paper discussed the problem of Spoken Conversational Question Answering (SCQA). On the one hand, conversational question answering (CQA) has been a challenging problem in language understanding and of many applications. On the other hand, speech modality in a natural form for conversation and contains additional information in audio signals. It is natural to explore the intersection of the two, i.e., SCQA. There are several existing works, but the research is usually limited by available dataset. This paper collects a Spoken-CoQA dataset based on CoQA and Google TTS for generating the training set and human speakers for collecting the test set audio. The authors also proposed DDNET approach which is a unification of knowledge distillation and dual attention techniques, and showed the efficacy of DDNET empirically by applying DDNET to various spoken QA models. \nsummary_of_strengths\n- The paper investigates a novel and challenging problem, Spoken Conversational Question Answering (SCQA). The problem is crucial for understanding natural/spoken conversation. The area is relevant to several ACL communities and could be interesting for industry application.  -The authors compile a dataset (Spoken-CoQA) to evaluate SCQA problem with much bigger sizes than previously available datasets. If the dataset is released as the authors promised, it will signifcantly benefit the research and benchmarking in SCQA problem.\n-The authors proposed a novel approach, DDNET. The approach leverages knowledge distillation and dual attention techniques to combine text representation and audio representation. The authors further showed the efficacy of DDNET empirically by applying DDNET to various spoken QA models and benchmarking with Spoken-CoQA dataset and a public available spoken SQuAD dataset. \nsummary_of_weaknesses\n- Although the authors showed consistent improvement with proposed DDNET approach over the baselines, it is difficult to interpret how good the results are. It would be great if the authors can provide some qualitiative analysis to help readers understanding the quality of result. Also, it would be interesting to see how the audio modality helps the answer prediction to better interpret the pros and cons of proposed method and the addition of audio modality.\n-There is a recent trend to bypass ASR, in order to make the underlying methods more scalable to low resourced languages. The proposed method still relies on ASR and it would be interesting to see if SCQA can achieve reasonable performance without ASR system. \ncomments,_suggestions_and_typos\nSee above weakness section ", "label": [[914, 1018, "Eval_pos_1"], [1019, 1187, "Jus_pos_1"], [1190, 1317, "Jus_pos_2"], [1318, 1445, "Eval_pos_2"], [1447, 1492, "Eval_pos_3"], [1493, 1625, "Jus_pos_3"], [1845, 1999, "Jus_neg_1"], [2000, 2305, "Jus_neg_1"]]}
{"id": 385, "review": "paper_summary\nThe paper presents a new corpus with anaphoric relations, annotating recipes for coreference and bridging. The utility of the corpus is demonstrated by training a ML classifier. \nsummary_of_strengths\nThe new corpus can benefit others studying anaphora beyond identify. The paper is well written. \nsummary_of_weaknesses\n- Transfer learning does not look to bring significant improvements. Looking at the variance, the results with and without transfer learning overlap. \ncomments,_suggestions_and_typos\nCan you please clarify/expand the evaluation methodology, e.g., given gold anaphors, you only evaluate the antecedents? At least you should state why the standard evaluation metrics fail in this case, e.g., they cannot score plurals, i.e., split-antecedent references Typo: line 513 'pretrained' ", "label": [[214, 282, "Eval_pos_1"], [283, 310, "Eval_pos_2"], [335, 401, "Eval_neg_1"], [402, 482, "Jus_neg_1"]]}
{"id": 386, "review": "paper_summary\nThis paper proposes a framework for zero shot cross-lingual natural language understanding that makes use of translation systems and bases on learning the alignment from unlabeled parallel data. The authors evaluated their methods on a bunch of datasets about intent recognition and entity recognition tasks and showed some promising results. They also included an interesting qualitative analysis for more insights. \nsummary_of_strengths\n- The authors evaluated carefully their framework on different datasets and their results showed that their proposed framework is promising.\n-The authors provided an error analysis that shows interesting insights for readers. \nsummary_of_weaknesses\n- It is not clear for me about the novelty of the proposed methods.  -The proposed method relies on the quality of translation systems.  -I'm not sure whether the differences of some results are significant (see Table 1).  -The differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective. \ncomments,_suggestions_and_typos\n- Did the authors run their experiments several times with different random initializations? ", "label": [[455, 524, "Eval_pos_1"], [529, 592, "Eval_pos_2"], [595, 679, "Eval_pos_3"], [704, 769, "Eval_neg_1"], [772, 837, "Eval_neg_2"], [840, 924, "Eval_neg_3"], [926, 978, "Jus_neg_4"], [989, 1035, "Eval_neg_4"], [1037, 1114, "Eval_neg_5"]]}
{"id": 387, "review": "paper_summary\nThis paper describes a semi-supervised approach to machine translation (MT) quality estimation (QE) at word-level. The approach hinges on the assumption that successful QE models rely on translation errors to predict overall sentence quality. It uses the sentence-level features as a weak label for getting predictions for the words in the sentence by exploring a set of feature attribution methods. These methods assign relevance scores that explain model predictions. Overall the paper shows that explanations extracted from sentence-level QE models can be used to detect translation errors. \nsummary_of_strengths\nThe paper introduces a novel way to approach word-level QE without having to resort to human annotation. it works losely as a semi-supervised word-level QE method using the sentence-level scores as weak labels for the word-level. This is the major contribution of the paper and its biggest strength. The main reason is because it enables practictioners and researchers of the field to have a word-level prediction without having to incur in the costs of annotating word-level data for training.\nExperimental settings look sound and results indicate that rationale extraction methods are comparable to using unsupervised glass-box methods to word-level QE but inferior to supervised black-box methods (that use human-produced or human-derived labels). \nsummary_of_weaknesses\nIt would be great if the code is indeed released if the paper is accepted. This would help a lot with the reproducibility of the method proposed in the paper. Other than that, it is unclear how the approach described here compares with the ones proposed for the Explainable Quality Estimation shared task which propose similar approaches. If the paper is accepted it would be good to position this in relation to these approaches. \ncomments,_suggestions_and_typos\n- Any idea about the difference in performance between using XLMR-base and XLMR-large? ", "label": [[630, 734, "Eval_pos_1"], [735, 1124, "Jus_pos_1"], [1125, 1157, "Eval_pos_2"], [1580, 1708, "Eval_neg_1"], [1709, 1834, "Jus_neg_1"]]}
{"id": 388, "review": "paper_summary\nThis paper proposes an unsupervised knowledge distillation framework with a multiple-task multiple-teacher model for cross-language named entity recognition. An entity similarity evaluator is proposed to help NER as an auxiliary task. The results show that the approach outperforms baselines on seven languages. \nsummary_of_strengths\n1. \tThis is the first work that using multi-task learning in cross-lingual NER. \n2. \tThe similarity metric model boosts the performance as an auxiliary task part. \n3. \tThe experiments show that the proposed approach performs better than the baselines. \nsummary_of_weaknesses\n1. \tInnovation is weak, only the modification of existing methods. Although existing cross-lingual NER methods don\u2019t consider multi-task learning, it has been widely used in transfer learning/unsupervised learning. \n2. \tThe experiment is not sufficient. A key contribution of this work is the teacher-student framework. But little experiments are carried out to prove its effectiveness. It seems that MTST in ablation study aims to prove it but the description \u201cmultiple-teacher to single-teacher\u201d and \u201cteacher and student have the same neural network structure\u201d is confusing. \ncomments,_suggestions_and_typos\nThere are some grammatical mistakes, such as a)\tin line 115, it should be \"that introduces an entity similarity evaluator\"; b)\tetc. ", "label": [[352, 428, "Eval_pos_1"], [433, 511, "Eval_pos_2"], [516, 600, "Eval_pos_3"], [627, 645, "Eval_neg_1"], [647, 838, "Jus_neg_1"], [843, 876, "Eval_neg_2"], [877, 1200, "Jus_pos_3"]]}
{"id": 390, "review": "paper_summary\nThis paper proposed the use of a set of auxiliary tasks to capture some form of interdependency between arcs in semantic dependency parsing in order to provide a simpler implementation that can obtain robust performance, which achieved near-SOTA and systematic performance on one English SemEval-2015 dataset and one French deep syntactic cyclic graphs. \nsummary_of_strengths\nFor a short paper, this paper is well organized and written. The main research question is motivated by efficiently and clearly discussing pros and cons of the previous work and how the current work is related and differs from them. The overall flow of the presentation of the motivations and arguments is smooth, making it easy to follow. The description of the experiments and the discussion of the results are clear and succinct. The use of auxiliary tasks to capture some form of interdependence between arcs for semantic dependency parsing while remaining a reasonable complexity and competitive results seems a contribution to me. \nsummary_of_weaknesses\n1. A more explicit definition / description / explanation of what is meant by \u201csystematic\u201d performance gains or improvements seems to be necessary since this is one of the proposed advantages of the proposed method, and it would also be better if you could explicitly describe the connection between \"systematic performance gain\" and the robustness of model performance. The aforementioned two aspects could make the contribution of the proposed method more obvious and salient.   2. A brief discussion of the motivation and selection of the auxiliary tasks seem needed in Section 3. Some of the issues were described in the first paragraph of Section 3, which could be viewed as implicitly motivating the use of the auxiliary tasks, but it still seems not very sufficient: the jump from problem description to \u201cHence the idea of using auxiliary tasks taking into account all the heads'' seems abrupt. I think a brief elaboration on why these 4 tasks are designed and chosen would make the picture clearer and would also echo the argument mentioned at the beginning, which is to capture some form of interdependence between arcs. \ncomments,_suggestions_and_typos\nContent: 1. Line 027: what did you mean by \u201cplain dependency parsing\u201d? Syntactic dependency? It would be clearer if a reference is added here to provide an example.  2. Line 047: it would be better and more explicit if the task and/or dataset is mentioned as part of the \u201cachieve state-of-the-art results\u201d statement.  3. Lines 078-079 / Line 08: For clarity, it would be better if the evaluation metric is mentioned here to better understand the scale of the improvement; this would also be helpful to understand the results reported in this paper for comparability: the expression \u201clabelled F-measure scores (LF1) (including ROOT arcs)\u201d was used in Fern\u00e1ndez-Gonz\u00e1lez and G\u00f3mez-Rodr\u00edguez (2020).  4. Line 079: Since ID and OOD are not widely used acronyms and this is the first time of them being used, it would be better to define them first and use ID or OOD thereafter: e.g. in-domain (ID) and out-of-domain (OOD)  5. Lines 130-135: The example wrt the mwe label can probably benefit from a visualization of the description, if the space is allowed. Also, the clause \u201cwhich never happens in the training set\u201d may benefit from a little rephrasing - if I\u2019m not mistaken, this example was mentioned here to support the claim that \u201cimpossible sets of labels for the set of heads of a given dependent\u201d. Does this mean that such a combination is incorrect and thus not possible? If so, saying that such scenarios never happen in the training set could mean that it is likely to conceptually have such combinations exist, but it\u2019s just that it\u2019s not seen in the training data.   6. Is there a specific reason for choosing 9 as the number of runs? ( Conventionally, 3, 5, or 10 are common).  Typos: -Line 018: near-soa > did you mean \u201cnear-state-of-the-art (sota)\u201d? Define before using an acronym -Line 036: decision > decisions  -Line 062: propose > proposes   -Line 097: biafine > biaffine  -footnote2: a dot (.) is missing after \u201cg\u201d in \u201ce.g\u201d - only pointed this out as a typo as you used \u201ce.g.\u201d throughout the paper as in Lines 157 / 225 / 230 / 231  -Line 202: experimented > experiment - only pointed this out as a typo as you seem to use present tense to describe other people\u2019s work as in Lines 047 / 078 / 081 -Line 250: provide > provides/provided; significative > significant (also caption in Table 1) Miscellaneous: 1. Inconsistent use of uppercase / lowercase in the title: tasks > Task; boost > Boost 2. Inconsistent intext citation styles:     - Lines 077-078 & 277-278: (Fern\u00e1ndez-Gonz\u00e1lez and G\u00f3mez-Rodr\u00edguez, 2020) > Fern\u00e1ndez-Gonz\u00e1lez and G\u00f3mez-Rodr\u00edguez (2020)     - Lines 276-277: (He and Choi, 2020) > He and Choi (2020) 3. Placement of footnotes: the footnotes follow the ending punctuation, as shown in the *ACL Template Section 4.1: Lines 104 / 166 / 170 / 231 / 239 / 249 / 250 / 266.  4. FYI - the link to the code in footnote 5 didn\u2019t seem to work; it said \u201cTransfer expired: Sorry, this transfer has expired and is not available any more\u201d. Not sure if I need to register an account to get to the code.   5. Add the abbreviation \u201cLF\u201d after \u201clabeled Fscore\u201d on Line 245 so that the use of \u201cLF\u201d later can be attributed to. ", "label": [[390, 450, "Eval_pos_1"], [451, 490, "Eval_pos_2"], [491, 621, "Jus_pos_2"], [623, 702, "Jus_pos_3"], [704, 728, "Eval_pos_3"], [730, 822, "Eval_pos_4"], [1053, 1196, "Eval_neg_1"], [1197, 1528, "Jus_neg_1"], [1534, 1633, "Eval_neg_2"], [1634, 1782, "Jus_neg_2"], [1784, 1951, "Eval_neg_2"], [1952, 2180, "Jus_neg_2"]]}
{"id": 391, "review": "- Strengths: An interesting and comprehensive study of the effect of using special-domain corpora for training word embeddings.  Clear explanation of the assumptions, contributions, methodology, and results.  Thorough evaluation of various aspects of the proposal.\n- Weaknesses: Some conclusions are not fully backed up by the numerical results.  E.g., the authors claim that for Catalan, the improvements of using specific corpora for training word vectors is more pronounced than English.  I am not sure why this conclusion is made based on the results.  E.g., in Table 6, none of the combination methods outperform the baseline for the 300-dimension vectors.\n- General Discussion: The paper presents a set of simple, yet interesting experiments that suggest word vectors (here trained using the skip-gram method) largely benefit from the use of relevant (in-domain) and subjective corpora. \nThe paper answers important questions that are of benefit to practitioners of natural language processing.  The paper is also very well-written, and very clearly organized. ", "label": [[13, 127, "Eval_pos_1"], [129, 207, "Eval_pos_2"], [209, 264, "Eval_pos_3"], [279, 345, "Eval_neg_1"], [347, 661, "Jus_neg_1"], [894, 1001, "Major_claim"], [1002, 1066, "Eval_pos_4"]]}
{"id": 392, "review": "paper_summary\nIn this paper, the authors challenge the basis of the seq2seq-based ABSA framework and propose the simple yet effective Seq2Path framework. By carefully designing the output format and discriminator tokens, the Seq2Path could generate multiple semantic tuples from the input sentences. The training methods and the decoding algorithm are reasonable and sound. Extensive experimental results demonstrate its superiority over the previous baselines. \nsummary_of_strengths\n- the problem of the previous Seq2Seq model for the ABSA model is reasonable   - experimental results are rich to prove the effectiveness of the proposed Seq2Path framework. The baselines are very detailed. \n  - Seq2Path is simple yet effective for the ABSA tasks. \nsummary_of_weaknesses\nIt makes me confused to understand the meaning of Figure 2. In Figure2, why do the searched results point to the augmented dataset? \ncomments,_suggestions_and_typos\nWhat is the purpose of your paragraphing in the abstract section? ", "label": [[300, 373, "Eval_pos_1"], [374, 462, "Eval_pos_2"], [565, 657, "Eval_pos_3"], [658, 691, "Eval_pos_4"], [696, 749, "Eval_pos_5"], [772, 831, "Eval_neg_1"], [832, 903, "Jus_neg_1"]]}
{"id": 394, "review": "This paper proposes a heterogeneous-event graph network to help solve the missing event prediction task. Based on the reported experimental results, the proposed method achieved slightly better performance than previous works (e.g., EventTransE) on this task. However, I still have some concerns about both the experimental details and the paper writing, details are as follows.\n1. The term \"heterogeneous\" is a little bit misleading. Typically, we use that term to refer to the graphs whose edges have multiple complex types. However, in the proposed model, there are only three relation types (word-word, word-event, event-event). If I understand correctly, both the word-word and event-event are just homogeneous networks and what the author did is adding an additional connection between these two homogeneous graphs. So a better term might be something like \"cross-graph\"?\n2. It seems like the author didn't mention how they construct the graphs in the paper. As the main contribution of this paper is using the word/event graphs, without knowing how did the author create them (e.g., what is the meaning of edges and how to get those connections), we couldn't evaluate the technical soundness of the proposed model.\n3. The author did experiments on the NYT dataset. My concern is that the quality of that dataset is not very satisfying because the test set is also automatically extracted from the documents. I suggest the author try other human-curated event sequence datasets (e.g., RocStory, Wikihow) as the evaluation dataset.\n4. I am not sure whether the evaluation is fair for baseline models. For example, to the best of my knowledge, none of the baseline methods mentioned in this paper use strong pre-trained language models (BERT), and the proposed model does use BERT as part of its model. And as suggested by the ablation study, BER contributes a lot to the final success and I found it is hard to be convinced that the main improvement is coming from the graph model rather than BERT. A better way to do the evaluation is adding one more baseline, which uses BERT to encode the sequence and directly make predictions. A comparison of that can better prove the value of the proposed graph model.\n5. By the way, another suggestion is that maybe you may want to use stronger pre-trained models (RoBERTa) if you want to get better performance.\nTo conclude, I think this paper proposes an interesting solution to the missing event prediction task, but it still needs some further improvement to be good enough to be published in COLING. ", "label": [[382, 434, "Eval_neg_1"], [435, 877, "Jus_neg_1"], [881, 964, "Eval_neg_2"], [965, 1152, "Jus_neg_1"], [1154, 1221, "Eval_neg_2"], [1540, 1605, "Eval_neg_3"], [1606, 2213, "Jus_neg_3"], [2359, 2551, "Major_claim"]]}
{"id": 396, "review": "paper_summary\nThis paper proposes a novel approach that jointly utilizes annotator information, that labels and elicited rationales are used to speed up the training of deep learning models with limited training data. The ranking constraint is the most important mechanism introduced. Performance evaluation shows that the proposed method achieves state-of-the-art performance with higher efficiency and lower demand for data. \nsummary_of_strengths\n1. The proposed approach is well-designed and novel. This paper derives the constraint formulas in a rigorous logical flow. \n2. This paper contributes a new text classification dataset with rationales and makes it publicly available. \n3. The experiments are sufficiently and reasonably conducted to verify the effectiveness of this method. \nsummary_of_weaknesses\n1. Experimental part is less convincing. The LR and SVM based baselines are too weak compared with deep learning approaches. \n2. Figure 2 does not show complete trends w.r.t the training documents. It seems that LwR-RC might be outperformed with RB-WAVG or even Lw/oR-BERT. Moreover, analysis of this trend is insufficiently discussed. \ncomments,_suggestions_and_typos\n1. Please provide some statistics of three benchmarks as truncation has been used in both sentence length and sentence number for each instance. \n2. Please reorganize the section of performance evaluation to highlight the strengths of the proposed methods. ", "label": [[452, 501, "Eval_pos_1"], [502, 573, "Eval_pos_2"], [576, 682, "Eval_pos_3"], [687, 789, "Eval_pos_4"], [815, 852, "Eval_neg_1"], [853, 937, "Jus_neg_1"], [941, 1085, "Jus_neg_2"], [1086, 1148, "Eval_neg_2"]]}
{"id": 398, "review": "paper_summary\nThis paper presents two extensions for improving non-autoregressive machine translation (NAT) with lexical constraints, with a focus on low frequency words. The authors take as base model an approach very similar to that of Xu & Carpuat 2021 and expand it with a) \"constrained training\", where the system already sees lexicon constraints at training time and b) \"alignment prompting\", where alignment information is included into the model. The motivation for this last method is to inform the model as to what source word(s) correspond to the constraint, so that the context can be better generated. Experimental results shows that the method is able to improve the generation quality (measured in BLEU), specially for out-of-domain tasks, while retaining the efficiency advantages of NAT systems. \nsummary_of_strengths\n- The paper is clearly written and well-motivated. Section 3.3 gives a clear indication of shortcomings of current models, and Section 6.1 revisits the issue and analyzes how the model addresses it.\n-The approach is general enough that it can be applied to different models.\n-Table 5 shows good improvements for out-of-domain test sets.\n-The approach is (mostly) clearly described and the authors plan to release the code. \nsummary_of_weaknesses\n- The evaluation of the paper could be made stronger by using some of the standard datasets for terminology translation (e.g. wmt21 shared task) and evaluation metrics (Alam et al. 2021).\n-The description of the alignment embedding seems a bit under-specified. Am I understanding it correctly that the constraints in the target sentence get an additional index that gets embedded (similar in concept to positional embeddings)? Are unaligned words in the constraints marked in a special manner? Are these embeddings recomputed in every refinement step of the LevT? \ncomments,_suggestions_and_typos\n- Line 232: The are surely sentences where not every bucket is represented, right? Would it be then more correct to say that you have **approximately** 6x the data? Or am I misunderstanding something?\n-Line 248: This explanation, while plausible, could be relatively easy to check just by looking at the words themselves. Have you done that? Have you tried filtering those words out (e.g. using stopword lists or similar) as they are unlikely to appear as constraints in real life situations?\n-Line 263: It would be better to use l_i.\n-Subsubsection starting at 324: Would it make sense to use the lexicon information directly if available (which it is for some test conditions) and resort to automatic tools only if necessary? Related to this, have you measured how sensitive the method is against alignment errors? This is also specially relevant for out-of-domain settings, where the alignment model is also operating in out-of-domain conditions.\n-Table 4: Please also include bold numbers for the baselines of previous work. Specifically for WMT17-WIKT the best result in terms of BLEU is actually in the baselines. ", "label": [[837, 885, "Eval_pos_1"], [886, 1033, "Jus_pos_1"], [1035, 1065, "Eval_pos_2"], [1066, 1109, "Jus_pos_2"], [1283, 1333, "Eval_neg_1"], [1334, 1468, "Jus_neg_1"], [1470, 1541, "Eval_neg_2"], [1542, 1845, "Jus_neg_2"]]}
{"id": 399, "review": "paper_summary\nThis paper proposed a method to elicit knowledge from language models and used the generated knowledge to enhance commonsense reasoning tasks\u2019 performance. Specifically, they designed templates for each task they consider and prompted GPT3 to produce relevant knowledge for each question. The generated knowledge is then appended to the question individually and sent to the inference model (T5 11B or its finetuned version). They show that the additional knowledge improves the performance for both zero-shot and finetuned models. \nsummary_of_strengths\n1. The proposed method is simple and straightforward to implement, yet it achieves performance gain compared to several baselines and got SOTA results on 3 tasks. \n2. The analysis showed that the model gets better performance with more knowledge (up to 20) and the improvements on smaller inference models are even larger. The human evaluation also suggests that the generated knowledge correlated well with human judgment in terms of helpfulness. \nsummary_of_weaknesses\n1. Since this paper is proposing a knowledge prompting method, the only real comparison is with self-talk method, however in table 3, only CSQA results are reported (because self-talk templates are only available for CSQA).  Thus advantage over other knowledge prompting methods is less well supported. Also, I think the results of self-talk+UnifiedQA should be included. \n2. Intuitively, I\u2019m not surprised that GPT3 can produce helpful knowledge for these tasks, however it\u2019s unclear whether the proposed method would still be effective when using a smaller LM (as the knowledge source)? \ncomments,_suggestions_and_typos\nIt would be great to address the weaknesses above. ", "label": [[571, 731, "Eval_pos_1"], [1042, 1262, "Jus_neg_1"], [1264, 1341, "Eval_neg_1"], [1342, 1411, "Jus_neg_1"], [1415, 1628, "Eval_neg_2"]]}
{"id": 400, "review": "Summary: The paper applies a sequence to sequence (seq2seq) approach for German historical text normalization, and showed that using a grapheme-to-phoneme generation as an auxiliary task in a multi-task learning (MTL) seq2seq framework improves performance. The authors argue that the MTL approach replaces the need for an attention menchanism, showing experimentally that the attention mechanism harms the MTL performance. The authors also tried to show statistical correlation between the weights of an MTL normalizer and an attention-based one.\nStrengths: 1) Novel application of seq2seq to historical text correction, although it has been applied recently to sentence grammatical error identification [1].  2) Showed that using grapheme-to-phoneme as an auxiliary task in a MTL setting improves text normalization accuracy.\nWeaknesses: 1) Instead of arguing that the MTL approach replaces the attention mechanism, I think the authors should investigate why attention did not work on MTL, and perhaps modify the attention mechanism so that it would not harm performance.\n2) I think the authors should reference past seq2seq MTL work, such as [2] and [3]. The MTL work in [2] also worked on non-attention seq2seq models.\n3) This paper only tested on one German historical text data set of 44 documents. It would be interesting if the authors can evaluate the same approach in another language or data set.\nReferences: [1] Allen Schmaltz, Yoon Kim, Alexander M. Rush, and Stuart Shieber. 2016. \nSentence-level grammatical error identification as sequence-to-sequence correction. In Proceedings of the 11th Workshop on Innovative Use of NLP for Building Educational Applications.\n[2] Minh-Thang Luong, Ilya Sutskever, Quoc V. Le, Oriol Vinyals, and Lukasz Kaiser. Multi-task Sequence to Sequence Learning. ICLR\u201916.  [3] Dong, Daxiang, Wu, Hua, He, Wei, Yu, Dianhai, and Wang, Haifeng. \nMulti-task learning for multiple language translation. ACL'15 --------------------------- Here is my reply to the authors' rebuttal: I am keeping my review score of 3, which means I do not object to accepting the paper. However, I am not raising my score for 2 reasons: - the authors did not respond to my questions about other papers on seq2seq MTL, which also avoided using attention mechanism. So in terms of novelty, the main novelty lies in applying it to text normalization.\n- it is always easier to show something (i.e. attention in seq2seq MTL) is not working, but the value would lie in finding out why it fails and changing the attention mechanism so that it works. ", "label": [[562, 709, "Eval_pos_1"], [1077, 1135, "Eval_neg_1"], [1137, 1222, "Jus_neg_1"]]}
{"id": 401, "review": "- Strengths: This paper introduced a novel method to improve zero pronoun resolution performance.. The main contributions of this papers are: 1) proposed a simple method to automatically generate a large training set for zero pronoun resolution task; 2) adapted a two step learning process to transfer knowledge from large data set to the specific domain data; 3) differentiate unknown words using different tags. In general, the paper is well written. Experiments are thoroughly designed.  - Weaknesses: But I have a few questions regarding finding the antecedent of a zero pronoun: 1. How will an antecedent be identified, when the prediction is a pronoun? The authors proposed a method by matching the head of noun phrases. It\u2019s not clear how to handle the situation when the head word is not a pronoun. \n2. What if the prediction is a noun that could not be found in the previous contents? \n3. The system achieves great results on standard data set. I\u2019m curious is it possible to evaluate the system in two steps? The first step is to evaluate the performance of the model prediction, i.e. to recover the dropped zero pronoun into a word; the second step is to evaluate how well the systems works on finding an antecedent.\nI\u2019m also curious why the authors decided to use attention-based neural network. A few sentences to provide the reasons would be helpful for other researchers.\nA minor comment: In figure 2, should it be s1, s2 \u2026 instead of d1, d2 \u2026.?  - General Discussion: Overall it is a great paper with innovative ideas and solid experiment setup. ", "label": [[13, 98, "Eval_pos_1"], [145, 249, "Eval_pos_2"], [413, 452, "Eval_pos_3"], [453, 490, "Eval_pos_4"], [898, 953, "Eval_pos_5"], [1483, 1561, "Major_claim"]]}
{"id": 402, "review": "paper_summary\nThis paper proposes a latent-variable model based on a variational probabilistic modeling framework for conditional compute in multi-domain and multilingual machine translation. They introduce latent variables which are learned during model training. These latent variables decide which sub-network of the model is selected for each task / language pair. \nThe results on multi-domain machine translation show promise compared to vanilla Transformer and Adapter baselines. But most of the improvements can be captured by a heuristic multi-task group dropout baseline, and there is a small improvement (0.5 bleu) over this baseline with the variational approach proposed. \nThe paper includes some interesting analysis of the models as well. On the other hand, some one important ablations on whether group dropout helps vs a baseline with group size=1 is missing and the improvements on multilingual translation are not significant enough. Overall, this leaves me unsure whether the added complexity of this approach is worth it over the simpler baselines (vanilla Transformer or heuristic multi-task group dropout). \nsummary_of_strengths\n1. This work proposes a technique to learn which subnetworks to use for which task / language pair based on learning from the data. \n2. The proposed probabilistic framework is mathematically sound and explained clearly and completely. \n3. There are experiments compared with decent baselines and multiple benchmarks and there has been a good attempt to evaluate the new technique well. \n4. The analysis showing that accuracy of Adapter baselines declines on a domain when that domain is divided into two pseudo-domains, where as the accuracy on this proposed framework remains the same motivates the reason of using the proposed technique very well 5. A nice advantage of this approach is that it doesn\u2019t need additional parameters to the added to the baseline Transformer model. There are some additional hyper-parameters to tune correctly per setup though. \n6. There is also analysis showing whether the learned subnetworks for related domains/languages overlap significantly. \n7. [ Updated] A nice baseline - heuristic multi-task group dropout has been added in the revised version of the paper that helps disentangle the impact of the variational approach. \nsummary_of_weaknesses\n1. While the authors mention leaving this to future work, the study of how many groups to divide each layer into is missing and how sensitive is model performance to the choice of number of groups is missing \u2192 how does the proposed group dropout compare to just a regular dropout with group size = 1; I think this is important because the paper proposes \u201cLatent Group Dropout\u201d and whether group dropout is needed or not is a key question that is not experimentally verified. \n2. \u201c heuristic multi-task group dropout\u201d (HMGD) captures most of the improvements between Transformer vs proposed LaMGD with a 0.5 BLEU difference between HMGD and LaMGD - It is worth making a case that the simpler approach HMGD is also a win 3. It is unclear if there was an attempt to tune the sizes and dropout for the adapter modules in the adapters baseline 4. Table 5 - very few pairs (7 out of 32) show significant gains over the Transformer baseline - it is actually unclear how much language specific model capacity helps on this TED benchmark at all. Even the Adapters baseline is very similar to the Transformer baseline. \ncomments,_suggestions_and_typos\n1. Regarding \u201cFinally, we set the weight of the entropy term to 0.0001 in the training loss in every experiments.\u201d - ideally, you should refer back to the exact term in the exact equation described in the previous sections 2. I don\u2019t understand what \u201c0^nd\u201d means in section 3.1.4  3. Boldface in Table 3 doesn\u2019t always refer to the best BLEU per column ", "label": [[1287, 1386, "Eval_pos_1"], [1389, 1466, "Eval_pos_2"], [1471, 1535, "Eval_pos_3"], [1541, 1736, "Jus_pos_4"], [1737, 1799, "Eval_pos_4"], [1803, 1836, "Eval_pos_5"], [1837, 2009, "Jus_pos_5"], [2145, 2160, "Eval_pos_6"], [2163, 2312, "Jus_pos_6"], [2338, 2542, "Eval_neg_1"], [2545, 2810, "Jus_neg_1"], [3057, 3173, "Eval_neg_2"], [3177, 3201, "Eval_neg_3"], [3203, 3214, "Jus_neg_3"], [3216, 3268, "Eval_neg_3"], [3271, 3371, "Eval_neg_4"], [3372, 3443, "Jus_neg_4"]]}
{"id": 403, "review": "This paper presents a graph-based approach for producing sense-disambiguated synonym sets from a collection of undisambiguated synonym sets.  The authors evaluate their approach by inducing these synonym sets from Wiktionary and from a collection of Russian dictionaries, and then comparing pairwise synonymy relations (using precision, recall, and F1) against WordNet and BabelNet (for the English synonym sets) or RuThes and Yet Another RussNet (for the Russian synonym sets).\nThe paper is very well written and structured.              The experiments and evaluations (or at least the prose parts) are very easy to follow.              The methodology is sensible and the analysis of the results cogent.  I was happy to observe that the objections I had when reading the paper (such as the mismatch in vocabulary between the synonym dictionaries and gold standards) ended up being resolved, or at least addressed, in the final pages.\nThe one thing about the paper that concerns me is that the authors do not seem to have properly understood the previous work, which undercuts the stated motivation for this paper.\nThe first instance of this misunderstanding is in the paragraph beginning on line 064, where OmegaWiki is lumped in with Wiktionary and Wikipedia in a discussion of resources that are \"not formally structured\" and that contain \"undisambiguated synonyms\".  In reality, OmegaWiki is distinguished from the other two resources by using a formal structure (a relational database) based on word senses rather than orthographic forms.              Translations, synonyms, and other semantic annotations in OmegaWiki are therefore unambiguous.\nThe second, and more serious, misunderstanding comes in the three paragraphs beginning on lines 092, 108, and 120.  Here the paper claims that both BabelNet and UBY \"rely on English WordNet as a pivot for mapping of existing resources\" and criticizes this mapping as being \"error-prone\".  Though it is true that BabelNet uses WordNet as a pivot, UBY does not.  UBY is basically a general-purpose specification for the representation of lexical-semantic resources and of links between them.  It exists independently of any given lexical-semantic resource (including WordNet) and of any given alignment between resources (including ones based on \"similarity of dictionary definitions\" or \"cross-lingual links\").  Its maintainers have made available various databases adhering to the UBY spec; these contain a variety of lexical-semantic resources which have been aligned with a variety of different methods.  A given UBY database can be *queried* for synsets, but UBY itself does not *generate* those synsets.  Users are free to produce their own databases by importing whatever lexical-semantic resources and alignments thereof are best suited to their purposes.  The three criticisms of UBY on lines 120 to 125 are therefore entirely misplaced.\nIn fact, I think at least one of the criticisms is not appropriate even with respect to BabelNet.  The authors claim that Watset may be superior to BabelNet because BabelNet's mapping and use of machine translation are error-prone.  The implication here is that Watset's method is error-free, or at least significantly less error-prone.  This is a very grandiose claim that I do not believe is supported by what the authors ought to have known in advance about their similarity-based sense linking algorithms and graph clustering algorithms, let alone by the results of their study.  I think this criticism ought to be moderated.              Also, I think the third criticism (BabelNet's reliance on WordNet as a pivot) somewhat misses the point -- surely the most important issue to highlight isn't the fact that the pivot is English, but rather that its synsets are already manually sense-annotated.\nI think the last paragraph of \u00a71 and the first two paragraphs of \u00a72 should be extensively revised. They should focus on the *general* problem of generating synsets by sense-level alignment/translation of LSRs (see Gurevych et al., 2016 for a survey), rather than particularly on BabelNet (which uses certain particular methods) and UBY (which doesn't use any particular methods, but can aggregate the results of existing ones).  It may be helpful to point out somewhere that although alignment/translation methods *can* be used to produce synsets or to enrich existing ones, that's not always an explicit goal of the process.  Sometimes it's just a serendipitous (if noisy) side-effect of aligning/translating resources with differing granularities.\nFinally, at several points in the paper (lines 153, 433), the \"synsets\" of TWSI of JoBimText are criticized for including too many words that are hypernyms, co-hypnomyms, etc. instead of synonyms.  But is this problem really unique to TWSI and JoBimText?  That is, how often do hypernyms, co-hypernyms, etc. appear in the output of Watset?  (We can get only a very vague idea of this from comparing Tables 3 and 5, which analyze only synonym relations.)  If Watset really is better at filtering out words with other semantic relations, then it would be nice to see some quantitative evidence of this.\nSome further relatively minor points that should nonetheless be fixed: - Lines 047 to 049: The sentence about Kiselev et al. (2015) seems rather useless.  Why bother mentioning their analysis if you're not going to tell us what they found?\n- Line 091: It took me a long time to figure out how \"wat\" has any relation to \"discover the correct word sense\".  I suppose this is supposed to be a pun on \"what\".  Maybe it would have been better to call the approach \"Whatset\"?  Or at least consider rewording the sentence to better explain the pun.\n- Figure 2 is practically illegible owing to the microscopic font.  Please increase the text size!\n- Similarly, Tables 3, 4, and 5 are too small to read comfortably.  Please use a larger font.              To save space, consider abbreviating the headers (\"P, \"R\", \"F1\") and maybe reporting scores in the range 0\u2013100 instead of 0\u20131, which will eliminate a leading 0 from each column.\n- Lines 517\u2013522: Wiktionary is a moving target.  To help others replicate or compare against your work, please indicate the date of the Wiktionary database dump you used.\n- Throughout: The constant switching between Times and Computer Modern is distracting.  The root of this problem is a longstanding design flaw in the ACL 2017 LaTeX style file, but it's exacerbated by the authors' decision to occasionally set numbers in math mode, even in running text.  Please fix this by removing \\usepackage{times} from the preamble and replacing it with either \\usepackage{newtxtext} \\usepackage{newtxmath} or \\usepackage{mathptmx} References: I Gurevych, J. Eckle-Kohler, and M. Matuschek, 2016. Linked Lexical Knowledge Bases: Foundations and Applications, volume 34 of Synthesis Lectures on Human Language Technologies, chapter 3: Linking Algorithms, pages 29-44. Morgan & Claypool.\n---- I have read the author response. ", "label": [[479, 525, "Eval_pos_1"], [539, 625, "Eval_pos_2"], [639, 666, "Eval_pos_3"], [671, 705, "Eval_pos_4"], [708, 779, "Eval_pos_5"], [781, 867, "Jus_pos_5"], [869, 936, "Eval_pos_5"], [937, 1116, "Eval_neg_1"], [1117, 2898, "Jus_neg_1"]]}
{"id": 404, "review": "paper_summary\nThis paper proposed a hierarchical recurrent aggregation framework to reduce the few-shot NLG task. The proposed framework consists of three modules, lexicalization, aggregation, and post-editing, which are composed of PLMs and do not share parameters. The experimental results show that the proposed method is slightly better than the baseline under low-resource settings. \nsummary_of_strengths\nThe method performs better than the compared baseline under low-resource settings, especially for MER metrics. \nsummary_of_weaknesses\n1. ** Poor efficiency**. The proposed method contains three different modules, each containing an unshared PLM so that the amount of parameters is three times that of the End2End method. As T5 is good at multi-task learning, I think the author should consider how to improve the performance of the model in the case of parameter sharing, rather than simply stacking multiple models. \n2. ** Time-consuming**. The proposed method requires recurrent generation so that the speed of response generation will be significantly reduced, especially when the amount of attribute-value pairs is relatively large. It should be noted that the NLG model is only a part of the dialogue system of the pipeline structure, and time efficiency also needs to be taken into consideration. \n3. ** Lack of strong baselines**. The baseline for comparison is only E2E-T5, which is a naive E2E-NLG method and may not be strong enough. Therefore, the improvement of the proposed method over a single T5 model(E2E-T5) may not be a very valuable conclusion.  I think the author should investigate and compare more few-shot NLG methods to enhance the persuasiveness of the experimental results. \n4. Lack of some additional insight into the effectiveness analysis for three modules. \n5. The innovations in this paper are more like tricks for improving metrics. \ncomments,_suggestions_and_typos\n**Suggestions**: See above.\n**Questions**: In table 1, The MER indicators of +aggregation and +post-edit are both higher than 1.1, while the MER of +selection is very low. From Line.402-Line.405, the selection is selecting the one with lower MER from aggregation and post-edit. Why the best MER result is reduced to 0.14 after selection ", "label": [[547, 568, "Eval_neg_1"], [569, 927, "Jus_neg_1"], [931, 951, "Eval_neg_2"], [952, 1313, "Jus_neg_2"], [1317, 1347, "Eval_neg_3"], [1348, 1709, "Jus_neg_3"], [1714, 1797, "Eval_neg_5"], [1801, 1875, "Eval_neg_6"]]}
{"id": 405, "review": "This paper proposes a supervised deep learning model for event factuality identification.  The empirical results show that the model outperforms state-of-the-art systems on the FactBank corpus, particularly in three classes (CT-, PR+ and PS+).  The main contribution of the paper is the proposal of an attention-based two-step deep neural model for event factuality identification using bidirectional long short-term memory (BiLSTM) and convolutional neural network (CNN).\n[Strengths:] - The structure of the paper is (not perfectly but) well organized.\n- The empirical results show convincing (statistically significant) performance gains of the proposed model over strong baseline.\n[Weaknesses:] See below for details of the following weaknesses: - Novelties of the paper are relatively unclear.\n- No detailed error analysis is provided.\n- A feature comparison with prior work is shallow, missing two relevant papers.\n- The paper has several obscure descriptions, including typos.\n[General Discussion:] The paper would be more impactful if it states novelties more explicitly.  Is the paper presenting the first neural network based approach for event factuality identification?  If this is the case, please state that.\nThe paper would crystallize remaining challenges in event factuality identification and facilitate future research better if it provides detailed error analysis regarding the results of Table 3 and 4.              What are dominant sources of errors made by the best system BiLSTM+CNN(Att)?  What impacts do errors in basic factor extraction (Table 3) have on the overall performance of factuality identification (Table 4)?  The analysis presented in Section 5.4 is more like a feature ablation study to show how useful some additional features are.\nThe paper would be stronger if it compares with prior work in terms of features.  Does the paper use any new features which have not been explored before?  In other words, it is unclear whether main advantages of the proposed system come purely from deep learning, or from a combination of neural networks and some new unexplored features.  As for feature comparison, the paper is missing two relevant papers: - Kenton Lee, Yoav Artzi, Yejin Choi and Luke Zettlemoyer. 2015 Event Detection and Factuality Assessment with Non-Expert Supervision. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1643-1648.\n- Sandeep Soni, Tanushree Mitra, Eric Gilbert and Jacob Eisenstein. 2014. \nModeling Factuality Judgments in Social Media Text. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 415-420.\nThe paper would be more understandable if more examples are given to illustrate the underspecified modality (U) and the underspecified polarity (u).  There are two reasons for that.  First, the definition of 'underspecified' is relatively unintuitive as compared to other classes such as 'probable' or 'positive'. \nSecond, the examples would be more helpful to understand the difficulties of Uu detection reported in line 690-697.  Among the seven examples (S1-S7), only S7 corresponds to Uu, and its explanation is quite limited to illustrate the difficulties.\nA minor comment is that the paper has several obscure descriptions, including typos, as shown below: - The explanations for features in Section 3.2 are somewhat intertwined and thus confusing.  The section would be more coherently organized with more separate paragraphs dedicated to each of lexical features and sentence-level features, by:   - (1) stating that the SIP feature comprises two features (i.e., lexical-level and sentence-level) and introduce their corresponding variables (l and c) *at the beginning*;   - (2) moving the description of embeddings of the lexical feature in line 280-283 to the first paragraph; and   - (3) presenting the last paragraph about relevant source identification in a separate subsection because it is not about SIP detection.\n- The title of Section 3 ('Baseline') is misleading.  A more understandable title would be 'Basic Factor Extraction' or 'Basic Feature Extraction', because the section is about how to extract basic factors (features), not about a baseline end-to-end system for event factuality identification.\n- The presented neural network architectures would be more convincing if it describes how beneficial the attention mechanism is to the task.\n- Table 2 seems to show factuality statistics only for all sources.  The table would be more informative along with Table 4 if it also shows factuality statistics for 'Author' and 'Embed'.\n- Table 4 would be more effective if the highest system performance with respect to each combination of the source and the factuality value is shown in boldface.\n- Section 4.1 says, \"Aux_Words can describe the *syntactic* structures of sentences,\" whereas section 5.4 says, \"they (auxiliary words) can reflect the *pragmatic* structures of sentences.\"  These two claims do not consort with each other well, and neither of them seems adequate to summarize how useful the dependency relations 'aux' and 'mark' are for the task.\n- S7 seems to be another example to support the effectiveness of auxiliary words, but the explanation for S7 is thin, as compared to the one for S6.  What is the auxiliary word for 'ensure' in S7?\n- Line 162: 'event go in S1' should be 'event go in S2'.\n- Line 315: 'in details' should be 'in detail'.\n- Line 719: 'in Section 4' should be 'in Section 4.1' to make it more specific.\n- Line 771: 'recent researches' should be 'recent research' or 'recent studies'.  'Research' is an uncountable noun.\n- Line 903: 'Factbank' should be 'FactBank'. ", "label": [[488, 553, "Eval_pos_1"], [556, 683, "Eval_pos_2"], [751, 797, "Eval_neg_1"], [800, 839, "Eval_neg_2"], [842, 889, "Eval_neg_3"], [891, 918, "Eval_neg_4"], [921, 982, "Eval_neg_5"], [1005, 1078, "Eval_neg_6"], [1080, 1221, "Jus_neg_6"], [1222, 1423, "Eval_neg_7"], [1436, 1771, "Jus_neg_7"], [1772, 1852, "Eval_neg_8"], [1854, 2180, "Jus_neg_8"], [2659, 2807, "Eval_neg_9"], [2809, 3220, "Jus_neg_9"], [3324, 3414, "Eval_neg_10"], [3415, 3988, "Jus_neg_10"]]}
{"id": 406, "review": "The paper presents a new approach that improves the past transition-based RST parser. The authors propose a new nuclear type and a new action to handle specific cases that the past systems have ignored.  - The authors did a good job conducting a series of experiments along the ablation studies that help us understand how each component contributes to the improvement.\n-The reviewer is quite confused with the description for the \"Separate\" system.\n-The paper will benefit from a diagram similar to the one presented in Dozat's paper. Bi-Affine model is not a well-known model. The authors should include a diagram also.\n-It takes some guesswork to understand how EDU representation is derived from the sentence representation. We simply average across all of the words in the EDU? Should there be a better way to derive that? Could we instead run BiLSTM over the EDU instead and then pass it through the Bi-Affine model?  -What's the differences between row1 and row2 in Table2? Please explain more.\n-The claim of this paper is that the new nuclear type and new action (and Bi-Affine features too?) contributes to the improvement. Table2 should label more clearly which feature is involved here.  -The idea of R^ and N^ is somewhat similar to the idea of markovization in phrase structure parsing, which is a classic technique in CYK parsing with PCFG. It would be nice to relate to the idea or argue why it is different. ", "label": [[371, 449, "Eval_neg_1"], [451, 535, "Eval_neg_2"], [536, 621, "Jus_neg_2"], [623, 728, "Eval_neg_3"], [729, 1001, "Jus_neg_3"]]}
{"id": 407, "review": "paper_summary\nThe paper proposed a flexible framework for multi-task BERT serving. Specifically, instead of training a single multi-task BERT or training several independent fine-tuned BERT models for different tasks, the proposed framework only partially fine-tune the top layers in the BERT model, which allows these sub-task models to share the parameters in the bottm layers. The paper further proposes to compress the fine-tuned topmost layers into less layers to reduce model parameters. On the GLUE benchmark, the proposed method outperfroms other smaller BERT variants with a better accuracy-model_size trade-off. Overall, the review believes this is a strong short paper. \nsummary_of_strengths\n1. The writing of the paper is clear and the motivation is clear. The proposed method (i.e., combining partial weight sharing and compression) is novel.\n2. The proposed method can compress BERT-base by 60% with only 0.3 performance degeneration on GLUE tasks. The empirical results are quite good. \nsummary_of_weaknesses\n1. For flexible multi-task, the paper proposes to initialize the student from the bottommost Ns layers of the teacher. Besides, the paper proposes to determine the number of task-specific layers for each task based on the marginal benefit. These two design choices seem to be a bit arbitrary. Have the authors tried other strategies such as 1) initialize the student like in PKD-BERT 2) determine the number of layers by total budgets (for example, fine-tune students with different number of layers and set an hard overhead budget for all sub-tasks, like the current 34.3%, and do a brute-force search to fine the optimal student layer configuration)? \ncomments,_suggestions_and_typos\nSee Weaknesses ", "label": [[622, 681, "Major_claim"], [706, 739, "Eval_pos_1"], [744, 767, "Eval_pos_2"], [769, 788, "Eval_pos_3"], [790, 844, "Jus_pos_3"], [846, 855, "Eval_pos_3"], [963, 1001, "Eval_pos_4"], [1027, 1263, "Jus_neg_1"], [1264, 1316, "Eval_neg_1"], [1317, 1677, "Jus_neg_1"]]}
{"id": 408, "review": "This paper proposes a hybrid model for the representation of contextual embeddings (BERT) that aims to disambiguate French nouns at supersense level. The model's performance does not improve the BERT state of the art performance on the same corpus (FlauBERT). However, it has the advantage of making embeddings interpretable, helping in the error analysis. In the error analysis section, the main finding is that errors are due to the polysemous nature of nouns: it is not surprising since polysemy is the primary source of errors for all the WSD systems. The paper does not propose how to improve on that aspect - just using a different corpus is mentioned as a possibility, but not changes in the methodology are tried or suggested -. In other words, even if the methodology is interesting and it is true that makes the embeddings less opaque, at the moment it is not able to guarantee improvements in the results because, from error analysis, it is not possible to get insights on how to improve the hybrid embedding representations of polysemous words. ", "label": [[737, 915, "Eval_neg_1"], [916, 1057, "Jus_neg_1"]]}
{"id": 409, "review": "This paper presents a unified annotation that combines macrostructures and RST structure in Chinese news articles. Essentially, RST structure is adopted for each paragraph and macrostructure is adopted on top of the paragraphs. \nWhile the view that nuclearity should not depend on the relation label itself but also on the context is appealing, I find the paper having major issues in the annotation and the experiments, detailed below: - The notion of \u201cprimary-secondary\u201d relationship is advocated much in the paper, but later in the paper that it became clear this is essentially the notion of nuclearity, extended to macrostructure and making it context-dependent instead of relation-dependent. Even then, the status nuclear-nuclear, nuclear-satellite, satellite-nuclear are \u201credefined\u201d as new concepts.\n- Descriptions of established theories in discourse are often incorrect. For example, there is rich existing work on pragmatic functions of text but it is claimed to be something little studied. There are errors in the related work section, e.g., treating RST and the Chinese Dependency Discourse Treebank as different as coherence and cohesion; the computational approach subsection lacking any reference to work after 2013; the performance table of nuclearity classification confusing prior work for sentence-level and document-level parsing.\n- For the annotation, I find the macro structure annotation description confusing; furthermore, statistics for the macro labels are not listed/reported. The agreement calculation is also problematic; the paper stated that \"Our measurement data is only taken on the layer of leaf nodes\". I don't think this can verify the validity of the annotation. There are multiple mentions in the annotation procedure that says \u201cprelim experiments show this is a good approach\u201d, but how? Finally it is unclear how the kappa values are calculated since this is a structured task; is this the same calculation as RST discourse treebank?\n- It is said in the paper that nuclearity status closely associates with the relation label itself. So what is the baseline performance that just uses the relation label? Note that some features are not explained at all (e.g., what are \u201chierarchical characteristics\u201d?)\n- The main contribution of the paper is the combination of macro and micro structure. However, in the experiments only relations at the micro level are evaluated; even so, only among 5 handpicked ones. I don't see how this evaluation can be used to verify the macro side hence supporting the paper.\n- The paper contains numerous grammatical errors. Also, there is no text displayed in Figure 7 to illustrate the example. ", "label": [[229, 419, "Major_claim"], [809, 879, "Eval_neg_1"], [880, 1001, "Jus_neg_1"], [1002, 1046, "Eval_neg_2"], [1048, 1351, "Jus_neg_2"], [1354, 1551, "Eval_neg_3"], [1552, 1973, "Jus_neg_3"], [1976, 2144, "Jus_neg_4"], [2145, 2193, "Eval_neg_4"], [2195, 2241, "Jus_neg_4"], [2245, 2444, "Jus_neg_5"], [2445, 2541, "Eval_neg_5"]]}
{"id": 410, "review": "paper_summary\nThis paper introduces **natural instructions**, a collection of 61 tasks in natural language processing, featuring the actual instructions used in the crowdsourcing process of the original NLP tasks. These instructions are mapped to a unified schema which include definition, prompt, positive/negative examples etc. ( Fig. 4). Extensive experiments are done with BART and GPT-3. Results suggest that (1) instructions help improve cross-task generalization; (2) increasing the number of tasks leads to better generalization; (3) negative examples are unexpectedly not helpful. \nsummary_of_strengths\n1. \tThe paper studies the interesting and important problem of cross-task generalization (generalization to held-out unseen tasks). The findings may be useful to the ACL audience. \n2. \tNatural instructions re-uses existing resources (instructions to crowd-workers) smartly. It provides a realistic learning/instruction-taking environment. Natural instructions is a valuable and unique resource complementary to the concurrent work of PromptSource and FLAN. \n3. \tCareful experiment design (three aspects of generalization: category, dataset, task). Detailed ablation and analysis in the appendix. \nsummary_of_weaknesses\nBelow are some minor suggestions or questions.\n-In the future version, if space allows,    * please add examples of each category (table 2) so the reader can better understand the difficulty of each category. For the analysis in Line 469-486, it\u2019s hard to understand the hypothesis without looking at what the instruction looks like for each category. \n  * please move some interesting analysis in the appendix to the main body (ablation of elements, error analysis).  -The datasets included in the paper are mainly related to QA. The classification category is also quite specific, e.g., classifying answer type/answerability. I wonder if the schema is extensive enough for general text classification tasks and general NLP tasks. \ncomments,_suggestions_and_typos\nSee comments above. ", "label": [[616, 743, "Eval_pos_1"], [744, 791, "Eval_pos_2"], [796, 885, "Eval_pos_3"], [886, 1069, "Jus_pos_3"], [1074, 1099, "Eval_pos_4"], [1101, 1157, "Jus_pos_4"], [1160, 1208, "Eval_pos_5"]]}
{"id": 411, "review": "paper_summary\nThis paper presents a new dataset on skill extraction and also explains the annotation guidelines of the dataset. They also provide baselines for the dataset. \nsummary_of_strengths\nThe strengths are as follows: 1) Very well written paper. \n2) Novel dataset with helpful guidelines that are made public which are very rare 3) Baseline model has been provided as well. \nsummary_of_weaknesses\nNo weaknesses as such found. This is a resubmission and the authors have dealt with the earlier questions effectively. \ncomments,_suggestions_and_typos\nPretty impressive paper. ", "label": [[228, 253, "Eval_pos_1"], [257, 335, "Eval_pos_2"], [404, 433, "Major_claim"], [556, 581, "Major_claim"]]}
{"id": 413, "review": "This paper describes interesting and ambitious work: the automated conversion of Universal Dependency grammar structures into [what the paper calls] semantic logical form representations.  In essence, each UD construct is assigned a target construction in logical form, and a procedure is defined to effect the conversion, working \u2018inside-out\u2019 using an intermediate form to ensure proper nesting of substructures into encapsulating ones.  Two evaluations are carried out: comparing the results to gold-standard lambda structures and measuring the effectiveness of the resulting lambda expressions in actually delivering the answers to questions from two QA sets.   It is impossible to describe all this adequately in the space provided.  The authors have taken some care to cover all principal parts, but there are still many missing details.  I would love to see a longer version of the paper! \nParticularly the QA results are short-changed; it would have been nice to learn which types of question are not handled, and which are not answered correctly, and why not.  This information would have been useful to gaining better insight into the limitations of the logical form representations.   That leads to my main concern/objection.  This logical form representation is not in fact a \u2018real\u2019 semantic one.                          It is, essentially, a rather close rewrite of the dependency structure of the input, with some (good) steps toward \u2018semanticization\u2019, including the insertion of lambda operators, the explicit inclusion of dropped arguments (via the enhancement operation), and the introduction of appropriate types/units for such constructions as eventive adjectives and nouns like \u201crunning horse\u201d and \u201cpresident in 2009\u201d.  But many (even simple) aspects of semantic are either not present (at least, not in the paper) and/or simply wrong.  Missing: quantification (as in \u201cevery\u201d or \u201call\u201d); numbers (as in \u201c20\u201d or \u201cjust over 1000\u201d); various forms of reference (as in \u201che\u201d, \u201cthat man\u201d, \u201cwhat I said before\u201d); negation and modals, which change the semantics in interesting ways; inter-event relationships (as in the subevent relationship between the events in \u201cthe vacation was nice, but traveling was a pain\u201d; etc. etc.  To add them one can easily cheat, by treating these items as if they were just unusual words and defining obvious and simple lambda formulas for them.  But they in fact require specific treatment; for example, a number requires the creation of a separate set object in the representation, with its own canonical variable (allowing later text to refer to \u201cone of them\u201d and bind the variable properly).  For another example, Person A\u2019s model of an event may differ from Person B\u2019s, so one needs two representation symbols for the event, plus a coupling and mapping between them.  For another example, one has to be able to handle time, even if simply by temporally indexing events and states.  None of this is here, and it is not immediately obvious how this would be added.  In some cases, as DRT shows, quantifier and referential scoping is not trivial.   It is easy to point to missing things, and unfair to the paper in some sense; you can\u2019t be expected to do it all.  But you cannot be allowed to make obvious errors.  Very disturbing is the assignment of event relations strictly in parallel with the verb\u2019s (or noun\u2019s) syntactic roles.  No-one can claim seriously that \u201che broke the window\u201d and \u201cthe window broke\u201d has \u201che\u201d and \u201cthe window\u201d filling the same semantic role for \u201cbreak\u201d. \nThat\u2019s simply not correct, and one cannot dismiss the problem, as the paper does, to some nebulous subsequent semantic processing.                          This really needs adequate treatment, even in this paper.  This is to my mind the principal shortcoming of this work; for me this is the make-or-break point as to whether I would fight to have the paper accepted in the conference.  (I would have been far happier if the authors had simply acknowledged that this aspect is wrong and will be worked on in future, with a sketch saying how: perhaps by reference to FrameNet and semantic filler requirements.)                           Independent of the representation, the notation conversion procedure is reasonably clear.  I like the facts that it is rather cleaner and simpler than its predecessor (based on Stanford dependencies), and also that the authors have the courage of submitting non-neural work to the ACL in these days of unbridled and giddy enthusiasm for anything neural. ", "label": [[943, 1067, "Eval_neg_1"], [1069, 1192, "Jus_neg_1"], [3682, 3913, "Major_claim"], [4163, 4253, "Eval_pos_1"], [4254, 4362, "Eval_pos_2"], [4378, 4515, "Eval_pos_3"]]}
{"id": 414, "review": "paper_summary\nThis paper describes a self-generation framework for training ODD and TOD systems, as well as the prediction of the transition turns from ODD to TOD turns. The proposed framework makes use of self-chatting approaches using a SotA system, as well as fine-tuned versions of the chatbot to play the role of a salesman and a user. In addition, an interesting approach for predicting the best turn where to insert the transition and use of zero-shot approaches for intent detection complete the work done by the authors. Finally, human evaluations show the feasibility of the proposed approach. \nsummary_of_strengths\n- The generated dataset is interesting for people working on dialogue evaluation -The proposed framework for self-generating the dataset is very interesting and the results are feasible -The proposed method for intent detection and transition turns are also correct and interesting -The paper is well written and clear to follow. Experimentation and human evaluation is correct \nsummary_of_weaknesses\n- Although author state that components can be replaced by other models for flexibility, authors did not try any change or alternative in the paper to proof the robustness of the proposed framework.\n-Did authors tried using BlenderBot vs 2.0 with incorporated knowledge? it would be very interesting to see how the dialogs can be improved by using domain ontologies from the SGD dataset.  -Although BlenderBot is finetuned on the SGD dataset, it is not clear how using more specific TOD chatbots can provide better results -Lines 159-162: Authors should provide more information about the type/number of personas created, and how the personas are used by the chatbot to generate the given responses.  -It is not clear if authors also experimented with the usage of domain ontologies to avoid the generation of placeholders in the evaluated responses  -Line 211: How many questions were created for this zero-shot intent classifier and what is the accuracy of this system?\n-Line 216: How many paraphrases were created for each question, and what was their quality rate?\n-Line 237: How critical was the finetuning process over the SQuad and CommonsenseQA models?\n-Line 254-257: How many templates were manually created?  -Line 265: How the future utterances are used during evaluation? For the generation part, are the authors generating some sort of sentence embedding representation (similar to SkipThoughs) to learn the generation of the transition sentence? and is it the transition sentence one taken from the list of manual templates? ( In general, this section 2.2.2 is the one I have found less clear)  -Merge SGD: Did authors select the TOD dialogue randomly from those containing the same intent/topic? did you tried some dialogue embedding from the ODD part and tried to select a TOD dialogue with a similar dialogue embedding? if not, this could be an idea to improve the quality of the dataset. this could also allow the usage of the lexicalized version of the SGD and avoids the generation of placeholders in the responses -Line 324: how the repeated dialogues are detected?  -Line 356: how and how many sentences are finally selected from the 120 generated sentences?\n-Lines 402-404: How the additional transitions are generated? using the T5 model? how many times the manual sentences were selected vs the paraphrased ones? \ncomments,_suggestions_and_typos\n- The paper: Fusing task-oriented and open-domain dialogues in conversational agents is not included in the background section and it is important in the context of similar datasets -Probably the word salesman is misleading since by reading some of the generated dialogues in the appendixes, it is not clear that the salesman agent is in fact selling something. It seems sometimes that they are still doing chitchat but on a particular topic or asking for some action to be done (like one to be done by an intelligent speaker) ", "label": [[628, 706, "Eval_pos_1"], [708, 782, "Eval_pos_2"], [787, 811, "Eval_pos_3"], [813, 907, "Eval_pos_5"], [909, 955, "Eval_pos_6"], [956, 1003, "Eval_pos_7"], [1729, 1876, "Eval_neg_1"]]}
{"id": 415, "review": "This paper compares different ways of inducing embeddings for the task of polarity classification. The authors focus on different types of corpora and find that not necessarily the largest corpus provides the most appropriate embeddings for their particular task but it is more effective to consider a corpus (or subcorpus) in which a higher concentration of subjective content can be found. The latter type of data are also referred to as \"task-specific data\". \nMoreover, the authors compare different embeddings that combine information from \"task-specific\" corpora and generic corpora. A combination outperforms embeddings just drawn from a single corpus. This combination is not only evaluated on English but also on a less resourced language (i.e. Catalan).\n- Strengths: The paper addresses an important aspect of sentiment analysis, namely how to appropriately induce embeddings for training supervised classifers for polarity classification. The paper is well-structured and well-written. The major claims made by the authors are sufficiently supported by their experiments.\n- Weaknesses: The outcome of the experiments is very predictable. The methods that are employed are very simple and ad-hoc. I found hardly any new idea in that paper. Neither are there any significant lessons that the reader learns about embeddings or sentiment analysis. The main idea (i.e. focusing on more task-specific data for training more accurate embeddings) was already published in the context of named-entity recognition by Joshi et al. (2015). The additions made in this paper are very incremental in nature.\nI find some of the experiments inconclusive as (apparently) no statistical signficance testing between different classifiers has been carried out. In Tables 2, 3 and 6, various classifier configurations produce very similar scores. In such cases, only statistical signficance testing can really give a proper indication whether these difference are meaningful. For instance, in Table 3 on the left half reporting results on RT, one may wonder whether there is a significant difference between \"Wikipedia Baseline\" and any of the combinations. Furthermore, one doubts whether there is any signficant difference between the different combinations (i.e. either using \"subj-Wiki\", \"subj-Multiun\" or \"subj-Europarl\") in that table. \nThe improvement by focusing on subjective subsets is plausible in general. \nHowever, I wonder whether in real life, in particular, a situation in which resources are sparse this is very helpful. Doing a pre-selection with OpinionFinder is some pre-processing step which will not be possible in most languages other than English. There are no equivalent tools or fine-grained datasets on which such functionality could be learnt. The fact that in the experiments for Catalan, this information is not considered proves that.  Minor details: - lines 329-334: The discussion of this dataset is confusing. I thought the task is plain polarity classification but the authors here also refer to \"opinion holder\" and \"opinion targets\". If these information are not relevant to the experiments carried out in this paper, then they should not be mentioned here.\n- lines 431-437: The variation of \"splicing\" that the authors explain is not very well motivated. First, why do we need this? In how far should this be more effective than simple \"appending\"?\n- lines 521-522: How is the subjective information isolated for these configurations? I assume the authors here again employ OpinionFinder? However, there is no explicit mention of this here.\n- lines 580-588: The definitions of variables do not properly match the formula (i.e. Equation 3). I do not find n_k in Equation 3.\n- lines 689-695: Similar to lines 329-334 it is unclear what precise task is carried out. Do the authors take opinion holders and targets in consideration?\n***AFTER AUTHORS' RESPONSE*** Thank you very much for these clarifying remarks. \nI do not follow your explanations regarding the incorporation of opinion holders and targets, though.\nOverall, I will not change my scores since I think that this work lacks sufficient novelty (the things the authors raised in their response are just insufficient to me). This submission is too incremental in nature. ", "label": [[776, 837, "Eval_pos_1"], [839, 947, "Jus_pos_1"], [949, 995, "Eval_pos_2"], [996, 1081, "Eval_pos_3"], [1096, 1147, "Eval_pos_3"], [1148, 1205, "Eval_pos_4"], [1206, 1248, "Eval_neg_1"], [1249, 1353, "Eval_neg_2"], [1354, 1367, "Eval_neg_3"], [1369, 1447, "Jus_neg_3"], [1449, 1537, "Eval_neg_3"], [1538, 1602, "Eval_neg_4"], [1603, 1646, "Eval_neg_5"], [1647, 2330, "Jus_neg_5"], [2331, 2406, "Eval_pos_5"], [4038, 4253, "Major_claim"]]}
{"id": 416, "review": "paper_summary\nThe authors focus on adversarial attacks on dialog systems. The authors first present a method that generates universal adversarial triggers with language model loss or selection criteria. The proposed attack method can generate natural-looking attacks. Furthermore, the authors propose a defense model, which can improve the robustness of conversational models. \nsummary_of_strengths\n1. The authors present a method to generate natural-looking attacks for dialog systems, with satisfactory coherency and relevancy. \n2. The proposed defender is effective for both adversarial agents and regular users. \nsummary_of_weaknesses\n1. The attack effectiveness of the proposed models seems to be limited, with no more than 50%. \n2. The authors only compare the proposed model with UAT. It would be more convincing if the authors can compare the proposed method with other competitive baselines. \ncomments,_suggestions_and_typos\nPlease refer to the weaknesses. ", "label": [[402, 530, "Eval_pos_1"], [534, 616, "Eval_pos_2"], [641, 709, "Eval_neg_1"], [711, 732, "Jus_neg_1"]]}
{"id": 417, "review": "paper_summary\nThe authors are presenting a paper on two multilingual probing tasks, e.g. sentence sorting and modeling of priming of Jabberwocky sentences, in order to test  the reality of argument constructions in the sentence embedding representations of modern pre-trained language models.  Several models trained on different amounts of data have been tested, with the goal of simulating different levels of proficiency in human speakers.\nIn the first sentence sorting experment, inspired by the experiment by Bencini and Goldberg (2000), the authors generate sentences by using verbs that are compatible with 4 different constructions and create sentence embeddings by averaging the token vectors. The results of vector clustering show that in all the tested languages sentences that share the same constructions are more similar than sentences sharing the same main verb, with the only exception of the LM model trained with the  smallest amount of data. Such a finding is certainly interesting as a construction-oriented preference in the original task was observed only in more proficient speakers.\nFor the Jabberwocky experiment, as a difference from the Johnson and Goldberg (2013) setting, sentences for each construction are generated by randomly filling real words in the templates. The contextual embeddings of such sentences are then compared to prototype embeddings of the constructions, which are obtained by averaging the verb embeddings of a prototypical verb for each construction. The authors found that for congruent prototype-Jabberwocky pairings distance are significantly lower, even in the scenario where construction prototypes are built by using relatively low frequency verbs.  The paper is overall well-structured and the experiments are clear and well-presented. Personally, I am not aware of any other work that probes contextualized word representations for argument constructions, and I appreciate that the authors take their inspiration directly from the psycholinguistic literature on the psychological reality of argument constructions. \nAlthough I would still have a couple of questions (see the section below), I feel I can say this work deserves publication. \nsummary_of_strengths\nThe paper makes an original contribution to the literature on probing the contextualized representations of pre-trained language models. \nThe experimental hypotheses are clear and the experiments well-designed. \nsummary_of_weaknesses\nNo important weaknesses, as far as I can see. \ncomments,_suggestions_and_typos\nQUESTIONS FOR THE AUTHORS - recently, [1] criticized the usage of standard similarity measures such as cosine and Euclidean distance with contextualized embeddings, given that such metrics might be dominated by a small number of outlier dimensions. Do you think this could be affecting your results? Have you thought about using  standardized metrics or a rank-based metric (e.g. Spearman correlation between vectors)?\n-Section 5.1: \"... in cases where a verb is split into multiple subwords, we take the embedding of the first subword token as the verb embedding\". I am not sure I understand the motivation of this step. Could you please elaborate more? Why not taking e.g. the average of the subword embeddings?\nTYPOS AND MINOR ISSUES: l. 264 LSTMS --> LSTMs l. 301 is most similar --> is the most similar EXTRA REFERENCES [1] Timkey and Van Schjindel, 2021. All Bark and No Bite: Rogue Dimensions in Transformer Language Models Obscure Representational Quality. Proceedings of EMNLP. ", "label": [[961, 1000, "Eval_pos_1"], [1001, 1105, "Jus_pos_1"], [1707, 1743, "Eval_pos_2"], [1748, 1792, "Eval_pos_3"], [2075, 2199, "Major_claim"], [2221, 2358, "Eval_pos_4"], [2359, 2396, "Eval_pos_5"], [2401, 2430, "Eval_pos_6"], [2455, 2501, "Major_claim"]]}
{"id": 418, "review": "paper_summary\nThis paper proposes a novel task called singing voice beautifying, the task of improving of quality of amuteur singing performance. It includes two subtasks, automatic pitch correction and vocal tone modification. To solve this challenge, they propose a conditional variational autoencoder with adversarial training to perform singing beautifying. During inference, they also propose shape-aware dynamitc time warping to better align parallel pitch contours. To validate the model, two datasets in both Chinese and English songs were collected from professional singers.  While most neural singing models focus on automatic pitch correction, the authors also extend their model to modify vocal tone and achieves good performance. This is a contribution to the field of modeling singing. The singing dataset in two languages, if released, will be helpful for singing researchers, as singing datasets are rare. In my opinion, this is a solid work.\nThe writing is mostly clear and concise. While some sections are technical and dense, they are accessible to researchers in speech processing in general. The model details are thoroughly described and the actual parameters can be found in source code. \nsummary_of_strengths\n- The task is novel and the proposed solution is effective. Based on the evaluation results and the given audio samples, the proposed method significantly improves the quality of amuteur singing. Furthermore, the model has been evaluated in two languages, Chinese and English, strengthening their arguments.  -The authors have performed extensive evaluation using a variety of metrics, including both objective measures and subjective measures from trained singers.  -This study is open and transparent. The authors have provided code and singing samples, which is helpful for understanding and assessing the proposed method. They also promise to release the data later. \nsummary_of_weaknesses\nI see no major weaknesses. But there are some minor issues that could be improved.\n- The description of the dataset is incomplete. It will be helpful to describe some demographic information of the singers in the dataset, such as gender and age range. What is the percetange of male and female singers? Are English songs recorded by native speakers? Male and female singers may pose slightly different challenges, so it will be good to know this. \n - Have the forced alignment results been manually checked? Montreal Forced Aligner works well with modal speech but may not work well on singing. Will this affect the evaluation results? \ncomments,_suggestions_and_typos\n- Figure 4 is not easy to interpret. All pitch contours are mixed together. Is it possible to hightly the specific areas where DTW and CTW make errors? In this way it will be easier to spot the errors and validate the alignment result.  -The same for Figure 3. Since this is a new distance measure, it might not be familiar to readers. It will be immensely helpful if a numerical example could be provided. For example, this could be \"if a points falls into region n with an angle of 30 degree, what is the numerical distance between this point and the anchor point?\" ", "label": [[801, 891, "Eval_pos_1"], [893, 921, "Jus_pos_1"], [923, 959, "Major_claim"], [960, 1000, "Eval_pos_2"], [1114, 1156, "Eval_pos_3"], [1236, 1253, "Eval_pos_4"], [1258, 1292, "Eval_pos_5"], [1294, 1354, "Jus_pos_6"], [1355, 1428, "Eval_pos_6"], [1544, 1618, "Eval_pos_7"], [1619, 1698, "Jus_pos_7"], [1702, 1737, "Eval_pos_8"], [1738, 1905, "Jus_pos_8"], [1928, 1954, "Major_claim"], [2013, 2058, "Eval_neg_1"], [2059, 2375, "Jus_neg_1"]]}
{"id": 419, "review": "This paper proposes a framework for evaluation of word embeddings based on data efficiency and simple supervised tasks. The main motivation is that word embeddings are generally used in a transfer learning setting, where evaluation is done based on how faster is to train a target model. The approach uses a set of simple tasks evaluated in a supervised fashion, including common benchmarks such as word similarity and word analogy. Experiments on a broad set of embeddings show that ranks tend to be task-specific and change according to the amount of training data used.\nStrengths - The transfer learning / data efficiency motivation is an interesting one, as it directly relates to the idea of using embeddings as a simple \"semi-supervised\" approach.\nWeaknesses - A good evaluation approach would be one that propagates to end tasks. \nSpecifically, if the approach gives some rank R for a set of embeddings, I would like it to follow the same rank for an end task like text classification, parsing or machine translation. However, the approach is not assessed in this way so it is difficult to trust the technique is actually more useful than what is traditionally done.\n-The discussion about injective embeddings seems completely out-of-topic and does not seem to add to the paper's understanding.\n-The experimental section is very confusing. Section 3.7 points out that the analysis results in answers to questions as \"is it worth fitting syntax specific embeddings even when supervised datset is large?\" but I fail to understand where in the evaluation the conclusion was made.\n-Still in Section 3.7, the manuscript says \"This hints, that purely unsupervised large scale pretraining might not be suitable for NLP applications\". This is a very bold assumption and I again fail to understand how this can be concluded from the proposed evaluation approach.\n-All embeddings were obtained as off-the-shelf pretrained ones so there is no control over which corpora they were trained on. This limits the validity of the evaluation shown in the paper.\n-The manuscript needs proofreading, especially in terms of citing figures in the right places (why Figure 1, which is on page 3, is only cited in page 6?).\nGeneral Discussion I think the paper starts with a very interesting motivation but it does not properly evaluate if their approach is good or not. As mentioned above, for any intrinsic evaluation approach I expect to see some study if the conclusions propagate to end tasks and this is not done in the paper. The lack of clarity and proofreading in the manuscript also hinders the understanding. In the future, I think the paper would vastly benefit from some extrinsic studies and a more controlled experimental setting (using the same corpora to train all embeddings, for instance). But in the current state I do not think it is a good addition to the conference. ", "label": [[585, 657, "Eval_pos_1"], [659, 752, "Jus_pos_1"], [767, 837, "Eval_neg_1"], [838, 1024, "Jus_neg_1"], [1025, 1074, "Jus_neg_2"], [1075, 1172, "Eval_neg_2"], [1175, 1301, "Eval_neg_3"], [1303, 1346, "Eval_neg_4"], [1347, 1583, "Jus_neg_4"], [1585, 1733, "Jus_neg_5"], [1734, 1860, "Eval_neg_5"], [1862, 1987, "Jus_neg_6"], [1988, 2050, "Eval_neg_6"], [2226, 2285, "Eval_pos_2"], [2290, 2352, "Eval_neg_7"], [2516, 2602, "Eval_neg_8"], [2792, 2873, "Major_claim"]]}
{"id": 421, "review": "paper_summary\nAs I am a returning reviewer for the paper I will keep any of my previous points that still hold for this version of the paper and add new ones where appropriate.\nI thank the authors for their response and the changes in the paper. I found much of the response (and changes) to be reasonable, with many of my complaints (especially regarding lack of information on the human evaluation) addressed in this version of the paper. The statistical significance tests are appreciated and they seem to support the initial version's claims.  ====== \t This paper proposes a novel approach to the task of knowledge-grounded dialogue generation, that additionally exploits commonsense external knowledge augmented with named-entity triples extracted from co-reference chains detected in the dialogues themselves. The proposed approach additionally employs a multi-hop attention layer over the dialogue history and corresponding external knowledge to inform the decoding process.\nThe paper presents both automatic and human evaluation over 2 datasets, and discusses a case study. \nsummary_of_strengths\n- The augmentation of common sense knowledge and the application of multi-hop attention over the dialogue history and unstructured knowledge for the purposes of knowledge-grounded dialogue generation are novel and interesting.\n-The proposed approach is experimentally shown to outperform previous work in automatic evaluation. While automatic evaluation is known to be unreliable in single-reference generation tasks, human evaluation results also indicate that the proposed approach outperforms related work on multiple criteria that effectively measure knowledge precision and recall. \nsummary_of_weaknesses\n- Human evaluation suggests that the proposed methods underperforms in fluency. This could be due to their use of a GRU as a decoder. Replacing the decoder with a pretrained language model similar to related work should help fluency. \ncomments,_suggestions_and_typos\n- The paper now has an ablation study that supports each components efficacy, but it's presentation can be improved. Please present the ablation study results in a table and clearly state how each experiment performed numerically. As it is the comparison is quite vague for most of the configurations.\n-The D.2 appendix mentions two human annotators. Based on the author responses, that number should be four.\n-I suggested for the previous version that the paper would be stronger if it also presented results on other knowledge-grounded benchmarks, e.g. DSTC9 and the Doc2Dial benchmark. The author's response to this was reasonable, and I would suggest that the include their reasoning in the paper as a potential limitation of the approach. ", "label": [[1106, 1330, "Eval_pos_1"], [1716, 1793, "Eval_neg_1"], [1794, 1948, "Jus_neg_1"], [1983, 2211, "Jus_neg_2"], [2212, 2282, "Eval_neg_2"]]}
{"id": 422, "review": "paper_summary\nThe authors propose a black-box adversarial attack method for structured prediction tasks such as POS-tagging and dependency parsing. The authors formulate the adversarial attack under the black-box setting as a combinatorial optimization problem and proposed three search methods (beam search, Metropolis-Hastings sampling, and their combination) to generate textual adversarial examples. They advocate preserving the meaning and fluency of the original text when generating adversarial examples. However, it seems that the adversarial examples against dependency parsing models only need to preserve the same syntactic structure as the original ones, and the constraint on their similarity in semantic properties can be relaxed (Zheng et al., 2020). Besides, the novelty of this study is limited. The objective function used is similar to that of many previous studies. They claim that their attack method does not need to know the training dataset used by the victim models. However, it seems that they train the reference models on the same dataset used by the victim model (at least for the dependency parsing task). They did not compare their search strategy to others, such as genetic algorithm (Alzantot et al., 2018) both in the performance and computational cost.\nIn their revised version (2nd round), they only added a paragraph in the introduction (Line 54-75) and an experiment to investigate \u201cthe impact of metric for reference models\u201d (Line 471-496). In the introduction, the authors claimed that structured prediction models are more vulnerable to adversarial examples than text classifiers. In the experiment, they introduced a metric, called OIoU (Opposite Intersection over Union), to evaluate \u201cthe diversity degree of reference parsers.\u201d However, the questions raised in the 1st sound still have not been well answered. \nsummary_of_strengths\nThe authors proposed a method that combines Metropolis-Hastings (MH) sample and beam search to generate textual adversarial examples, and their experimental results show that the proposed method can better preserve the meaning and fluency as the original texts. \nsummary_of_weaknesses\nThe novelty of this study is limited. The objective function used is similar to that of many previous studies. \nThey claim that their attack method does not need to know the training dataset used by the victim models. However, they follow the work of Han et al. (2020) and train the reference models on the same dataset used by the victim model (at least for the dependency parsing task). \nThey did not compare their search strategy to others, such as genetic algorithm (Alzantot et al., 2018). \nIt seems that their method requires much more computational cost to generate adversarial examples, which makes the proposed method harder to be used in practice.\nReference: Moustafa Alzantot, Yash Sharma, Ahmed Elgohary, Bo-Jhang Ho, Mani Srivastava, and Kai-Wei Chang. Generating natural language adversarial examples. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, 2018. \ncomments,_suggestions_and_typos\nN/A ", "label": [[766, 812, "Eval_neg_1"], [813, 885, "Jus_neg_1"], [1136, 1188, "Eval_neg_2"], [1190, 1287, "Jus_neg_2"], [2161, 2198, "Eval_neg_3"], [2199, 2272, "Jus_neg_3"], [2551, 2603, "Eval_neg_4"], [2605, 2654, "Jus_neg_4"], [2657, 2754, "Jus_neg_5"], [2756, 2817, "Eval_neg_5"]]}
{"id": 423, "review": "paper_summary\nThis paper proposes a method to improve the performance of the retriever typically used in the recent open-domain question answering systems. It uses the hyperlink structure in the Web documents to create pseudo question-passage pairs to train the retriever. The paper tests two approaches: (1) dual-link that extracts the question-passage pairs from two pages that link to each other, and (2) co-mention that extracts the pairs from two pages that link to a same page. It adopts a standard bi-encoder trained using the extracted question-passage pairs. Experiments are conducted using five standard question answering datasets under zero-shot, few-shot, multi-hop, and out-of-domain scenarios, and report enhanced empirical performance. \nsummary_of_strengths\n- Dataset creation method that is highly useful to train the retriever for open domain question answering -Enhanced retrieval performance in popular question answering benchmarks -Extensive experiments that validate the effectiveness of the proposed method -Well-written paper \nsummary_of_weaknesses\nThe method significantly underperform BM25 in the MS MARCO dataset, which is one of the most popular retrieval benchmark. This suggests that the applicability of the proposed method to general QA domain is limited. \ncomments,_suggestions_and_typos\nN/A ", "label": [[776, 880, "Eval_pos_1"], [881, 952, "Eval_pos_2"], [954, 1030, "Eval_pos_3"], [1032, 1050, "Eval_pos_4"], [1074, 1195, "Jus_neg_1"], [1196, 1289, "Eval_neg_1"]]}
{"id": 425, "review": "paper_summary\nSee my previous summary.\nAfter reading the authors' response, most questions and concerns are addressed.  The remaining concern I have is the instability of major PLMs (e.g., https://openreview.net/forum?id=nzpLWnVAyah), where PLMs are very sensitive to hyper-parameters both in pre-training and fine-tuning. \nThis paper explores language abilities of PLM(s), but each PLM is based on a single hyper-parameter configuration (learning rate, batch size, number of updates etc.). Considering the instability of PLMs, my concern is that the findings of this paper might not be supported when we use different hyper-parameters (e.g., using different random seeds, learning rates, batch sizes).  It would be great to mention this phenomenon/possibility in the paper. \nsummary_of_strengths\nsee above \nsummary_of_weaknesses\nsee above \ncomments,_suggestions_and_typos\nI appreciate authors' response. I raised my score. ", "label": [[120, 181, "Eval_neg_1"], [235, 490, "Jus_neg_1"], [491, 703, "Eval_neg_1"], [704, 775, "Jus_neg_1"]]}
{"id": 426, "review": "paper_summary\nThis paper compiles and annotates a multi-reference Chinese Grammatical Error Correction (MuCGEC) evaluation dataset from three different Chinese Second Language sources. Multiple annotators and reviewers are involved in rewriting ungrammatical sentences and creating the most multi-referenced GEC dataset for Chinese. \nThis work ensembles SOTA Seq2Edit and Seq2Seq models with Structured BERT and beats previous SOTAs on an earlier benchmark dataset NLPCC18. Moreover, char-based evaluation metrics are suggested, and ablation experiments are implemented on the number of references, error types, text sources, etc. \nsummary_of_strengths\nThe strict and (hopefully) accessible annotation guidelines and multiple annotators and reviewers ensure annotation quality. \nTheir ensemble model compensates for the drawbacks of Seq2Edit and Seq2Seq models on Recall. \nSince the previous SOTA models do not provide code access, this paper's data and release contribute largely to GEC for Chinese. \nsummary_of_weaknesses\nHere are a few suggestions: 1) How was the quality of reviewers' final golden decisions assessed? For example, the annotators' agreements in Figure 2 look decent, but they are compared against reviewers' final golden answers. Would it be possible to compare different reviewers' decisions on approving the same set of annotated corrections? And see how much the reviewers agree?\n2) Comparing the rewritten corrections to NLPCC18 and CGED's original error-coded correction would be an interesting analysis to show how much they overlap in reference corrections and whether MuCGEC provides further diversity.   3) Briefly discussing what kinds of data augmentation techniques  ( \u2665 )  other works use in Table 4 would be great (and maybe why not used in this paper). \ncomments,_suggestions_and_typos\nLine 17: add a space \"datasets. We\" --> \"datasets. We\" ", "label": [[653, 778, "Eval_pos_1"], [873, 930, "Jus_pos_2"], [932, 999, "Eval_pos_2"]]}
{"id": 428, "review": "- Update after rebuttal I appreciate the authors taking the time to clarify their implementation of the baseline and to provide some evidence of the significance of the improvements they report. These clarifications should definitely be included in the camera-ready version. I very much like the idea of using visual features for these languages, and I am looking forward to seeing how they help more difficult tasks in future work.\n- Strengths: - Thinking about Chinese/Japanese/Korean characters visually is a great idea!\n- Weaknesses: - Experimental results show only incremental improvement over baseline, and the choice of evaluation makes it hard to verify one of the central arguments: that visual features improve performance when processing rare/unseen words.\n- Some details about the baseline are missing, which makes it difficult to interpret the results, and would make it hard to reproduce the work.\n- General Discussion: The paper proposes the use of computer vision techniques (CNNs applied to images of text) to improve language processing for Chinese, Japanese, and Korean, languages in which characters themselves might be compositional. The authors evaluate their model on a simple text-classification task (assigning Wikipedia page titles to categories). They show that a simple one-hot representation of the characters outperforms the CNN-based representations, but that the combination of the visual representations with standard one-hot encodings performs better than the visual or the one-hot alone. They also present some evidence that the visual features outperform the one-hot encoding on rare words, and present some intuitive qualitative results suggesting the CNN learns good semantic embeddings of the characters.\nI think the idea of processing languages like Chinese and Japanese visually is a great one, and the motivation for this paper makes a lot of sense. However, I am not entirely convinced by the experimental results. The evaluations are quite weak, and it is hard to say whether these results are robust or simply coincidental. I would prefer to see some more rigorous evaluation to make the paper publication-ready. If the results are statistically significant (if the authors can indicate this in the author response), I would support accepting the paper, but ideally, I would prefer to see a different evaluation entirely.\nMore specific comments below: - In Section 3, paragraph \"lookup model\", you never explicitly say which embeddings you use, or whether they are tuned via backprop the way the visual embeddings are. You should be more clear about how the baseline was implemented. If the baseline was not tuned in a task-specific way, but the visual embeddings were, this is even more concerning since it makes the performances substantially less comparable.\n- I don't entirely understand why you chose to evaluate on classifying wikipedia page titles. It seems that the only real argument for using the visual model is its ability to generalize to rare/unseen characters. Why not focus on this task directly? E.g. what about evaluating on machine translation of OOV words? I agree with you that some languages should be conceptualized visually, and sub-character composition is important, but the evaluation you use does not highlight weaknesses of the standard approach, and so it does not make a good case for why we need the visual features.  - In Table 5, are these improvements statistically significant?\n- It might be my fault, but I found Figure 4 very difficult to understand. \nSince this is one of your main results, you probably want to present it more clearly, so that the contribution of your model is very obvious. As I understand it, \"rank\" on the x axis is a measure of how rare the word is (I think log frequency?), with the rarest word furthest to the left? And since the visual model intersects the x axis to the left of the lookup model, this means the visual model was \"better\" at ranking rare words? Why don't both models intersect at the same point on the x axis, aren't they being evaluated on the same set of titles and trained with the same data? In the author response, it would be helpful if you could summarize the information this figure is supposed to show, in a more concise way.  - On the fallback fusion, why not show performance for for different thresholds? 0 seems to be an edge-case threshold that might not be representative of the technique more generally.\n- The simple/traditional experiment for unseen characters is a nice idea, but is presented as an afterthought. I would have liked to see more eval in this direction, i.e. on classifying unseen words - Maybe add translations to Figure 6, for people who do not speak Chinese? ", "label": [[448, 523, "Eval_pos_1"], [540, 608, "Eval_neg_1"], [610, 768, "Jus_neg_1"], [771, 814, "Jus_neg_2"], [816, 912, "Eval_neg_2"], [1745, 1835, "Eval_pos_2"], [1837, 1891, "Eval_pos_3"], [1893, 1958, "Eval_neg_3"], [1959, 1989, "Eval_neg_4"], [1991, 2068, "Eval_neg_5"], [2070, 2158, "Eval_neg_6"], [2159, 2299, "Jus_neg_6"], [2300, 2367, "Eval_neg_6"], [2400, 2564, "Jus_neg_7"], [2565, 2629, "Eval_neg_7"], [2630, 2807, "Jus_neg_7"], [2810, 2901, "Eval_neg_8"], [2902, 3459, "Jus_neg_8"], [3462, 3535, "Eval_neg_9"], [3536, 4445, "Jus_neg_9"], [4448, 4556, "Eval_neg_10"], [4557, 4720, "Jus_neg_10"]]}
{"id": 430, "review": "paper_summary\nThis paper studies the text-to-SQL generalization problem, focusing on the Column Operations problem. \nThe contribution of this paper: 1.  Propose two new benchmarks: a synthetic dataset and a train/test repartitioning of the SQUALL dataset, both capable of quantifying out-of-domain generalization on column operations. \n2. Propose a schema expansion method using heuristics to expand columns into sets of derived columns. \n3. Propose a schema pruning method to prune the relevant columns that the final parser needs to look at. \nsummary_of_strengths\nThe contribution of this paper: 1.  Propose two new benchmarks: a synthetic dataset and a train/test repartitioning of the SQUALL dataset, both capable of quantifying out-of-domain generalization on column operations. \n     These benchmarks are useful for evaluating the column operation problem. \n2. Propose a schema expansion method using heuristics to expand columns into sets of derived columns. \n     Schema expansion is a good idea for this problem. \n3. Propose a schema pruning method to prune the relevant columns that the final parser needs to look at. \n    Schema pruning is also a good idea. \nsummary_of_weaknesses\nThe column operation problem is worth studying, and the proposed ideas are also good, but the implementation is not exciting.\nFirst, Schema Extension requires manual annotation, which means that reasonable schema design and annotation can eliminate the problem of column operation. However,  this conclusion can be also inferred from the previous works about schema linking. So, while this is indeed a solution, the manual schema expansion method is not exciting.  Secondly, I think schema pruning should be directly integrated into the text-to-SQL model and trained together, just like many models on the Spider leaderboard. During the training, the model can learn which columns should be left. Then we do not need to manually find a parameter for the schema pruning module. Unless detailed comparative experimental, I do not think the current schema pruning design is reasonable. \ncomments,_suggestions_and_typos\nAn auto Schema Extension method would be more exciting. ", "label": [[602, 784, "Jus_pos_1"], [790, 862, "Eval_pos_1"], [867, 966, "Jus_pos_2"], [972, 1022, "Eval_pos_2"], [1026, 1128, "Jus_pos_3"], [1133, 1169, "Eval_pos_3"], [1192, 1317, "Eval_neg_1"], [1318, 1969, "Jus_neg_1"], [1969, 2075, "Eval_neg_1"]]}
{"id": 433, "review": "paper_summary\nThe paper proposes a simple and efficient method, BitFit, for fine-tuning language models on tasks with small-or-low training data. BitFit fine-tunes only the bias-terms of the language model and shows comparable performance to full fine-tuning on some tasks in GLUE.  The bias terms constitute very few parameters of a pre-trained LM and so BitFit is more efficient than DiffPrune and Adapters in terms of the number of parameters fine-tuned. \nsummary_of_strengths\n- The paper introduces a simple technique for fine-tuning language models and comparisons with other techniques show that the method is parameter efficient on the GLUE benchmark. \nsummary_of_weaknesses\n- The paper provides some unsubstantiated conjectures about the \"fine-tuning as exposure of existing capabilities in LMs\". Without adequate reasoning or references, it is hard to comprehend what the author(s) refer to in this case.\n-Generalization gap: No reference to a graph or table that the Generalization gap talks about. However, I did understand later that the generalization gap referred to here is from validation to test in Table 1. The difference between full fine-tuning validation and test also seems to be within 3% from this table and given that experiments haven't been run over multiple seeds, the conclusion about the lower generalization gap is unformed. On QQP, both full fine-tuning and BitFit suffer very similar drops. \n    - Have the author(s) run the codes for multiple seeds and reported the mean? \n    - If not run for multiple seeds, is the <1% difference a significant result?\nOverall, while this paper proposes an interesting empirical result, it makes some unsubstantiated claims and some of the conclusions from the experiments are not adequately justified. The paper could benefit from a more substantial discussion of fewer results in the main paper with a more detailed discussion deferred to the Appendix for other experiments. \ncomments,_suggestions_and_typos\n- Perhaps this point is not explicitly mentioned, but it seems like BitFit also has the added advantage of *not* introducing any new parameters to the model. This would have been a nice positive to stress on.\n-Is there any reason why the paper stresses on Masked Language Models in particular? Did experiments not work out with an autoregressive LM? ", "label": [[482, 659, "Eval_pos_1"], [684, 913, "Eval_neg_1"], [1588, 1654, "Eval_pos_2"], [1656, 1692, "Eval_neg_2"], [1693, 1771, "Eval_neg_3"]]}
{"id": 434, "review": "paper_summary\nThis paper explores the effects of using a generative model that provides question prompts to human annotators in either the standard or adversarial data collection for training a QA model. The proposed approach combines the benefits of dynamic adversarial data collection, i.e. human annotators generating data points for the model and generative models that are traditionally used to augment datasets with adversarial data points. \nThis paper provides a thorough analysis of using Generative Annotation Assistants (GAAs) during data annotation process, examining the effects on annotation speedup, effectiveness in fooling the model, and performance improvement on downstream tasks. This paper collects data points from several experimental settings to compare the effects of sampling strategies, using model-in-the-loop (standard vs adversarial data collection), datasets used to train the GAAs, and whether answer prompts are provided or not. \nThis paper shows that the use of GAA without adversarial model in the loop is can achieve near-adversarial accuracy but at much higher speed of generating the adversarial dataset. Thus this paper addresses limitations of the adversarial data collection process, which is the large amount of time needed to collect the dataset. \nThis paper shows that use of GAA in both the standard and adversarial data collection helps improve model performance on downstream tasks. Additionally, in the standard data collection setting, GAA provides a measurable speedup in collecting the dataset in general, but also the effective examples that fool the model by a significant margin. \nDespite the exhaustive experimentation, all the experimental settings are done using one model-in-the-loop, one question prompt generator and 2 datasets of similar domains. This limitation has been called out in the paper. \nsummary_of_strengths\n- The major takeaway is that the use of GAAs in the standard data collection process results in annotation efficiency, as measured by vMER, i.e. model error rate on the collected data, and time per model fooling example. Additionally, this results in significant improvement in model performance on downstream QA task, which is almost as performant as adversarial data collection without use of GAAs.\n-Authors demonstrate effectiveness of using GAAs by exploring effects on the adversarial data collection process as well. Speedup isn't observed here as much as the increase in downstream task performance.\n-Authors even experiment with providing answer prompts in addition to question prompts and observe a performance improvement on most downstream tasks for most of the experimental settings. So overall, providing answer prompts in addition to question prompts helps.\n-This is the first work to investigate the use of generative assistants for crowdworkers doing the data annotation.\n-This paper uses several experimental settings, albeit only few data points per setting to conduct the study.\n-The paper has clear language, explaining in detail all the components used in the experimental setup and evaluation strategy. This paper also provides the motivation behind the experimental settings which makes the objective of the exercise much clear. \nsummary_of_weaknesses\n- This approach is compute-intensive as it requires pretraining of the GAA model on the same dataset as the QA model needs to be trained on. It assumes that the resultant GAA model is performant enough to provide meaningful prompts to annotators. If this assumption doesn't hold true, then it can result in larger annotation times, thus bringing into question the speedup claim of the proposed approach.\n-From table 3, it looks like using GAA with adversarial data collection hurts out-of-domain generalization, as seen by the dip in performance on MRQA dataset.\n-This approach has been tested only on the SQuAD dataset for all experimental settings, and just additionally MRQA for evaluation. While it is understandable that all these experimental settings require lot of time and human effort to conduct, the fact that this has been tested only on one dataset raises concerns about generalizability of this framework. Especially when use of GAAs seems to hurt performance on the out of domain dataset MRQA for adversarial data collection. So in situations where adversarial data collection cost doesn't matter as much as quality of resultant QA model, using GAAs won't work.\n-While some mild correlations emerge between sampling strategy and downstream performance, overall it seems like the best experimental setting in terms of a trade-off between annotation speed, efficiency and downstream performance depends highly on the downstream task at hand. Practically, its not feasible to try out all these experimental settings for data annotation for QA datasets for industrial/real-world applications. \ncomments,_suggestions_and_typos\n- In the setting where annotators were provided with both the question and answer prompts, doesn't this limit the example diversity? This is referring to the drawbacks of generative models mentioned in the introduction section. I think maybe more data is needed to answer this. If we have some metrics showing how many times users accepted the suggestions by GAA, and if we see the most of the GAA suggestions were accepted, then it means that it is the GAA that is generating data points for the most part, with minimal human intervention.\n-In table 4, there is a spike in model performance in terms of annotation efficiency for likelihood sampling strategy in the Adversarial QA dataset. Such a trend was not seen for question prompts only setting, for both standard and adversarial data collection processes. Can you please provide a reason/explanation for this observation? ", "label": [[2752, 2866, "Eval_pos_1"], [2978, 3006, "Eval_pos_2"], [3008, 3103, "Jus_pos_2"], [3256, 3290, "Eval_neg_1"], [3291, 3394, "Jus_neg_1"], [3395, 3585, "Jus_neg_2"], [3586, 3657, "Eval_neg_2"], [3818, 3947, "Jus_neg_3"], [3948, 4173, "Eval_neg_3"]]}
{"id": 435, "review": "paper_summary\nThe authors present a strong method for unsupervised constituency parsing that relies on RoBERTa and constituent classification based on an initial set of constituents / distuents. Throughout training, this labeled set is updated with the models own predictions. Extensive results and analysis show the competitiveness of this approach with other unsupervised parsing methods. That being said, some details needed for reproducibility are needed, and claims about the initial set being a \u201cminor\u201d source of supervision are contradicted throughout the text.\nTo clarify the last part above, the issue may be that \u201cminor\u201d is underspecified. Clearly their supervision is relatively easy to acquire, although it appears their approach is sensitive to choice of initial labeled set. \nsummary_of_strengths\n- Simple yet effective method for unsupervised constituency parsing that is EM-like. Competitive with existing models.\n-Provides additional value through interesting ablations and extensive analysis. \nsummary_of_weaknesses\n- It is not clear what effect the initial set of candidate constituents and distuents has on final performance. Multiple times the authors claim this form of supervision is \u201cminimal\u201d, and although it is certainly simple to use, perhaps different initial sets would yield different results. Based on Appendix A.1 it does seem like several ways of exploring initial set were explored, although no results reported. There are some results with this respect in section 5.3 showing that left-branching and random strategies to build initial seed set yield very low performance, and authors also state \u201cthe manner in which we perform the initial classification has a strong impact on the final tree structure\u201d here. It would greatly help to be more consistent about the effect of the initial seed set throughout the text.\n-It\u2019s not clear how h_in(i, j) and h_out(i, j) are computed. For h_in, two likely possibilities are: a) RoBERTa is run over only the tokens i:j and the last vector is used to represent the phrase, or b) RoBERTa is run over an entire sentence x, and two or more output vectors are concatenated to get phrase vectors. If (a) is used, this seems clean although a bit expensive. If (b) is used, it seems like this would have trouble distinguishing between inside and outside. In either case, the text should be more clear about how this is done otherwise reproducibility would be quite difficult. \ncomments,_suggestions_and_typos\n- Do you think that validation using performance on the dev set (using early stopping or hyperparam selection) would be helpful? There are many reasons why people are interested in methods that could benefit from validation (even for unsupervised learning) because it can help find a model with a good inductive bias that does not overfit to given labels. I suppose by reporting three settings (inside, inside w/ self-training, inside + outside) you are doing a type of validation over hyperparams.  -Do you think your method of seed bootstrapping would be generally applicable to other types of unsupervised parsing models?\n-What does it mean to re-normalize class probability scores in Fig 3?\n-How much does the set of labels grow each epoch?\n-How did you choose min/max threshold values, and what are these values? Also, how did you decide 1:10 ratio of constituents to distituents? ", "label": [[812, 895, "Eval_pos_1"], [896, 929, "Eval_pos_2"], [931, 1011, "Eval_pos_3"], [1036, 1145, "Eval_neg_1"], [1146, 1849, "Jus_neg_1"], [1851, 1910, "Eval_neg_2"], [1911, 2443, "Jus_neg_2"]]}
{"id": 436, "review": "paper_summary\nThe authors present a method of adaptively truncating a BERT forward pass (or any deep-net with an iteratively-modified hidden vector a-la self-attention) at inference time in a classification setting; this method in theory allows a practitioner to trade off accuracy for latency: early-stopping takes fewer FLOPs but does slightly worse on average.\nThe model is, like the baselines compared to, adaptive, in the sense of allowing different test examples to use more or less computation as needed. The way it works is by training a per-layer classifier during finetuning for each layer; each hidden layer can be used to directly predict the label. Once a sufficient number of layers yield sufficiently low-entropy predictions, computation can be terminated (with both of these \"sufficiencies\" in the previous sentence corresponding to a different inference-time hyperparameter).\nThe classification tasks on GLUE are analyzed under this setting; the setup does a bit better than the competing adaptive-computation methods in the regime of heavily-truncated computations (only going through about 25% of the BERT's layers at test-time), and does comparably or marginally better, in general, than the other adaptive methods at less aggressive speedups.\nThe authors also demonstrate that the method is applicable to a ResNet trained on CIFAR to do vision classification.\nGenerally, the performance gains are not unambiguous and the settings in which this method is strongly preferred to other comparable systems could be clarified. I believe the comparison would be greatly strengthened by comparing directly to the more obvious way of reducing inference-time computation (namely, simply using a smaller fixed-sized model, comparable in FLOPs or wall-clock throughput to the adaptive large model).  I suspect the body of researchers interested directly in studying the performance-computation tradeoff of problems may find the work of interest---this method seems to consistently outperform the other comparable systems across a few of the tasks probed. \nsummary_of_strengths\n- Comparison of this method to a number of directly comparable published adaptive computation methods on a number of baselines (GLUE tasks and a basic CIFAR-10/100 vision setup).\n-Method outperforms competing methods in very-restricted-computation regime (that is, when very aggressively early-stopping inference forward-passes).\n-Method does comparably or better than a few comparable systems for a few tasks across different accuracy/performance tradeoffs.\n-The method is intuitive and the presentation was generally clear (except a few smaller points of clarification I outline below in \"suggestions\"). \nsummary_of_weaknesses\n- Not totally clear exactly when you'd want this scheme over the various other related early-stopping-adaptive-inference BERT setups. It seems like this setup generally works about the same as (or marginally better than) the others on many of the tasks (Table 2), and better in the regime of very-truncated computations (only doing a forward pass through 25% of the network on average). Could be clarified a bit if there are other clear differences in behavior beyond doing better in the very-aggressive-early-stopping regime.\n-The main comparison I'd really like to see is a comparison to smaller fixed-size models, ideally of different sizes. If the point of the early-exiting program is to save computation, then the most straightforward to reduce computation is to try a smaller model (simpler than a large model that does adaptive inference-time things which are not present during learning). It's possible that just using fixed-size smaller BERTs will do better in the performance-versus-latency tradeoff. I don't think this is a rejection-worthy shortcoming but I think it would really improve the strength of the claims in the paper.\n-I'm concerned by the speedup metric (this is not a rejection-level concern---I'm just arguing that they're using a proxy measurement without providing proof that it stands as a linearly-related proxy). The central results are task-specific-performance versus relative speedup afforded by the early stopping model. However, the speedup used as the x axis of the graphs is essentially the average percentage of layers the model goes through---that is, for a 50-layer BERT, if examples stop on average at layer 25, this will have a speedup ratio of 0.5. The authors assert \"according to our experiments, it is proportional to actual wall-clock runtime\". I think it's very important to show this, particularly because in the world of GPU usage and batching, there's often a weak coupling and non-linear relationship between the number of FLOPs required total (which is what this is) and the actual runtime (which depends on GPU utilization, which depends on the shape of the calculation, which depends on batching), and in particular this can vary from task-to-task. So this is a proxy measurement of efficiency (the \"efficiency\" we would actually care about in any setting is throughput or wall-clock time, rather than the number of flops required). The authors claim they observed a proportionality between the two, but the extent to which one actually stands as a proxy for the other (in particular, as a linearly-related proxy) seems very important to explicitly quantify and present. \ncomments,_suggestions_and_typos\n- There's two task-specific hyperparams to set to parameterize early stopping, aren't there, the $\\tau$ param which binarizes the \"is this a low-entropy-enough\" decision, and the number of layers after which stopping occurs if the entropy's sufficiently low, I believe. These assumedly interact and each of them contribute to the speedup. How was this two-parameter decision turned into a single-parameter speedup? That is, the paper/system is motivated by there being a single knob to turn to control accuracy/computation tradeoffs, but this is two knobs, I think. Could be clarified in the \"experiments\" writeup.\n-Generally, I found the motivations somewhat contrived (the part about keeping up with high demand for flu queries via adaptively scaling back latency during flu season, for example, or sec 3.1 saying that the fact that there's a discontinuity in the PABEE latency-accuracy tradeoff curve, etc). It's not at all clear that \"the accuracy on this GLUE task must be exactly 60%\", as given in 3.1 as a desideratum limiting industrial applicability of other techniques, ever makes sense. I thing it really suffices to say \"we are investigating the accuracy/latency tradeoff, motivated by the fact that some applications will require lower latency\" and leave it at that? I would expect most readers would recognize this to be one of the central tradeoffs across the entire field and acknowledge that it's useful as an object of study.\n-Just to be clear -- the different early-existing classifiers $f^{(i)}()$, for $i=1,\\ldots,m$ each get the training label during fine-tuning right (that is to say, each of them is trained to predict the exact same label as the others, conditioned on a different intermediate vector)? This could be stated more explicitly in subsecs 2.2 and 2.2.1 (I may have just missed it).\n-What are the relative weights $m$ in 2.2.1? are they hyperparams? 2.2.2 seems to describe different schemes for using weights $m$ during inference, but aren't they needed during training? Are they fixed the same across the different setups?\n-hard to read table 2 because of its size. maybe if there's room in next revision or appendix turn each of these cols into a small fig like fig 3? ", "label": [[1381, 1433, "Eval_neg_1"], [1438, 1540, "Eval_neg_2"], [1542, 1681, "Eval_neg_3"], [1683, 1805, "Jus_neg_3"], [1809, 1952, "Eval_neg_4"], [1956, 2064, "Jus_neg_4"], [2717, 2848, "Eval_neg_5"], [2849, 3241, "Jus_neg_5"], [3243, 3359, "Eval_neg_6"], [3360, 3856, "Jus_neg_6"], [3858, 4059, "Eval_neg_7"], [4060, 5343, "Jus_neg_7"], [5992, 6045, "Eval_neg_8"], [6047, 6284, "Jus_neg_8"], [6287, 6366, "Eval_neg_9"], [6368, 6819, "Jus_neg_9"], [7438, 7458, "Eval_neg_10"], [7459, 7584, "Jus_neg_10"]]}
{"id": 438, "review": "paper_summary\nThe authors use a variety of NLP methods to answer a number of linguistic questions regarding the phonology-syntax interface. Specifically, they use machine learning methods (varying the kinds of information available to the models) to demonstrate the learnability of well-researched word-pairing phenomena in Southeast Asian languages, namely \"coordinate compounds\" and \"elaborate expressions\". Their results are highly suggestive that \"although language users may use phonological hierarchies like those proposed in Mortensen (2006) to select appropriate orders for EEs and CCs, it is clearly not the case that they must (though they will perform a bit better if they do)\". \nsummary_of_strengths\nThe paper is very clearly written, both from the perspective of language use and argument structure. The flow of the article is easy to follow, and the logic is methodical and explicit. The methods used are all well-established in the NLP community, which makes interpretation and evaluation straightforward, but the application is novel and the results are very interesting from a linguistic standpoint. The authors do an excellent job of situating their argument in the context of other published research, and it seems clear that their results are an important contribution to that dialog. \nsummary_of_weaknesses\nNone. \ncomments,_suggestions_and_typos\nNone. ", "label": [[712, 812, "Eval_pos_1"], [813, 854, "Eval_pos_2"], [856, 896, "Eval_pos_3"], [898, 1019, "Eval_pos_4"], [1021, 1049, "Eval_pos_5"], [1050, 1116, "Eval_pos_6"], [1117, 1219, "Eval_pos_7"], [1221, 1304, "Eval_pos_8"]]}
{"id": 439, "review": "paper_summary\nTo analyze the attribution from a token to another token in transformer networks more accurately than previous studies, the authors proposed a new method named GlobEnc. Locally (for each layer), they extended the method of Kobayashi et al., 2021 by considering the contribution of residual connection #2 and layer normalization #2 (Figure 1). Globally (for the whole network), they corrected for the heuristic in the method of Abnar and Zuidema 2020.\nExperimental results showed that the proposed method is a faithful index that correlates well with the gradient x input method (Kindermans et al., 2016). In addition, they confirmed that the vector norm-based method (Kobayashi et al. 2020) and the consideration of residual connection (Kobayashi et al., 2021), which were proposed as local analysis methods, contribute to global faithfulness. They also pointed out that considering only layer normalization #1 (Kobayashi et al., 2021) is insufficient, and layer normalization #2 should be considered simultaneously. \nsummary_of_strengths\n1. Transformer networks are now a fundamental tool used not only in NLP but also in a wide range of data sciences, and improving analysis methods is an important research direction. \n1. Well written and easy to understand 1. Clearly mentioned the connections to related literatures 1. Thorough comparison with abundant previous methods in Experiments 1. Provide the insightful result that two Layer normalization in the layer counteract each other. \nsummary_of_weaknesses\n1. The models addressed in the experiments are limited; only the (fine-tuned) BERT-base models are used. It's not clear how generalizable the findings are to other transformer models (larger or smaller models; variants such as RoBERTa). \n1. No justifications for the way to measure the faithfulness of the attribution methods. In experiments, the faithfulness of the attribution methods are measured by calculating the correlation with the gradient x input method (Kindermans et al., 2016); however, it has been pointed out that this method can only capture the global behavior of the model in a very rough way (Sundararajan et al. ICML 2017). I am wondering that if the authors use a more accurate method, such as integrated gradient (Sundararajan et al.), will they get the same results? \n1. Insufficient description about qualitative analysis. How were two input sentences for qualitative analysis (Sec 4.5.5, Figure 1) determined? At least, the authors should mention whether these examples occurred during the fine-tuning process. Furthermore, the attribution values in Fig. 1 are 0 to 1. Did you normalize the vector norms? Also, does the attribution map for the input examples that failed to be classified also work well? \n1. Some points are slightly over-claimed. The proposed method does not consider feed-forward layers; however, some text is described as if the proposed method considers all components in transformer networks. \n    - Title: \"Incorporating the whole encoder layer\"     - l.08:  \"incorporates all the components\" \ncomments,_suggestions_and_typos\n- The authors can mention Geva et al. (EMNLP 2021) to justify excluding the Feed-Forward layer.\n-Although the mixing ratio of the methods with Fixed Residual (W_ {FixedRes} and N_ {FixedRes}) is explained as 0.5, it is actually lower than 0.5 because attention also has the effect of preserving. ", "label": [[1056, 1167, "Jus_pos_1"], [1168, 1235, "Eval_pos_1"], [1239, 1274, "Eval_pos_2"], [1278, 1334, "Eval_pos_3"], [1338, 1403, "Eval_pos_4"], [1407, 1502, "Eval_pos_5"], [1528, 1579, "Eval_neg_1"], [1581, 1629, "Jus_neg_1"], [1630, 1707, "Eval_neg_2"], [1709, 1759, "Jus_neg_2"], [1766, 1851, "Eval_neg_3"], [1852, 2315, "Jus_neg_3"], [2319, 2371, "Eval_neg_4"], [2372, 2754, "Jus_neg_4"], [2758, 2796, "Eval_neg_5"], [2797, 2964, "Jus_neg_5"]]}
{"id": 440, "review": "This paper presents evaluation metrics for lyrics generation exploring the need for the lyrics to be original,but in a similar style to an artist whilst being fluent and co-herent. The paper is well written and the motivation for the metrics are well explained.   The authors describe both hand annotated metrics (fluency, co-herence and match) and an automatic metric for \u2018Similarity'. Whilst the metric for Similarity is unique and interesting the paper does not give any evidence of this as an effective automatic metric as correlations between this metric and the others are low, (which they say that they should be used separately). The authors claim it can be used to meaningfully analyse system performance but we have to take their word for it as again there is no correlation with any hand-annotated performance metric.  Getting worse scores than a baseline system isn\u2019t evidence that the metric captures quality (e.g. you could have a very strong baseline).\nSome missing references, e.g. recent work looking at automating co-herence, e.g. using mutual information density (e.g. Li et al. 2015). In addition, some reference to style matching from the NLG community are missing (e.g. Dethlefs et al. 2014 and the style matching work by Pennebaker). ", "label": [[181, 206, "Eval_pos_1"], [211, 260, "Eval_pos_2"], [387, 523, "Eval_neg_1"], [524, 967, "Jus_neg_1"], [968, 991, "Eval_neg_2"], [993, 1103, "Jus_neg_2"], [1105, 1185, "Eval_neg_3"], [1187, 1254, "Jus_neg_3"]]}
{"id": 442, "review": "paper_summary\nThis paper proposes a pre-training objective based on QA, aiming at learning general-purpose contextual representations. They use BART model to generate QA pairs with context passages and use a cross-encoder RoBERTa to relabel the examples. After that, bi-encoder model is trained to match the cross-encoder predictions on the generated questions. And this paper conducts many experiments on multiple datasets of four downstream tasks. \nsummary_of_strengths\n1. The experimental effect is considerable under four different downstream tasks. \n2. Experiments require a lot of computing resources and time. \nsummary_of_weaknesses\n1. The article is very trivial and didn't write a paragraph to summarize the contribution of the paper, which makes it difficult to focus on the core ideas. \n2. It is suggested to combine pictures and other forms to assist the description, which will be clearer to understand. \n3. Will the code and dataset of this paper be published? \ncomments,_suggestions_and_typos\nPlease see my comments and questions in the core review. ", "label": [[643, 797, "Eval_neg_1"]]}
{"id": 444, "review": "- Strengths: This paper presents an extension of many popular methods for learning vector representations of text.  The original methods, such as skip-gram with negative sampling, Glove, or other PMI based approaches currently use word cooccurrence statistics, but all of those approaches could be extended to n-gram based statistics.  N-gram based statistics would increase the complexity of every algorithm because both the vocabulary of the embeddings and the context space would be many times larger.  This paper presents a method to learn embeddings for ngrams with ngram context, and efficiently computes these embeddings.  On similarity and analogy tasks, they present strong results.\n- Weaknesses: I would have loved to see some experiments on real tasks where these embeddings are used as input beyond the experiments presented in the paper.  That would have made the paper far stronger.\n- General Discussion: Even with the aforementioned weakness, I think this is a nice paper to have at ACL.\nI have read the author response. ", "label": [[506, 628, "Eval_pos_1"], [630, 691, "Eval_pos_2"], [706, 896, "Eval_neg_1"], [919, 1002, "Major_claim"]]}
{"id": 445, "review": "This paper is outside of my area of expertise, and my comments should be taken accordingly. It describes an improved system for word segmentation of \"Chinese\" (I guess this means Standard Written Chinese, but this isn't specified). Obviously, even small improvements on performance of word segmentation for Chinese could have significant positive impacts given the number of users of Chinese language resources. So, given that the paper reports improved performance over what it identifies as the previous state-of-the-art, it seems like a good paper for COLING. ( I have no reason to doubt that the comparison is against the actual state-of-the-art. However, I lack the expertise to know if this is the case or not.)\nSome potential issues I noted about this work are as follows: 1. I didn't see any indication as to whether the resources used in this work would be made public or not, including the new annotation guidelines. The impact of this work would be significantly greater if these resources would be made widely available, and that should be a factor in its acceptance, in my view. My low score on \"Reproducibility\" is due to this specific concern. The underlying algorithms seem clear enough, at least.\n2. I believe that the paper is, at present, aimed at a relatively small audience already familiar with previous work in this area. It would be hard for someone who is not aware of the literature on this topic, both in terms of work on Chinese and the parsing algorithms, to follow, in my view.\n3. The paper could benefit from more explicit example of the challenges that Chinese data poses for word segmentation for readers who are not familiar with this. This would, in particular, clarify the extent to which the results of this paper could be applied to other languages. ( This seems likely, especially for man Southeast Asian languages.)\n4. The paper categorizes Chinese as \"polysynthetic\", which is really bizarre. Most sources treat it as the \"opposite\" of polysynthetic, i.e., as analytic or isolating. Perhaps one can argue that the standard categorization is wrong, but no reasonable definition of polysynthetic would include Chinese.\n5. The term \"multi-grained word segmentation\" is not standard, and never appears to be defined in this paper. It would be good for it to be defined. ", "label": [[412, 717, "Major_claim"], [783, 1091, "Jus_neg_1"], [1092, 1158, "Eval_neg_1"], [1159, 1213, "Jus_pos_1"], [1217, 1344, "Eval_neg_2"], [1345, 1507, "Eval_neg_3"], [1511, 1669, "Eval_neg_4"], [1670, 1855, "Jus_neg_4"], [1858, 1933, "Eval_neg_5"], [1934, 2157, "Jus_neg_5"], [2161, 2307, "Eval_neg_6"]]}
{"id": 446, "review": "paper_summary\nThe paper measures the out-of-domain faithfulness of post-hoc explanations and the explanations of select-then-predict models. For post-hoc explanations, results show that in many cases, sufficiency and comprehensiveness are better for out-of-domain than for in-domain explanations. This leads to the suggestions,that these models generalize better than other models. Select-then-predict models also perform surprisingly well in out-of-domain settings. However, the authors find that using sufficiency and comprehensiveness for evaluation of faithfulness can be misleading. They hence suggest comparing the faithfulness of post-hoc explanations to a random attribution baseline to ensure robustness. \nsummary_of_strengths\nThe paper is well written and easy to understand. It follows a logical structure which makes it easy to follow the experiments and line of argumentation. There are only few grammatical mistakes. A very detailed evaluation is given for both normalized sufficiency and normalized comprehensiveness, as well as full-text F1. The models were trained and evaluated on multiple datasets and domains to ensure unbiased results. A zip file has been attached to reproduce results. The file contains Python code, along with required packages and a detailed README.md. References are given where necessary. All experiments are well explained, and details for datasets, models, and analyses are given in the appendix. \nsummary_of_weaknesses\nThere are quite a few punctuation errors and grammatical errors (l. 41, 137, 457), as well as orthographic inconsistencies (e.g., versions of \u201cfull-text\u201d l. 95, 343, Table 3, 459, etc.). Some of the findings are printed in italic letters in an inconsistent way (e.g., 391-396, 427-438, 555-561, but not 567-581), also, this makes reading a little inconvenient. Spaces between tables, formulas, and text are also inconsistent (e.g., l. 285, 372). \ncomments,_suggestions_and_typos\nPlease correct the punctuation errors and review the consistency of words (orthography with or without dashes, upper and lowercase letters). Remove italic conclusions to ensure better readability. ", "label": [[736, 785, "Eval_pos_1"], [786, 889, "Eval_pos_2"], [890, 930, "Eval_neg_1"], [931, 1057, "Eval_pos_3"], [1332, 1366, "Eval_pos_4"], [1465, 1528, "Eval_neg_2"], [1530, 1545, "Jus_neg_2"], [1548, 1587, "Eval_neg_3"], [1589, 1649, "Jus_neg_3"], [1652, 1725, "Eval_neg_4"], [1727, 1775, "Jus_neg_4"], [1778, 1824, "Eval_neg_4"], [1826, 1889, "Eval_neg_5"], [1891, 1908, "Jus_neg_5"]]}
{"id": 447, "review": "paper_summary\nThis paper argues that existing benchmarks for the text-to-SQL task have failed to capture the out-of-domain problem on column operations, matching domain-specific phrases to composite operation over columns. To tackle this problem, the authors propose a synthetic dataset together with a repartitioned train/test SQUALL dataset. On the methodology, prior domain knowledge is injected via column expansion templates, which requires a small amount of manual effort. For example, a \"time_span\" column can be expanded as \"start_time\" and \"end_time\" columns. Schema pruning is also proposed to drop irrelevant columns. In this way, column operations can be reduced to regular column matching. Empirical results on the two datasets show that the proposed method can significantly outperform two strong baseline parsers, seq2seq of Shi et al. (2020) and SmBop of Rubin and Berant (2021). \nsummary_of_strengths\n1. This paper reveals a new perspective of out-of-domain generalization on the text-to-SQL parsing, which I feel is a practical problem and valuable to the community. \n2. A synthetic dataset covering three domains (finance, sports, and science) has been newly created. The existing SQUALL has been repartitioned considering column operations such as field accessors or arithmetic operations. Both datasets target the domain generalization of column operation. \n3. The proposed method may seem to be engineering, where a small amount of manual effort is needed to design templates for schema expansion. But it works well on the two proposed datasets. The improvement against the baseline parsers is significant. And it helps the parser to relieve the burden of reasoning and reduces the column operations to a column matching problem. Schema pruning is also proposed to boost performance. \n4. The experimental study, as well as the discussion, is comprehensive. \nsummary_of_weaknesses\n1. My biggest concern is whether the templates are domain or dataset-specific. The motivation is to address the domain generalization on column operations, but it is implemented by manually crafted ad-hoc templates that could be tailored for the datasets. I understand this paper is the first step to tackle this problem. The authors also say that improvement does exist and automatic schema expansion is worth future study. \n2. I have not extensively studied the benchmarks in the text-to-SQL task. So I'm not sure whether repartitioning the SQUALL is the best choice or any other resources are also available. \ncomments,_suggestions_and_typos\n1. I have a question regarding the schema expansion for the synthetic dataset. According to my understanding,  Table 2 shows the templates used for the SQUALL. What are the templates used for the synthetic dataset? Are they the same as the formulas used in the dataset construction (Table 5)? If so, will this setting make the synthetic dataset easier? The figures in Table 3 demonstrates that the performance on the synthetic dataset is much better than that on the repartitioned SQUALL. \n2. I haven't seen any typos. The paper is clearly written. ", "label": [[921, 1085, "Eval_pos_1"], [1088, 1186, "Eval_pos_2"], [1382, 1567, "Eval_pos_3"], [1568, 1806, "Jus_pos_3"], [1810, 1879, "Eval_pos_4"], [1905, 1980, "Eval_neg_1"], [1981, 2327, "Jus_neg_1"], [3040, 3065, "Jus_pos_5"], [3066, 3096, "Eval_pos_5"]]}
{"id": 448, "review": "paper_summary\nThe paper considers the problem of structural bias in NLI, such as predicting the entailment from hypothesis only. \nThe problem is addressed by reformulating the NLI task as a generative task where a model is conditioned on the biased subset of the input and the label and generates the remaining subset of the input.\nWith this formulation, they achieve a bias-free model, with the downside of decreased performance on the actual task. Then they further analyze the cause for this poor performance and propose ways to mitigate it and partially close the gap with the performance of the discriminative model by discriminative fine-tuning.\nThe contributions of the paper are addressing the structural bias in NLI, building a generative model that leads to unbiased models, and improving its performance with discriminative fine-tuning (allowing some bias into the model). \nsummary_of_strengths\nThe examined problem is interesting and well-motivated.\nThe paper is well-written and overall easy to follow.\nThe paper provides a way to remove structural bias for the NLI task, examines the cause for its poor performance and proposes ways to improve it by balancing the bias-performance trade-off.\nThe proposed formulation and analysis can lead to interesting extensions in future work. \nsummary_of_weaknesses\nThe proposed bias-free model, although eliminating the bias, suffers from poor performance. One thing that the paper would benefit from would be stressing the practical implication of the approach, having in mind that it is not useful in itself for solving the task. \ncomments,_suggestions_and_typos\n255-257: Can you please explain why this is the case? ", "label": [[906, 961, "Eval_pos_1"], [962, 1015, "Eval_pos_2"], [1206, 1295, "Eval_pos_3"], [1318, 1409, "Eval_neg_1"], [1410, 1584, "Jus_neg_1"]]}
{"id": 449, "review": "paper_summary\nThis work focuses on spoken NER task and explores the use of external data in various form which is not annotated for the task such as plain speech audio, plain text, and speech with transcripts. The paper compares the pipeline approach and end-2-end approach to analyze how they benefit from external data. \nsummary_of_strengths\nS1: Impressive improvement in both E2E and pipeline models, up to 16% and 6% respectively.  S2: Experimenting with both E2E and pipeline approaches including respective baseline shows to impact of external data regardless of the approach.  S3: Error analysis in different forms gives some insight about where each approach (E2E or pipeline) differs or fails. \nsummary_of_weaknesses\nW1: Considering that E2E model benefits from mostly training over pseudo-labels (\u201cdistillation\u201d), the resulting model actually represents another pipeline, which requires the text-NER. This hurts the main arguments favoring the E2E approach over the pipeline model.\nW2: For the E2E model, for methods that do not use pseudo-labeling, improvements are rather limited. This point needs to be discussed more comprehensively, especially to understand what makes the difference when looking at the comparison for \u201cUn-Sp\u201d external data type \ncomments,_suggestions_and_typos\n- Different fonts for the captions make it harder to read. I suggest using a consistent font for the caption and the main text.  -A sentence that introduces the findings is missing before enumerating points in line 433 -Positions of the figures are not fitting with the explanations, which makes the paper harder to read. ", "label": [[348, 402, "Eval_pos_1"], [404, 433, "Jus_pos_1"], [996, 1092, "Eval_neg_1"], [1093, 1261, "Jus_neg_1"], [1296, 1352, "Eval_neg_2"], [1353, 1421, "Jus_neg_2"], [1424, 1512, "Eval_neg_3"], [1514, 1576, "Jus_neg_4"], [1578, 1616, "Eval_neg_4"]]}
{"id": 450, "review": "paper_summary\nThis paper introduces LexGLUE, a new benchmark for language understanding for English legal texts. \nThe authors claim it is important to have a benchmark for this, as NLU is more and more used for applications in legal domains, but it is not so clear what works and what not. \nWith the easier access to datasets and tools for legal NLP, hope is also to introduce more interdisciplinary researchers to legal NLP and  speed up the adoption and transparent evaluation of new legal NLP methods. \nThe authors collect 7 datasets for legal NLP (Multi-label classification, Multi class classification, multiple choice QA). \nThey use a SVM baseline and different pretrained transformers on this task to get an initial performance picture \nsummary_of_strengths\n- Authors chose only publicly available datasets -Datasets have a variety of sizes and sub domains -Baseline evaluation with many different models -Nice ethics statement \nsummary_of_weaknesses\n- Only English, it is understandable and explained in the paper why that is but still sad -Only 2.5 different tasks (Multi-label classification, Multi class classification, multiple choice QA) -Datasets are not new, but just aggregated from other people \ncomments,_suggestions_and_typos\nIt would be nice to hedge a bit more the NLU claim, as the tasks in this benchmark are mostly classification. NLU for me is more sematnic tasks NLI/QA/semantic parsing, relation extraction, ... ", "label": [[913, 935, "Eval_pos_1"], [960, 972, "Eval_neg_1"], [974, 1047, "Jus_neg_1"], [1049, 1073, "Eval_neg_2"], [1074, 1149, "Jus_neg_2"], [1152, 1172, "Eval_neg_3"], [1174, 1211, "Jus_neg_3"]]}
{"id": 451, "review": "paper_summary\nThis paper proposed improvements to the Levenshtein transformer for lexically constraint translation task. The proposed improvements through alignment constraints during training and or inference \nsummary_of_strengths\n- Clear writing and captivating content; the motivating study section is a nice approach to architecture and experimental design and the analysis section is a great read - The experiments/analyses are well-taught out to validate stated hypothesis and comparative studies with different ablations are applied to support analyses - Based on Ding et al. 2021 analysis of low-freq words, the hunch of injecting alignments knowledge outside of the AT / NAT model is a good one \nsummary_of_weaknesses\n- Nothing much, a very well-written and thought out papers and experiments; although it's incremental improvements to the LevT, it's an extensive study with solid empirical evidence for the conjectures stated in the paper.\n- [Possibly out of scope] The question still remains why can't a NAT learn the alignment mapping with its cross attention to different token positions? It's possibly out of scope of the paper but a good food for thought. \ncomments,_suggestions_and_typos\n- I might have missed it but the results section stated \"When translating without constraints, however, adding ACT does not bring consistent improvements as hard and soft constraints do, which could be attributed to the discrepancy between training and inference.\"; does that mean all the +ACT results only have alignments constraints and prompting during training and no alignments prompting at inference? Actually, it'll be kind of tough to do alignment prompting at inference anyways because should the prompting at every iteration or the first iteration of the decoder?  - Would the code and experiments be open sourced like Susanto et al?  - Maybe I'm reading too much into Table 4 results, the improvements clearly comes from the constraint training, esp. the soft constraint setup. Thinking out loud, is GIZA++ or any external aligner necessary for the alignment? Can some mechanism/layer be proposed to replace that alignment from GIZA?\n- Section 6.3 is honest limitation but the improved from 94.25 -> 96.90 is also quite a feat since it's inching at a strong baseline.  - Figure 2 also shows where the BLEU is lost in the 10-30% bin, would it be possible to identify all terms in that bin and list them in the appendix with the (source, term) -> LevT vs LevT + ACT outputs? A few possible reason that the model is finding it hard to learn that 10-30% bin, (i) term is mapped to too many multiple targets in training data or mapped to targets that's not in the constraint list of terms in a skewed manner, (ii) the translated outputs though not in the reference are valid translations though not the preferred one ", "label": [[234, 271, "Eval_pos_1"], [273, 360, "Eval_pos_2"], [365, 401, "Eval_pos_3"], [404, 478, "Eval_pos_4"], [483, 559, "Eval_pos_5"], [562, 703, "Eval_pos_6"], [729, 801, "Major_claim"], [803, 949, "Eval_pos_6"]]}
{"id": 452, "review": "paper_summary\nThe paper 'Rewire-then-Probe: A Contrastive Recipe for Probing Biomedical Knowledge of Pre-trained Language Models' studies the biomedical knowledge contained in PLMs with factual probing techniques. To this end, the authors propose a new biomedical probing suite derived from UMLS and evaluate different PLMs with multiple probing techniques on it. After discovering that these probing techniques lead to very low accuracy values, the authors design a novel 'rewiring' pretraining objective that boosts accuracy scores for retrieval-based probing. \nsummary_of_strengths\n- Factual probing is an important technique to study the factual knowledge internalized by PLMs and it is underresearched in the biomedical domain.\n-The authors provide a novel large factual probing dataset for the biomedical domain that contains manually verified triples.\n-The proposed 'rewiring' pretraining objective improves probing accuracy by a large margin without resorting to supervised training.\n-The paper is well written and code and data are available \nsummary_of_weaknesses\nI only have minor concerns with this paper: -The authors investigate only one prompt per relation which may be suboptimal -Supervised prompting methods were excluded from the evaluation, but they might provide interesting points of comparison \ncomments,_suggestions_and_typos\nNone ", "label": [[734, 858, "Eval_pos_1"], [993, 1018, "Eval_pos_2"], [1119, 1195, "Eval_neg_1"]]}
{"id": 454, "review": "- Strengths: - this article puts two fields together: text readability for humans and machine comprehension of texts - Weaknesses: - The goal of your paper is not entirely clear. I had to read the paper 4 times and I still do not understand what you are talking about!\n-The article is highly ambiguous what it talks about - machine comprehension or text readability for humans -you miss important work in the readability field -Section 2.2. has completely unrelated discussion of theoretical topics.\n-I have the feeling that this paper is trying to answer too many questions in the same time, by this making itself quite weak. Questions such as \u201cdoes text readability have impact on RC datasets\u201d should be analyzed separately from all these prerequisite skills.\n- General Discussion: - The title is a bit ambiguous, it would be good to clarify that you are referring to machine comprehension of text, and not human reading comprehension, because \u201creading comprehension\u201d and \u201creadability\u201d usually mean that.\n-You say that your \u201cdataset analysis suggested that the readability of RC datasets does not directly affect the question difficulty\u201d, but this depends on the method/features used for answer detection, e.g. if you use POS/dependency parse features.\n-You need to proofread the English of your paper, there are some important omissions, like \u201cthe question is easy to solve simply look..\u201d on page 1.\n-How do you annotate datasets with \u201cmetrics\u201d??\n-Here you are mixing machine reading comprehension of texts and human reading comprehension of texts, which, although somewhat similar, are also quite different, and also large areas.\n-\u201creadability of text\u201d is not \u201cdifficulty of reading contents\u201d. Check this: DuBay, W.H. 2004. The Principles of Readability. Costa Mesa, CA: Impact information.  -it would be good if you put more pointers distinguishing your work from readability of questions for humans, because this article is highly ambiguous. \nE.g. on page 1 \u201cThese two examples show that the readability of the text does not necessarily correlate with the difficulty of the questions\u201d you should add \u201cfor machine comprehension\u201d -Section 3.1. - Again: are you referring to such skills for humans or for machines? If for machines, why are you citing papers for humans, and how sure are you they are referring to machines too?\n-How many questions the annotators had to annotate? Were the annotators clear they annotate the questions keeping in mind machines and not people? ", "label": [[133, 178, "Eval_neg_1"], [179, 268, "Jus_neg_1"], [270, 321, "Eval_neg_2"], [378, 427, "Eval_neg_3"], [428, 499, "Eval_neg_4"], [501, 626, "Eval_neg_5"], [627, 761, "Jus_neg_5"], [786, 814, "Eval_neg_6"], [816, 1006, "Jus_neg_6"], [1256, 1303, "Eval_neg_7"], [1305, 1402, "Jus_neg_7"], [1797, 1904, "Jus_neg_8"], [1906, 1946, "Eval_neg_8"]]}
{"id": 455, "review": "- Strengths: This paper has high originality, proposing a fundamentally different way of predicting words from a vocabulary that is more efficient than a softmax layer and has comparable performance on NMT. If successful, the approach could be impactful because it speeds up prediction.\nThis paper is nice to read with great diagrams. it's very clearly presented -- I like cross-referencing the models with the diagrams in Table 2. Including loss curves is appreciated.\n- Weaknesses: Though it may not be possible in the time remaining, it would be good to see a comparison (i.e. BLEU scores) with previous related work like hierarchical softmax and differentiated softmax.\nThe paper is lacking a linguistic perspective on the proposed method. Compared to a softmax layer and hierarchical/differentiated softmax, is binary code prediction a natural way to predict words? Is it more or less similar to how a human might retrieve words from memory? Is there a theoretical reason to believe that binary code based approaches should be more or less suited to the task than softmax layers?\nThough the paper promises faster training speeds in the introduction, Table 3 shows only modest (less than x2) speedups for training. Presumably this is because much of the training iteration time is consumed by other parts of the network. It would be useful to see the time needed for the output layer computation only.\n- General Discussion: It would be nice if the survey of prior work in 2.2 explicitly related those methods to the desiderata in the introduction (i.e. specify which they satisfy).\nSome kind of analysis of the qualitative strengths and weaknesses of the binary code prediction would be welcome -- what kind of mistakes does the system make, and how does this compare to standard softmax and/or hierarchical and differentiated softmax?\nLOW LEVEL COMMENTS Equation 5: what's the difference between id(w) = id(w') and w = w' ?\n335: consider defining GPGPU Table 3: Highlight the best BLEU scores in bold Equation 15: remind the reader that q is defined in equation 6 and b is a function of w. I was confused by this at first because w and h appear on the LHS but don't appear on the right, and I didn't know what b and q were. ", "label": [[13, 44, "Eval_pos_1"], [46, 205, "Jus_pos_1"], [207, 253, "Eval_pos_2"], [254, 286, "Jus_pos_2"], [287, 334, "Eval_pos_3"], [335, 362, "Eval_pos_4"], [366, 431, "Eval_pos_5"], [432, 469, "Eval_pos_6"], [674, 743, "Eval_neg_1"], [744, 1084, "Jus_neg_1"], [1085, 1218, "Eval_neg_2"], [1219, 1405, "Jus_neg_2"]]}
{"id": 456, "review": "The paper presents a complicated matter in a remarkably clear fashion, which makes it easy to follow, even for non-specialists. The extension of NLP procedures to languages with less resources is welcome, and the approach taken here is well argued, justified and evaluated. I cannot comment on the mathematical details, though.\nThe term \"backtranslation\" is mentioned in Section 2, but is explicated in detail as late as in 6.3. Consider adding a short sentence in Section 2 explaining the nature and motivation of a back-translation approach.\nIn section 3, a reference is needed for the reader to look up the history of the Arabic language(s). The following chapter might be sufficient (or references therein): van Putten. Marijn. 2020. Classical and Modern Standard Arabic. In Christopher Lucas & Stefano Manfredi (eds.), Arabic and contact-induced change, 57\u201382. Berlin: Language Science Press. https://langsci-press.org/catalog/book/235 A couple of references use \"(Smith 2000)\" where \"Smith (2000)\" would have been appropriate. Please check all references.  The literature entries for Ferguson, Guzman, Habash have problems with upper/lower case letters. Check these and also the remaining entries.  Appendices D & E  would profit from lines with transliteration. I can read some Arabic, but not as fast as I would need to make the appendix a worthwhile addition for a reader like me. ", "label": [[0, 127, "Eval_pos_1"], [128, 203, "Eval_pos_2"], [209, 272, "Eval_pos_3"], [544, 644, "Eval_neg_1"], [645, 1061, "Jus_neg_1"]]}
{"id": 457, "review": "This paper proposed to explore discourse structure, as defined by Rhetorical Structure Theory (RST) to improve text categorization. A RNN with attention mechanism is employed to compute a representation of text. The experiments on various of dataset shows the effectiveness of the proposed method. Below are my comments: (1) From Table 2, it shows that \u201cUNLABELED\u201d model performs better on four out of five datasets than the \u201cFULL\u201d model. The authors should explain more about this, because intuitively, incorporating additional relation labels should bring some benefits. Is the performance of relation labelling so bad and it hurts the performance instead?\n(2) The paper also transforms the RST tree into a dependency structure as a pre-process step. Instead of transforming, how about keep the original tree structure and train a hierarchical model on that?\n(3) For the experimental datasets, instead of comparing with only one dataset with each of the previous work, the authors may want to run experiments on more common datasets used by previous work. ", "label": [[325, 481, "Eval_neg_1"], [483, 658, "Jus_neg_1"]]}
{"id": 458, "review": "paper_summary\nThis paper introduces two grammatical error correction probing tasks for     Chinese masked language models: one for replacing erroneous characters or     words, and another for inserting the correct character or word in order to     make a sentence grammatical. The authors employ these tasks to investigate     models trained via different masking strategies: character-based masking,     word-based masking or both. They find that, for spans of length 1, their     character-masked models perform best for both task, while for spans of     length 2-4, the combined character and word model generally fares better. \n    The authors also experiment with fine-tuning existing pretrained RoBERTA     models, and demonstrate that these models fare worse on both tasks. \nsummary_of_strengths\nThis paper articulates an interesting distinction between character and     word-level masking for Chinese masked language modeling, and investigates     the implications of such modeling decisions in a practical way. Generally,     it is concise and easy to follow. The illustrations are very informative and     provide a good overview of the tasks. The examples are likewise very useful for non-Chinese speakers. \n    Naturally, the dataset and models the authors create can be useful for     future research. \nsummary_of_weaknesses\nA study focusing on grammatical error correction should include an error     analysis that attempts to characterize the types of a mistakes the model     makes. The authors likely had to go forego this step due to space     constraints, but without it the insights produced here are generally     limited.  Furthermore, the authors do not include any sort of context for what kind of     performance the models are expected to achieve for these tasks in favorable     circumstances, e.g. in a supervised learning regime. Without this     information, it is difficult to ascertain if the out-of-the-box models'     probing performance is at all good, or if it pales in comparison to simple     supervised baselines. I imagine this would be easy to achieve by fine-tuning     a model on some splits of the dataset, or training another architecture for     the task, e.g. a BiLSTM.  Lastly, it would be useful to see how their models fare on downstream tasks,     how they compare with other Chinese-language MLMs like RoBERTa, and what     implications this might hold for Chinese MLM design. Indeed, the authors     provide this information in the appendix, but nothing is shown in the main     body of the paper, and the discussion is limited to a sentence. \ncomments,_suggestions_and_typos\n- How is P@10 calculated for spans of length 2+? This should be mentioned.\n-Is the [MASK] token applied to the omitted characters in the insertion task?\nLine 249: Michelle, not Micheal ", "label": [[1020, 1069, "Eval_pos_1"], [1070, 1154, "Eval_pos_2"], [1155, 1219, "Eval_pos_3"], [1224, 1316, "Eval_pos_4"], [1339, 1499, "Eval_neg_1"], [1500, 1644, "Jus_neg_1"], [1646, 1820, "Eval_neg_2"], [1822, 2217, "Jus_neg_2"], [2219, 2294, "Eval_neg_3"], [2300, 2597, "Jus_neg_3"]]}
{"id": 460, "review": "paper_summary\nThis paper presents a technique for adapting the learning process based on CBMI (Conditional Bilingual Mutual Information), which is defined to be the ratio of the translation probability over the language model probability for each token.  The motivation of this method is that neural models tend to already excel at producing fluent text, but sometimes fail to produce accurate translations.  Experimentally, the approach shows a small but measurable difference (according to BLEU) over the baseline with no adaptive training, and performs similarly (perhaps better) than other similar methods.  One of the models is also compared via human evaluation to the baseline, scoring with adequacy and fluency, and the technique increases adequacy more than fluency (though both are already high). \nsummary_of_strengths\n- Reasonable motivation for the technique from a theoretical view -Technique is practical, involves training a small LM model during training but no test-time overhead.\n-Many similar techniques are reviewed, briefly explained, and compared \nsummary_of_weaknesses\n- Some important details are omitted, e.g., pNMT is \"the probability output by the NMT model\" and pLM \"the probability output by an additional target-side language model\" without any real implementation details.  Are these computed using the full transformer models (embedding, encoder, decoder, softmax)?  Is the BPE vocabulary of the LM fixed to the same vocab as the NMT model?  Is the NMT model used in CMBI the one being trained, or the fixed one after 100k steps?  Learning rates and decay are also not discussed.  I have a good idea of the answer to some of these questions, but the details could be important.\n-Is the baseline transformer model only trained for 100k steps?  If so, how do we know that training for another 100-200k steps alone might not account for the score improvement.\n-Some explanations of techniques are too brief, e.g., Focal and Anti-focal loss are described exactly the same (same formula and even hyperparameters) yet give different results, Self-paced learning receives a too-brief single sentence description.  Also, section 5.4 and appendix C are not detailed enough to understand the experiment. \ncomments,_suggestions_and_typos\n- I'd like to see some discussion/visualization of learning curves, since the kind of differences in scores reported can be due to converged vs. non-converged models -Are the Self-paced learning results all from another paper?  In Table 1 only the base results are flagged as such, but in table 6, it looks like both results are.\n-Using p(x|y_<j) -- the source probability conditioned on the target prefix -- seems a strange choice.  I would think the unconditional source probability would be better.  Yet, based on Eq 7, the source probability factor is cancelled out anyway, so it doesn't really matter.\n-Eq 15 and 16 seem identical, but they are called \"focal loss\" and \"anti-focal loss\".  Shouldn't they be opposite?  Otherwise how are they different in practice\u2026 even the hyperparams from the appendix are largely the same.  Please highlight the difference in the description in Section 4.4.\n-Typo: +0.55 -> 0.54 BLEU (line 473) -Section 5.4 - What is really happening here?  How are these priors calculated and used in training?  Is this related to equation 18 (from Appendix C)?  Also, the LM prior seems to give equivalent results to the \"prior selection\" and even to the full CBMI adaptive training method, why is this LM prior not compared to the LM prior in table 1?\n-Are the CMBI values used in figure 4 and prior selection the non-normalized values -Appendix C:  hard to understand how Figure 5 is produced, since CBMI values are calculated per token, not per sentence.  Also how is equation 18 used? \nFigure 4 shows the LM prior dominating < 0, the TM prior dominating > 8, and CMBI prior dominating between them.  Why dos the \"prior selection\" method then have the TM prior between 0 and 6, and the CMBI prior above 6?  This doesn't match figure 4 (at least as labeled). ", "label": [[831, 894, "Eval_pos_1"], [896, 997, "Eval_pos_2"], [999, 1069, "Eval_pos_3"], [1094, 1128, "Eval_neg_1"], [1130, 1709, "Jus_neg_1"], [1890, 1935, "Eval_neg_2"], [1937, 2137, "Jus_neg_2"], [2139, 2226, "Eval_neg_3"], [2590, 2691, "Eval_neg_4"], [2693, 2865, "Jus_neg_4"]]}
{"id": 461, "review": "paper_summary\nThis paper tackles the problem of measuring social biases in sense embeddings. It presents the new Sense-Sensitive Social Bias (SSSB) dataset and extends existing bias evaluation datasets (WEAT and WAT) to be compatible with word sense-level evaluation. It presents new evaluation approaches for measuring social biases in static and contextualized sense embeddings, and reports on experimental analysis of state-of-the-art sense embedding models under these evaluation schemes. The findings demonstrate that sense embeddings do not conform to the behavior of word embeddings with respect to social biases, and that sense embeddings often show higher degrees of bias under the presented evaluations. \nsummary_of_strengths\nThis is a strong paper. The text is well written and clearly structured, with a logical flow that makes it easy to follow the arguments. Related work is presented in appropriate detail and the paper is clearly situated with respect to existing work in computational analysis of social biases within the ACL community. The experiments are thorough and well-motivated, and the methodology is clear and reproducible. he analysis is well described and appropriate, and includes a thoughtful discussion of differences in observed behavior between word and sense embeddings on the various datasets evaluated. The three categories included in SSSB are well-motivated, and the occupation/action distinction is an especially clever choice. The tables and figures are (largely) appropriate and informative. \nsummary_of_weaknesses\nThe main weakness of this paper is that it is not sufficiently grounded in the sociolinguistic side of bias measurement, and therefore makes some significant claims and design decisions that are not entirely appropriate.\nThe proposed task and the experimental design are predicated on the assumption that word senses as semantically disjoint with respect to social biases, but this is not really true (which is actually nodded to in the paper's discussion section). References to nationality and language can often be used metonymically to imply one another (particularly for voicing negative opinions), and colour and race are most certainly strongly correlated in visual design and storytelling (eg the use of dark colours, especially black, for evil in Western European storytelling, which is less common in African stories). Occupations and associated actions, while not used metonymically, may still be correlated: it is likely higher surprisal to refer to a woman \"engineering a solution\" than a man, for example. ( It is worth noting that the maximum pairwise similarity approach used in WAT calculation runs counter to this assumption of disjoint senses.)\nThere is also an important distinction between observability of a bias (e.g., through surveying/prompting native speakers) and measuring it using computational tools. Social biases may be observable without being measured by current approaches. These nuances are important to discuss throughout the paper: social bias assessment is not a computational task alone, but must incorporate human judgment and be framed through specific human viewpoints with acknowledged limitations.\nThere are several further issues with the precision of the language used in the paper that are important to address when discussing a sensitive topic such as social bias measurement.\n-The term \"bias\" itself needs some context. Biases may be positive or neutral (eg preferring to complete \"they sat at the computer to write ___\" with \"code\" vs \"a book\" when trained on different data), as well as negative. As such, it is not clear what it means to have an \"unbiased\" model, or when that is desirable. A clearer definition of what kinds of bias are being assessed, what their impact is, and what mitigation of these biases might look like will help set the context better.\n-The term \"stereotype\" is not used correctly. A stereotype is a commonly-held association between one group and some attribute or behavior; although most stereotypes involve negative attributes or disfavored behaviors, a stereotype is not inherently negative. The labels of \"stereotype/anti-stereotype\" are therefore not appropriate; positive/negative would be better. ( An anti-stereotype would be something that contradicts the stereotypical attribute or behavior.) Stereotypes are also socially-derived based on common beliefs/associations; for example, \"Japanese people are stupid\" is not a commonly-held belief, so this is not a stereotype.\n-The terms \"pleasant\" and \"unpleasant\" are used, but are not clearly defined and feel subjective enough to mask the import of the difference being discussed. \" Positive\" and \"negative\" are the standard words used to discuss emotional valence of this type; these should either be used or the use of alternatives should be justified and clearly defined.\n-Race and ethnicity are distinct constructs; the evaluation described in Section 4.2 is comparing race senses with colour senses, not ethnicity. ( Ethnicity is related more to heritage and origin, while race is a social group that one can be assigned to by others or identify with voluntarily.)\n-Referring to nationality groups as \"disadvantaged groups\" (Line 286) doesn't quite work; while the racial identities that nationalities are correlated with may be disadvantaged, non-US nationalities themselves are not disadvantaged. ( Eg Scandinavian nationalities are very unlikely to be considered disadvantaged.)\nIt is not clear what the impact could be of measuring bias in sense embeddings or of debiasing sense embeddings. These models are much more rarely used than word embeddings, and in much more specialized settings.\nOther issues with the presentation: -Section 3 is difficult to follow. It is not clear how (X and Y) and (A and B) are related, or what a high or low score on the s and w functions means.\n-Figures 2 and 3 are unreadable.\n-P-value calculation is discussed in Section 3, but no p-values are reported in the experimental results. \ncomments,_suggestions_and_typos\n- The inclusion of the verb sense of \"carpenter\" in Section 4.3 is a little questionable; this is a very rarely-used sense. The other occupation/action words are commonly used and could reasonably be measured, but very few texts (for embedding training) will use carpenter as a verb, and many native speakers will not recognize it as correct.\n-There is growing consensus to capitalize racial identifiers (certainly \"Black\", increasingly \"White\").\n-Lines 442-445: This actually does not accurately simulate the word-level embedding case, as it assigns equal weight to all senses. In practice, word senses are more likely to be Zipf-distributed, so a word-level embedding model will be exposed to much more training data for some senses than for others.\n-It is not clear how the categories in WEAT (Table 2) are associated with the social biases this paper is framed around.\n-It would be helpful to mark the extended WEAT/WAT contributed by this paper using a modified label (eg WEAT*), in Table 2 and in text.\n-The Ethical Statement is well written, but should be extended with some more concrete discussion of challenges not addressed in the dataset (eg gender beyond the binary, races other than Black, other sources of social bias).\n-It would be helpful to have a listing of nationalities/racial identities/occupations included in the dataset (along with the adjectives used) as part of the paper, such as in supplementary tables.\n-It would be interesting to see what human behavior is for these prompts/comparisons... -Dev et al 2019 and Nadeem et al 2020 references are missing publication information.\nTypos -Figure 1, graph titles \"embembeddings\" -Line 051: extra \"is\" -Line 122: missing \"are\" -Line 603: \"bises\" -> \"biases\" -Line 610: missing \"being\" -Spaces not needed between section symbol and number -Spaces after non-terminal periods (\"vs.\", \"cf.\") should be escaped to avoid spacing issues ", "label": [[736, 759, "Eval_pos_1"], [760, 872, "Eval_pos_2"], [873, 920, "Eval_pos_3"], [925, 1052, "Eval_pos_4"], [1054, 1101, "Eval_pos_5"], [1107, 1148, "Eval_pos_6"], [1150, 1338, "Eval_pos_7"], [1339, 1395, "Eval_pos_8"], [1401, 1465, "Eval_pos_9"], [1467, 1533, "Eval_pos_10"], [1556, 1776, "Major_claim"], [1777, 2021, "Eval_neg_1"], [2022, 3198, "Jus_neg_1"], [3199, 3381, "Major_claim"], [3383, 3425, "Eval_neg_2"], [3426, 3870, "Jus_neg_2"], [3872, 3916, "Eval_neg_3"], [3917, 4516, "Jus_neg_3"], [4518, 4674, "Eval_neg_4"], [4675, 4868, "Jus_neg_4"], [4870, 4913, "Eval_neg_5"], [4914, 5163, "Jus_neg_5"], [5165, 5253, "Eval_neg_6"], [5254, 5693, "Jus_neg_6"], [6056, 6143, "Eval_neg_7"], [6144, 6396, "Jus_neg_7"], [6807, 6926, "Eval_neg_8"], [7064, 7204, "Eval_neg_9"], [7205, 7286, "Jus_neg_9"]]}
{"id": 462, "review": "paper_summary\nThe paper mainly proposes an approximation approach for transformer model inference so that it can apply homomorphic encryption. First, the author introduces the current practical homomorphic encryptions and transformer-based model. Second, the author proposes an approximation workflow by separating the fine-tuning stage into several subphases. Third, the author evaluates the approximation performance on different NLP tasks and datasets. \nsummary_of_strengths\nHomomorphic Encryption is widely used for security and privacy protection and this problem is meaningful for real-world applications.  The experiment results in table 1 and table 2 are promising.\nThe paper is well-organized and easy to follow. \nsummary_of_weaknesses\nThe author only evaluates tinyBERT and doesn't consider the larger model BERT_base and BERT_large. When the number of encoders increases, the performance loss could drop significantly. \ncomments,_suggestions_and_typos\nin Figure 4, the blue line and orange line are almost overlapping, it is difficult to distinguish. Increasing the width of the line can help. ", "label": [[613, 673, "Eval_pos_1"], [674, 722, "Eval_pos_2"]]}
{"id": 463, "review": "The paper describes a human evaluation comparison of AMR-to-English NLG systems. It is well structured and clearly written, but does not provide much new insights. In my opinion, the paper would be much better if it thoroughly described the systems for which it compares the outputs, providing a kind of review of AMR-NLG systems. Now the descriptions of the systems are too short and cryptical. The paper would be acceptable for me if this were provided. \nThe description of the metrics used in AMR-NLG evaluation is very similar to discussions in MT about reference-based metrics. It would be good to refer to that field. Although there is reference to WMT to argue for human evaluation, there is no reference to the MT metrics shared task, which seems relevant as an argument for the weaknesses of automated reference-based metrics. \nAs you are comparing different systems on relatively small test sets, significance levels for at least some of the metrics should be provided, especially when you are using subsets of only 100 sentences.\nDetails: -Put the figures and tables nearer to where they are discussed in the text.\n-The violin plots seem to suggest a scale of below 0 to above 100, which should not be possible according to the description of the annotations.\n-Are the annotators native English speakers? If not, then it might be difficult for them to judge fluency.\n-Provide a url or reference to LDC2015E86 and LDC2017T10 -FORGe in section 4.1 is not mentioned anywhere else. ", "label": [[81, 122, "Eval_pos_1"], [124, 162, "Eval_neg_1"], [164, 330, "Jus_neg_2"], [331, 395, "Eval_neg_2"], [396, 456, "Major_claim"], [457, 623, "Jus_neg_1"], [624, 836, "Eval_neg_1"], [837, 905, "Jus_neg_2"], [907, 978, "Eval_neg_2"], [980, 1039, "Jus_neg_2"]]}
{"id": 464, "review": "paper_summary\nThis paper proposes to use the Best-first search algorithm to improve the Beam search method for generating diverse sentences. It constructs a lattice graph along with the hypothesis recombination method to shrink the size of the graph. In experiments, the authors compare with several baselines, showing the advantage of the proposed method. \nsummary_of_strengths\nthe paper is clearly written and good to follow. the experiments and analysis are impressive. \nsummary_of_weaknesses\nThe methodology part is a little bit unclear. The author could describe clearly how the depth-first path completion really works using Figure 3. Also, I'm not sure if the ZIP algorithm is proposed by the authors and also confused about how the ZIP algorithm handles multiple sequence cases. \ncomments,_suggestions_and_typos\n- Figure 2, it is not clear about \"merge target\". If possible, you may use a shorter sentence.\n-Line 113 (right column), will the lattice graph size explode? For a larger dataset, it may impossible to just get the lattice graph, am I right? How should you handle that case?\n-Algorithm 1 step 4 and 5, you may need to give the detailed steps of *isRecomb* and *doRecomb* in the appendix.\n-Line 154 left, \"including that it optimizes for the wrong objective\". Can you clearly state what objective? why the beam search algorithm is wrong? Beam search is a greedy algorithm that can recover the best output with high probability.\n-For the ZIP method, one thing unclear to me is how you combine multiple sequences by if they have different lengths of shared suffixes?\n-Line 377, is BFSZIP an existing work? If so, you need to cite their work.  -In figure 5, the y-axis label may use \"Exact Match ratio\" directly.\n-Line 409, could you cite the \"R2\" metric?\n-Appendix A, the authors state \"better model score cannot result in better hypothesis\". You'd better state clearly what idea hypothesis you want. \" a near-optimal model score\" this sentence is unclear to me, could you explain in detail?\n-In line 906, it is clear from the previous papers that Beam search results lack diversity and increase the beam size does not work. Can you simplify the paragraph? ", "label": [[379, 427, "Eval_pos_1"], [428, 473, "Eval_pos_2"], [496, 541, "Eval_neg_1"], [542, 787, "Jus_neg_1"]]}
{"id": 465, "review": "paper_summary\nThis paper studies translationese effects (differences between original and translations in the same language) in machine translation. Since one cannot have parallel corpora with originals in the 2 languages simultaneously, the authors analyse which is the best configuration in data and model alignments for maximising machine translation quality. Recent literature has studied the effect that translationese has in the test sets for the evaluation, but training data and models have been less explored. For this work, the authors first compile a corpus tagged for translationese in five language pairs (causalMT) and then run several experiments on different combinations of the data to see the effect on BLEU scores. \nsummary_of_strengths\n- The number of experiments is comprehensive - The conclusions might help MT researchers in designing their systems. This is true for language pairs where translationese information is available, but also in general when one uses back-translation or self-training \nsummary_of_weaknesses\n- Given the nature of the work, evaluation is important. Authors argue that they cannot perform human evaluation, but at least the combination of other metrics would be relevant. COMET for instance correlates better with human judgements than BLEU [1] and goes further string matching - A couple of relevant references are missing and it would be nice to see them included in the revised version and emphasize your contributions after their works: [2] which create a similar corpus from Europarl with the addition of information about the possibility of pivot translation (which might translationese effects) and [3] which also studied the relevance of the train-test alignment in SMT translation quality (see the references in the Comments section) \ncomments,_suggestions_and_typos\n- Section 4, lines 404-411. \nI don't understand this part, I'm not able to reproduce you Corr column in Table 4 from the numbers in Table 2. Table 4's caption helps a bit, but I think the explanation here is not clear enough. Also, the term correlation is confusing (at least to me). I understand your point in the definition but, still, when I read \"correlation\" I expect something between -1 and 1 that shows me how much two variables relate to each other.\n- Still in Table 4, the 3r column shouldn't be named ATE? If I understood properly Corr is also Cau-Ant.\n- Causal effect results. How does (1) and the last part of (2) match? \n\"The data-model alignment is a clear cause for MT performance.\" vs \"For other language pairs, the data-model alignment can sometimes have a distinct positive impact and can also sometimes have a negative impact.\"\n- Related to the previous point: it makes a lot of sense to me that data-model alignment affects MT performance. The fact that it does not always happen couldn't be the effect of more \"others\" in Eq.3 besides length and content?\n- I think it would be good to include some of the appendices in the main text. Some parts of the main text are duplicated (Section 6 is already found in pieces in previous sections) on the other hand some relevant information is ignored here. \nFor instance, I found Appendix E1 especially interesting as the performance quality of method used to find topic and length equivalents might affect the final quality of the generated corpora.\nReferences [1] Tom Kocmi, Christian Federmann, Roman Grundkiewicz, Marcin Junczys-Dowmunt, Hitokazu Matsushita, Arul Menezes. To Ship or Not to Ship: An Extensive Evaluation of Automatic Metrics for Machine Translation. WMT@EMNLP 2021: 478-494 [2] Kwabena Amponsah-Kaakyire, Daria Pylypenko, Cristina Espa\u00f1a-Bonet and Josef van Genabith. Do not Rely on Relay Translations: Multilingual Parallel Direct Europarl. Proceedings of the Workshop on Modelling Translation: Translatology in the Digital Age (MoTra21), pages 1-7, Iceland (online), May 2021.\n[3] Sylwia Ozdowska, and Andy Way. Optimal Bilingual Data for French-English PB-SMT. Annual Conference of the European Association for Machine Translation EAMT (2009). ", "label": [[758, 800, "Eval_pos_1"], [803, 872, "Eval_pos_2"], [873, 1020, "Jus_pos_2"], [1045, 1099, "Jus_neg_1"], [1100, 1221, "Eval_neg_1"], [1222, 1327, "Jus_neg_1"], [1330, 1373, "Eval_neg_2"], [1374, 1793, "Jus_neg_2"], [1828, 1883, "Eval_neg_3"], [1885, 1996, "Jus_neg_3"], [1998, 2050, "Eval_neg_3"], [2052, 2109, "Eval_neg_4"], [2110, 2284, "Jus_neg_4"]]}
{"id": 466, "review": "paper_summary\nThis paper proposes a conversation-based VQA (Co-VQA) framework to decompose a complex question into a sequence of sub-questions and get the final result by answering the sub-questions. \nThe Co-VQA contains three components: Questioner, Oracle, and Answerer, to decompose the question, provide oracle answers in the training procedure and perform reasoning model on the sequence of sub-questions seperately. \nBut there are several dependancies (e.g. SQS, Oracle) that bound the performance of the proposed model. \nsummary_of_strengths\nThe author introduces an interactive mechanism into the VQA task. \nsummary_of_weaknesses\n- What is the percentage of complex questions in the VQA dataset ? If the proportion is very small, it is really hard for the proposed method to gain significant improvement.\n-There are several dependancies (e.g. SQS, Oracle) that bound the performance of the proposed model.\n-The improvement of Table 1 and 2 is not significant. If the author insists on the significancy of those performance, please provide corresponding statistical analysis, like p-value. \ncomments,_suggestions_and_typos\nN/A ", "label": [[640, 704, "Jus_neg_1"], [705, 812, "Eval_neg_1"], [814, 913, "Eval_neg_2"], [915, 967, "Eval_neg_3"], [968, 1097, "Jus_neg_3"]]}
{"id": 467, "review": "paper_summary\nThis paper investigates the ability of the raw sequence-to-sequence model in document-level translation. The perspective is interesting. It proposes a multi-resolutional data processing step which iteratively split document corpus into k (1, 2, 4, 8,...) parts and feeds them to the sentence-to-sentence translation model. They also provide a new document-level dataset focusing on 3 phenomena: tense consistency, conjunction presence, pronoun translation. Empirically, they show that the multi-resolutional data processing can effectively address the learning problem with long sentences, and obtain improvements over some baselines. \nHowever, there are some issues with the paper: 1) it uses different experiment settings (e.g., a 32k batch size, a beam size of 5) and does not mention some details (e.g., the number of training steps), these different settings may contribute the performance and the comparisons in Table 2 may not be sufficiently reliable. \n2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations. \n3) the multi-resolutional data processing approach may somehow increase the instance weight of the document-level data, and how this affects the performance is not studied. \nsummary_of_strengths\nThis paper investigates the ability of the raw sequence-to-sequence model in document-level translation. The perspective is interesting. It proposes a multi-resolutional data processing step which iteratively split document corpus into k (1, 2, 4, 8,...) parts and feeds them to the sentence-to-sentence translation model. They also provide a new document-level dataset focusing on 3 phenomena: tense consistency, conjunction presence, pronoun translation. Empirically, they show that the multi-resolutional data processing can effectively address the learning problem with long sentences, and obtain improvements over some baselines. \nsummary_of_weaknesses\n1) it uses different experiment settings (e.g., a 32k batch size, a beam size of 5) and does not mention some details (e.g., the number of training steps), these different settings may contribute the performance and the comparisons in Table 2 may not be sufficiently reliable. \n2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations. \n3) the multi-resolutional data processing approach may somehow increase the instance weight of the document-level data, and how this affects the performance is not studied. \ncomments,_suggestions_and_typos\n1) It's better to adopt experiment settings consistent with previous work. \n2) There is still a large performance gap compared to Voita et al. (2019) in linguistic evaluations, while BLEU may not be able to reflect these document-level phenomena and linguistic evaluations are important. The issues for the performance gap shall be investigated and solved. \n3) It's better to investigate whether the model really leverages document-level contexts correctly, probably refer to this paper: Do Context-Aware Translation Models Pay the Right Attention? In ACL 2021. ", "label": [[119, 150, "Eval_pos_1"], [151, 336, "Jus_pos_1"], [650, 696, "Major_claim"], [699, 737, "Eval_neg_1"], [739, 779, "Jus_neg_1"], [781, 814, "Eval_neg_1"], [816, 850, "Jus_neg_1"], [851, 974, "Eval_neg_1"], [977, 1041, "Eval_neg_2"], [1042, 1532, "Jus_neg_2"], [1536, 1706, "Eval_neg_3"], [1833, 1864, "Eval_pos_2"], [1865, 2051, "Jus_pos_2"], [2389, 2426, "Eval_neg_4"], [2428, 2468, "Jus_neg_4"], [2470, 2503, "Eval_neg_4"], [2505, 2539, "Jus_neg_4"], [2540, 2663, "Eval_neg_4"], [2667, 2730, "Eval_neg_5"], [2731, 3221, "Jus_neg_5"], [3225, 3395, "Eval_neg_6"]]}
{"id": 468, "review": "paper_summary\nThe paper presents an evaluation framework that probes state of the art video-text retrieval and multiple choice models (CLIP and Multi-modal Transformer based) on MSR-VTT and LSMDC. The authors build a pipeline that automatically creates contrast sets, meaning that video captions are manipulated to change their semantics while maintaining plausibility. The contrast sets are created by swiping the gender of a person or using pre-trained LMs to replace verb phrases with top K most probable phrases. They also use AMT to create human generated contrast sets and ask humans to evaluate if the automated contrast sets are valid. \nThe results show that the models performance drops significantly on contrast sets compared to original setting (the random negatives). They also show that model performance is strongly correlated with semantic proximity and stronger retrieval performance does not guarantee robustness to word-level manipulation. \nsummary_of_strengths\nThe process of creating the contrast sets is clear, easy to implement and well explained. \nThe paper evaluates strong state of the art models for  video-text retrieval and multiple choice models (CLIP and Multi-modal Transformer based) on specialized datasets (MSR-VTT and LSMDC). \nThe conclusions seem to be of interest for the community and can stimulate useful future work directions. \nsummary_of_weaknesses\nMissing explanations about why the models found the automately generated contrast sets more difficult than the human contrast sets. Humans still perform well on both cases, but more analysis on why the automatic sets are difficult would be interesting. \nOverall, the results require more analysis, but for a short paper it could be enough. \nMy opinion is that this short paper could be organized better in a long paper by including the information about annotations and contrast set construction from the appendix, together with more analysis and ablations. \ncomments,_suggestions_and_typos\nN/A ", "label": [[980, 1070, "Eval_pos_1"], [1071, 1174, "Eval_pos_2"], [1176, 1214, "Jus_pos_2"], [1216, 1239, "Eval_pos_2"], [1241, 1258, "Jus_pos_2"], [1262, 1368, "Eval_pos_3"], [1391, 1523, "Eval_neg_1"], [1523, 1644, "Jus_neg_1"], [1645, 1731, "Major_claim"]]}
{"id": 469, "review": "- Strengths: The authors address a very challenging, nuanced problem in political discourse reporting a relatively high degree of success.\nThe task of political framing detection may be of interest to the ACL community.\nThe paper is very well written.\n- Weaknesses: Quantitative results are given only for the author's PSL model and not compared against any traditional baseline classification algorithms, making it unclear to what degree their model is necessary. Poor comparison with alternative approaches makes it difficult to know what to take away from the paper.\nThe qualitative investigation is interesting, but the chosen visualizations are difficult to make sense of and add little to the discussion. Perhaps it would make sense to collapse across individual politicians to create a clearer visual.\n- General Discussion: The submission is well written and covers a topic which may be of interest to the ACL community. At the same time, it lacks proper quantitative baselines for comparison.  Minor comments: - line 82: A year should be provided for the Boydstun et al. citation - It\u2019s unclear to me why similar behavior (time of tweeting) should necessarily be indicative of similar framing and no citation was given to support this assumption in the model.\n- The related work goes over quite a number of areas, but glosses over the work most clearly related (e.g. PSL models and political discourse work) while spending too much time mentioning work that is only tangential (e.g. unsupervised models using Twitter data).\n- Section 4.2 it is unclear whether Word2Vec was trained on their dataset or if they used pre-trained embeddings.\n- The authors give no intuition behind why unigrams are used to predict frames, while bigrams/trigrams are used to predict party.\n- The authors note that temporal similarity worked best with one hour chunks, but make no mention of how important this assumption is to their results. If the authors are unable to provide full results for this work, it would still be worthwhile to give the reader a sense of what performance would look like if the time window were widened.\n- Table 4: Caption should make it clear these are F1 scores as well as clarifying how the F1 score is weighted (e.g. micro/macro). This should also be made clear in the \u201cevaluation metrics\u201d section on page 6. ", "label": [[13, 138, "Eval_pos_1"], [139, 219, "Eval_pos_2"], [220, 251, "Eval_pos_3"], [266, 464, "Eval_neg_1"], [465, 569, "Eval_neg_2"], [570, 711, "Eval_neg_3"], [711, 808, "Jus_neg_3"], [831, 1000, "Major_claim"], [1090, 1267, "Eval_neg_4"], [1546, 1645, "Eval_neg_5"], [1648, 1775, "Eval_neg_6"]]}
{"id": 471, "review": "The paper describes an attempt to quantitatively measure the amount of exaggeration that happens between the contents of a scientific article and its accompanying press release. The authors focus on the cases when a paper describes a correlation between factors, which is then related as causation in press release. This is solved by a collection of heuristics, but also classifiers based on data coming from manual annotation.  The weakest points of the paper I found are the following: -In the introduction, the authors state that they are 'comparing claims in press releases to the _corresponding_ claims in the original research papers'. In my view such a task would however be very hard and require a great deal of language understanding. The authors bypass it by heuristically choosing sentences that summarise the documents and assuming that if an correlational study is described by causal sentences, we have an example of exaggeration. But it's never verified whether the compared sentences indeed correspond to each other, i.e. talk about the same phenomena. This is an acceptable simplification that may work in most cases (i.e. for papers with only one claim), but it should be explicitly stated in the introduction and discussed as limitation further on. Especially since this may lead to errors, e.g. in situations when the correlational sentences from the paper and causal from the press release are all accurate, but simply refer to different parts of the study.\n-The paper mentions no code or data released with it. I'm afraid the preparation of the data and its analysis involves so many degrees of freedom it would be nearly impossible to replicate it simply by following the article. To give just one example: there is 60k+861k papers in PubMed with the labels needed for study type classification, but only 100k of them are chosen to build a classifier with no information on how this choice was made.\n-The paper is imbalanced in terms of the amount of background description and new contribution. The initial background description takes full three pages and is then followed by numerous fragments further on. The new findings, on the other hand, are limited to a handful of observations about the degree of exaggeration, which basically confirm previous findings. This is especially unfortunate as the exaggeration measurement method could be used to perform many more interesting experiments, e.g. comparing different fields of science, countries, types of studies etc.\nThe strongest points are the following: -The study is very robust technically and methodologically: statistical significance is tested when appropriate, the annotation is done with checking the agreement, the dataset sizes are substantial and the description of details is clear and abundant enough to inspire confidence in the results (though not enough to replicate; see above).\n-The authors establish clear links with the non-computational literature and show how it justifies their design decisions.\n-The obtained results have clear correspondence to the real world and are non-obvious, -The problem tackled here is very 'fuzzy' and hard to precisely define, which makes it commendable how robustly it was quantified here.\n-The language used is crystal clear.\nOverall, in my opinion the strengths of the study are more significant and I'd recommend accepting this paper at the conference. However, I can do that only with limited confidence, since while I found the technical part very clear and easy to follow, I'm not familiar with the application area here, which seems to contain quite a few previous articles judging by the provided bibliography. ", "label": [[1480, 1532, "Jus_neg_1"], [1533, 1703, "Eval_neg_1"], [1704, 1922, "Jus_neg_1"], [1924, 2018, "Eval_neg_2"], [2019, 2493, "Jus_neg_2"], [2535, 2592, "Eval_pos_1"], [2594, 2874, "Jus_pos_1"], [2876, 2997, "Eval_pos_2"], [2999, 3085, "Eval_pos_3"], [3222, 3257, "Eval_pos_4"], [3258, 3650, "Major_claim"]]}
{"id": 473, "review": "paper_summary\nThis paper is similar to \"Learning Music Helps You Read\", that is, it tests the transferability of structure from synthetic pretraining data. It uses similar synthetic data to this prior work, but adds a dependency dataset. This dependency dataset is similar to the parenthetical dataset that exists, but it uses different symbols for opening and closing, rather than the same symbol. Their results are mostly similar to the existing results, but they find that introducing different opening and closing symbols does make the nested structure important for transfer, which is different from the previous results.  They then go over results after pretraining on masked language model objectives and then fine tuning on dependency parsing. \nsummary_of_strengths\nI was very interested in the result that a simple and well-justified modification to the parenthetical language actually reveals nested structure to be valuable.\nThey run multiple seeds for the pretraining objective, allowing them to perform statistical tests over pretrained parameters. This kind of rigor is unfortunately often lacking in many papers on transfer learning, and I was pleased to see it here. \nsummary_of_weaknesses\nI'm not convinced that this should be an entire long paper, given that it is largely a replication of existing results.  There are a lot of stray line numbers all over the pages, which indicates to me that the authors may have modified the template in a way that perhaps could have resulted in a desk reject.\n\"We adopt sentence-level modeling [over between-sentences] because we would like to focus on the learning of sentence structures and also simplify the task setting.\" \nThis isn\u2019t entirely clear. Do they mean that the prior work was using multiple sentences as input at a time and they are using shorter inputs? And if so, does this actually generalize to a more typical setting, which uses fixed length multiple sentence inputs? If you are making a significant change, justifying it verbally is not as good as testing the effect of it.\nThe authors describe the Transformer performance as evidence that \"Transformer encoders exhibit surprisingly good transferability to other languages\". I disagree with this characterization. It looks from the plot as though the random weights have much better performance in the Transformer model, indicating that the Transformer is not necessarily transferring the data better, but is just better at learning the fine tuned task generally. Even the authors seem to disagree with their own characterization, in lines 494-496. \ncomments,_suggestions_and_typos\nA few citations on existing synthetic training: https://arxiv.org/pdf/2106.01044.pdf and https://cs.stanford.edu/~nfliu/papers/liu+levy+schwartz+tan+smith.repl4nlp2018.pdf both of which look at the inductive biases through synthetic training data.\nBe clear whether the language modeling task in section 4 is a causal model.  418: s/is/are/ There are a lot of stray line numbers all over the pages, which indicates to me that the authors may have modified the template in a way that perhaps could have resulted in a desk reject.\nFigure 2a \"unifrom\" To what degree does TILT actually differ from model stitching https://arxiv.org/abs/2106.07682 procedures? I guess it doesn\u2019t have the stitching layer?\nWhere are the results training on the parenthetical data in section 5? Only the dependency data is there. ", "label": [[774, 935, "Eval_pos_1"], [936, 1061, "Jus_pos_2"], [1062, 1183, "Eval_pos_2"], [1206, 1264, "Eval_neg_1"], [1266, 1324, "Jus_neg_1"], [1682, 1708, "Eval_neg_2"], [1709, 2049, "Jus_neg_2"], [2200, 2239, "Eval_neg_3"], [2240, 2575, "Jus_neg_3"]]}
{"id": 474, "review": "The authors propose a Sentimental Words Aware Fusion Network (SWAFN) to solve the multimodal sentiment analysis problem by using an auxiliary task of determining whether each word is sentimental word, in additional to the coattention-based fusion. The coattention and multitask mechanisms both show improvements over the start-of-the-art models. The paper is well-written with sufficient experiment results to support the proposed method. ", "label": [[346, 439, "Eval_pos_1"]]}
{"id": 475, "review": "This paper delves into the mathematical properties of the skip-gram model, explaining the reason for its success on the analogy task and for the general superiority of additive composition models. It also establishes a link between skip-gram and Sufficient Dimensionality Reduction.\nI liked the focus of this paper on explaining the properties of skip-gram, and generally found it inspiring to read. I very much appreciate the effort to understand the assumptions of the model, and the way it affects (or is affected by) the composition operations that it is used to perform. In that respect, I think it is a very worthwhile read for the community.\nMy main criticism is however that the paper is linguistically rather naive. The authors' use of 'compositionality' (as an operation that takes a set of words and returns another with the same meaning) is extremely strange. Two words can of course be composed and produce a vector that is a) far away from both; b) does not correspond to any other concept in the space; c) still has meaning (productivity wouldn't exist otherwise!) Compositionality in linguistic terms simply refers to the process of combining linguistic constituents to produce higher-level constructs. It does not assume any further constraint, apart from some vague (and debatable) notion of semantic transparency. The paper's implication (l254) that composition takes place over sets is also wrong: ordering matters hugely (e.g. 'sugar cane' is not 'cane sugar'). This is a well-known shortcoming of additive composition.  Another important aspect is that there are pragmatic factors that make humans prefer certain phrases to single words in particular contexts (and the opposite), naturally changing the underlying distribution of words in a large corpus. For instance, talking of a 'male royalty' rather than a 'king' or 'prince' usually has implications with regard to the intent of the speaker (here, perhaps highlighting a gender difference). This means that the equation in l258 (or for that matter the KL-divergence modification) does not hold, not because of noise in the data, but because of fundamental linguistic processes. \nThis point may be addressed by the section on SDR, but I am not completely sure (see my comments below).\nIn a nutshell, I think the way that the authors present composition is flawed, but the paper convinces me that this is indeed what happens in skip-gram, and I think this is an interesting contribution.  The part about Sufficient Dimensionality Reduction seems a little disconnected from the previous argument as it stands. I'm afraid I wasn't able to fully follow the argument, and I would be grateful for some clarification in the authors' response. If I understand it well, the argument is that skip-gram produces a model where a word's neighbours follow some exponential parametrisation of a categorical distribution, but it is unclear whether this actually reflects the distribution of the corpus (as opposed to what happens in, say, a pure count-based model). The fact that skip-gram performs well despite not reflecting the data is that it implements some form of SDR, which does not need to make any assumption about the underlying form of the data. But then, is it fair to say that the resulting representations are optimised for tasks where geometrical regularities are important, regardless of the actual pattern of the data? I.e. there some kind of denoising going on?\nMinor comments: - The abstract is unusually long and could, I think, be shortened.\n- para starting l71: I think it would be misconstrued to see circularity here. \nFirth observed that co-occurrence effects were correlated with similarity judgements, but those judgements are the very cognitive processes that we are trying to model with statistical methods. Co-occurrence effects and vector space word representations are in some sense 'the same thing', modelling an underlying linguistic process we do not have direct observations for. So pair-wise similarity is not there to break any circularity, it is there because it better models the kind of judgements humans known to make.\n- l296: I think 'paraphrase' would be a better word than 'synonym' here, given that we are comparing a set of words with a unique lexical item.\n- para starting l322: this is interesting, and actually, a lot of the zipfian distribution (the long tail) is fairly uniform.\n- l336: it is probably worth pointing out that the analogy relation does not hold so well in practice and requires to 'ignore' the first returned neighbour of the analogy computation (which is usually one of the observed terms).\n- para starting l343: I don't find it so intuitive to say that 'man' would be a synonym/paraphrase of anything involving 'woman'. The subtraction involved in the analogy computation is precisely not a straightforward composition operation, as it involves an implicit negation.  - A last, tiny general comment. It is usual to write p(w|c) to mean the probability of a word given a context, but in the paper 'w' is actually the context and 'c' the target word. It makes reading a little bit harder... Perhaps change the notation?\nLiterature: The claim that Arora (2016) is the only work to try and understand vector composition is a bit strong. For instance, see the work by Paperno & Baroni on explaining the success of addition as a composition method over PMI-weighted vectors: D. Paperno and M. Baroni. 2016. When the whole is less than the sum of its parts: How composition affects PMI values in distributional semantic vectors. \nComputational Linguistics 42(2): 345-350.\n*** I thank the authors for their response and hope to see this paper accepted. ", "label": [[283, 399, "Eval_pos_1"], [400, 575, "Eval_pos_2"], [576, 648, "Eval_pos_3"], [649, 724, "Major_claim"], [725, 871, "Eval_neg_1"], [872, 1540, "Jus_neg_1"], [2261, 2463, "Eval_pos_4"], [2464, 2583, "Eval_neg_2"], [2584, 3440, "Jus_neg_2"], [5599, 5676, "Major_claim"]]}
{"id": 476, "review": "This paper proposes a method for discovering correspondences between languages based on MDL. The author model correspondences between words sharing the same meaning in a number of Slavic languages. They develop codes for rules that match substrings in two or more languages and formulate an MDL objective that balances the description of the model and the data given the model. \nThe model is trained with EM and tested on a set of 13 Slavic languages. The results are shown by several distance measures, a phylogenetic tree, and example of found correspondences.  The motivation and formulation of the approach makes sense. MDL seems like a reasonable tool to attack the problem and the motivation for employing EM is presented nicely. I must admit, though, that some of the derivations were not entirely clear to me. \nThe authors point out the resemblance of the MDL objective to Bayesian inference, and one thinks of the application of Bayesian inference in (biological) phylogenetic inference, e.g. using the MrBayes tool. An empirical comparison here could be insightful.   Related work:  -Lacking comparison to methods for borrowing and cognate detection or other computational methods for historical linguistics. For example, the studies by Alexandre Bouchard-Cote, Tandy Warnow, Luay Nakhleh and Andrew Kitchen. Some may not have available tools to apply in the given dataset, but one can mention List and Moran (2013). There are also relevant tools for biological phylogeny inference that can be applied (paup, MrBayes, etc.).  Approach and methodology -Alignment procedure: the memory/runtime bottleneck appears to be a major drawback, allowing the comparison of only 5 languages at most. As long as multiple languages are involved, and phylogenetic trees, it would be interesting to see more languages. I'm curious what ideas the authors have for dealing with this issue.  -Phylogenetic tree: using neighbor joining for creating phylogenetic trees is known to have disadvantages (like having to specify the root manually). How about more sophisticated methods?   -Do you run EM until convergence or have some other stopping criterion?  Data -Two datasets are mixed, one of cognates and one not necessarily (the Swadesh lists). Have you considered how this might impact the results?  -The data is in orthographic form, which might hide many correspondences. This is especially apparent in languages with different scripts. Therefore the learned rules might indicate change of script more than real linguistic correspondences. This seems like a shortcoming that could be avoided by working on the level of phonetic transcriptions.\nUnclear points -What is the \"optimal unigram for symbol usages in all rules\"? ( line 286) -The merging done in the maximization step was not entirely clear to me.  Minor issue -\"focus in on\" -> \"focus on\" (line 440) Refs Johann-Mattis List, Steven Moran. 2013. An Open Source Toolkit for Quantitative Historical Linguistics. Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 13\u00e2\u0080\u009318, Sofia, Bulgaria. Association for Computational Linguistics. \nhttp://www.aclweb.org/anthology/P13-4003.   Andrew Kitchen, Christopher Ehret, Shiferaw Assefa and Connie J. Mulligan. \n2009. Bayesian phylogenetic analysis of Semitic languages identifies an Early Bronze Age origin of Semitic in the Near East ", "label": [[564, 623, "Eval_pos_1"], [1094, 1219, "Eval_neg_1"], [1219, 1534, "Jus_neg_1"], [1583, 1643, "Eval_neg_2"], [1645, 1883, "Jus_neg_2"], [2294, 2534, "Jus_neg_3"], [2535, 2638, "Eval_neg_3"]]}
{"id": 477, "review": "This paper proposes the new (to my knowledge) step of proposing to treat a number of sentence pair scoring tasks (e.g. Answer Set Scoring, RTE, Paraphrasing, among others) as instances of a more general task of understanding semantic relations between two sentences. Furthermore, they investigate the potential of learning generally- applicable neural network models for the family of tasks. I find this to be an exciting proposal that's worthy of both presentation at CoNLL and further discussion and investigation.\nThe main problem I have with the paper is that it in fact feels unfinished. It should be accepted for publication only with the proviso that a number of updates will be made for the final version: 1 - the first results table needs to be completed 2 - given the large number of individual results, the written discussion of results is terribly short. Much more interpretation and discussion of the results is sorely needed. \n3 - the abstract promises presentation of a new, more challenging dataset which the paper does not seem to deliver. This incongruity needs to be resolved. \n4 - the results vary quite a bit across different tasks - could some investigation be made into how and why the models fail for some of the tasks, and how and why they succeed for others? \nEven if no solid answer is found, it would be interesting to hear the authors' position regarding whether this is a question of modeling or rather dissimilarity between the tasks. Does it really work to group them into a unified whole? \n5 - please include example instances of the various datasets used, including both prototypical sentence pairs and pairs which pose problems for classification 6 - the Ubu. RNN transfer learning model is recommended for new tasks, but is this because of the nature of the data (is it a more general task) or rather the size of the dataset? How can we determine an answer to that question?\nDespite the unpolished nature of the paper, though, it's an exciting approach that could generate much interesting discussion, and I'd be happy to see it published IN A MORE FINISHED FORM. \nI do recognize that this view may not be shared by other reviewers!\nSome minor points about language: -\"weigh\" and \"weighed\" are consistently used in contexts that rather require \"weight\" and \"weighted\" -there are several misspellings of \"sentence\" (as \"sentene\") -what is \"interpunction\"?\n-one instance of \"world overlap\" instead of \"word overlap\" ", "label": [[392, 516, "Major_claim"], [517, 592, "Eval_neg_1"], [593, 940, "Jus_neg_1"], [945, 1096, "Eval_neg_2"], [1100, 1152, "Eval_neg_3"], [1155, 1522, "Jus_neg_3"]]}
{"id": 479, "review": "paper_summary\nThis paper used chi-squared measures, t-statistics, and raw frequency to build a token merging pre-processing step, in order to improve the results of LDA in languages without marked word boundaries. It is valuable to leverage collocations for languages without marked word boundaries (e.g., Chinese and Thai), and the experimental results indicated positive contributions of token emerging. However, this paper only compared the proposed approaches with LDA, without employing the existing similar approaches, e.g., the methods proposed in (Lau et al., 2013), as baselines. Besides, the related work section is quite sketchy. \nsummary_of_strengths\n1. The experiments were conducted on 7 languages, which was valuable to test whether representing bigrams collocations in the input could improve topic coherence or not in general. \n2. The experimental results indicated that the t-statistic and raw frequency approaches for token merging could improve the topic modeling results across 7 languages. \nsummary_of_weaknesses\n1. The descriptions of several important sentences are unclear, e.g., in \u201cFor all languages, we use the reduced version of Wikipedia database\u201d, it is suggested to provide the links to the reduced version of Wikipedia database for all languages (except for English). To evaluate the influence of these large collocation training corpora, it is also suggested to compute the collocation measures for all bigrams on each document collection (i.e., without any external corpora). \n2. Figure 1 should be illustrated in more details. Why only show the results of three languages in Figure 1? Are the results of other languages consistent with these three languages? \n3. The related work section is quite sketchy. There are limited reviews on collocations and topic models in recent years. One of the relevant early studies was cited, i.e., (Lau et al., 2013), in which, Lau et al. compared topic models learned from unigram bag-of-words data, with topic models learned from bag-of-words data that includes preextracted bigram collocations. As shown in (Lau et al., 2013), they considered four different bigram replacement methods. Particularly, they first extracted bigrams for each document collection using the N-gram Statistics Package, identifying the top bigrams based on the Student\u2019s t-test. Then, they used the top 1k, 10k, and 100k as the three different bigram replacement methods. Unfortunately, the above method was not employed as the baseline in this study, in order to validate the effectiveness of using large collocation training corpora to compute the collocation measures for all bigrams. \ncomments,_suggestions_and_typos\n1. The presentation can be improved, e.g., \u201cTo train word embeddings\u201d (line 298) and \u201cto obtain word embeddings\u201d (line 301) are repetitive. \n2. The reference (El-Kishky et al., 2014) had been published at Proc. VLDB Endow. 8(3): 305-316 (2014). Besides, the information of reference (Merity et al., 2016) are incomplete. ", "label": [[588, 639, "Eval_neg_1"], [1038, 1097, "Eval_neg_2"], [1099, 1511, "Jus_neg_2"], [1515, 1561, "Eval_neg_3"], [1563, 1695, "Jus_neg_3"], [1699, 1741, "Eval_neg_4"], [1742, 2637, "Jus_neg_4"]]}
{"id": 480, "review": "The authors present a new formula for assessing readability of Vietnamese texts. The formula is developed based on a multiple regression analysis with three features. Furthermore, the authors have developed and annotated a new text corpus with three readability classes (easy, middle, hard).\nResearch on languages other than English is interesting and important, especially when it comes to low-resource languages. Therefore, the corpus might be a nice additional resource for research (but it seems that the authors will not publish it - is that right?). However, I don't think the paper is convincing in its current shape or will influence future research. Here are my reasons: - The authors provide no reasons why there is a need for delevoping a new formula for readability assessments, given that there already exist two formulas for Vietnamese with almost the same features. What are the disadvantages of those formulas and why is the new formula presented in this paper better?\n- In general, the experimental section lacks comparisons with previous work and analysis of results. The authors claim that the accuracy of their formula (81% on their corpus) is \"good and can be applied in practice\". What would be the accuracy of other formulas that already exist and what are the pros and cons of those existing formulas compared to the new one?\n- As mentioned before, an analysis of results is missing, e.g. which word / sentence lengths / number of difficult words are considered as easy/middle/hard by their model?\n- A few examples how their formula could be applied in a practical application would be nice as well.\n- The related work section is rather a \"background\" section since it only presents previously published formulas. What I'm missing is a more general discussion of related work. There are some papers that might be interesting for that, e.g., DuBay 2004: \"The principles of readability\", or Rabin 1988: \"Determining difficulty levels of text written in languages other than English\" - Since Vietnamese is syllable-based and not word-based, I'm wondering how the authors get \"words\" in their study. Do they use a particular approach for merging syllables? And if yes, which approach do they use and what's the accuracy of the approach?\n- All in all, the content of the paper (experiments, comparisons, analysis, discussion, related work) is not enough for a long paper.\nAdditional remarks: - The language needs improvements - Equations: The usage of parentheses and multiplying operators is inconsistent - Related works section: The usage of capitalized first letters is inconsistent ", "label": [[556, 658, "Major_claim"], [682, 789, "Eval_neg_1"], [791, 984, "Jus_neg_1"], [987, 1085, "Eval_neg_2"], [1086, 1349, "Jus_neg_2"], [1352, 1407, "Eval_neg_3"], [1408, 1521, "Jus_neg_3"], [1626, 1800, "Eval_neg_4"], [1801, 2004, "Jus_neg_4"], [2259, 2390, "Major_claim"]]}
{"id": 482, "review": "This paper explores the use of graph neural networks to identify missing elements in an event chain in a multiple-choice scenario. Overall, it is an interesting approach to the task, which surpasses the performance of previous systems. Furthermore, the paper presents an interesting discussion of the results, including ablation studies that reveal the usefulness of every system component. However, the paper also has some issues, which I discuss below: First of all, the task is defined as a multiple-choice task in which, for each missing event, there is a golden choice and four wrong choices. However, there are no examples of the wrong choices nor information regarding how they vary in relation to the correct one. This makes it hard to assess the actual difficulty of the task.\nFurthermore, although the evaluation is performed on the MCNC dataset, the model is trained on a portion of the Gigaword corpus. In this case, the event chains are obtained by mapping the coreference chains identified by the Stanford CoreNLP library. This means that the system is probably also learning to identify those coreference chains. Thus, it would be interesting to compare the performance of the system with a simple selection based on the coreference chains identified using Stanford CoreNLP.\nThe system relies on BERT to generate the embedding representation of the text. Furthermore, the pre-trained model is fine-tuned on the used corpus. However, this tuning is still based on masked language modeling task. It would be more interesting to fine-tune the model to a task closer to event chain prediction, such as next sentence prediction, which was also used to pre-train the original model.  Finally, regarding typos/grammar/style, when references are used as part of the sentences, the format should be different (e.g. \"(Lv et al. 2019) found\" -> Lv et al. (2019) found). Furthermore, the placing of references should be consistent across the paper. For instance, in the second paragraph of the introduction, the placement of the references varies between before and after \"based\", which makes it confusing. More importantly, the paper has several issues regarding number and verb tense mismatches or inconsistencies, as well as \"a\" that should be \"an\". Thus, it requires a thorough proof-reading and revision. ", "label": [[131, 235, "Eval_pos_1"], [236, 309, "Eval_pos_2"], [310, 390, "Jus_pos_2"], [391, 454, "Major_claim"], [455, 721, "Jus_neg_1"], [722, 785, "Eval_neg_1"], [1874, 1951, "Eval_neg_2"], [1952, 2109, "Jus_neg_2"], [2110, 2255, "Jus_neg_3"], [2256, 2312, "Eval_neg_3"]]}
{"id": 483, "review": "paper_summary\nMotivated by the assumption that all the components in the encoder block, in general, are meaningful for token attribution analysis, this work proposes a GlobEnc, which incorporates all the components in the encoder block in the model. \nsummary_of_strengths\n- The proposed method, GlobEnc, expands the scope of analysis from attention block in Transformers to the whole encoder.\n-This paper shares intriguing insights from various experiments.\n-Writing quality is high and validates the proposed method on many different criteria to investigate the role of each component in the encoder. \nsummary_of_weaknesses\n- In token attribution analysis task, most researchers will intuitively agree that self-attention plays a primary role in Transformer architecture. However, why is the sub-network on top of the self-attention block important, and what role the authors expect from them are not well-motivated. A qualitative experiment comparing the token attribution analysis with and without subsequent sub-network (i.e., FFN, residual connection #2, and output LN) would be helpful to elucidate the role of remaining components other than self-attention block.\n- The work appears to be incremental: As mentioned in the paper, Kobayashi et al. (2020, 2021) proposed the analysis method leveraging attention block. While the proposed method extends the existing approach to the entire encoder block, GlobEnc seems to reuse the tactics used in the previous method without any distinctive improvement. \ncomments,_suggestions_and_typos\n- In Equation 6, it would be better to note what \u03b3 and \u2299 (element-wise product) represent.\n-In Figure 1, it would be better to compare the attribution map of GlobEnc with the attribution map of other comparable methods (e.g., N_{res}). ", "label": [[394, 457, "Eval_pos_1"], [459, 602, "Eval_pos_2"], [773, 917, "Eval_neg_1"], [918, 1170, "Jus_neg_2"], [1172, 1208, "Eval_neg_2"], [1209, 1508, "Jus_neg_2"]]}
{"id": 485, "review": "This paper proposes an approach for classifying literal and metaphoric adjective-noun pairs. The authors create a word-context matrix for adjectives and nouns where each element of the matrix is the PMI score. They then use different methods for selecting dimensions of this matrix to represent each noun/adjective as a vector. The geometric properties of average, nouns, and adjective vectors and their normalized versions are used as features in training a regression model for classifying the pairs to literal or metaphor expressions. Their approach performs similarly to previous work that learns a vector representation for each adjective.\nSupervision and zero-shot learning. The authors argue that their approach requires less supervision (compared to previous work)  and can do zero-shot learning. I don\u2019t think this is quite right and given that it seems to be one of the main points of the paper, I think it is worth clarifying. The approach proposed in the paper is a supervised classification task: The authors form vector representations from co-occurrence statistics, and then use the properties of these representations and the gold-standard labels of each pair to train a classifier. The model (similarly to any other supervised classifier) can be tested on words that did not occur in the training data; but, the model does not learn from such examples. Moreover, those words are not really \u201cunseen\u201d because the model needs to have a vector representation of those words.\nInterpretation of the results. The authors provide a good overview of the previous related work on metaphors. However, I am not sure what the intuition about their approach is (that is, using the geometric properties such as vector length in identifying metaphors). For example, why are the normalized vectors considered? It seems that they don\u2019t contribute to a better performance. \nMoreover, the most predictive feature is the noun vector; the authors explain that this is a side effect of the data which is collected such that each adjective occurs in both metaphoric and literal expressions. ( As a result, the adjective vector is less predictive.) It seems that the proposed approach might be only suitable for the given data. This shortcoming is two-fold: (a) From the theoretical perspective (and especially since the paper is submitted to the cognitive track), it is not clear what we learn about theories of metaphor processing. ( b) From the NLP applications standpoint, I am not sure how generalizable this approach is compared to the compositional models.\nNovelty. The proposed approach for representing noun/adjective vectors is very similar to that of Agres et al. It seems that the main contribution of the paper is that they use the geometric properties to classify the vectors. ", "label": [[681, 937, "Eval_neg_1"], [938, 1487, "Jus_neg_1"], [1598, 1753, "Eval_neg_2"], [1754, 1871, "Jus_neg_2"], [1872, 2219, "Eval_neg_3"], [2220, 2555, "Jus_neg_3"], [2564, 2666, "Eval_neg_4"], [2667, 2783, "Jus_neg_4"]]}
{"id": 486, "review": "paper_summary\nThe paper explored the approaches to use external data to improve the spoken NER performance of both pipeline and E2E-based systems. These approaches include using unlabeled and text data to perform self-training, knowledge distillation and transfer learning. The experimental results showed that all these approaches lead to better performance in both pipeline and E2E-based spoken NER systems. \nsummary_of_strengths\nThis work explores many possible approaches in exploring how to use external data for spoken NER task. It covers both the speech and text from data aspects. In terms of the training approaches, the authors tried the self-training, transfer learning and knowledge distillation. This work is meaningful to the community. \nsummary_of_weaknesses\nIt is a bit confusing about the word error rates in Fig. 3. The description in the paper use word error rate (WER) all the time. But in Fig. 3, the authors used (100 - WER) as the vertical axis. It is not clear about the motivation of doing this. Note: the WER can exceed 100% in some cases. \ncomments,_suggestions_and_typos\nSome grammar issues need to be reviewed. For example, at line 394 in section 4.2:           Like the baseline models Shon et al. (2021), we to train on the finer label set (18 entity tags) and evaluate on the com396 bined version (7 entity tags) -> \"we to ....\" ", "label": [[432, 534, "Eval_pos_1"], [536, 707, "Jus_pos_1"], [709, 751, "Major_claim"], [774, 833, "Eval_neg_1"], [834, 968, "Jus_neg_1"], [969, 1020, "Eval_neg_1"], [1021, 1066, "Jus_neg_1"], [1099, 1139, "Eval_neg_2"], [1140, 1360, "Jus_neg_1"]]}
{"id": 487, "review": "- Summary:  The paper introduces a new dataset for a sarcasm interpretation task and a system (called Sarcasm SIGN) based on machine translation framework Moses. The new dataset was collected from 3000 sarcastic tweets (with hashtag `#sarcasm) and 5 interpretations for each from humans. The Sarcasm SIGN is built based on Moses by replacing sentimental words by their corresponding clusters on the source side (sarcasm) and then de-cluster their translations on the target side (non-sarcasm). Sarcasm SIGN performs on par with Moses on the MT evaluation metrics, but outperforms Moses in terms of fluency and adequacy.  - Strengths: the paper is well written the dataset is collected in a proper manner the experiments are carefully done and the analysis is sound.\n- Weaknesses: lack statistics of the datsets (e.g. average length, vocabulary size) the baseline (Moses) is not proper because of the small size of the dataset the assumption \"sarcastic tweets often differ from their non sarcastic interpretations in as little as one sentiment word\" is not supported by the data.  - General Discussion: This discussion gives more details about the weaknesses of the paper.  Half of the paper is about the new dataset for sarcasm interpretation. \nHowever, the paper doesn't show important information about the dataset such as average length, vocabulary size. More importantly, the paper doesn't show any statistical evidence to support their method of focusing on sentimental words.  Because the dataset is small (only 3000 tweets), I guess that many words are rare. Therefore, Moses alone is not a proper baseline. A proper baseline should be a MT system that can handle rare words very well. In fact, using clustering and declustering (as in Sarcasm SIGN) is a way to handle rare words.\nSarcasm SIGN is built based on the assumption that \"sarcastic tweets often differ from their non sarcastic interpretations in as little as one sentiment word\". Table 1 however strongly disagrees with this assumption: the human interpretations are often different from the tweets at not only sentimental words. I thus strongly suggest the authors to give statistical evidence from the dataset that supports their assumption. Otherwise, the whole idea of Sarcasm SIGN is just a hack.\n-------------------------------------------------------------- I have read the authors' response. I don't change my decision because of the following reasons:  - the authors wrote that \"the Fiverr workers might not take this strategy\": to me it is not the spirit of corpus-based NLP. A model must be built to fit given data, not that the data must follow some assumption that the model is built on.\n- the authors wrote that \"the BLEU scores of Moses and SIGN are above 60, which is generally considered decent in the MT literature\": to me the number 60 doesn't  show anything at all because the sentences in the dataset are very short. And that, if we look at table 6, %changed of Moses is only 42%, meaning that even more than half of the time translation is simply copying, the BLUE score is more than 60.\n- \"While higher scores might be achieved with MT systems that explicitly address rare words, these systems don't focus on sentiment words\": it's true, but I was wondering whether sentiment words are rare in the corpus. If they are, those MT systems should obviously handle them (in addition to other rare words). ", "label": [[634, 659, "Eval_pos_1"], [660, 703, "Eval_pos_2"], [704, 738, "Eval_pos_3"], [743, 764, "Eval_pos_4"], [780, 810, "Eval_neg_1"], [812, 848, "Jus_neg_1"], [850, 884, "Eval_neg_2"], [885, 925, "Jus_neg_2"], [926, 1077, "Eval_neg_3"], [1788, 2211, "Jus_neg_4"], [2212, 2269, "Eval_neg_4"]]}
{"id": 488, "review": "- Strengths: 1) The paper is trying to bridge the gap between Stochastic Gradient MCMC and Stochastic Optimization in deep learning context. Given dropout/dropConnect and variational inference are commonly used to reduce the overfit, the more systematic way to introduce/analyse such bayesian learning based algorithms would benefit deep learning community. \n2) For language modeling tasks, the proposed SG-MCMC optimizer + dropout outperforms RMSProp + dropout, which clearly shows that uncertainty modeling would help reducing the over-fitting, hence improving accuracy. \n3) The paper has provided the details about the model/experiment setups so the results should be easily reproduced.\n- Weaknesses: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm. \n2) The paper only shows the comparison between SG-MCMC vs RMSProp and did not conduct other comparison. It should explain more about the relation between pSGLD vs RMSProp other than just mentioning they are conterparts in two families. \n2) The paper does not talk about the training speed impact with more details.\n- General Discussion: ", "label": [[577, 645, "Jus_pos_1"], [646, 689, "Eval_pos_1"], [706, 814, "Eval_neg_1"], [819, 919, "Eval_neg_2"], [920, 1052, "Jus_neg_2"], [1056, 1130, "Eval_neg_3"]]}
{"id": 489, "review": "paper_summary\nThe paper deals with misinformation detection in Twitter, using multimodal data (images and text). \nAuthors focus on three topics: COVID, climate change, and military vehicles. \nsummary_of_strengths\nAuthors perform experiements on multiple settings, using several quality metrics. \nThe results show a superiority of the proposed method. \nPartial ablation study is also reported. \nsummary_of_weaknesses\nThe paper is not very clearly writtem. Too much information is moved to Appendix. \nSome important details of data collection and data generation are omitted. \nThe main issue is a lack of novelty - authors use exisiting tools and approaches for tweets classification. \ncomments,_suggestions_and_typos\nI did not notice many typos. However, the paper organization can be improved. \nFor example, data generation and data collection can be described in the same section. \nAlso, motivation behind some experiments is not very clear. Interpretation of the results are offen missing. \nBelow are some detailed comments/questions:  1. What \"in-the-wild\" term means? \n2. Evaluation metrics can be explained in a few words (or a relevant reference can be provided). \n3. I did not understand why Multiply option was selected for the experiments, when Concat+Dot clearly outperforms it in 4 cases, as shown in Table 3. \n4. Conclusions from Table 4 are missing. \n5. The motivation behind experiment with clustering and interpretation of its results are missing/unclear. ", "label": [[213, 295, "Eval_pos_1"], [416, 454, "Eval_neg_1"], [455, 498, "Eval_neg_2"], [499, 574, "Eval_neg_3"], [575, 610, "Eval_neg_4"], [716, 794, "Eval_neg_5"], [795, 882, "Jus_neg_5"], [883, 942, "Eval_neg_6"], [943, 992, "Eval_neg_7"]]}
{"id": 490, "review": "paper_summary\nThis paper introduced LimitedInk, a self-explaining model that outperforms the existing baseline on both end-task predictions and human-annotated rationale agreement. More importantly, the paper shows that unlike the finding from the previous studies, the shortest rationales can not present the best explanation for human understanding which call for more careful design on evaluating human rationales. \nsummary_of_strengths\nThe paper conducts a human study on rationales. The finding contradicts the typical intuition about rationales that the shortest rationales are the best to explain human understanding. This would encourage a fundamental rethinking and revolution of rationale research. \nsummary_of_weaknesses\nI guess that is due to content limited. It would be better to see some qualitative analysis on why 10-30% rationales can already give decent performance whereas human needs at least 40% rationales. It seems to indicates that the downstream model is making predictions in a way that is very different from how human make predictions. \ncomments,_suggestions_and_typos\nI would suggest adding some qualitative analysis to explain the title of the paper in later version. ", "label": [[440, 624, "Jus_pos_1"], [625, 709, "Eval_pos_1"]]}
{"id": 491, "review": "paper_summary\nThis paper proposes a new pre-trained variational encoder-decoder dialog model which has continuous latent variables to deal with the one-to-many mapping problem in dialogue response generation. \nThis paper conducts empirical experiments on 3 datasets to show their proposed model has better performances both on relevance and diversity than previous state-of-the-art dialog systems. \nThey also conducted additional analysis to show the impact of latent variable sizes, different decoding strategies and position embeddings for their proposed model. \nHowever, in the model evaluation part, the paper does not show how their model can generate diverse responses given the same context. \nsummary_of_strengths\n1. This paper proposes a new variational encoder-decoder dialog model for open-domain dialogue generation tasks, which shows better performance in terms of relevancy and diversity than previous state-of-the-art models. The new model leverages several techniques to better alleviate the one-to-many mapping problem in dialogue response generation, including conditional variational autoencoder, n-stream self-attention, memory scheme, masked language modeling, free bits and bag-of-words loss for reducing KL-vanishing, and position embeddings for Transformers. \n2. This paper conducts empirical experiments on three benchmark datasets to validate the effectiveness of the proposed model. The proposed model achieves better performances than previous baselines (e.g. PLATO) in both automatic metrics (e.g. BLEU-1/2, Distinct-1/2) and human evaluation (fluency, coherence, informativeness, overall). \n3. This paper analyzes the effect of latent variable sizes, sizes of K in top-k sampling and different combinations of position embeddings, which are helpful empirical observations for training Transformer-based variational encoder-decoders. \nsummary_of_weaknesses\n1. This paper lacks a discussion on its novelty, e.g., why continuous latent variables are better than discrete latent variables for solving the one-to-many mapping problem. Though empirically the proposed DialogVED performs better than PLATO, DialogVED incorporates more other techniques (e.g. n-stream self-attention, memory scheme, masked language modeling, free bits) than PLATO. It is not very clear to me why we should choose those techniques to train a continuous variational encoder-decoder. Maybe some ablation studies on the training objective and model architecture can help explain it. \n2. The model evaluation does not show the model's superiority in solving the one-to-many mapping problem in open-domain dialogue response generation. Distinct-1/2 are corpus-level metrics and are not showing the diversity of generated responses given the same context. To verify their claim, the authors may consider using self-bleu and the ratio of unique generated sentences to better evaluate the diversity. \ncomments,_suggestions_and_typos\n1. It is unclear to me what's the input to the prior network. Fig 1 suggests the [CLS] token is at the end of the context, but line234 says the [CLS] token is at the beginning of the context. ", "label": [[1888, 1932, "Eval_neg_1"], [1933, 2058, "Jus_neg_1"], [2059, 2268, "Jus_neg_2"], [2269, 2384, "Eval_neg_2"], [2385, 2483, "Jus_neg_2"], [2487, 2633, "Eval_neg_3"], [2634, 2895, "Jus_neg_3"], [2931, 2989, "Jus_neg_4"], [2990, 3120, "Eval_neg_4"]]}
{"id": 492, "review": "This paper is a resource paper. The resource it introduces is a corpus of Russian texts, annotated with semantic change judgements, following the DURel framework. The authors describe the annotation procedure and the corpus, and also provide some baseline experiments on the automatic detection of semantic change. The corpus is promised to be published upon acceptance.\nThis is a very good paper and should definitely be accepted. Since it follows an already established annotation framework, it is not innovative in that sense, but having corpora in different languages annotated according to the same framework is a big win for the community.  The only real issue with the paper that I see is that the pre-Sowiet era is much longer than the other two. It is conceivable that this early data set is inconsistent in itself, and it would be good if the authors comment on this.\nMinor things: -There are some missing articles in the text (4.1: \u201eloses old sense\u201c \u2192 \u201eloses an old sense\u201c; \u201enaturally captures the type of meaning change\u201c \u2192 \u201enaturally captures the following types of meaning changes\u201c, ...). Critical proofreading would be good for the final version.\n-The authors state that the corpus will be published unter a \u201epermissive\u201c license, which is surprisingly vague. I\u2018d assume that the authors know which license can be used by now. The wish of course would be to use a creative commons license. ", "label": [[371, 431, "Major_claim"], [432, 645, "Eval_pos_1"], [647, 754, "Eval_neg_1"], [755, 877, "Jus_neg_1"], [893, 936, "Eval_neg_2"], [937, 1160, "Jus_neg_2"]]}
{"id": 493, "review": "paper_summary\nThe paper proposed a method for referring image segmentation. The basic architecture is based on the idea of cross-modal transformer. It processed linguistic feature and multi-layers of visual embeddings through the cross-modal transformer to obtain the multi-layer visual-linguistic embeddding. The multi-layer visual-linguistic embedding is further aggregated through the proposed hierarchical cross-modal aggregation. The proposed model achieves better performance than previous baselines. \nsummary_of_strengths\nThe task of referring image segmentation is very interesting. I still wonder whether it is necessary to conduct visual grounding on pixel level. Even though, the task is very challenging and interesting. \nThe motivation example in Fig. 1 is pretty interesting. It shows the advantage of the proposed approach. \nsummary_of_weaknesses\nThe proposed architecture seems like a cross-modal transformer (3.2 synchronous multi-modal fusion) and a multi-layer feature aggregation (3.3 hierarchical cross-modal aggregation). \nWith this standard cross-modal architecture, it is unclear to me why the proposed approach could resolve the example in Fig. 1, while the other approaches failed to achieve.\nSect. 3.3 puzzled me the most. It is unclear to me why the affinity weights should be defined as Eq.2? Why is the Convolution necessary? What is the motivation of choosing the specific form of hierarchical cross-modal exchange?  I feel the major issue of this paper is the motivation example is not closely connected to the proposed approach. Therefore, it is not clear to me why the proposed approach could solve the motivation example. \ncomments,_suggestions_and_typos\nNone ", "label": [[529, 590, "Eval_pos_1"], [687, 733, "Eval_pos_2"], [734, 789, "Eval_pos_3"], [790, 839, "Jus_pos_3"], [1045, 1218, "Eval_neg_1"], [1448, 1657, "Major_claim"]]}
{"id": 494, "review": "paper_summary\nThe paper presents a sentence-level matching method for fine-grain document similarity of research papers (named ASPIRE). The method relies on co-citation sentences as learning signal to train a multi-vector model. The key difference to related work is the ability of determining paper\u2019s similarity not only on a single aspect but multiple ones, whereby the set of aspects is not predefined.  The method uses a combination of a SciBERT encoder, contrastive learning, multi-instance learning, and optimal transport. In experiments with four datasets all baselines are outperformed. Besides the promising experimental results, one strength of presented method is its scalable inference that could easily enable future applications based on the method. \nsummary_of_strengths\n- ASPIRE outperforms the strong baselines (e.g., SPECTER).\n-A series of relevant ablations are presented. \n-Even though the paper presents a niche topic, the approach is important and could be also applied on other domains apart from scientific documents. \n-The multi vector approach makes aspect similarity feasible for production environments due to scalable inference (with ANN). \nsummary_of_weaknesses\n- The approach description (\u00a7 3) is partially difficult to follow and should be revised. The additional page of the camera-ready version should be used to extend the approach description (rather than adding more experiments).\n-CSFCube results are not reported with the same metrics as in the original publication making a comparison harder than needed. \ncomments,_suggestions_and_typos\n- The standard deviation from the Appendix could be added to Table 1 at least for one metric. There should be enough horizontal space.\n-Notation of BERT_\\theta and BERT_\\epsilon is confusing. Explicitly calling them the co-citation sentence encoder and the paper encoder could make it clearer.  -How are negative sampled for BERT_\\epsilon?\nAdditional relevant literature: - Luu, K., Wu, X., Koncel-Kedziorski, R., Lo, K., Cachola, I., & Smith, N.A. (2021). Explaining Relationships Between Scientific Documents. ACL/IJCNLP.\n-Malte Ostendorff, Terry Ruas, Till Blume, Bela Gipp, Georg Rehm. Aspect-based Document Similarity for Research Papers. COLING 2020.\nTypos: -\tLine 259: \u201ccotation\u201d -\tLine 285: Missing \u201c.\u201d ", "label": [[1194, 1280, "Eval_neg_1"], [1281, 1417, "Jus_neg_1"]]}
{"id": 495, "review": "paper_summary\nThis paper presents a new dataset namely HiTab with 3,597 hierarchical tables and 10,686 questions. They construct this dataset by collecting a wealth of statistical reports and Wikipedia pages, and then asking human annotators to convert sentence descriptions into question-answer pairs. Nearly all tables in the dataset are hierarchical, with fine-grained annotations of quantity and entity alignment. They evaluate two tasks on this dataset: table QA and natural language generation. They show that complex hierarchical structures increase the difficulty of both table QA and text generation. \nsummary_of_strengths\nIn general, I think there are some unique features to make this HiTab dataset valuable to the NLP community, including 1) this is the first dataset that focuses on hierarchical tables, 2) it contains a detailed annotation of entity and quantity alignment for each table-question pair, and 3) instead of crowd-sourcing, HiTab obtain the questions from real sentence descriptions of each table. This avoids annotation artifacts and bias to some extent.  There are some other strengths as follows:  - Good efforts are spent to build this dataset, with 18 annotators and 2400 working hours. Besides the hierarchical tables,  -Automatic Evaluation (Section 2.4) and Human Inspections (Section 2.5) are conducted in the dataset construction process to ensure the data quality.  -The details of dataset construction are well-presented in the paper and the Appendix. The authors also provide a good comparison with existing Table QA datasets.  -The dataset and codes are provided alongside with the paper. I randomly checked some data samples, and they are in high quality. \nsummary_of_weaknesses\nIt is still not quite obvious to me what additional challenges are posed by the hierarchical table structure in table QA. Although the authors mentioned three challenges in the introduction, it seems that they are not well-reflected in the experiments. I think Appendix B.4 is a good example to empirically show hierarchical structures indeed increases the difficulty of table QA. I suggest the authors put this part into the main paper, and I expect to see more results and analysis towards this direction to reveal how SOTA QA models fail in facing with complex hierarchical structures. \ncomments,_suggestions_and_typos\nI am not an expert of table QA. But to me, the authors seem to miss a straightforward baseline, that is to firstly flat the table into a text sequence like Figure 7 and then train a text-based QA model on it. For example, we can fine-tune the pretrained QA models like SpanBERT for table QA after flatting the table into a text sequence. In this case, we might be able to learn the hierarchical information through the self-attention mechanism built in Transformers. ", "label": [[632, 740, "Eval_pos_1"], [741, 1082, "Jus_pos_1"], [1130, 1174, "Eval_pos_2"], [1176, 1251, "Jus_pos_2"], [1405, 1490, "Eval_pos_3"], [1491, 1566, "Eval_pos_4"], [1630, 1698, "Eval_pos_5"], [1721, 1842, "Eval_neg_1"], [1843, 2310, "Jus_neg_1"]]}
{"id": 496, "review": "This paper proposes a neural method for generating distractors for multiple choice questions for reading comprehension exams. Given a text passage, a question, and the correct answer, this approach generates distractors for the given, correct answer. The novel aspect of the proposed approach is that after encoding the input, the resulting representations of the passage and question get \"rewritten\" to de-emphasize the correct information. The goal is to ensure that distractor items are incorrect (as distractor answers for multiple choice questions should be).\nContributions ------------- - a neural approach to generating distrator items for multiple choice questions - an evaluation of this approach (using both automated metrics and human judgments) that suggests that the proposed approach outperforms the state of the art Strengths --------- - Generating good distractors for multiple choice questions is a task with real-world applications that would greatly support teachers.\n- The paper is clearly written. In particular, the proposed model is quite complicated, but the general ideas are relatively well explained and the intentions behind technical decisions are articulated.  Weaknesses ---------- I was somewhat underwhelmed by the evaluation. While the numbers show that the proposed method outperforms the state of the art, I did not get a good sense what the generated distractors really look like or whether they would be useful in the creation of actual exams.\n- The evaluation does not directly assess the aspects of distractor generation that the authors want to improve on. In the introduction the authors say that they are specifically interested in generating distractors that are incorrect, but plausible. The proposed method is designed to improve those aspects. However, neither the automatic evaluation nor the human evaluation tries to directly assess those aspects.\n- While the evaluation compares the proposed approach to a number of other recently proposed approaches to distractor generation, how exactly the proposed model differs from these prior approaches could be explained more explicitly and with a bit more detail.\n- I wonder about the variety of distractors that can be generated by this approach in comparison to the variety among human generated distractors. It seems that the proposed approach would always generate distractors which draw on information from other parts of the text passage. However, there may be questions where the answer as well as any potential distractors do not so much rely on specific parts of the reading passage. For example, questions that ask for the meaning of words or phrases may rely more on the test takers prior knowledge. Or the distractors for question 2 from Figure 1, which asks for a title, seem to not be tied to closely to a specific part of the given passage either.\n- I wish the paper would give more examples (than just one) of generated distractors. And I wish there were some examples of distractors generated by some of the prior approaches for comparison.\nOther questions and comments ---------------------------- - It seems that this approach is focused on multiple choice questions in reading comprehension exams (where an answer has to be given based on a text passage). I don't think that is explicitly said anywhere.\n- The term \"reform\" in connection with the processing of the passage representation and the question representation seems a bit awkward to me. I have to admit though that I don't have a really good proposal for an alternative. Maybe \"rewrite\", \"amend\", \"alter\", or \"adjust\"?\n- Sec. 4.1.1. Is the language of the dataset English? Please say so and specify how the lengths in Figure 3 are measured (# of words?).\n- Sec. 4.2: \"SS (Seq2Seq): the basic model that generates a distractor from the passage;\" ==> Only from the passage? Is it not given the question and correct answer?\n- Sec. 4.5: \"five annotators with good English background (at least holding a bachelor's degree)\" ==> Does that mean the annotators were non-native speakers who had bachelor's degrees in English?\n- Sec. 4.5: What instructions were given to the annotators, especially wrt. to judging the \"distracting ability\". Creating good multiple choice questions is not easy and part of the problem is writing good distractors. I doubt that someone without any experience or training would do a good job. And so I wonder to what degree they are able to judge what's a good distractor.\n- Sec. 4.5: \"to score these distractors with three gears ...\" ==> I don't understand what \"with three gears\" means here.\n- Why did you choose HSA and CHN for the human evaluation? Why not include SEQA, which also performs well in the automated tests, maybe even better than HSA?\n- I also would have liked to see human evaluation results for human generated distractors to get a sense of what the systems are aiming for.\n- Fig. 4: This text passage seems to be a sequence of job ads. It is hard to see in the figure where one job ad ends and the next begins. Therefore, it is hard to understand which answers are correct. ", "label": [[853, 986, "Eval_pos_1"], [989, 1018, "Eval_pos_2"], [1019, 1126, "Eval_pos_3"], [1130, 1188, "Eval_pos_4"], [1213, 1259, "Eval_neg_1"], [1260, 1481, "Jus_neg_1"]]}
{"id": 497, "review": "This paper applies the idea of translation model pruning to neural MT. The authors explore three simple threshold and histogram pruning schemes, two of which are applied separately to each weight class, while the third is applied to the entire model. The authors also show that retraining the models produces performance equal to the full model, even when 90% of the weights are pruned. \nAn extensive analysis explains the superiority of the class-blind pruning scheme, as well as the performance boost through retraining.  While the main idea of the paper is simple, it seems quite useful for memory-restricted applications of NMT. I particularly liked the analysis section which gives further insight into the model components that are usually treated like black boxes. While these insights are interesting by themselves, the paper's main motivation is model compression. This argument would be stronger if the paper included some numbers on actual memory consumption of the compressed model in comparison to the uncompressed model.      Some minor remarks: -There is a substantial amount of work on pruning translation models in phrase-based SMT, which could be referenced in related work, e.g.  Johnson, J., Martin, J., Foster, G. and Kuhn, R.: Improving Translation Quality by Discarding Most of the Phrasetable. EMNLP 07 or Zens, R., Stanton, D. and Peng X.: A Systematic Comparison of Phrase Table Pruning Techniques. EMNLP 12 - It took me a while to understand Figure 5. I would find it more informative to add an additional barplot under figure 4 showing highest discarded weight magnitude by class. This would also allow a comparison across all pruning methods. ", "label": [[388, 523, "Eval_pos_1"], [524, 632, "Eval_pos_2"], [633, 771, "Eval_pos_3"], [1436, 1478, "Eval_neg_1"], [1479, 1671, "Jus_neg_1"]]}
{"id": 499, "review": "paper_summary\nThe authors describe a novel approach to Cross-Language Information Retrieval (CLIR). They propose a single-model-based approach. That is, in contrast with classical MT-based CLIR systems, where we can clearly distinguish two phases/stages/subprocesses involved (translation + monolingual retrieval), this is now solved altogether. For simplicity they have used English as the target language for their experiments. The results obtained show state-of-the-art performane according to their metrics.\nTheir proposal takes as basis a ColBERT IR architecture (with proved state-of-the-art performance) and a pre-trained multilingual masked language model (XML-RoBERTa in their experiments). The idea consists on using knowledge distillation (KD) to teach/fine-tune the underlying multilingual model to \"imitate\" the behavior (and, theoretically, the performance) of a monolingual IR system in the target language, even when the CLIR system is taking as input queries in source language, not the target language. This fine-tuning/training process is done in two phases.  A first KD process is focused on teaching the CLIR system to select and rank relevant documents in the same way as the target-language monolingual system. Ideally, the resulting fine-tuned [cross-language] model should be able to obtain the same output results as a monolingual model. In other words, we are teaching the system how to solve the retrieval part of the process. For this purpose, a regular CLIR dataset is needed (parallel queries and their corresponding relevant documents).\nThe second KD process is focused on teaching the CLIR system to encode (internally) a source-language text (e.g. the input query to the CLIR system) in the same way as the target-language monolingual system. Ideally, the resulting fine-tuned [cross-language] model should be able to generate the same internal representation of a text as the monolingual model even when they are taking as input texts (queries) in different languages (but being mutual translations, of course). In other words, we are teaching the system how to solve the translation part of the process. For this purpose, a parallel corpora is needed.  Logically, there are still two issues involved in the joint CLIR process (translation + retrieval), but they are now solved at a time after having teached the model how to do it.   The XOR-TyDi CLIR dataset (7 languages + English) is used for in-domain experiments, and the MKQA dataset (5 common languages with the previous one + English) is used for zero-shot experiments. For simplicity, English is always used as the target language. The following baselines, in increasing performance order, are used: (1) [bottom] ColBERT IR model + XML-RoBERTa multilingual masked language model, fine-tuned with English data       (2) ColBERT IR model + XML-RoBERTa multilingual masked language model, fine-tuned with both English and cross-language data  (3) [top] classical approach: MT + ColBERT IR model, fine-tuned with English data  The results obtained are clearly positive, notably outperforming the lower baselines and approaching the top monolingual state-of-the-art approach. An ablation study is also performed, proving that both KD processes actually contribute to the results. \nsummary_of_strengths\n- Novelty.\n-State-of-the-art results.\n-Quite complete experiments (given the length of the article).\n-Well-written. \nsummary_of_weaknesses\n- I miss the use of more metrics -Given the length of the main article, 5 pages, 3 pages of Appendix may be a bit excessive. \ncomments,_suggestions_and_typos\n- When reading the first part of the article, the authors tend to associate the term \"CLIR\" mainly with theirs and similar approaches, in contrast with the classical MT+monolingual IR approach. However, both of them are CLIR systems and this creates some confusion in the reader.  - Sect. 1 \"Introduction\" contains too many redundancies with respect to the rest of the paper. Taking into account the length of the paper and the Appendix, a reduction of Sect.1 through the removal of unnecessary redundancies would provide space enough to move part of the appendix back to tha main paper, thus making it more complete and self-contained.\n- l.166-181: No data are given here about the parallel corpus used for the term-level alignment. This is explained later, in the appendix, but no clue is given to the reader at this point. The authors should move that part back to the main paper or, at least, indicate that those data are provided in the Appendix. Apart from that, nothing is said about the size of the resultant dictionary.\n- Figure 2: It has not been useful at all, at least for me.\n- l.182-195: According to this, it seems that the resulting CLIR system proposed by the authors would be also able to work not only with queries in the source-language (i.e. non-English), but also with queries in the target-language (i.e. English). Is this the case or just a confusion?\n- Table 1: The reader must be able to easily identify which configuration described in the text corresponds with each raw, and this is not the case. This must be corrected.\n- I have some concerns about the (only) use of R@5kt as metric. Why no Precision-based metric is used? This information would be very useful to have a better view of the performance of the approach. Moreover, which criteria is used to count the tokens? Do you join the text of all the snippets retrieved and, then, you count 5000 tokens?\n- l.266-268: The authors are surprised about the fact that the contribution of the KD with parallel corpus is greater than that of IR triples. I think it makes much sense since you're using a Recall-based metric. Since it is a Recall-based metric, the important thing is that you retrieve the document, regardless of its ranking. In a CLIR context, a more accurate translation (e.g. being able to translate one more term of the input query) allows new matchings, thus improving recall, regardless of the ranking position where the new documents are retrieved.\n- References: They contain the usual missing-uppercases errors.\n- footnotes 3,4: When a reference to a footnote is inserted immediately before a punctuation mark, they should switch positions; e.g. \"(...) OPUS^3. The (...)\" --> \"(...) OPUS.^3 The (...)\"  - l.479-480: How many cases have been analyzed? ", "label": [[100, 313, "Jus_pos_1"], [315, 344, "Eval_pos_1"], [3294, 3302, "Eval_pos_1"], [3304, 3329, "Eval_pos_2"], [3331, 3392, "Eval_pos_3"], [3394, 3408, "Eval_pos_4"], [3872, 3964, "Eval_pos_1"], [3965, 4225, "Jus_pos_1"]]}
{"id": 500, "review": "This paper introduces UCCA as a target representation for semantic parsing and also describes a quite successful transition-based parser for inference into that representation. I liked this paper a lot. I believe there is a lot of value simply in the introduction of UCCA (not new, but I believe relatively new to this community), which has the potential to spark new thinking about semantic representations of text. I also think the model was well thought out. \nWhile the model itself was fairly derivative of existing transition-based schemes, the extensions the authors introduced to make the model applicable in this domain were reasonable and well-explained, at what I believe to be an appropriate level of detail.\nThe empirical evaluation was pretty convincing -- the results were good, as compared to several credible baselines, and the authors demonstrated this performance in multiple domains. My biggest complaint about this paper is the lack of multilingual evaluation, especially given that the formalism being experimented with is exactly one that is supposed to be fairly universal. I'm reasonably sure multilingual UCCA corpora exist (in fact, I think the \"20k leagues\" corpus used in this paper is one such), so it would be good to see results in a language other than English.\nOne minor point: in section 6, the authors refer to their model as \"grammarless\", which strikes me as not quite correct. It's true that the UCCA representation isn't derived from linguistic notions of syntax, but it still defines a way to construct a compositional abstract symbolic representation of text, which to me, is precisely a grammar. ( This is clearly a quibble, and I don't know why it irked me enough that I feel compelled to address it, but it did.)\nEdited to add: Thanks to the authors for their response. ", "label": [[177, 202, "Eval_pos_1"], [203, 416, "Eval_pos_2"], [417, 462, "Eval_pos_3"], [463, 719, "Eval_pos_4"], [720, 766, "Eval_pos_5"], [903, 1096, "Eval_neg_1"], [1097, 1293, "Eval_neg_1"], [1311, 1414, "Eval_neg_2"], [1415, 1756, "Jus_neg_2"]]}
{"id": 501, "review": "paper_summary\nThe purpose of this paper is to build a system to detect negation cues and negation focus by gathering additional data and utilizing negation masking strategies. Results indicate some improvement as well as some transferability of negation across domains. \nsummary_of_strengths\nIn order to build AugNB, the authors collected a large number of negation sentences. This large corpus might be useful for future research.   Using their proposed models (e.g., AugNB and CueNB), the authors conduct extensive experiments on a variety of datasets.   Based on the reported results, detecting negation cues and detecting focus of negation appear promising for most of the datasets. \nsummary_of_weaknesses\nWhile the results are promising, the innovation of this paper is somewhat limited as the approaches are quite well known.   The paper indicates that only gold cue information is used for scope resolution (I. 219). Now, the question is what happens if you use predicted cues? Is it the case that a few wrong cues can give devastating results on focus detection?  More details should be there for the approach \"negation-focused data collection\" approach. It seems additional cues are collected only from a biomedical domain. Therefore, the model AugNB can be biased to perform better on the biomedical datasets only. \nFurther, there should be more information on CueNB model as well. How is the masking objective utilized on top of AugNB? \ncomments,_suggestions_and_typos\nNumbers in Tables 2 and 3 look more crowded. I would round to one decimal point in these tables. ", "label": [[377, 431, "Eval_pos_1"], [434, 554, "Eval_pos_2"], [557, 687, "Eval_pos_3"], [710, 791, "Eval_neg_1"], [792, 830, "Jus_neg_1"]]}
{"id": 502, "review": "paper_summary\nThis paper proposes to examine the problem of Automatic Readability Assessment (ARA) as a neural pairwise ranking task, with the goal of addressing multiple transferability issues with classification-based ARA.  The paper compares a pairwise neural ranking model with existing non-ranking ARA methods and explores the monolingual and cross-lingual transferability of their ARA ranking model.  From a technical contributions standpoint, the paper mostly provides benchmarking results for ranking vs. other ARA paradigms, rather than a new model. \nsummary_of_strengths\nThe paper is thorough in what ARA experiments are run across different paradigms (classification, regression, ranking) and transferability settings (monolingual/cross-lingual transfer).  The paper also provides a well-motivated argument for the benefits of using a ranking-based paradigm for readability assessment, especially for settings involving transfer. \nsummary_of_weaknesses\nThe paper lacks significance tests in their comparisons between models, which may draw some of the conclusions in the paper into question.  The results themselves could be presented more clearly with a little more reflection on why certain methods seem to work better than others.  Most of my concerns are covered in more detail in Comments, Suggestions, and Typos. \ncomments,_suggestions_and_typos\nLines 207-208: Isn't this just mathematically equivalent to taking Binary Cross Entropy loss over the sigmoid?\nLines 225-232: One limitation of the current ranking based approach is that while it's easier to compare across different datasets with potentially different label scales, running comparisons over multiple instances may be less efficient.  For example, if we want to get an idea of the absolute readability for a new document compared to a set of S other documents that have \"already been scored\", we would have to re-run the NPRM over the S documents in your comparison basis.  Have you thought of methods of aggregating or scaling the NPRM score to get around this drawback?  Or are there particular applications you can see where this comparison problem as not being as much of an efficiency issue.\nSection 5.3: One suggestion I had to make things a little easier to follow in this section is to add a table comparing the performance of the best models for each paradigm (classification, regression, pairwise-ranking) on the ranking metrics.  Currently, it's a little hard to follow the written results here because the reader has to check between multiple tables to make the comparison.\nTables 1-6: I would recommend bolding the best performance numbers in these tables.\nResults: Overall, did you run any significance tests when making comparisons between two different models?  Some of the numbers, such as the pairwise ranking evaluations and the comparisons between classification, regression, and ranking on the ranking metrics are quite close.  I would recommend adding a note in the text whenever you make a comparison between two models as to whether the difference in results is significant.\nLine 559-561: Do you think the drop in performance for Vikidia-Fr is because of the domain difference between Newsela and Vikidia? ", "label": [[581, 661, "Eval_pos_1"], [663, 698, "Jus_pos_1"], [700, 728, "Eval_pos_1"], [730, 764, "Jus_pos_1"], [767, 940, "Eval_pos_2"], [964, 1102, "Eval_neg_1"], [1104, 1244, "Eval_neg_2"], [1489, 1712, "Eval_neg_3"], [1714, 2175, "Jus_neg_3"]]}
{"id": 503, "review": "- Strengths: A new encoder-decoder model is proposed that explicitly takes  into account monotonicity.\n- Weaknesses: Maybe the model is just an ordinary BiRNN with alignments de-coupled. \nOnly evaluated on morphology, no other monotone Seq2Seq tasks.\n- General Discussion: The authors propose a novel encoder-decoder neural network architecture with \"hard monotonic attention\". They evaluate it on three morphology datasets.\nThis paper is a tough one. One the one hand it is well-written, mostly very clear and also presents a novel idea, namely including monotonicity in morphology tasks.  The reason for including such monotonicity is pretty obvious: Unlike machine translation, many seq2seq tasks are monotone, and therefore general encoder-decoder models should not be used in the first place. That they still perform reasonably well should be considered a strong argument for neural techniques, in general. The idea of this paper is now to explicity enforce a monotonic output character generation. They do this by decoupling alignment and transduction and first aligning input-output sequences monotonically and then training to generate outputs in agreement with the monotone alignments. \nHowever, the authors are unclear on this point. I have a few questions: 1) How do your alignments look like? On the one hand, the alignments seem to be of the kind 1-to-many (as in the running example, Fig.1), that is, 1 input character can be aligned with zero, 1, or several output characters. However, this seems to contrast with the description given in lines 311-312 where the authors speak of several input characters aligned to 1 output character. That is, do you use 1-to-many, many-to-1 or many-to-many alignments?\n2) Actually, there is a quite simple approach to monotone Seq2Seq. In a first stage, align input and output characters monotonically with a 1-to-many constraint (one can use any monotone aligner, such as the toolkit of Jiampojamarn and Kondrak). Then one trains a standard sequence tagger(!) to predict exactly these 1-to-many alignments. For example, flog->fliege (your example on l.613): First align as in \"f-l-o-g / f-l-ie-ge\". Now use any tagger (could use an LSTM, if you like) to predict \"f-l-ie-ge\" (sequence of length 4) from \"f-l-o-g\" (sequence of length 4). Such an approach may have been suggested in multiple papers, one reference could be [*, Section 4.2] below. \nMy two questions here are:  2a) How does your approach differ from this rather simple idea?\n2b) Why did you not include it as a baseline?\nFurther issues: 3) It's really a pitty that you only tested on morphology, because there are many other interesting monotonic seq2seq tasks, and you could have shown your system's superiority by evaluating on these, given that you explicitly model monotonicity (cf. also [*]).\n4) You perform \"on par or better\" (l.791). There seems to be a general cognitive bias among NLP researchers to map instances where they perform worse to \"on par\" and all the rest to \"better\". I think this wording should be corrected, but otherwise I'm fine with the experimental results.\n5) You say little about your linguistic features: From Fig. 1, I infer that they include POS, etc.  5a) Where did you take these features from?\n5b) Is it possible that these are responsible for your better performance in some cases, rather than the monotonicity constraints?\nMinor points: 6) Equation (3): please re-write $NN$ as $\\text{NN}$ or similar 7) l.231 \"Where\" should be lower case 8) l.237 and many more: $x_1\\ldots x_n$. As far as I know, the math community recommends to write $x_1,\\ldots,x_n$ but $x_1\\cdots x_n$. That is, dots should be on the same level as surrounding symbols.\n9) Figure 1: is it really necessary to use cyrillic font? I can't even address your example here, because I don't have your fonts.\n10) l.437: should be \"these\" [*]  @InProceedings{schnober-EtAl:2016:COLING,    author    = {Schnober, Carsten  and  Eger, Steffen  and  Do Dinh, Erik-L\\^{a}n  and  Gurevych, Iryna},   title     = {Still not there? Comparing Traditional Sequence-to-Sequence Models to Encoder-Decoder Neural Networks on Monotone String Translation Tasks},   booktitle = {Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers},   month     = {December},   year                                                      = {2016},   address   = {Osaka, Japan},   publisher = {The COLING 2016 Organizing Committee},   pages     = {1703--1714},   url                                               = {http://aclweb.org/anthology/C16-1160} } AFTER AUTHOR RESPONSE Thanks for the clarifications. I think your alignments got mixed up in the response somehow (maybe a coding issue), but I think you're aligning 1-0, 0-1, 1-1, and later make many-to-many alignments from these. \nI know that you compare to  Nicolai, Cherry and Kondrak (2015) but my question would have rather been: why not use 1-x (x in 0,1,2) alignments as in  Schnober et al. and then train a neural tagger on these (e.g. BiLSTM). I wonder how much your results would have differed from such a rather simple baseline. ( A tagger is a monotone model to start with and given the monotone alignments, everything stays monotone. In contrast, you start out with a more general model and then put hard monotonicity constraints on this ...) NOTES FROM AC Also quite relevant is Cohn et al. (2016), http://www.aclweb.org/anthology/N16-1102 .\nIsn't your architecture also related to methods like the Stack LSTM, which similarly predicts a sequence of actions that modify or annotate an input?   Do you think you lose anything by using a greedy alignment, in contrast to Rastogi et al. (2016), which also has hard monotonic attention but sums over all alignments? ", "label": [[188, 250, "Eval_neg_1"], [425, 451, "Eval_pos_1"], [452, 589, "Eval_pos_2"], [592, 1195, "Jus_pos_2"], [1196, 1243, "Eval_neg_2"], [1244, 2534, "Jus_neg_2"], [2554, 2608, "Eval_neg_3"], [2610, 2811, "Jus_neg_3"], [3103, 3148, "Eval_neg_4"], [3150, 3243, "Jus_neg_4"]]}
{"id": 504, "review": "Update after author response:  1. My major concern about the optimization of model's hyperparameter (which are numerous) has not been addressed. This is very important, considering that you report results from folded cross-validation.  2. The explanation that benefits of their method are experimentally confirmed with 2% difference -- while evaluating via 5-fold CV on 200 examples -- is quite unconvincing.\n======================================================================== Summary: In this paper authors present a complex neural model for detecting factuality of event mentions in text. The authors combine the following in their complex model:                          (1) a set of traditional classifiers for detecting event mentions, factuality sources, and source introducing predicates (SIPs), (2) A bidirectional attention-based LSTM model that learns latent representations for elements on different dependency paths used as input, (2) A CNN that uses representations from the LSTM and performs two output predictions (one to detect specific from underspecified cases and another to predict the actual factuality class).  From the methodological point of view, the authors are combining a reasonably familiar methods (att-BiLSTM and CNN) into a fairly complex model. However, this model does not take raw text (sequence of word embeddings) as input, but rather hand-crafted features (e.g., different dependency paths combining factuality concepts, e.g., sources, SIPs, and clues). The usage of hand-crafted features is somewhat surprising if coupled with complex deep model. The evaluation seems a bit tainted as the authors report the results from folded cross-validation but do not report how they optimized the hyperparameters of the model. Finally, the results are not too convincing -- considering the complexity of the model and the amount of preprocessing required (extraction of event mentions, SIPs, and clues), a 2% macro-average gain over the rule-based baseline and overall 44% performance seems modest, at best (looking at Micro-average, the proposed model doesn't outperform simple MaxEnt classifier).\nThe paper is generally well-written and fairly easy to understand. Altogether, I find this paper to be informative to an extent, but in it's current form not a great read for a top-tier conference.    Remarks: 1. You keep mentioning that the LSTM and CNN in your model are combined \"properly\" -- what does that actually mean? How does this \"properness\" manifest? What would be the improper way to combine the models?\n2. I find the motivation/justification for the two output design rather weak:      - the first argument that it allows for later addition of cues (i.e manually-designed features) kind of beats the \"learning representations\" advantage of using deep models. \n        - the second argument about this design tackling the imbalance in the training set is kind of hand-wavy as there is no experimental support for this claim.  3. You first motivate the usage of your complex DL architecture with learning latent representations and avoiding manual design and feature computation.  And then you define a set of manually designed features (several dependency paths and lexical features) as input for the model. Do you notice the discrepancy?  4. The LSTMs (bidirectional, and also with attention) have by now already become a standard model for various NLP tasks. Thus I find the detailed description of the attention-based bidirectional LSTM unnecessary. \n5. What you present as a baseline in Section 3 is also part of your model (as it generates input to your model). Thus, I think that calling it a baseline undermines the understandability of the paper.  6. The results reported originate from a 5-fold CV. However, the model contains numerous hyperparameters that need to be optimized (e.g., number of filters and filter sizes for CNNs). How do you optimize these values? Reporting results from a folded cross-validation doesn't allow for a fair optimization of the hypeparameters: either you're not optimizing the model's hyperparameters at all, or you're optimizing their values on the test set (which is unfair).  7. \" Notice that some values are non-application (NA) grammatically, e.g., PRu, PSu, U+/-\" -- why is underspecification in ony one dimension (polarity or certainty) not an option? I can easily think of a case where it is clear the event is negative, but it is not specified whether the absence of an event is certain, probable, or possible.  Language & style: 1. \" to a great degree\" -> \"great degree\" is an unusual construct, use either \"great extent\" or \"large degree\" 2. \" events that can not\" -> \"cannot\" or \"do not\" 3. \" describes out networks...in details shown in Figure 3.\" - > \"...shown in Figure 3 in details.\" ", "label": [[1591, 1625, "Eval_neg_1"], [1626, 1759, "Jus_neg_1"], [1760, 1803, "Eval_neg_2"], [1807, 2131, "Jus_neg_2"], [2132, 2198, "Eval_pos_1"], [2199, 2330, "Major_claim"], [2552, 2626, "Eval_neg_3"], [2631, 2969, "Jus_neg_3"], [3288, 3405, "Jus_neg_4"], [3406, 3498, "Eval_neg_4"], [3502, 3611, "Jus_neg_5"], [3612, 3699, "Eval_neg_5"]]}
{"id": 505, "review": "- Strengths: - technique for creating dataset for evaluation of out-of-coverage items, that could possibly be used to evaluation other grammars as well.  -the writing in this paper is engaging, and clear (a pleasant surprise, as compared to the typical ACL publication.)\n- Weaknesses: -The evaluation datasets used are small and hence results are not very convincing (particularly wrt to the alchemy45 dataset on which the best results have been obtained) -It is disappointing to see only F1 scores and coverage scores, but virtually no deeper analysis of the results. For instance, a breakdown by type of error/type of grammatical construction would be interesting.  -it is still not clear to this reviewer what is the proportion of out of coverage items due to various factors (running out of resources,  lack of coverage for \"genuine\" grammatical constructions in the long tail, lack of coverage due to extra-grammatical factors like interjections, disfluencies, lack of lexical coverage, etc.  - General Discussion: This paper address the problem of \"robustness\" or lack of coverage for a hand-written HPSG grammar (English Resource Grammar). The paper compares several approaches for increasing coverage, and also presents two creative ways of obtaining evaluation datasets (a non-trivial issue due to the fact that gold standard evaluation data is by definition available only for in-coverage inputs).  Although hand-written precision grammars have been very much out of fashion for a long time now and have been superseded by statistical treebank-based grammars, it is important to continue research on these in my opinion. The advantages of high precision and deep semantic analysis provided by these grammars has not been reproduced by non-handwritten grammars as yet. For this reason, I am giving this paper a score of 4, despite the shortcomings mentioned above. ", "label": [[155, 270, "Eval_pos_1"], [286, 324, "Eval_neg_1"], [335, 366, "Eval_neg_2"], [368, 454, "Jus_neg_2"], [457, 568, "Eval_neg_3"], [569, 666, "Jus_neg_3"], [669, 778, "Eval_neg_4"], [780, 996, "Jus_neg_4"], [1778, 1874, "Major_claim"]]}
{"id": 506, "review": "- Strengths: This paper presents a 2 x 2 x 3 x 10 array of accuracy results based on systematically changing the parameters of embeddings models: (context type, position sensitive, embedding model, task), accuracy - context type \u2208 {Linear, Syntactic} -position sensitive \u2208 {True, False} -embedding model \u2208 {Skip Gram, BOW, GLOVE} -task \u2208 {Word Similarity, Analogies, POS, NER, Chunking, 5 text classific. \ntasks} The aim of these experiments was to investigate the variation in performance as these parameters are changed. The goal of the study itself is interesting for the ACL community and similar papers have appeared before as workshop papers and have been well cited, such as Nayak et al.'s paper mentioned below.\n- Weaknesses: Since this paper essentially presents the effect of systematically changing the context types and position sensitivity, I will focus on the execution of the investigation and the analysis of the results, which I am afraid is not  satisfactory.\nA) The lack of hyper-parameter tuning is worrisome. E.g.    - 395 Unless otherwise notes, the number of word embedding dimension is set to 500. \n   - 232 It still enlarges the context vocabulary about 5 times in practice. \n   - 385 Most hyper-parameters are the same as Levy et al' best configuration.\n  This is worrisome because lack of hyperparameter tuning makes it difficult to make statements like method A is better than method B. E.g. bound methods may perform better with a lower dimensionality than unbound models, since their effective context vocabulary size is larger.\nB) The paper sometimes presents strange explanations for its results. E.g.    - 115 \"Experimental results suggest that although it's hard to find any  universal insight, the characteristics of different contexts on different models are concluded according to specific tasks.\"\n   What does this sentence even mean?     - 580 Sequence labeling tasks tend to classify words with the same syntax  to the same category. The ignorance of syntax for word embeddings which  are learned by bound representation becomes beneficial.     These two sentences are contradictory, if a sequence labeling task    classified words with \"same syntax\" to same category then syntx becomes    a ver valuable feature. Bound representation's ignorance of syntax    should cause a drop in performance just like other tasks which does not    happen.\nC) It is not enough to merely mention Lai et. al. 2016 who have also done a    systematic study of the word embeddings, and similarly the paper     \"Evaluating Word Embeddings Using a Representative Suite of Practical    Tasks\", Nayak, Angeli, Manning. appeared at the repeval workshop at     ACL 2016. should have been cited. I understand that the focus of Nayak    et al's paper is not exactly the same as this paper, however they    provide recommendations about hyperparameter tuning and experiment    design and even provide a web interface for automatically running    tagging experiments using neural networks instead of the \"simple linear    classifiers\" used in the current paper.\nD) The paper uses a neural BOW words classifier for the text classification tasks    but a simple linear classifier for the sequence labeling tasks. What is    the justification for this choice of classifiers? Why not use a simple    neural classifier for the tagging tasks as well? I raise this point,    since the tagging task seems to be the only task where bound    representations are consistently beating the unbound representations,    which makes this task the odd one out.  - General Discussion: Finally, I will make one speculative suggestion to the authors regarding the analysis of the data. As I said earlier, this paper's main contribution is an analysis of the following table. \n(context type, position sensitive, embedding model, task, accuracy) So essentially there are 120 accuracy values that we want to explain in terms of the aspects of the model. It may be beneficial to perform factor analysis or some other pattern mining technique on this 120 sample data. ", "label": [[523, 588, "Eval_pos_1"], [734, 977, "Major_claim"], [981, 1029, "Eval_neg_1"], [1030, 1558, "Jus_neg_1"], [1562, 1628, "Eval_neg_2"], [1629, 2382, "Jus_neg_2"]]}
{"id": 508, "review": "paper_summary\nTo alleviate the problem of error propagation due to Automatic Speech Recognition (ASR) system for multi-modal sentiment analysis task the paper provides a refinement approach by detecting the positions of the sentiment words in the text and dynamically refine the word embeddings in the detected positions by incorporating multimodal clues such as low voice, sad face and textual context. Their approach outperforms the baselines. \nsummary_of_strengths\nThe approach does try to address the real-world problem of error propagation due to ASR. The  architecture is novel. Good benchmarking and ablation. \nsummary_of_weaknesses\nIt would be difficult to detect the positions of sentiment words if an overall sentence (more than one word) has become noisy due to ASR. The same would be true if there are inserts and deletes in the resulting noisy sentence. This approach may fail in such cases. What is the solution for the same?  Some qualitative analysis of the results and/or error analysis will be useful to understand these type of cases better. The case study section can be replaced with qualitative and /or error analysis. \ncomments,_suggestions_and_typos\nThe last sentence of the abstract \u201cFurthermore, our approach can be adapted for other multimodal feature fusion models easily.\u201d needs more explanation somewhere in the paper. This is a very vague sentence without any proper basis.  There is no mention of making the built datasets publicly available. Though the dataset can be built to reproduce the results it would be better if authors can share the same publicly. ", "label": [[557, 584, "Eval_pos_1"], [641, 866, "Jus_neg_1"], [867, 904, "Eval_neg_1"], [905, 1141, "Jus_neg_1"]]}
{"id": 509, "review": "- Strengths: The authors have nice coverage of a different range of language settings to isolate the way that relatedness and amount of morphology interact (i.e., translating between closely related morphologically rich languages vs distant ones) in affecting what the system learns about morphology. They include an illuminating analysis of what parts of the architecture end up being responsible for learning morphology, particularly in examining how the attention mechanism leads to more impoverished target side representations. \nTheir findings are of high interest and practical usefulness for other users of NMT.  - Weaknesses: They gloss over the details of their character-based encoder. \nThere are many different ways to learn character-based representations, and omitting a discussion of how they do this leaves open questions about the generality of their findings. Also, their analysis could've been made more interesting had they chosen languages with richer and more challenging morphology such as Turkish or Finnish, accompanied by finer-grained morphology prediction and analysis.\n- General Discussion: This paper brings insight into what NMT models learn about morphology by training NMT systems and using the encoder or decoder representations, respectively, as input feature representations to a POS- or morphology-tagging classification task. This paper is a straightforward extension of \"Does String-Based Neural MT Learn Source Syntax?,\" using the same methodology but this time applied to morphology. Their findings offer useful insights into what NMT systems learn. ", "label": [[301, 421, "Eval_pos_1"], [423, 533, "Jus_pos_1"], [534, 618, "Eval_pos_2"], [634, 696, "Eval_neg_1"]]}
{"id": 510, "review": "paper_summary\nThis paper proposes a novel pyramid-BERT to achieve the sequence length reduction across different encoder layers, which benefits the memory and decoding time reduction for downstream classification and ranking tasks. This method is based on the core-set based token selection method which is justified by theoretical results. Experiments on GLUE benchmarks and Long Range Arena datasets demonstrate the effectiveness of the proposed method. \nsummary_of_strengths\n- This paper designs a novel speedup method with core-set based token selection which is justified by theoretical results.\n-Nice experiment results on GLUE benchmarks and Long Range Arena datasets. \nsummary_of_weaknesses\n- This paper lacks experiment comparisons with some very similar approaches, such as centroid transformers and representation pooling, although the authors claim that a thorough comparison is not required.\n-The proposed method seems not to improve the training speed of traditional BERT in the pre-training stage. It means that we only apply this method in the downstream tasks. I wonder about the effectiveness of combining this method with other speedup ways, such as DeeBERT (dynamic early exiting). \ncomments,_suggestions_and_typos\nThis paper is well written and easy to follow. The authors explore to speed up BERT by selecting core-set tokens and reducing the sequence length across different encoder layers. This idea is interesting, and the motivation is reasonable.  My main concern is the experiment comparisons with similar approaches, such as centroid transformers and representation pooling. Although these methods do not release the code, I think this comparison could strengthen the effectiveness of the proposed method. \nBesides, this method seems to only fit downstream classification and ranking tasks. Thus, I wonder about the performance of combining this method with other speedup ways, such as DeeBERT (dynamic early exiting). If we adopt the way of dynamic early exiting, the sequence length reduction across different layers seems to be marginal somehow.\nThe token selection algorithm used in the paper also reminds me of another way that combines the selective encoding method and Gumbel-Softmax for BERT.  Each token predicts the probability of surviving at each layer and adopt the Gumbel-Softmax to obtain the final token that feeds to the next layer.  This method may enhance the robustness of the proposed method, since it traverses more different combinations. ", "label": [[480, 600, "Eval_pos_1"], [602, 676, "Eval_pos_2"], [701, 774, "Eval_neg_1"], [775, 904, "Jus_neg_1"], [906, 1012, "Eval_neg_2"], [1013, 1202, "Jus_neg_2"], [1235, 1281, "Eval_pos_3"], [1414, 1438, "Eval_pos_4"], [1444, 1472, "Eval_pos_5"], [1475, 1544, "Eval_neg_3"], [1546, 1602, "Jus_neg_3"]]}
{"id": 511, "review": "- Strengths: The paper is thoroughly written and discusses its approach compared to other approaches. The authors are aware that their findings are somewhat limited regarding the mean F values.\n- Weaknesses: Some minor orthographical mistakes and some repetive clauses. In general the paper would benefit if the sections 1 and 2 would be shortened to allow the extension of sections 3 and 4. \nThe main goal is not laid out clearly enough, which may be a result of the ambivalence of the paper's goals.\n- General Discussion: Table 1 should only be one column wide, while the figures, especially 3, 5, and 6 would greatly benefit from a two column width. \nThe paper was not very easy to understand during first read. Major improvements could be achieved by straightening up the content. ", "label": [[13, 101, "Eval_pos_1"], [393, 501, "Eval_neg_1"], [654, 715, "Eval_neg_2"]]}
{"id": 512, "review": "paper_summary\nThe paper proposed boundary smoothing as a regularization technique for span-based neural NER models. The boundary smoothing can mitigate the over-confidence issue by reassigning the entity probabilities from annotated spans to the surrounding ones. The approach is simple, and mainly extends the Yu et al (2020) by adding the boundary smoothing. The experiments show the proposed approach can improve the performance of Yu et al (2020). \nsummary_of_strengths\nThe idea of the boundary smoothing is simple, and can improve the performance of the existing NER approach.\nThe in-depth analysis for the over-confidence issue is attractive.\nThe paper is well written. \nsummary_of_weaknesses\nThe idea of using boundary information is not completely innovative. For example, the idea of effectively using boundary information has been proposed in Shen et al.(2021).\nThe experiments don\u2019t prove the claims. Firstly, the paper lacks adequate verification for the proposed idea, and evaluated it with only one baseline. Secondly, the paper used RoBERTa while the previous methods used BERT. Thirdly, Yu et al. (2020) conducted the experiments on GENIA, but the paper doesn\u2019t. \ncomments,_suggestions_and_typos\nThe paper lacks adequate verification for the idea. The paper claims that the boundary smoothing can be easily integrated into any span-based neural NER systems, but the paper only integrated it into the Yu et al. (2020). The paper should further evaluate the boundary smoothing idea in several SOTA approaches.\nIn addition, the idea of effectively using boundary information has been proposed in Shen et al.(2021), which constructs soft examples for partially matched spans, and trains a boundary regressor with boundary-level Smooth L1 loss.\nIn the experiments, I have some concerns: (1)\tThe paper uses RoBERTa for the experiments, while the baselines used BERT. It may be unfair. \n(2)\tWhy the Baseline outperforms Yu et al. (2020) on ACE 04 and ACE 05 greatly, but has comparable performance on OntoNotes 5 and CoNLL 2003? Yu et al. (2020) also conducted experiments on the GENIA dataset, but the paper doesn\u2019t report the results on this dataset. \n(3)\tAccording to the Table 4, the improvement of BS against LS is marginal.  Some statements are not very accurate. For example, Line 313, the submission says \u201cthis work is among the first to introduce a span-based approach to Chinese NER tasks and establish SOTA results.\u201d, however, Shen et al. 2021 proposed a span-based approach previously, and conducted the NER experiment on Chinese Weibo dataset. ", "label": [[474, 581, "Eval_pos_1"], [582, 648, "Eval_pos_2"], [649, 676, "Eval_pos_3"], [699, 767, "Eval_neg_1"], [768, 871, "Jus_neg_1"], [872, 911, "Eval_neg_2"], [912, 1179, "Jus_neg_2"], [1212, 1263, "Eval_neg_3"], [1264, 1523, "Jus_neg_3"]]}
{"id": 513, "review": "paper_summary\nThe paper proposes a pipeline for zero-shot data-to-text generation. \nThe framework follows traditional data-to-text diagram. It contains 4 steps in general. 1) Template verbalization with  2) ordering module, then the 3) aggregation and 4) sentence compression. \nTo enable training the pipeline model, the paper contributes a large dataset named Wikifluent corpus. The data-text pairs are collected from Wikipedia dump. It also applies some split-and-repharase and coreference models on corresponding texts.  The framework proposed in this paper is similar to rule-based data-to-text generation. The main difference of the proposed method compared to traditional methods, each module is trained using neural networks by leveraging recent pre-trained generation models like BART. The novelty of this paper is limited. The neural pipeline method not entirely new, some of previous papers combine important modules such as planning and ordering of traditional data-to-text generation into an end-to-end neural network. [ 1,2]. \nThe paper show improvements of the proposed method on two public datasets with COPY baseline in automatic evaluation. In the manual evaluation, the paper fails to compare with existing baseline method, leaving the evaluation incomplete. The overall paper structure is messy. A lot of detailed model descriptions are described in experiments, which is quite confusing.  [1] Neural data-to-text generation: A comparison between pipeline and end-to-end architectures. Ferreira, et al., 2019. \n[2] Data-to-text with content content selection and planning. Puduppully, et al., 2019. \nsummary_of_strengths\n1. The paper contributes a large scale data-to-text generation dataset WikiFluent.  2. The paper proposes a neural pipeline method leveraging recent pre-trained language models. \nsummary_of_weaknesses\n1. The idea of neural pipeline method for data-to-text generation is not entirely new. The novelty is limited in this paper. \n2. In the experiments, the paper only compare with a weak baseline COPY. There are a lot potential baselines are missing in the experiments. Some existing methods leveraging pre-trained language models (PLMs) should be compared as a baseline. For example, training the PLMs on WIKIFLUENT datasets and then test on the evaluation dataset is a straight forward baseline. \n3. The human evaluation part is quite confusing. No baseline methods are compared (for example COPY). \n4. The paper writing is really confusing. First is the paper structure. A lot of model descriptions are describe in the experiment part. And the section 5.4 is the description of ablation methods, which would be more clear by listing together with other baseline methods. \ncomments,_suggestions_and_typos\nIn sentence aggregation model, the details are not quite clear. Given the input sentences, the supervision is comming from the delimiter term. What is the standard of defining the delimiter term? Is it only using strict sentence string matching or semantic matching?  There are some missing references in this paper. \n[1] Neural data-to-text generation: A comparison between pipeline and end-to-end architectures. Ferreira, et al., 2019. \n[2] Data-to-text with content content selection and planning. Puduppully, et al., 2019. \n[3] An architecture for data-to-text systems. Reiter et al., 2007.  Typos Line 409 we adopt BART-base for -> we adopt BART-base model for ", "label": [[794, 831, "Eval_neg_1"], [832, 876, "Eval_neg_2"], [877, 1039, "Jus_neg_2"], [1277, 1314, "Eval_neg_3"], [1315, 1407, "Jus_neg_3"], [1844, 1927, "Eval_neg_4"], [1928, 1966, "Eval_neg_5"], [1970, 2039, "Eval_neg_6"], [2040, 2336, "Jus_neg_6"], [2340, 2385, "Eval_neg_7"], [2386, 2439, "Jus_neg_7"], [2443, 2481, "Eval_neg_8"], [2482, 2712, "Jus_neg_8"], [2745, 2808, "Eval_neg_9"], [2809, 3011, "Jus_neg_9"], [3013, 3062, "Eval_neg_10"], [3063, 3411, "Jus_neg_10"]]}
{"id": 514, "review": "This paper proposes two dictionary-based methods for estimating multilingual word embeddings, one motivated in clustering (MultiCluster) and another in canonical correlation analysis (MultiCCA). \nIn addition, a supersense similarity measure is proposed that improves on QVEC by substituting its correlation component with CCA, and by taking into account multilingual evaluation. \n The evaluation is performed on a wide range of tasks using the web portal developed by the authors; it is shown that in some cases the proposed representation methods outperform two other baselines.\nI think the paper is very well written, and represents a substantial amount of work done. The presented representation-learning and evaluation methods are certainly timely. I also applaud the authors for the meticulous documentation.\nMy general feel about this paper, however, is that it goes (perhaps) in too much breadth at the expense of some depth. I'd prefer to see a thorougher discussion of results (e.g. regarding the conflicting outcome for MultiCluster between 59- and 12-language set-up; regarding the effect of estimation parameters and decisions in MultiCluster/CCA). So, while I think the paper is of high practical value to me and the research community (improved QVEC measure, web portal), I frankly haven't learned that much from reading it, i.e. in terms of research questions addressed and answered.\nBelow are some more concrete remarks.\nIt would make sense to include the correlation results (Table 1) for monolingual QVEC and QVEC-CCA as well. After all, it is stated in l.326--328 that the proposed QVEC-CCA is an improvement over QVEC.\nMinor: l. 304: \"a combination of several cross-lingual word similarity datasets\" -> this sounds as though they are of different nature, whereas they are really of the same kind, just different languages, right?\np. 3: two equations exceed the column margin Lines 121 and 147 only mention Coulmance et al and Guo et al when referring to the MultiSkip baseline, but section 2.3 then only mentions Luong et al. So, what's the correspondence between these works?\nWhile I think the paper does reasonable justice in citing the related works, there are more that are relevant and could be included: Multilingual embeddings and clustering: Chandar A P, S., Lauly, S., Larochelle, H., Khapra, M. M., Ravindran, B., Raykar, V. C., and Saha, A. (2014). An autoencoder approach to learning bilingual word representations. In NIPS. \nHill, F., Cho, K., Jean, S., Devin, C., and Bengio, Y. (2014). Embedding word similarity with neural machine translation. arXiv preprint arXiv:1412.6448. \nLu, A., Wang, W., Bansal, M., Gimpel, K., & Livescu, K. (2015). Deep multilingual correlation for improved word embeddings. In NAACL. \nFaruqui, M., & Dyer, C. (2013). An Information Theoretic Approach to Bilingual Word Clustering. In ACL.\nMultilingual training of embeddings for the sake of better source-language embeddings: Suster, S., Titov, I., and van Noord, G. (2016). Bilingual learning of multi-sense embeddings with discrete autoencoders. In NAACL-HLT. \nGuo, J., Che, W., Wang, H., and Liu, T. (2014). Learning sense-specific word embeddings by exploiting bilingual resources. In COLING.\nMore broadly, translational context has been explored e.g. in Diab, M., & Resnik, P. (2002). An unsupervised method for word sense tagging using parallel corpora. In ACL. ", "label": [[580, 669, "Eval_pos_1"], [670, 752, "Eval_pos_2"], [753, 813, "Eval_pos_3"], [814, 932, "Major_claim"]]}
{"id": 516, "review": "paper_summary\nThis paper takes advantage of data (utterances, annotations) that is produced as part of language documentation to use for automatic linguistic segmentation, mainly at the word level. The authors worked with two languages that are still being documented Mboshi and Japhug. Both languages have data available but at different levels of granularity. They used Bayesian non-parametric approaches with different variations of the n-gram models and data used in addition to an incremental data training approach. The evaluation is reported in terms of F1 on three different segmentation levels (morphological boundary level, token level, and type-level). The discussion points out that the weakly supervised performs better than fully supervised and of course better than unsupervised. On the other hand, morpheme-based segmentation benefits more from underserviced training. \nsummary_of_strengths\n- The paper is well written and easy to follow.\n-The languages that were targeted are still being documented and they resemble a real low-resource setting.\n-The core approach is easy to follow and replicate. It lends itself to being easily explainable in terms of behavior and performance.\n-This effort can be helpful as an enabling technology for the language documentation process. \nsummary_of_weaknesses\nThere are no major weaknesses in this version of the paper. \nThe weaknesses mentioned in the previous review have been addressed in the current version for the most part. \ncomments,_suggestions_and_typos\nI appreciate the authors taking the time to improve upon the paper. I believe this work would definitely facilitate the documentation process whether in progress or as a post-processing step. It would be really useful for a future version of this work to include a use case study on using this process and reporting how useful it was. This would definitely increase the value of this work among both NLP and language documentation communities. ", "label": [[909, 954, "Eval_pos_1"], [1064, 1114, "Eval_pos_2"], [1115, 1196, "Jus_pos_2"], [1198, 1291, "Eval_pos_3"], [1314, 1374, "Major_claim"], [1587, 1962, "Major_claim"]]}
{"id": 518, "review": "In this paper the authors present a method for training a zero-resource NMT system by using training data from a pivot language. Unlike other approaches (mostly inspired in SMT), the author\u2019s approach doesn\u2019t do two-step decoding. Instead, they use a teacher/student framework, where the teacher network is trained using the pivot-target language pairs, and the student network is trained using the source-pivot data and the teacher network predictions of the target language.\n- Strengths: The results the authors present, show that their idea is promising. Also, the authors present several sets of results that validate their assumptions.\n- Weaknesses: However, there are many points that need to be address before this paper is ready for publication.\n1)            Crucial information is missing Can you flesh out more clearly how training and decoding happen in your training framework? I found out that the equations do not completely describe the approach. It might be useful to use a couple of examples to make your approach clearer.\nAlso, how is the montecarlo sampling done?  2)            Organization The paper is not very well organized. For example, results are broken into several subsections, while they\u2019d better be presented together. \u00a0The organization of the tables is very confusing. Table 7 is referred before table 6. This made it difficult to read the results.\n3)            Inconclusive results: After reading the results section, it\u2019s difficult to draw conclusions when, as the authors point out in their comparisons, this can be explained by the total size of the corpus involved in their methods (621 \u00a0).  4)            Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.  - General Discussion: Other: 578: \u00a0We observe that word-level models tend to have lower valid loss compared with sentence- level methods\u2026. \nIs it valid to compare the loss from two different loss functions?\nSec 3.2, the notations are not clear. What does script(Y) means? \nHow do we get p(y|x)? this is never explained Eq 7 deserves some explanation, or better removed. \n320: What approach did you use? You should talk about that here 392 : Do you mean 2016?\nNitty-gritty: 742\u00a0 : import => important 772 \u00a0: inline citation style 778: can significantly outperform  275: Assumption 2 needs to be rewritten \u2026 a target sentence y from x should be close to that from its counterpart z. ", "label": [[490, 557, "Eval_pos_1"], [655, 753, "Major_claim"], [768, 798, "Eval_neg_1"], [799, 1040, "Jus_neg_1"], [1112, 1149, "Eval_neg_2"], [1150, 1251, "Jus_neg_2"], [1252, 1301, "Eval_neg_3"], [1302, 1381, "Jus_neg_3"], [1645, 1670, "Eval_neg_4"], [1672, 1824, "Jus_neg_4"]]}
{"id": 519, "review": "- Strengths: The paper addresses a long standing problem concerning automatic evaluation of the output of generation/translation systems.\nThe analysis of all the available metrics is thorough and comprehensive.\nThe authors demonstrate a new metric with a higher correlation with human judgements The bibliography will help new entrants into the field.\n- Weaknesses: The paper is written as a numerical analysis paper, with very little insights to linguistic issues in generation, the method of generation, the differences in the output from a different systems and human generated reference.\nIt is unclear if the crowd source generated references serve well in the context of an application that needs language generation.\n- General Discussion: Overall, the paper could use some linguistic examples (and a description of the different systems) at the risk of dropping a few tables to help the reader with intuitions. ", "label": [[138, 210, "Eval_pos_1"], [592, 722, "Eval_neg_1"]]}
{"id": 520, "review": "paper_summary\nThis paper proposes a smoothing regularization technique for NER. The technique proposed targets boundary smoothing where the authors claim that inconsistent boundaries are often seen as a problem in annotated NLP NER datasets. The results after applying the method shows less over-confidence, better model calibration, flatter neural minima and more smoothed loss landscapes which plausibly explain performance improvement rather than directly/only addressing the inconsistent boundary span labeling problem in NER. The result attributes are empirically verified. \nsummary_of_strengths\n1. After 95% performance ranges, bringing improvements to any problem systematically is certainly a challenging dilemma. In this regard, the work proposed in this paper is already notable.\n2. The problem of inconsistent span annotations identified in this paper indeed can also be intuited as an NLP annotation problem hence is a valid one that needs to be addressed. The solution to regularize this problem with smoothing technique for span-based NER method is sound and well justified throughout the paper. The reader would benefit from reading the theory and methods in this paper.\n3. The code is publicly released.\n4. The empirical analysis is well performed. I especially appreciate section 5.2 which directly tries to shed light on the question that formed the problem premise of this work and I quote \"How does boundary smoothing improve the model performance?\" It offers a slight indication of a negative result as in not being able to quantitatively verify it owing to irregularities in distribution between synthesized boundary noise and actual noise in the datasets. Nevertheless, it shows another promising result with smoothed loss landscapes indicating sound machine learning settings.\n5. The fact that boundary smoothing addresses the over-confidence issue of target span predictions is quite meaningful, hence again a plus for this paper. \nsummary_of_weaknesses\nNot quite a weakness of this work, just a question I am wondering about. Considering that the boundary smoothing regularization distributes itself around the entity span, is it that the actual problem of whether such a function addresses the boundary irregularity problem in the NLP datasets can never be verified? It is indeed understood from the method explanation in the paper that it can to some extent. \ncomments,_suggestions_and_typos\nTable 5. Note the caption. There are no results for LS in the table. ", "label": [[604, 721, "Jus_pos_1"], [722, 789, "Eval_pos_1"], [969, 1109, "Eval_pos_2"], [1110, 1185, "Eval_pos_3"], [1223, 1264, "Eval_pos_4"], [1804, 1956, "Eval_pos_5"]]}
{"id": 521, "review": "paper_summary\nThe paper proposes to trace an agenda for cross-cultural NLP, first by identifying four axes of variation for cultures (linguistic form and style, common ground, aboutness, objectives and values) and describing the challenges related to them; then by highlighting possible strategies for mitigating cultural biases in modern NLP in the areas of data collection, model training and translation.\nI am not an expert on the issues under discussion in this paper, so it is not easy for me to provide a balanced evaluation. I enjoyed reading it, especially for the attempt of brining awareness on cultural issues, which are often neglected in current NLP practices. I think the authors did a very extensive and comprehensive work on the bibliography, which could be useful for correcting biases.\nOn the other hand, I feel that is often difficult to disentangle between linguistic and cultural dimension, and in many cases  the problems/solutions are overlapping with the typical low-resource language scenarios (e.g. where a particular language/dialect/sociolect can be associated with a specific cultural group), and in such cases the general heuristic could be described as \"the more cultural-specific data we can acquire, the better\"; while in other cases it is difficult to propose any conclusive solution (e.g. see the case for model training in Section 6.2, where methods balancing for bias would require access to demographic attributes, but the problem would persist because one cannot really be sure that such attributes adequately reflect culture).  I guess this is a general problematic when one has to deal with something like culture, which is notoriously difficult to define. \nAt the end of my reading, I still feel that I did not get what are, specifically, the directions indicated by the authors (this might well be a limitation of mine) ... \nsummary_of_strengths\nThe topic is highly relevant for the special theme of this year, and the issue of cross-cultural NLP is a new and interesting perspective. \nThe survey takes into account a wide body of references that might be useful for NLP researchers. \nsummary_of_weaknesses\nThe difficulty in defining culture and in disentangling between linguistic and cultural dimension make it also difficult, in many of the cases exemplified by the authors, to trace a clear roadmap for a cross-cultural NLP. \ncomments,_suggestions_and_typos\nl. 141 varies --> vary l. 466 datasts --> datasets ", "label": [[532, 552, "Eval_pos_1"], [554, 673, "Jus_pos_1"], [674, 803, "Eval_pos_2"], [1958, 2028, "Eval_pos_3"], [2029, 2127, "Eval_pos_4"]]}
{"id": 522, "review": "paper_summary\nThis paper contributes two experiments to investigate how annotator\u2019s backgrounds and sociopolitical attitudes affect their perception of toxic language. The paper finds that having certain characteristics is correlated with perceptions towards racism, for example, believing in freedom of speech, predisposes one to be more lax in annotating anti-Black toxicity. The paper also finds that the widely used toxicity detection tool, Perspective API, mimics the conservative attitudinal profile when it comes anti-Black toxicity. \nsummary_of_strengths\n- Important and valuable work -Going beyond demographic characteristics of annotators, but also including sociopolitical attitudes -Carefully thought-out experiments -Detailed description of some metrics and results -Well-described and contextualized implications of disregarding annotator background and beliefs when creating \nsummary_of_weaknesses\n- Underdefined and conflation of concepts -Several important details missing -Lack of clarity in how datasets were curated prevents one from assessing their validity -Too many results which are not fully justified or explained \ncomments,_suggestions_and_typos\nThis is a very important, interesting, and valuable paper with many positives. First and foremost, annotators\u2019 backgrounds are an important factor and should be taken into consideration when designing datasets for hate speech, toxicity, or related phenomena. The paper not only accounts for demographic variables as done in previous work but other attitudinal covariates like attitude towards free speech that are well-chosen. The paper presents two well-thought out experiments and presents results in a clear manner which contain several important findings.\nIt is precisely because of the great potential and impact of this paper, I think the current manuscript requires more consideration and fine-tuning before it can reach its final stage. At this point, there seems to be a lack of important details that prevent me from fully gauging the paper\u2019s findings and claims. Generally: - There were too many missing details (for example, what is the distribution of people with \u2018free off speech\u2019 attitudes? What is the correlation of the chosen scale item in the breadth-of-posts study?). On a minor note, many important points are relegated to the appendix.\n-Certain researcher choices and experiment design choices were not justified (for example, why were these particular scales used?)\n-The explanation of the creation of the breadth-of-posts was confusing. How accurate was the classification of AAE dialect and vulgarity?  -The toxicity experiment was intriguing but there was too little space to be meaningful.\nMore concretely, - With regard to terminology and concepts, toxicity and hate speech may be related but are not the same thing. The instructions to the annotators seem to conflate both. The paper also doesn\u2019t present a concrete definition of either. While it might seem redundant or trivial, the wording to annotators plays an important role and can confound the results presented here.\n-Why were the particular scales chosen for obtaining attitudes? Particularly, for empathy there are several scale items [1], so why choose the Interpersonal Reactivity Index?\n-What was the distribution of the annotator\u2019s background with respect to the attitudes? For example, if there are too few \u2018free of speech\u2019 annotators, then the results shown in Table 3, 4, etc are underpowered.  -What were the correlations of the chosen attitudinal scale item for the breadth-of-posts study with the toxicity in the breadth-of-workers study?\n-How accurate are the automated classification in the breadth-of-posts experiment, i.e., how well does the states technique differentiate identity vs non-identity vulgarity or AAE language for that particular dataset. Particularly, how can it be ascertained whether the n-word was used as a reclaimed slur or not?  -In that line, Section 6 discusses perceptions of vulgarity, but there are too many confounds here. Using b*tch in a sentence can be an indication of vulgarity and toxicity (due to sexism).\n-In my opinion, the perspective API experiment was interesting but rather shallow. My suggestion would be to follow up on it in more detail in a new paper rather than include it in this one. The newly created space could be used to enter the missing details mentioned in the review.   -Finally, given that the paper notes that MTurk tends to be predominantly liberal and the authors (commendably) took several steps to ensure greater participation from conservatives, I was wondering if \u2018typical\u2019 hate speech datasets are annotated by more homogenous annotators compared to the sample in this paper. What could be the implications of this? Do this paper's findings then hold for existing hate speech datasets?\nBesides these, I also note some ethical issues in the \u2018Ethical Concerns\u2019 section. To conclude, while my rating might seem quite harsh, I believe this work has great potential and I hope to see it enriched with the required experimental details.\nReferences: [1] Gerdes, Karen E., Cynthia A. Lietz, and Elizabeth A. Segal. \" Measuring empathy in the 21st century: Development of an empathy index rooted in social cognitive neuroscience and social justice.\" Social Work Research 35, no. 2 (2011): 83-93. ", "label": [[565, 592, "Eval_pos_1"], [594, 694, "Eval_pos_2"], [695, 728, "Eval_pos_3"], [730, 778, "Eval_pos_4"], [780, 890, "Eval_pos_5"], [915, 954, "Eval_neg_1"], [956, 989, "Eval_neg_2"], [991, 1078, "Eval_neg_3"], [1080, 1140, "Eval_neg_4"], [1173, 1251, "Eval_pos_6"], [1252, 1732, "Jus_pos_6"], [1733, 1917, "Eval_neg_5"], [1918, 2046, "Jus_neg_5"], [2060, 2095, "Eval_neg_6"], [2096, 2330, "Jus_neg_6"], [2332, 2407, "Eval_neg_7"], [2409, 2460, "Jus_neg_7"], [2463, 2533, "Eval_neg_8"], [2534, 2599, "Jus_neg_8"], [2602, 2689, "Eval_neg_9"], [2690, 3076, "Jus_neg_9"], [4117, 4198, "Eval_neg_10"], [4199, 4398, "Jus_neg_10"], [4908, 5070, "Major_claim"]]}
{"id": 523, "review": "paper_summary\nThis paper proposes a data-efficient end-to-end event extraction method by designing event-specific generation templates. \nThese event-specific templates describe the event types in a natural language way. \nThe proposed method can use pre-trained language models and exploit label semantics from event type definitions. \nExperimental results show that DEGREE has better performance on low resource event extraction and can be compared with SOTA under the full supervised setting. \nsummary_of_strengths\nThis paper designs an event-specific template for generative event extraction. \nCompared with the previous methods, the proposed method is more consistent with natural language generation and achieves better performance in the low-resource setting. \nsummary_of_weaknesses\n- The proposed framework is based on the previous template-based information extraction method. \nThe template-based method's main drawback is the significantly increased training/inference cost (corresponding to the number of event types). The increase in the category of events (such as MAVEN with 100+ types) will exacerbate the problem.\n- For extreme low-resource settings (1%), different data sampling may heavily affect the experimental results. \nIt is better to conduct experiments multi-times on different subset samples. \ncomments,_suggestions_and_typos\nSome detailed questions about model inference: -Multi-events: Can DEGREE and DEGREE(ED) deal with multiple events of the same type in the same sentence? Although we can predict the same input sentence for different types one-by-one if a sentence contains multiple events of the same type, the same input (template and sentence) will correspond to various outputs. For example, some sentences have two death events. This is not a problem for other generation methods because they are trigger-driven (TANL, BART-GEN), or generate all extracted events in one step (Text2Event).\n-Converting generated results to events: Are there some generated results that cannot be parsed into events? Generation methods are more uncontrollable than span extraction and span classification methods for event extraction. For example, the generated span maybe not appear in the input sentence.\nThere are many template methods for information extraction. \nIt is suggested that authors should cite these articles: -Template-Based Named Entity Recognition Using BART. Findings of ACL_IJCNLP 2021 -Reading the Manual: Event Extraction as Definition Comprehension. spnlp@EMNLP 2020 ", "label": []}
{"id": 524, "review": "paper_summary\nBelow is a copy from the previous review: >In this paper the authors proposed a new fairness metric, accumulated prediction sensitivity. The authors formulate the metric and establish its properties in relationship with group fairness and individual fairness. Interestingly, the authors measure the correlation of the proposed metric with a human judgment of fairness. Since the proposed metric requires a choice of the way of computing two hyperparameter vectors, the authors experiment with different choices and show that this choice matters quite a lot. \nsummary_of_strengths\nBelow is a copy from the previous review: > - Important direction of research > - Relatively good correlation with human judgment > - The authors evaluated several choices of the metric > - Intuitive formulation of the metric > - Clearly written and easy to follow paper \nsummary_of_weaknesses\nBelow is a copy from the previous review: > - Need of choice of the two hyperparameters vectors w and v > - This choice, as evident from the authors experiments, is very important > - Process of obtaining v can be quite involved > - The proposed metric works only or gradient-based models > - Although the formulation is intuitive, the metric values themselves can be hard to interpret. \ncomments,_suggestions_and_typos\nThe authors addressed most of the reviewers' comments made for the previous submission. This is an important direction of research and I believe in the current state the paper can be accepted for publication. ", "label": [[640, 671, "Eval_pos_1"], [728, 779, "Eval_pos_2"], [784, 819, "Eval_pos_3"], [824, 865, "Eval_pos_4"], [1181, 1275, "Eval_neg_1"], [1396, 1517, "Major_claim"]]}
{"id": 525, "review": "paper_summary\nThis paper deals with the temporal misalignment problem, which occurs when an NLP model is trained on a dataset created from data of a certain time period and tested/used for data of another time period. \nThis paper discusses the temporal misalignment problem through experiments on a number of NLP tasks with temporal misalignment. For that, TD, a metric for temporal degradation (of the task performance), is defined and used. \nThe paper presents some findings about the temporal misalignment problem, such as answers to \"how does sensitivity to temporal misalignment vary with text domain and task?\" \nThe paper brings NLP community's attention to the temporal misalignment problem. \nsummary_of_strengths\nFirstly, the paper is beneficial to the NLP community, since it will bring NLP community's attention to the temporal misalignment problem. This is particularly important because currently people in this community extensively use the pretraining-DAPT-finetuning paradigm. \nThe arguments in the paper are supported by comprehensive experiments with a number of tasks. \nThe paper is well-written and well-organized. \nsummary_of_weaknesses\nThe problem caused by difference in training data and test data has been studied, although they might not focus on \"temporal\" aspect. \nThere are a number of papers, where terms like \"covariate shift\", labeling adaptation, and instance adaptation (e.g., Jiang and Zhai (2007) and Shimodaira (2000). See below). It would be interesting to see the connection between the arguments of the current paper and such papers.\n-Jiang and Zhai. Instance Weighting for Domain Adaptation in NLP. ACL. 2007.\n-Hidetoshi Shimodaira. 2000. Improving predictive inference under covariate shift by weighting the loglikelihood function. Journal of Statistical Planning and Inference, 90:227\u2013244.\nTD score is defined in Section 2.3, but there is no intuition or justification about why it is defined like this, at least in the same section. Descriptions about intuition can be found in the later sections, so this is simply a matter of organization of the paper.  It is not clear whether TD scores for different performance metrics can be compared with each other, across different tasks. There should be a discussion on the justification (or maybe limitation).\nSome specific examples of temporal misalignment would help readers understand the paper, if they (the examples) are presented in Introduction. \ncomments,_suggestions_and_typos\nD in the text body should be in italic, consistently. ", "label": [[721, 774, "Eval_pos_1"], [776, 992, "Jus_pos_1"], [993, 1087, "Eval_pos_2"], [1088, 1134, "Eval_pos_3"], [1832, 1976, "Eval_neg_1"], [1976, 2097, "Jus_neg_1"], [2100, 2223, "Eval_neg_2"], [2225, 2296, "Jus_neg_2"]]}
{"id": 526, "review": "- First line of page 2, this bit does not seem very fluent, maybe something is missing, please check and amend if/as needed: \"... enabling us to easily create large number of coherent test examples...\" - I am dubious about the orthodoxy of using the image with the pussycat's face in the title as well as to replace the name of the project/system in the running text of the paper: while it may look cute (to some), I suspect that other readers (like this innovation-averse reviewer) may find it annoying, if not outright inappropriate. If the paper is accepted (as incidentally I think it should be), I think that the author(s) would be well advised to consult with the conference chairs and proceedings editors to check that this rather unusual practice is deemed acceptable and technically feasible for the proceedings.\n- Similar to the previous point, I wonder if the deliberately eccentric heading of the very short and merely descriptive section 3 (i.e. \"Do Androids Dream of Coreference Translation Pipelines?\", which consists of merely 14 lines) is entirely justified and appropriate for a paper to be published in conference proceedings, although I accept that to some extent these are matters of personal taste.\n- Possible typo on page 2, please check and edit as required: \"follows this work, but _create_ the challenge set in an automatic way.\" >>> should it be 'creates'?\n- The last paragraph of Section 3 on page 3 seems to include some typos such as the following, so please check and amend the passage as appropriate: \"The coreference steps resembles\" (one of the last two words is incorrect), \"the rules-based approach\" ('rule-based'?), \" each of these phenomenon\" ('phenomena'?).\n- The caption of Figure 1 on page 4 seems too (and unnecessarily) long: a caption is a caption; data analysis and comments should be given in the running text of the paper, referring to the figure whose data is being illustrated and discussed.\n- Line 3 on page 5: \"This _filters_ our subset to 4,580 modified examples.\" >>> For clarity, should 'filters' be 'reduced' (or something similar) instead? ", "label": [[2, 123, "Eval_neg_1"], [127, 200, "Jus_neg_1"], [204, 379, "Eval_neg_2"], [381, 821, "Jus_neg_2"], [823, 1220, "Eval_neg_3"]]}
{"id": 527, "review": "This paper introduces the task of automatic \"pull quote selection\" from text. The task is to identify one or more spans of text pulled from an article and presented in a salient manner to the reader within the article, with the goal of engaging the reader and to provide emphasis on interesting aspects of the article. The authors introduce a new dataset for this research, and explore a variety of approaches to automatically perform this task. Using these approaches (ranging from hand-crafted feature based methods to mixture of experts), the authors provide interesting insights into properties of text that make for good pull pull-quotes.\nAt a high level, there are two key aspects of the paper worth mentioning first. The approach taken by the paper to analyze the novel data/task, and provide insights is extremely well done. However, the the paper also is dealing with the challenge of a formal definition for this task, which it does not quite achieve eventually.  The dataset is constructed from pull-quotes identified in existing well known publications (presumably, created by the editors of those publications). As such, there may or may not be a consistency among the strategy taken by these publications in selecting these pull quotes. Additionally, each may have a different goal/objective/motivation in selecting a particular span as a pull-quote. Because this paper only defines the task in terms of what is observed in these publications, and does not go beyond using these existing articles + pull-quotes in its definition of the task, it would be hard for someone to manually construct a dataset for this task by hand (with human annotators). This appears to be a fundamental weakness in this work. What would be human annotation guidelines for such data set creation? How would one assess agreement if one were to create such data with human experts?\nTherefore, it appears that the actual task taken on by this paper is that of learning the latent decisions behind the pull-quote identification of *these particular* publications.\nHaving said that, the approach and analysis undertaken by the paper is very insightful. While the task can be construed as learning to extract pull-quotes in a manner similar to that of these selected publications, the methodical approach taken in the paper is commendable. It was enjoyable to see the paper build from hand-crafted features used in a traditional ML classifier to more recent deep learning models with character and word-based features, to cross-task of approaches used in similar tasks (headline, clickbait, summarization).\nThe observations and conclusions from the experiments are perceptive, and readers of the paper would certainly learn about interesting linguistic characteristics that are useful in identifying noteworthy sentences in any given text.\nIt was great to see the human evaluation in Section 5.5 of the paper. This really helped to see the impact of the pull-quotes on human readers. It would have been neat to see such an analysis of the data as part of the task definition early on... to perhaps help more clearly define what a human reader (or a human writer) is expecting to highlight as quotable. ( a.k.a., crowd-sourcing pull-quote extraction?) ", "label": [[446, 643, "Eval_pos_1"], [724, 832, "Eval_pos_2"], [833, 972, "Eval_neg_1"], [974, 2052, "Jus_neg_1"], [2053, 2139, "Eval_pos_3"], [2141, 2326, "Eval_pos_4"], [2327, 2593, "Eval_pos_5"], [2594, 2662, "Eval_pos_6"], [2668, 2826, "Eval_pos_7"], [2827, 2896, "Eval_pos_8"], [2897, 3238, "Jus_pos_8"]]}
{"id": 528, "review": "This paper performs an overdue circling-back to the problem of joint semantic and syntactic dependency parsing, applying the recent insights from neural network models. Joint models are one of the most promising things about the success of transition-based neural network parsers.\nThere are two contributions here. First, the authors present a new transition system, that seems better than the Hendersen (2008) system it is based on. The other contribution is to show that the neural network succeeds on this problem, where linear models had previously struggled. The authors attribute this success to the ability of the neural network to automatically learn which features to extract. However, I think there's another advantage to the neural network here, that might be worth mentioning. In a linear model, you need to learn a weight for each feature/class pair. This means that if you jointly learn two problems, you have to learn many more parameters. The neural network is much more economical in this respect.\nI suspect the transition-system would work just as well with a variety of other neural network models, e.g. the global beam-search model of Andor (2016). There are many other orthogonal improvements that could be made. I expect extensions to the authors' method to produce state-of-the-art results.\nIt would be nice to see an attempt to derive a dynamic oracle for this transition system, even if it's only in an appendix or in follow-up work. At first glance, it seems similar to the arc-eager oracle. The M-S action excludes all semantic arcs between the word at the start of the buffer and the words on the semantic stack, and the M-D action excludes all semantic arcs between the word at the top of the stack and the words in the buffer. The L and R actions seem to each exclude the reverse arc, and no other. ", "label": [[1015, 1116, "Eval_neg_1"], [1118, 1169, "Jus_neg_1"]]}
{"id": 530, "review": "The authors use self-training to train a seq2seq-based AMR parser using a small annotated corpus and large amounts of unlabeled data. They then train a similar, seq2seq-based AMR-to-text generator using the annotated corpus and automatic AMRs produced by their parser from the unlabeled data. They use careful delexicalization for named entities in both tasks to avoid data sparsity. This is the first sucessful application of seq2seq models to AMR parsing and generation, and for generation, it most probably improves upon state-of-the art.\nIn general, I really liked the approach as well as the experiments and the final performance analysis. \nThe methods used are not revolutionary, but they are cleverly combined to achieve practial results. \nThe description of the approach is quite detailed, and I believe that it is possible to reproduce the experiments without significant problems. \nThe approach still requires some handcrafting, but I believe that this can be overcome in the future and that the authors are taking a good direction.\n(RESOLVED BY AUTHORS' RESPONSE) However, I have been made aware by another reviewer of a data overlap in the Gigaword and the Semeval 2016 dataset. This is potentially a very serious problem -- if there is a significant overlap in the test set, this would invalidate the results for generation (which are the main achievemnt of the paper). Unless the authors made sure that no test set sentences made their way to training through Gigaword, I cannot accept their results.\n(RESOLVED BY AUTHORS' RESPONSE)  Another question raised by another reviewer, which I fully agree with, is the  5.4 point claim when comparing to a system tested on an earlier version of the AMR dataset. The paper could probably still claim improvement over state-of-the art, but I am not sure I can accept the 5.4 points claim in a direct comparison to Pourdamghani et al. -- why haven't the authors also tested their system on the older dataset version (or obtained Pourdamghani et al.'s scores for the newer version)?\nOtherwise I just have two minor comments to experiments:  - Statistical significance tests would be advisable (even if the performance difference is very big for generation).\n- The linearization order experiment should be repeated with several times with different random seeds to overcome the bias of the particular random order chosen.\nThe form of the paper definitely could be improved. \nThe paper is very dense at some points and proofreading by an independent person (preferably an English native speaker) would be advisable. \nThe model (especially the improvements over Luong et al., 2015) could be explained in more detail; consider adding a figure. The experiment description is missing the vocabulary size used. \nMost importantly, I missed a formal conclusion very much -- the paper ends abruptly after qualitative results are described, and it doesn't give a final overview of the work or future work notes.\nMinor factual notes: - Make it clear that you use the JAMR aligner, not the whole parser (at 361-364). Also, do you not use the recorded mappings also when testing the parser (366-367)?\n- Your non-Gigaword model only improves on other seq2seq models by 3.5 F1 points, not 5.4 (at 578).\n- \"voters\" in Figure 1 should be \"person :ARG0-of vote-01\" in AMR.\nMinor writing notes: - Try rewording and simplifying text near 131-133, 188-190, 280-289, 382-385, 650-659, 683, 694-695.\n- Inter-sentitial punctuation is sometimes confusing and does not correspond to my experience with English syntax. There are lots of excessive as well as missing commas.\n- There are a few typos (e.g., 375, 615), some footnotes are missing full stops.\n- The linearization description is redundant at 429-433 and could just refer to Sect. 3.3.\n- When refering to the algorithm or figures (e.g., near 529, 538, 621-623), enclose the references in brackets rather than commas.\n- I think it would be nice to provide a reference for AMR itself and for the multi-BLEU script.\n- Also mention that you remove AMR variables in Footnote 3.\n- Consider renaming Sect. 7 to \"Linearization Evaluation\".\n- The order in Tables 1 and 2 seems a bit confusing to me, especially when your systems are not explicitly marked (I would expect your systems at the bottom). \nAlso, Table 1 apparently lists development set scores even though its description says otherwise.\n- The labels in Table 3 are a bit confusing (when you read the table before reading the text).\n- In Figure 2, it's not entirely visible that you distinguish month names from month numbers, as you state at 376.\n- Bibliography lacks proper capitalization in paper titles, abbreviations and proper names should be capitalized (use curly braces to prevent BibTeX from lowercasing everything).\n- The \"Peng and Xue, 2017\" citation is listed improperly, there are actually four authors.\n*** Summary: The paper presents first competitive results for neural AMR parsing and probably new state-of-the-art for AMR generation, using seq2seq models with clever preprocessing and exploiting large a unlabelled corpus. Even though revisions to the text are advisable, I liked the paper and would like to see it at the conference.  (RESOLVED BY AUTHORS' RESPONSE) However, I am not sure if the comparison with previous state-of-the-art on generation is entirely sound, and most importantly, whether the good results are not actually caused by data overlap of Gigaword (additional training set) with the test set.\n*** Comments after the authors' response: I thank the authors for addressing both of the major problems I had with the paper. I am happy with their explanation, and I raised my scores assuming that the authors will reflect our discussion in the final paper. ", "label": [[542, 645, "Eval_pos_1"], [646, 746, "Eval_pos_2"], [747, 797, "Eval_pos_3"], [801, 891, "Eval_pos_4"], [892, 1042, "Eval_pos_5"], [1075, 1190, "Jus_neg_1"], [1191, 1234, "Eval_neg_1"], [1237, 1514, "Jus_neg_1"], [1548, 1617, "Eval_neg_2"], [1619, 2035, "Jus_neg_2"], [5079, 5189, "Major_claim"]]}
{"id": 531, "review": "- Strengths: The paper makes several novel contributions to (transition-based) dependency parsing by extending the notion of non-monotonic transition systems and dynamic oracles to unrestricted non-projective dependency parsing. The theoretical and algorithmic analysis is clear and insightful, and the paper is admirably clear.\n- Weaknesses: Given that the main motivation for using Covington's algorithm is to be able to recover non-projective arcs, an empirical error analysis focusing on non-projective structures would have further strengthened the paper. And even though the main contributions of the paper are on the theoretical side, it would have been relevant to include a comparison to the state of the art on the CoNLL data sets and not only to the monotonic baseline version of the same parser.\n- General Discussion: The paper extends the transition-based formulation of Covington's dependency parsing algorithm (for unrestricted non-projective structures) by allowing non-monotonicity in the sense that later transitions can change structure built by earlier transitions. In addition, it shows how approximate dynamic oracles can be formulated for the new system. Finally, it shows experimentally that the oracles provide a tight approximation and that the non-monotonic system leads to improved parsing accuracy over its monotonic counterpart for the majority of the languages included in the study.\nThe theoretical contributions are in my view significant enough to merit publication, but I also think the paper could be strengthened on the empirical side. In particular, it would be relevant to investigate, in an error analysis, whether the non-monotonic system improves accuracy specifically on non-projective structures. Such an analysis can be motivated on two grounds: (i) the ability to recover non-projective structures is the main motivation for using Covington's algorithm in the first place; (ii) non-projective structures often involved long-distance dependencies that are hard to predict for a greedy transition-based parser, so it is plausible that the new system would improve the situation.  Another point worth discussion is how the empirical results relate to the state of the art in light of recent improvements thanks to word embeddings and neural network techniques. For example, the non-monotonicity is claimed to mitigate the error propagation typical of classical greedy transition-based parsers. But another way of mitigating this problem is to use recurrent neural networks as preprocessors to the parser in order to capture more of the global sentence context in word representations. Are these two techniques competing or complementary? A full investigation of these issues is clearly outside the scope of the paper, but some discussion would be highly relevant.\nSpecific questions: Why were only 9 out of the 13 data sets from the CoNLL-X shared task used? I am sure there is a legitimate reason and stating it explicitly may prevent readers from becoming suspicious.  Do you have any hypothesis about why accuracy decreases for Basque with the non-monotonic system? Similar (but weaker) trends can be seen also for Turkish, Catalan, Hungarian and (perhaps) German.\nHow do your results compare to the state of the art on these data sets? This is relevant for contextualising your results and allowing readers to estimate the significance of your improvements.\nAuthor response: I am satisfied with the author's response and see no reason to change my previous review. ", "label": [[13, 97, "Eval_pos_1"], [98, 228, "Jus_pos_1"], [229, 294, "Eval_pos_2"], [299, 328, "Eval_pos_3"], [1415, 1572, "Major_claim"]]}
{"id": 532, "review": "paper_summary\nThe authors of this paper propose an empirical study for the zero-shot capabilities of CLIP models and demonstrate that CLIP models have strong few-shot capabilities. The authors propose the TAP-C method for evaluating VQA and demonstrate zero-shot cross-modal transfer capabilities on the visual entailment. \nsummary_of_strengths\n1. The paper conducts rich experiments and presents several interesting insights, which have a certain value to the community. \n2. The paper is well-written and easy to follow. \n3. The proposed two-step prompt generation method is interesting for studying the zero-shot performance. \nsummary_of_weaknesses\n1. There should be more VLU tasks to prove the argument of the paper, e.g., NLVR. \ncomments,_suggestions_and_typos\n1. It is best to use vector graphics in the paper. ", "label": [[348, 472, "Eval_pos_1"], [476, 522, "Eval_pos_2"], [526, 628, "Eval_pos_3"]]}
{"id": 533, "review": "Review: Multimodal Word Distributions - Strengths:  Overall a very strong paper.\n- Weaknesses: The comparison against similar approaches could be extended.\n- General Discussion: The main focus of this paper is the introduction of a new model for learning multimodal word distributions formed from Gaussian mixtures for multiple word meanings. i. e. representing a word by a set of many Gaussian distributions. \nThe approach, extend the model introduced by Vilnis and McCallum (2014) which represented word as unimodal Gaussian distribution. By using a multimodal, the current approach attain the problem of polysemy.\nOverall, a very strong paper, well structured and clear. The experimentation is correct and the qualitative analysis made in table 1 shows results as expected from the approach.  There\u2019s not much that can be faulted and all my comments below are meant to help the paper gain additional clarity.  Some comments:  _ It may be interesting to include a brief explanation of the differences between the approach from Tian et al. 2014 and the current one. Both split single word representation into multiple prototypes by using a mixture model.  _ There are some missing citations that could me mentioned in related work as : Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector Space Neelakantan, A., Shankar. J. Passos, A., McCallum. EMNLP 2014 Do Multi-Sense Embeddings Improve Natural Language Understanding? Li and Jurafsky, EMNLP 2015 Topical Word Embeddings. Liu Y., Liu Z., Chua T.,Sun M. AAAI 2015 _ Also, the inclusion of the result from those approaches in tables 3 and 4 could be interesting.  _ A question to the authors: What do you attribute the loss of performance of w2gm against w2g in the analysis of SWCS?\nI have read the response. ", "label": [[52, 80, "Major_claim"], [617, 673, "Major_claim"]]}
{"id": 534, "review": "This paper presents an exploratory work made on the evaluation of justified (or not) disagreement in the task of annotations. \nAs far as my skills on the subject allowed me to evaluate and understand, here are the positive, neutral and negative aspects I could identify.\n--- Positive aspects --- (1) The paper tackles a very interesting subject for NLP and especially for CL. It thus is a good match for COLING.\n(2) As far as I could tell, the paper respects all formal submission criteria (e.g. abstract size, anonymity).\n(3) The paper is written in a very pleasant English that allowing a smooth reading (even though I have some concern about the content structure).\n(3) I found the methodology and results sound and convincing. There was a number of choices made by the authors that felt very adequate (e.g. boolean crowdsourcing, Prolific etc.).\n(4) Even though the results are probably only preliminary, my impression is that the amount of work to obtain them seems substantial.\n--- Neutral aspects --- (1) the citations made inside parenthesis do not display inline as they should and have their own pair of parentheses.  (2) in Section 2 the authors said \"Despite the central nature of phenomena triggering disagreement in annotation tasks, we are not aware of evaluation methods that do not mainly rely on agreement\". I don't know any too. Nonetheless, if the authors haven't already checked, I would suggest to give a look at the publications within the hcomp community => https://www.humancomputation.com/ (3) If I got that right, in Section 6.2 I suppose that \" The unit-quality-score (uas)\" should be changed as \" The unit-quality-score (uqs)\".\n(4) In section 7.2, something went wrong with this sentence \". In contrast, a simple majority vote achieves an f1-score of 0.78 and the unit-annotation-score. \"\n(5) \"Most importantly, it would be highly valuable if the existing metrics could be combined in such a way that we could use them for the identification of different types of disagreements. \" => sounds like something a Machine Learning algorithm could be used for by using the metrics as features.\n--- Negative aspects --- (1) The paper has one major issue: whereas the overall subject is coherent and well defined, its focus is quite blurry and it has been hard to understand what is exactly the contribution of the authors. Indeed the authors tend to present things in a not-so-consistent fashion from one section to the other. Also, the paper seems to be focusing on studying disagreement between annotations, yet a consequent part of the contribution described is about a filtering method to discard incoherent annotations or annotators and  thus improve aggregation. Likewise, diagnostic datasets are often mentioned but nothing is done about them in the results... Another example is Table 2 => why go into such details if they don't contribute directly to the results or argumentation presented in this paper? All this details tend to confuse the reader in the end.\n(2) While the paper tackles a very interesting subject for CL, I was under the impression that it is written in a very NLP-oriented fashion. I believe COLING try to bridge (as much as possible) NLP and Linguistics and the authors should consider that for their final version.    (3) The authors should have provided examples in Section 3 and 6.3 to help the reader understanding.\n--- Conclusion --- I think that the work presented is a good piece of research and this paper is already nice (and can become even nicer). \nI would thus gladly recommend to accept it and hope that, if it gets accepted, the final version will have amended the shortcomings mentioned above. ", "label": [[300, 375, "Eval_pos_1"], [376, 411, "Major_claim"], [527, 668, "Eval_pos_2"], [672, 730, "Eval_pos_3"], [731, 849, "Jus_pos_3"], [854, 983, "Eval_pos_4"], [2145, 2343, "Eval_neg_1"], [2344, 2990, "Jus_neg_1"], [3389, 3660, "Major_claim"]]}
{"id": 535, "review": "This paper presents a text classification method based on pre-training technique using both labeled and unlabeled data. The authors reported experimental results with several benchmark data sets including TREC data, and showed that the method improved overall performance compared to other comparative methods.\nI think the approach using pre-training and fine-tuning itself is not a novel one, but the originality is the use of both labeled and unlabeled data in the pre-training step. \nThe authors compare their results against three baselines, i.e. without pre-training and a deep learning with unsupervised pre-training using deep autoencoders, but I think that I would be interesting to compare the method against other methods presented in the introduction section. ", "label": []}
{"id": 536, "review": "paper_summary\nThis paper explores the knowledge-grounded conversation generation task. Two latent variables are employed for the controlling of types and boundaries for segments during generation, after an auxiliary task to classify segments into knowledge-relevant or knowledge-irrelevant. Such a strategy is fused into a pretrained encoder-decoder architecture, and realizes superior performance than strong baselines on two benchmark datasets. \nAlthough this paper is well-motivated, I do not see many significant revisions for the previous comments. \nsummary_of_strengths\n1. The motivation of this paper is solid, I believe the segmentation based on the relation of text pieces between knowledge is reasonable to solve knowledge-grounded dialogue generation tasks.\n2. The proposed model is basically complete and shows its superiority to strong baselines. It is based on a pre-trained BART model, and the proposed Module Indicator/ Boundary Indicator can help the base model to better combine information from both the context and knowledge in generated responses. \nsummary_of_weaknesses\n1. The writing still needs improvement. There are only some minor revisions compared to the last version, where they fix some typos, moving part of human evaluation results into the main part, and change few expressions to make them more clear. However, some major problems still exist. \n1) The descriptions of the model are difficult for readers to follow, including complex notations and some unclear definitions (e.g.,  the different styles mentioned). \n2) Although it adds more details about human evaluation, there is still no clear definition of metrics pklg and lklg. And I also have concerns about their justification.\n2. An ablation study is still missing. Without such analyses, readers will have no idea about how each component contributes to the final performance. E.g., how the style adapter affects the generated responses, or how it performs if one kind of latent is removed. \ncomments,_suggestions_and_typos\nPlease check the weakness section.\n1. A case study with only one sample is insufficient for a competent paper in the dialogue area. Consider adding more samples and putting more highlights on the samples to show your superiority.\n2. Still missing some references that utilize pretrained model in an encoder-decoder architecture for auxiliary-information-grounded dialogue generation.  Typo: L492: Sec ?? ", "label": [[579, 616, "Eval_pos_1"], [618, 768, "Jus_pos_1"], [1095, 1131, "Eval_neg_1"], [1132, 1336, "Jus_neg_1"], [1337, 1379, "Eval_neg_1"], [1383, 1448, "Eval_neg_2"], [1450, 1548, "Jus_neg_2"], [1552, 1666, "Eval_neg_3"], [1667, 1718, "Eval_neg_4"], [1722, 1757, "Eval_neg_5"], [1758, 1984, "Jus_neg_5"], [2250, 2400, "Eval_neg_6"]]}
{"id": 537, "review": "## General comments: This paper presents an exploration of the connection between part-of-speech tags and word embeddings. Specifically the authors use word embeddings to draw some interesting (if not somewhat straightforward) conclusions about the consistency of PoS tags and the clear connection of word vector representations to PoS. The detailed error analysis (outliers of classification) is definitely a strong point of this paper.\nHowever, the paper seems to have missing one critical main point: the reason that corpora such as the BNC were PoS tagged in the first place. Unlike a purely linguistic exploration of morphosyntactic categories (which are underlined by a semantic prototype theory - e.g. see Croft, 1991), these corpora were created and tagged to facilitate further NLP tasks, mostly parsing. The whole discussion could then be reframed as whether the distinctions made by the distributional vectors are more beneficial to parsing as compared to the original tags (or UPOS for that matter).  Also, this paper is missing a lot of related work in the context of distributional PoS induction. I recommend starting with the review Christodoulopoulos et al. 2010 and adding some more recent non-DNN work including Blunsom and Cohn (2011), Yatbaz et al. (2012), etc. In light of this body of work, the results of section 5 are barely novel (there are systems with more restrictions in terms of their external knowledge that achieve comparable results).\n## Specific issues In the abstract one of the contributed results is that \"distributional vectors do contain information about PoS affiliation\". Unless I'm misunderstanding the sentence, this is hardly a new result, especially for English: every distributionally-based PoS induction system in the past 15 years that presents \"many-to-one\" or \"cluster purity\" numbers shows the same result.\nThe assertion in lines 79-80 (\"relations between... vectors... are mostly semantic\") is not correct: the <MIKOLOV or COLOBERT> paper (and subsequent work) shows that there is a lot of syntactic information in these vectors. Also see previous comment about cluster purity scores. In fact you revert that statement in the beginning of section 2 (lines 107-108).\nWhy move to UPOS? Surely the fine-grained distinctions of the original tagset are more interesting.\nI do not understand footnote 3. Were these failed attempts performed by you or other works? Under what criteria did they fail? What about Brown cluster vectors? They almost perfectly align with UPOS tags.\nIs the observation that \"proper nouns are not much similar to common nouns\" (lines 331-332) that interesting? Doesn't the existence of \"the\" (the most frequent function word) almost singlehandedly explain this difference?\nWhile I understand the practical reasons for analysing the most frequent word/tag pairs, it would be interesting to see what happens in the tail, both in terms of the vectors and also for the types of errors the classifier makes. \nYou could then try to imagine alternatives to pure distributional (and morphological - since you're lemmatizing) features that would allow better generalizations of the PoS tags to these low-frequency words.\n## Minor issues Change the sentential references to \\newcite{}: e.g. \"Mikolov et al. (2013b) showed\" ", "label": [[337, 437, "Eval_pos_1"], [438, 502, "Eval_neg_1"], [504, 1011, "Jus_neg_1"], [1013, 1110, "Eval_neg_2"], [1111, 1281, "Jus_neg_2"], [1282, 1354, "Eval_neg_3"], [1356, 1465, "Jus_neg_3"], [1613, 1706, "Eval_neg_4"], [1708, 1857, "Jus_neg_4"]]}
{"id": 538, "review": "paper_summary\nThis paper addresses the issue of automatically mining values behind arguments. Relying on a taxonomy of values based on literature and consistent with those used in social sciences, and collecting data across 4 cultural domain, the paper presents a robust study on the automatic detection of values with very encouraging results. \nsummary_of_strengths\nThe paper is exquisitely well-written and presented. The experimental design is sound, and the data collection follows good practice, leading to very encouraging results.  The background literature review is excellent.  Overall, I think this is an excellent paper which I would like to see at ACL. Addressing the weaknesses below would strengthen the paper even further. \nsummary_of_weaknesses\nThere is a slight imbalance between the background and the description and discussion of results in the paper. Given the very strong set up, the final part of the paper seems somewhat underwhelming. Further discussion on the results and their significance, with more space allocated to what they mean for further work and applications would have made for an even stronger paper.\nAt alpha=.49 inter-annotator agreement is low, which is not unexpected given the complexity of the task. However, further discussion on how this could be addressed (e.g. with better annotation manuals or training) is missing. \ncomments,_suggestions_and_typos\nLine 458: equally effected -> equally affected Overall, no other issues with the writing, which is excellent. I would advise the authors to dedicate a bit more space on discussing the results and their significance for further work and applications, perhaps shortening the background sections slighlty. ", "label": [[367, 419, "Eval_pos_1"], [420, 452, "Eval_pos_2"], [458, 537, "Eval_pos_3"], [539, 585, "Eval_pos_4"], [587, 664, "Major_claim"], [761, 871, "Eval_neg_1"], [872, 1139, "Jus_neg_1"], [1140, 1244, "Jus_neg_2"], [1245, 1366, "Eval_neg_2"]]}
{"id": 539, "review": "paper_summary\nThe authors proposed an improved version of the $\\alpha$-entmax which does not require sorting during computation, which aims to reduce the latency of the operation. The proposed computation method is based on $\\alpha$-RELU, which replaces the normalization variable in $\\alpha$-entmax to a fixed value. Experiments show that the resultant operator is faster than 1.5-entmax on WMT14 En-De and WMT13 En-Ru translation tasks. \nsummary_of_strengths\n- The proposed method shows that even with a potentially unnormalized distribution, it doesn't seem to harm the decoding performance of the generation model -Experiment results show that the proposed method is constantly faster than $\\alpha$-entmax and the performance is not dropping \nsummary_of_weaknesses\nThe authors claim that the alpha entmax is slow because it requires sorting. I checked the code of  $\\alpha$-entmax implemented in https://github.com/deep-spin/entmax and find that the top-k approximation is the default option with K=100 as the default setting. In this case, `torch.topk` is called in the code, and only K numbers are sorted. Therefore, if the author position the proposed method as a fast approximation to the original  $\\alpha$-entmax, I think the authors shall perform more systematical comparison with the top-k approximation.\nIn the paper, the only comparison with top-k approximation is Figure 2 (bottom, center). However, it's actually difficult to make a conclusion for comparing performance and speed based on this figure. It seems that the proposed 1.5-ReLU is still faster in the first 20 hours of training, then the gap closes. Also, as a key performance indicator of the proposed method, it is desirable to have the actual numbers. For example, averaged seconds per batch for training and averaged seconds per 1k decoding requests for inference. It will be even better to profile just the computation time of softmax /  $\\alpha$-entmax /  $\\alpha$-ReLU operators.\nI hope to see more detailed comparison with the top-k variant of $\\alpha$-entmax on (1) translation performance (at least WMT14 En-De) and (2) execution speed (can be seconds per 1k batch, will be better if reporting the running time of just the $\\alpha$-entmax/$\\alpha$-ReLU layer). I might change my assessment according to the results.\nAnother concern is on the distribution, in Figure 7, authors showed that the sums are still concentrating to a certain value. However, the deviation is not small according to this figure. Will this impact the correctness of the language model scores (log p)? Some applications are relying on the scores for reranking purposes. \ncomments,_suggestions_and_typos\n- For evaluation, the datasets that the authors use are pretty old datasets. I'm not against using WMT14 En-De, however, WMT13 En-Ru is rare in the research community recently. ", "label": []}
{"id": 540, "review": "paper_summary\nIn this paper, the authors proposed Structural Information-augmented Syntax Controlled Paraphrasing (Si_SCP) which is a syntax-controlled paraphrase generation technique. They tackled two problems of such generation (a) encoding the structural information -- by using a tree transformer to capture parent-child and sibling relationships and (b) retrieving syntactic structure to guide the generation - by introducing a synthetic template retriever. \nsummary_of_strengths\n(a) The paper is well written and easy to follow. \n(b) Various experiments and ablation studies are done to establish the effectiveness of the proposed mechanism. \n(c) The authors worked on the previous weaknesses thoroughly. \nsummary_of_weaknesses\nThough the paper is detailed, I recommend the following areas to be addressed (a) Retrieval of the similar templates: The authors should clarify on why the query text and the whole query parse is important to retrieve templates? Is  it always the case that given a query, we can retrieve templates other than the own query template? Are all top K retrieved templates meaningful concerning the query or some thresholding on similarity is needed? \n(b) Human evaluation needs some more elaboration. The author should report the inter-annotator agreement along with the results.   (c) Syntactic evaluation: At the time of evaluating TED, did the authors use the top most retrieved template? They should elaborate on the process (d) Table 1: The performance of (Si_SCP) is slightly better than guiG in ParaNMT-small where as for QQP-OS the gains are higher. Is there any specific reasons or observation regarding the same? \n(e) Though the paper is focused on syntax guided paraphrase generation, it would be nice if the authors can discuss Si_SCP\u2019s gain in comparison to unsupervised paraphrase generations [Krishna, Kalpesh, John Wieting, and Mohit Iyyer. \" Reformulating Unsupervised Style Transfer as Paraphrase Generation.\" Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2020.] \ncomments,_suggestions_and_typos\nWhile I can see a detailed work in the paper, I would recommend the authors to address the points mentioned in the weaknesses. ", "label": [[489, 535, "Eval_pos_1"], [540, 648, "Eval_pos_2"], [653, 711, "Eval_pos_3"], [1184, 1229, "Eval_neg_1"], [1230, 1308, "Jus_neg_1"]]}
{"id": 542, "review": "This paper presents a conversion from a \"traditional\" POS tag set to UD tagset for Thai, and presents POS tagging experiments with a variety of models that differ in two dimensions: (1) the type of sub-word units, and (2) and type of pre-training. The authors conclude that sub-word units are beneficial for the task as well as language-specific pre-training.\nThe problem investigated is a standard problem, but the fact that the paper is on a relatively less-studied language increases the value of the paper. The conversion of a language-specific tagset to a multi-lingual, \"universal\", tagset is interesting, and the methods presented, although standard/straightforward, makes sense. However, the paper leaves many questions unanswered, it should probably be a long paper with further information, analyses and discussion. I will list some of the issues with the paper.\nThe conversion of ORCHID to UD has multiple issues: - First, there is no clear motivation for this conversion.  From Table   1, this seems this is a lossy conversion.  Given that there is not   strong motivation (e.g., creating a UD compliant treebank), the    purpose of the conversion is also unclear, similar to what authors   mean by \"manageable tagset\" (p.1, last paragraph).\n- There is little information on conversion process. It seems there    wasn't enough attention put into the conversion process.  In a   proper conversion to UD, one should also map some of the sub-types   to morphological features.   - The conversion also makes the results incomparable to the earlier   studies on this data set. Irrespective of the comparison to other    studies, it would have been interesting to see the results of the   systems tested with both tagsets. \n   -It is also surprising to see no mention of the existing Thai UD   treebank (Thai PUD). The treebank includes both coarse (UD) and fine    (presumably ORCHID) POS tags. A comparison of conversion used in    this treebank and in the present paper is needed for better   understanding the value of the conversion presented. \n   -Additional information, e.g., tag distribution (before and after   conversion), on the data set would be beneficial for interpretation   of the results discussed. \n   Investigating the effects of use of sub-word units is interesting. \nHowever, I have difficulties interpreting the results for two reasons: - No discussion, or analysis, of the results. We are given a number of   performance metrics, announcement of the best models, and no insight   into why these differences should be observed, or are there anything   contrary to expectations. For example, the failure of multi-lingual   BERT (even in OoV words) needs some explanation discussion. \n  Pre-trained model not only \"does not benefit from the cross lingual   transfer\", but hurts the performance of the model compared much   simpler ones. \n   -The models presented typically have large variation due to effects   like random initialization and test set split. As a result it is   difficult get a good sense of which model is better without having   an indication of how much the results vary. Furthermore, the authors   seem to have missed the fact that in the results they present in    Table 4, syllable-based (not pre-trained) model performs as well as   BERT on in-vocabulary words. \n   In general, given current presentation, I do not think results are conclusive.\nThere are also frequent typos and language mistakes. A thorough  proofreading is recommended. Here are a few examples: - title: \"Pre-trained Language Model\" -> \"Pre-trained Language Models\" -abstract: \"syllables representations\" -> \"syllable representations\" -Intro, paragraph  1: \"Modern approaches include\" -> \"Modern approaches includes\" -In general quite a few agreement mistakes, a through proofreading   would be good.\n-Intro, paragraph  1: multiple citations should be placed in a single   pair of parentheses, and citations should not be after the sentence final   punctuation.\n-'related work', paragraph 1 (and other places): when using a   citation as part of the sentence, it should not be in parentheses. \n  \"(Akbik et al., 2018) proposed\" -> \"Akbik et al. (2018) proposed\"   (see conference style/guidelines for more information) -Most tables are not referenced from the main text. The reader needs   to know what to look at in those tables, and in what way they   support the description or argumentation.\n-There are proper names or abbreviations in the references (likely   BibTeX normalization issues): \"thai\", \"bilstm\", ... ", "label": [[511, 610, "Eval_pos_1"], [616, 685, "Eval_pos_2"], [687, 826, "Major_claim"], [927, 983, "Eval_neg_1"], [985, 1253, "Jus_neg_1"], [1256, 1306, "Eval_neg_2"], [1307, 1486, "Jus_neg_2"], [1734, 1820, "Eval_neg_3"], [1821, 2055, "Jus_neg_3"], [2227, 2294, "Eval_pos_3"], [2368, 2411, "Eval_neg_4"], [2412, 2864, "Jus_neg_4"], [2869, 2984, "Jus_neg_5"], [2985, 3117, "Eval_neg_5"], [3118, 3311, "Eval_neg_6"], [3316, 3394, "Eval_neg_7"], [3395, 3447, "Eval_neg_8"], [3448, 3819, "Jus_neg_8"]]}
{"id": 545, "review": "- Strengths: Relatively clear description of context and structure of proposed approach. \nRelatively complete description of the math. Comparison to an extensive set of alternative systems.\n- Weaknesses: Weak results/summary of \"side-by-side human\" comparison in Section 5. Some disfluency/agrammaticality.\n- General Discussion: The article proposes a principled means of modeling utterance context, consisting of a sequence of previous utterances. Some minor issues: 1. Past turns in Table 1 could be numbered, making the text associated with this table (lines 095-103) less difficult to ingest. Currently, readers need to count turns from the top when identifying references in the authors' description, and may wonder whether \"second\", \"third\", and \"last\" imply a side-specific or global enumeration.\n2. Some reader confusion may be eliminated by explicitly defining what \"segment\" means in \"segment level\", as occurring on line 269. Previously, on line 129, this seemingly same thing was referred to as \"a sequence-sequence [similarity matrix]\". The two terms appear to be used interchangeably, but it is not clear what they actually mean, despite the text in section 3.3. It seems the authors may mean \"word subsequence\" and \"word subsequence to word subsequence\", where \"sub-\" implies \"not the whole utterance\", but not sure.\n3. Currently, the variable symbol \"n\" appears to be used to enumerate words in an utterance (line 306), as well as utterances in a dialogue (line 389). The authors may choose two different letters for these two different purposes, to avoid confusing readers going through their equations.\n4. The statement \"This indicates that a retrieval based chatbot with SMN can provide a better experience than the state-of-the-art generation model in practice.\" at the end of section 5 appears to be unsupported. The two approaches referred to are deemed comparable in 555 out of 1000 cases, with the baseline better than the proposed method in 238 our of the remaining 445 cases. \nThe authors are encouraged to assess and present the statistical significance of this comparison. If it is weak, their comparison permits to at best claim that their proposed method is no worse (rather than \"better\") than the VHRED baseline.\n5. The authors may choose to insert into Figure 1 the explicit \"first layer\", \"second layer\" and \"third layer\" labels they use in the accompanying text.\n6.  Their is a pervasive use of \"to meet\" as in \"a response candidate can meet each utterace\" on line 280 which is difficult to understand.\n7. Spelling: \"gated recurrent unites\"; \"respectively\" on line 133 should be removed; punctuation on line 186 and 188 is exchanged; \"baseline model over\" -> \"baseline model by\"; \"one cannot neglects\". ", "label": [[13, 89, "Eval_pos_1"], [90, 134, "Eval_pos_2"], [135, 189, "Eval_pos_3"], [204, 273, "Eval_neg_1"], [274, 306, "Eval_neg_2"]]}
{"id": 546, "review": "paper_summary\nThis paper proposes a neural pairwise ranking model to evaluate readability assessment. The model outperforms existing methods on two tasks - cross-corpus ranking and zero shot, cross-lingual ranking - based on ranking metrics. The paper also publish a new bilingual (English-French) readability dataset for cross-lingual evaluation. \nsummary_of_strengths\n1. The pairwise ranking method shows better robustness on various monolingual dataset and out-domain cross-lingual dataset than previous methods. \n2. The paper creates a new cross-lingual dataset for readability assessment. \nsummary_of_weaknesses\n1. The paper does not clearly introduce the dataset. For example, what is average length of each text in NewsEla? If the text is long, the paper should also mention how the text is encoded by BERT when it exceeds the maximum length. \n2. As the paper already pointed out, the nature of pairwise model restricts the inference for downstream applications, so that the contribution of this paper is not very strong. \ncomments,_suggestions_and_typos\n- L400: citation for \"for evaluating ranking or information-retrieval tasks in literature\"?\n-L502: Is NPRM (0.999, 0.995, 0.990, 0.948) better than regBERT (0.999, 0.997, 0.994, 0.977)?\n-L613: \"while\" -> \"\" ", "label": [[620, 669, "Eval_neg_1"], [670, 850, "Jus_neg_1"], [854, 968, "Jus_neg_2"], [970, 1029, "Eval_neg_2"]]}
{"id": 547, "review": "paper_summary\nThis paper propose a new pre-training objective for train Bi-encoder based QA model. The main claim is that the proposed pre-training objectives helps in improving the model's performance on non-QA downstream tasks like paraphrase detection, sentiment analysis etc. without extensive finetuing.  The main hypothesis is that the proposed way of pre-training helps the model in learning better token level representation needed for zero shot and few shot tasks. \nsummary_of_strengths\nSynthetically generated questions have been nicely used for pre-training QA model to improve performance on tasks like Sentiment analysis, paraphrase detection in a zero shot, few shot setup. A good analysis of using QA as the pre-training objective rather than MLM. \nsummary_of_weaknesses\nThe paper has incremental or limited novelty as the question generation from passages using BART is already a known, Bi-encoder for QA already exists. \nContributions of the paper are quite trivial. \nBi-encoder+Unsupervised QA didn't perfrom great. So whether choosing QA as the pre-training is helping QA task or not is not very clear. \nwhat is the motivation behind independently encoding questions and passage is not very clear. isn't that a shared common encoder would help the model learning better question tokens to passage token matching? \nFor QA task, It seems that the cross-encoder + MRQA is doing better than proposed QUIP. \ncomments,_suggestions_and_typos\nHighlight the best numbers in table 7.  -Have a strategy to filter out best quality question from the set of synthetically generated questions.\n-May be design a curriculum learning paradigm for pre-training so that the batch sampling is done in a manner that helps the pre-training learn from easy to complex questions. ", "label": [[496, 687, "Eval_pos_1"], [688, 763, "Eval_pos_2"], [786, 830, "Eval_neg_1"], [831, 936, "Jus_neg_1"], [938, 984, "Eval_neg_2"], [985, 1033, "Jus_neg_3"], [1034, 1121, "Eval_neg_3"], [1123, 1216, "Eval_neg_4"], [1217, 1332, "Jus_neg_4"]]}
{"id": 548, "review": "The paper present a curriculum learning method for NMT which is shown to work relatively well for low-resource settings. The method is applied to the standard Transformer and it is evaluated on several language pairs and different data sizes. The method determines what data samples should be presented to the model based on their difficulty and the model competence. The experiments show that the approach provides for consistent improvements.  The paper is interesting and relatively clear, but there are some points in the paper that need clarification.  The approach is interesting. It is based on Platanios et al. (2019) where sample difficulty and model competence were used. This paper proposes new ways to compute these values, and more importantly, sample difficulty is reestimated during training. \nThe improvements are consistent across different language pairs, although often relatively small. The highest improvements are when training on 50K WMT data.  My main concern is that the improvements are rather small, on many test sets it is under 1 BLEU. Also, I am concerned if the model used for 50K WMT is overparametrized. This is the setting where the highest improvements are obtained, but it seems like this is a large model for such a small dataset. Dropout should probably be larger as well. Sennrich and Zhang (2019) (Revisiting Low-Resource Neural Machine Translation: A Case Study) provide details on how to better train low-resource models. \nI am concerned if the method would prove as effective in low-resource settings if the models are more optimized for the low-resource setting.\nI am not sure why are there some differences in model sizes across the different language pairs. These seem rather arbitrary. How were these hyperparameters determined? I do not see a connection between training dataset size and model size or number of layers.\nThe analysis in Figure 2, 3 and 4 is interesting. However, I am curious if these effects would be noticeable in the other experimental settings, where in almost all cases, improvements are under 1 BLEU.\nThe results in Koehn and Knowles (2017) with regards to low BLEU scores in low-resource settings have been addressed in Sennrich and Zhang (2019). \nI agree with the sentiment that performance in low-resource settings is often lacking, but using this reference may not be appropriate.  Typos and writing style:  including language model -> including language modeling mini-batch contains sentences -> mini-batch containing sentences Algorithm 1, line 1: Randomly initial -> Randomly initialize I would suggest normalizing all scores in Table 3 to 2 decimal points.\nIn this paper, we propose a dynamic model competence (DMC) estimation method... - this paragraph is confusing. I did not understand what is the \"prior hypothesis of the training process\" nor how is BLEU superior if the loss is already the optimal method to estimate competence. ", "label": [[446, 556, "Eval_pos_1"], [558, 586, "Eval_pos_2"], [587, 806, "Jus_pos_2"], [968, 1025, "Eval_neg_1"], [1027, 1063, "Jus_neg_1"], [1065, 1136, "Eval_neg_2"], [1137, 1606, "Jus_neg_2"], [1607, 1732, "Eval_neg_3"], [1733, 1867, "Jus_neg_3"], [1868, 1917, "Eval_neg_4"]]}
{"id": 549, "review": "paper_summary\nIn this paper, the authors propose SWCC to improve event representation learning. The method mainly modifies the objective function, which can be broken down into a modified InfoNCE objective, a prototype-based clustering objective, and a masked language model objective. The experiment includes two intrinsic evaluations, based on event embedding similarity and one extrinsic evaluation, MCNC. The result shows that the proposed method outperforms baselines on all the evaluations. The ablation and visualization studies also identify the importance of each component (objective function). \nsummary_of_strengths\n1. The experiment results show strong performance of the proposed method. \n2. To the best of my knowledge, this is the first work that leverages prototype-based clustering in event representation learning. \nsummary_of_weaknesses\n1. The write-up has many typos and some formulas/explanations are confusing. \n2. The technical innovation of the proposed method is limited. The proposed objective function is basically a combination of two related works with tiny changes. \n3. Reproductivity is not ideal, as some essential parts are not addressed in the paper, such as training data. \n4. More strong baselines should be included/discussed in the experiments. \ncomments,_suggestions_and_typos\nComments 1. Line 18: it\u2019s better to specify the exact tasks rather than stating several tasks in the abstract 2. Line 69: \u201ccurrent\u201d -> \u201ccurrently\u201d 3. Line 112: margin-based loss can use one positive with MULTIPLE negatives. \n4. Figure 1: the relation types are not explicitly modeled in this work so the figure is kind of confusing. \n5. Eq. 1: what is z_p 6. Eq. 2: how do you convert BERT token embeddings into event embeddings? Concatenation? Any pooling? \n7. Line 236-237: \u201cconciser\u201d -> \u201cconsider\u201d 8. Line 209-211: I can understand what you mean but this part should be re-written. For example, \u201cgiven an anchor event, we generate 3 positive samples with different dropout masks.\u201d \n9. Table 1: needs to include a random baseline to show the task difficulty 10. Experiments: what training data you use for pre-training? \n11. Table 1: do the baselines and the proposed method use the same training data for pre-training? How did you get the results for the baselines? Did you have your own implementation, or directly use their released embeddings/code? \n12. Line 450: L_{pc} or L_{cp}| in Eq. 7? \n13. Table 2: what\u2019s the difference between \u201cSWCC w/o Prototype-based Clustering\u201d and \u201cBERT(InfoNCE)\u201d? \n14. Table 3: MCNC should have many strong baselines that are not compared here, such as the baselines in [1]. Can you justify the reason? \n15. Can you provide an analysis on the impact of the number of augmented samples (e.g., z_{a1}, z_{a2}) here? ", "label": [[705, 833, "Eval_pos_1"], [859, 886, "Eval_neg_1"], [891, 931, "Eval_neg_2"], [937, 996, "Eval_neg_3"], [997, 1096, "Jus_neg_3"], [1100, 1127, "Eval_neg_4"], [1129, 1208, "Jus_neg_4"]]}